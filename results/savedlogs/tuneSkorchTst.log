## SLURM PROLOG ###############################################################
##    Job ID : 9851912
##  Job Name : tuneSkorchTst.sh
##  Nodelist : gpu2106
##      CPUs : 1
##  Mem/Node : 65536 MB
## Directory : /gpfs/home/isears1/Repos/tabnet-sepsis
##   Job Started : Tue May  2 00:16:18 EDT 2023
###############################################################################
module: loading 'cuda/11.3.1'
module: cuda: To use: module load gcc/10.2
module: loading 'cudnn/8.2.0'
module: cudnn: To use: module load cuda/11.1.1 gcc/10.2
Tue May  2 00:16:18 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ...  On   | 00000000:01:00.0 Off |                  N/A |
| 43%   44C    P5    96W / 350W |     35MiB / 24576MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A     15959      G   X                                  33MiB |
+-----------------------------------------------------------------------------+
Python 3.10.4
[32m[I 2023-05-02 00:16:22,699][0m A new study created in memory with name: no-name-5ce9e989-9d56-4002-ac98-f5df1102b7b9[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 225
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2110[0m   [32m0.7134[0m        [35m0.3053[0m       [31m0.9214[0m        [94m0.2569[0m     +  16.4736
      2   [36m0.2512[0m   [32m0.7337[0m        [35m0.2476[0m       0.9202        [94m0.2506[0m     +  14.6989
      3   [36m0.2715[0m   [32m0.7478[0m        [35m0.2466[0m       0.9202        [94m0.2497[0m     +  14.0693
      4   [36m0.2769[0m   [32m0.7605[0m        [35m0.2423[0m       [31m0.9226[0m        [94m0.2462[0m     +  22.9211
      5   [36m0.3014[0m   [32m0.7675[0m        [35m0.2406[0m       [31m0.9250[0m        [94m0.2425[0m     +  13.1737
      6   [36m0.3034[0m   [32m0.7740[0m        [35m0.2388[0m       0.9214        [94m0.2405[0m     +  13.3392
      7   0.2993   [32m0.7756[0m        [35m0.2369[0m       0.9238        0.2417        13.7317
      8   [36m0.3219[0m   [32m0.7815[0m        [35m0.2360[0m       0.9250        [94m0.2391[0m     +  23.6633
      9   0.3198   0.7801        [35m0.2347[0m       0.9238        [94m0.2379[0m     +  13.9450
     10   [36m0.3335[0m   0.7803        [35m0.2313[0m       0.9226        0.2405        14.2418
     11   0.3243   0.7809        [35m0.2312[0m       0.9226        0.2438        14.2332
     12   0.3312   0.7803        [35m0.2311[0m       0.9214        0.2411        13.8998
     13   0.3329   0.7776        [35m0.2283[0m       0.9250        0.2473        24.8299
     14   0.3290   [32m0.7860[0m        0.2287       0.9238        0.2435        13.8097
     15   0.3149   [32m0.7886[0m        0.2287       0.9226        0.2405        13.6069
     16   0.3246   0.7860        [35m0.2255[0m       0.9238        0.2475        13.8705
     17   0.3113   0.7834        0.2277       0.9178        0.2442        23.3328
     18   0.3220   0.7863        0.2262       0.9238        0.2461        13.8683
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 00:21:30,334][0m Trial 0 finished with value: 0.23786523179607208 and parameters: {'lr': 0.00022773707365791534, 'dropout': 0.5046651723743011, 'd_model_multiplier': 8, 'num_layers': 2, 'n_heads': 16, 'dim_feedforward': 359, 'batch_size': 215, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0, 'top_n_features': 225}. Best is trial 0 with value: 0.23786523179607208.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 51
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.0707[0m   [32m0.3242[0m        [35m0.6362[0m       [31m0.8984[0m        [94m0.6225[0m     +  9.9891
      2   0.0705   0.3221        [35m0.6330[0m       [31m0.9033[0m        [94m0.6191[0m     +  8.8871
      3   0.0681   0.3143        [35m0.6234[0m       0.9033        [94m0.6137[0m     +  19.7077
      4   0.0673   0.3080        [35m0.6180[0m       [31m0.9069[0m        [94m0.6088[0m     +  8.8290
      5   0.0662   0.3037        [35m0.6119[0m       [31m0.9093[0m        [94m0.6035[0m     +  8.8500
      6   0.0643   0.2977        [35m0.6003[0m       [31m0.9117[0m        [94m0.5966[0m     +  9.2858
      7   0.0628   0.2931        [35m0.5959[0m       0.9117        [94m0.5918[0m     +  9.2271
      8   0.0615   0.2849        [35m0.5849[0m       [31m0.9129[0m        [94m0.5862[0m     +  9.0332
      9   0.0608   0.2847        [35m0.5758[0m       [31m0.9141[0m        [94m0.5806[0m     +  9.4835
     10   0.0585   0.2783        [35m0.5699[0m       [31m0.9166[0m        [94m0.5749[0m     +  9.3433
     11   0.0576   0.2744        [35m0.5586[0m       [31m0.9178[0m        [94m0.5696[0m     +  9.0509
     12   0.0569   0.2726        [35m0.5543[0m       0.9178        [94m0.5645[0m     +  8.7931
     13   0.0556   0.2698        [35m0.5462[0m       0.9178        [94m0.5588[0m     +  8.7516
     14   0.0548   0.2671        [35m0.5404[0m       0.9178        [94m0.5554[0m     +  8.7976
     15   0.0542   0.2637        [35m0.5305[0m       0.9178        [94m0.5499[0m     +  8.8159
     16   0.0539   0.2606        [35m0.5258[0m       0.9178        [94m0.5451[0m     +  8.8658
     17   0.0539   0.2613        [35m0.5153[0m       0.9178        [94m0.5406[0m     +  8.7367
     18   0.0536   0.2585        [35m0.5077[0m       0.9178        [94m0.5354[0m     +  8.9892
     19   0.0536   0.2578        [35m0.5068[0m       0.9178        [94m0.5310[0m     +  8.9367
     20   0.0537   0.2547        [35m0.4993[0m       0.9178        [94m0.5266[0m     +  8.7900
     21   0.0541   0.2540        [35m0.4949[0m       0.9178        [94m0.5219[0m     +  9.1685
     22   0.0541   0.2541        [35m0.4877[0m       0.9178        [94m0.5168[0m     +  9.3446
     23   0.0544   0.2527        [35m0.4808[0m       0.9178        [94m0.5134[0m     +  10.0319
     24   0.0540   0.2525        [35m0.4771[0m       0.9178        [94m0.5104[0m     +  9.4237
     25   0.0547   0.2498        [35m0.4710[0m       0.9178        [94m0.5041[0m     +  9.2395
     26   0.0548   0.2498        [35m0.4635[0m       0.9178        [94m0.5022[0m     +  9.3955
     27   0.0548   0.2503        [35m0.4610[0m       0.9178        [94m0.4985[0m     +  8.7562
     28   0.0543   0.2499        [35m0.4551[0m       0.9178        [94m0.4950[0m     +  8.8025
     29   0.0548   0.2501        [35m0.4487[0m       0.9178        [94m0.4896[0m     +  8.9452
     30   0.0548   0.2487        [35m0.4464[0m       0.9178        [94m0.4869[0m     +  9.0468
     31   0.0549   0.2500        [35m0.4401[0m       0.9178        [94m0.4835[0m     +  8.7509
     32   0.0548   0.2489        [35m0.4380[0m       0.9178        [94m0.4799[0m     +  8.5915
     33   0.0549   0.2491        [35m0.4330[0m       0.9178        [94m0.4773[0m     +  8.8815
     34   0.0549   0.2493        [35m0.4280[0m       0.9178        [94m0.4729[0m     +  8.7094
     35   0.0549   0.2494        [35m0.4275[0m       0.9178        [94m0.4711[0m     +  8.9565
     36   0.0550   0.2494        [35m0.4194[0m       0.9178        [94m0.4674[0m     +  8.5058
     37   0.0549   0.2494        [35m0.4175[0m       0.9178        [94m0.4644[0m     +  9.0725
     38   0.0550   0.2499        [35m0.4140[0m       0.9178        [94m0.4610[0m     +  8.8305
     39   0.0551   0.2507        [35m0.4109[0m       0.9178        [94m0.4578[0m     +  8.6223
     40   0.0551   0.2504        [35m0.4083[0m       0.9178        [94m0.4549[0m     +  8.8533
     41   0.0547   0.2506        [35m0.4071[0m       0.9178        [94m0.4526[0m     +  8.7068
     42   0.0547   0.2505        [35m0.4014[0m       0.9178        [94m0.4494[0m     +  9.3991
     43   0.0547   0.2513        [35m0.4004[0m       0.9178        [94m0.4471[0m     +  9.1131
     44   0.0548   0.2518        [35m0.3954[0m       0.9178        [94m0.4450[0m     +  9.1753
     45   0.0548   0.2524        [35m0.3905[0m       0.9178        [94m0.4428[0m     +  9.0545
     46   0.0549   0.2523        [35m0.3876[0m       0.9178        [94m0.4396[0m     +  9.0293
     47   0.0553   0.2525        [35m0.3872[0m       0.9178        [94m0.4368[0m     +  8.5728
     48   0.0554   0.2529        [35m0.3849[0m       0.9178        [94m0.4340[0m     +  8.6222
     49   0.0555   0.2537        [35m0.3823[0m       0.9178        [94m0.4323[0m     +  8.7042
     50   0.0555   0.2544        [35m0.3803[0m       0.9178        [94m0.4288[0m     +  8.9614
[32m[I 2023-05-02 00:29:17,276][0m Trial 1 finished with value: 0.4288449683097138 and parameters: {'lr': 8.197545550454173e-08, 'dropout': 0.42708637322043297, 'd_model_multiplier': 1, 'num_layers': 1, 'n_heads': 32, 'dim_feedforward': 471, 'batch_size': 14, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'BatchNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.01, 'top_n_features': 51}. Best is trial 0 with value: 0.23786523179607208.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 222
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
Warning, assumed runtime error: CUDA out of memory. Tried to allocate 1.49 GiB (GPU 0; 23.70 GiB total capacity; 20.99 GiB already allocated; 1.29 GiB free; 21.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[32m[I 2023-05-02 00:29:22,590][0m Trial 2 finished with value: 100.0 and parameters: {'lr': 1.102113016441554e-05, 'dropout': 0.4492830752032858, 'd_model_multiplier': 16, 'num_layers': 12, 'n_heads': 32, 'dim_feedforward': 279, 'batch_size': 126, 'pos_encoding': 'learnable', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 222}. Best is trial 0 with value: 0.23786523179607208.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 171
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2108[0m   [32m0.7011[0m        [35m0.3800[0m       [31m0.9178[0m        [94m0.2616[0m     +  34.6278
      2   [36m0.2402[0m   [32m0.7493[0m        [35m0.2520[0m       0.9178        [94m0.2518[0m     +  34.8677
      3   [36m0.2468[0m   [32m0.7708[0m        [35m0.2435[0m       0.9105        0.2519        34.9491
      4   [36m0.2594[0m   0.7516        [35m0.2421[0m       0.9141        0.2581        34.9240
      5   [36m0.2676[0m   0.7561        [35m0.2376[0m       0.9045        0.2579        34.9666
      6   [36m0.2697[0m   0.7581        [35m0.2359[0m       0.9069        0.2596        34.5496
      7   0.2636   0.7596        0.2360       0.9117        0.2542        35.0818
      8   0.2621   0.7599        [35m0.2320[0m       0.9081        0.2613        34.8354
      9   [36m0.2754[0m   0.7659        0.2331       0.9045        0.2590        35.4437
     10   0.2617   0.7549        [35m0.2317[0m       0.9069        0.2732        35.3515
     11   [36m0.2875[0m   0.7540        [35m0.2293[0m       [31m0.9202[0m        [94m0.2512[0m     +  34.8249
     12   0.2738   0.7561        [35m0.2289[0m       0.8972        0.2802        35.0332
     13   0.2804   0.7576        [35m0.2284[0m       0.9129        0.2567        34.9745
     14   0.2837   0.7589        [35m0.2265[0m       0.9117        0.2685        34.8052
     15   0.2830   0.7482        [35m0.2262[0m       0.9117        0.2609        35.0804
     16   0.2734   0.7582        0.2291       0.9021        0.2888        35.2012
     17   0.2682   0.7471        [35m0.2256[0m       0.8936        0.3023        35.0391
     18   0.2580   0.7531        0.2263       0.8912        0.2955        34.8613
     19   0.2761   0.7526        [35m0.2255[0m       0.8996        0.2918        35.1155
     20   0.2645   0.7524        [35m0.2234[0m       0.9081        0.2798        35.0265
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 00:41:37,864][0m Trial 3 finished with value: 0.25115755658051697 and parameters: {'lr': 0.00017808761944302582, 'dropout': 0.49935774679918776, 'd_model_multiplier': 4, 'num_layers': 8, 'n_heads': 32, 'dim_feedforward': 504, 'batch_size': 56, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.01, 'top_n_features': 171}. Best is trial 0 with value: 0.23786523179607208.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 98
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
Warning, assumed runtime error: CUDA out of memory. Tried to allocate 3.24 GiB (GPU 0; 23.70 GiB total capacity; 18.24 GiB already allocated; 3.04 GiB free; 19.57 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[32m[I 2023-05-02 00:41:43,892][0m Trial 4 finished with value: 100.0 and parameters: {'lr': 0.0001801566574221871, 'dropout': 0.36646972313053217, 'd_model_multiplier': 1, 'num_layers': 5, 'n_heads': 64, 'dim_feedforward': 184, 'batch_size': 137, 'pos_encoding': 'learnable', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.001, 'top_n_features': 98}. Best is trial 0 with value: 0.23786523179607208.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 132
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0598[0m   [32m0.4428[0m        [35m0.7704[0m       [31m0.0798[0m        [94m0.7782[0m     +  16.8635
      2   [36m0.0602[0m   [32m0.4438[0m        0.7705       [31m0.0810[0m        [94m0.7776[0m     +  16.1124
      3   [36m0.0604[0m   [32m0.4449[0m        [35m0.7695[0m       0.0810        [94m0.7768[0m     +  16.3486
      4   [36m0.0609[0m   [32m0.4464[0m        [35m0.7692[0m       0.0810        [94m0.7759[0m     +  15.5799
      5   [36m0.0615[0m   [32m0.4482[0m        [35m0.7679[0m       0.0810        [94m0.7749[0m     +  15.0037
      6   [36m0.0619[0m   [32m0.4501[0m        [35m0.7674[0m       0.0810        [94m0.7737[0m     +  15.1138
      7   [36m0.0626[0m   [32m0.4522[0m        [35m0.7662[0m       0.0810        [94m0.7724[0m     +  14.9867
      8   [36m0.0643[0m   [32m0.4551[0m        [35m0.7656[0m       [31m0.0822[0m        [94m0.7711[0m     +  15.2440
      9   [36m0.0659[0m   [32m0.4579[0m        [35m0.7646[0m       0.0822        [94m0.7696[0m     +  14.8949
     10   [36m0.0680[0m   [32m0.4608[0m        [35m0.7633[0m       0.0822        [94m0.7681[0m     +  16.2323
     11   [36m0.0719[0m   [32m0.4638[0m        [35m0.7619[0m       0.0822        [94m0.7665[0m     +  15.5499
     12   [36m0.0720[0m   [32m0.4661[0m        [35m0.7600[0m       0.0822        [94m0.7648[0m     +  16.2735
     13   [36m0.0723[0m   [32m0.4689[0m        [35m0.7589[0m       0.0822        [94m0.7631[0m     +  16.5280
     14   [36m0.0726[0m   [32m0.4722[0m        [35m0.7577[0m       0.0822        [94m0.7613[0m     +  15.4698
     15   0.0726   [32m0.4752[0m        [35m0.7562[0m       0.0810        [94m0.7594[0m     +  16.2669
     16   [36m0.0732[0m   [32m0.4784[0m        [35m0.7548[0m       0.0810        [94m0.7575[0m     +  15.8380
     17   [36m0.0736[0m   [32m0.4811[0m        [35m0.7527[0m       0.0810        [94m0.7556[0m     +  15.7816
     18   [36m0.0739[0m   [32m0.4843[0m        [35m0.7520[0m       0.0822        [94m0.7535[0m     +  15.9901
     19   [36m0.0741[0m   [32m0.4872[0m        [35m0.7501[0m       0.0822        [94m0.7515[0m     +  15.9334
     20   0.0740   [32m0.4888[0m        [35m0.7487[0m       [31m0.0834[0m        [94m0.7494[0m     +  15.7907
     21   [36m0.0744[0m   [32m0.4918[0m        [35m0.7470[0m       0.0834        [94m0.7473[0m     +  15.8989
     22   [36m0.0746[0m   [32m0.4939[0m        [35m0.7458[0m       [31m0.0859[0m        [94m0.7452[0m     +  16.1936
     23   [36m0.0748[0m   [32m0.4955[0m        [35m0.7430[0m       [31m0.0883[0m        [94m0.7430[0m     +  15.8563
     24   0.0720   [32m0.4975[0m        [35m0.7415[0m       0.0883        [94m0.7409[0m     +  15.9420
     25   0.0710   [32m0.4987[0m        [35m0.7394[0m       [31m0.0895[0m        [94m0.7387[0m     +  16.2384
     26   0.0707   [32m0.5007[0m        [35m0.7377[0m       0.0883        [94m0.7365[0m     +  16.2286
     27   0.0708   [32m0.5013[0m        [35m0.7356[0m       [31m0.0931[0m        [94m0.7343[0m     +  15.9291
     28   0.0695   [32m0.5021[0m        [35m0.7342[0m       [31m0.1016[0m        [94m0.7321[0m     +  16.7261
     29   0.0689   0.4995        [35m0.7316[0m       [31m0.1028[0m        [94m0.7299[0m     +  18.2027
     30   0.0688   0.4975        [35m0.7299[0m       [31m0.1076[0m        [94m0.7277[0m     +  16.4941
     31   0.0686   0.4962        [35m0.7284[0m       [31m0.1137[0m        [94m0.7255[0m     +  16.0759
     32   0.0682   0.4935        [35m0.7260[0m       [31m0.1185[0m        [94m0.7232[0m     +  16.2280
     33   0.0680   0.4923        [35m0.7235[0m       [31m0.1221[0m        [94m0.7210[0m     +  16.3257
     34   0.0677   0.4901        [35m0.7221[0m       [31m0.1306[0m        [94m0.7187[0m     +  16.4946
     35   0.0673   0.4865        [35m0.7200[0m       [31m0.1391[0m        [94m0.7165[0m     +  16.3405
     36   0.0662   0.4828        [35m0.7179[0m       [31m0.1548[0m        [94m0.7142[0m     +  16.9604
     37   0.0653   0.4770        [35m0.7156[0m       [31m0.1741[0m        [94m0.7119[0m     +  16.7738
     38   0.0649   0.4724        [35m0.7140[0m       [31m0.1935[0m        [94m0.7097[0m     +  16.7889
     39   0.0645   0.4677        [35m0.7116[0m       [31m0.2092[0m        [94m0.7074[0m     +  16.4253
     40   0.0642   0.4615        [35m0.7097[0m       [31m0.2406[0m        [94m0.7052[0m     +  15.9428
     41   0.0637   0.4553        [35m0.7076[0m       [31m0.2709[0m        [94m0.7029[0m     +  16.1699
     42   0.0634   0.4496        [35m0.7052[0m       [31m0.3120[0m        [94m0.7006[0m     +  16.2427
     43   0.0630   0.4450        [35m0.7029[0m       [31m0.3615[0m        [94m0.6984[0m     +  16.2794
     44   0.0627   0.4399        [35m0.7012[0m       [31m0.3978[0m        [94m0.6962[0m     +  16.4430
     45   0.0627   0.4352        [35m0.6985[0m       [31m0.4631[0m        [94m0.6940[0m     +  16.6142
     46   0.0632   0.4286        [35m0.6969[0m       [31m0.5079[0m        [94m0.6917[0m     +  16.4706
     47   0.0635   0.4222        [35m0.6943[0m       [31m0.5647[0m        [94m0.6895[0m     +  16.4781
     48   0.0628   0.4158        [35m0.6925[0m       [31m0.6143[0m        [94m0.6873[0m     +  16.1756
     49   0.0635   0.4106        [35m0.6902[0m       [31m0.6663[0m        [94m0.6851[0m     +  16.8421
     50   0.0648   0.4068        [35m0.6892[0m       [31m0.7110[0m        [94m0.6830[0m     +  15.9301
[32m[I 2023-05-02 00:55:14,511][0m Trial 5 finished with value: 0.6829633771147953 and parameters: {'lr': 9.253392554534966e-07, 'dropout': 0.11563479139426676, 'd_model_multiplier': 1, 'num_layers': 8, 'n_heads': 8, 'dim_feedforward': 367, 'batch_size': 236, 'pos_encoding': 'learnable', 'activation': 'gelu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0, 'top_n_features': 132}. Best is trial 0 with value: 0.23786523179607208.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 118
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0515[0m   [32m0.2589[0m        [35m0.5982[0m       [31m0.9226[0m        [94m0.5145[0m     +  13.2843
      2   [36m0.0530[0m   [32m0.2611[0m        [35m0.4681[0m       0.9226        [94m0.4158[0m     +  13.5541
      3   [36m0.0538[0m   [32m0.2692[0m        [35m0.3958[0m       0.9226        [94m0.3615[0m     +  13.5806
      4   [36m0.0552[0m   [32m0.2923[0m        [35m0.3537[0m       0.9226        [94m0.3303[0m     +  13.7377
      5   [36m0.0577[0m   [32m0.3335[0m        [35m0.3269[0m       0.9226        [94m0.3114[0m     +  13.7817
      6   [36m0.0618[0m   [32m0.3786[0m        [35m0.3096[0m       0.9226        [94m0.2994[0m     +  13.6506
      7   [36m0.0670[0m   [32m0.4294[0m        [35m0.2976[0m       0.9226        [94m0.2915[0m     +  13.5882
      8   [36m0.0730[0m   [32m0.4780[0m        [35m0.2902[0m       0.9226        [94m0.2859[0m     +  13.4246
      9   [36m0.0802[0m   [32m0.5171[0m        [35m0.2828[0m       0.9226        [94m0.2819[0m     +  14.0247
     10   [36m0.0913[0m   [32m0.5496[0m        [35m0.2795[0m       0.9226        [94m0.2790[0m     +  13.8940
     11   [36m0.1213[0m   [32m0.5774[0m        [35m0.2771[0m       0.9226        [94m0.2768[0m     +  13.5893
     12   [36m0.1324[0m   [32m0.5992[0m        [35m0.2728[0m       0.9226        [94m0.2751[0m     +  13.7423
     13   [36m0.1416[0m   [32m0.6193[0m        [35m0.2714[0m       0.9226        [94m0.2737[0m     +  13.8614
     14   [36m0.1526[0m   [32m0.6359[0m        [35m0.2697[0m       0.9226        [94m0.2728[0m     +  13.5282
     15   [36m0.1603[0m   [32m0.6502[0m        [35m0.2685[0m       0.9226        [94m0.2720[0m     +  13.7759
     16   [36m0.1698[0m   [32m0.6612[0m        [35m0.2676[0m       0.9226        [94m0.2713[0m     +  13.5897
     17   [36m0.1773[0m   [32m0.6719[0m        [35m0.2671[0m       0.9226        [94m0.2708[0m     +  13.6558
     18   [36m0.1846[0m   [32m0.6814[0m        [35m0.2654[0m       0.9226        [94m0.2705[0m     +  13.7521
     19   [36m0.1911[0m   [32m0.6900[0m        [35m0.2653[0m       0.9226        [94m0.2702[0m     +  13.7268
     20   [36m0.1943[0m   [32m0.6972[0m        [35m0.2649[0m       0.9226        [94m0.2698[0m     +  13.5173
     21   [36m0.1991[0m   [32m0.7034[0m        [35m0.2645[0m       0.9226        [94m0.2695[0m     +  13.6160
     22   [36m0.2047[0m   [32m0.7090[0m        [35m0.2643[0m       0.9226        [94m0.2692[0m     +  13.9140
     23   [36m0.2089[0m   [32m0.7142[0m        [35m0.2632[0m       0.9226        [94m0.2688[0m     +  13.6164
     24   0.2042   [32m0.7186[0m        0.2637       0.9226        [94m0.2684[0m     +  13.5039
     25   0.2085   [32m0.7238[0m        [35m0.2629[0m       0.9226        [94m0.2679[0m     +  13.5985
     26   [36m0.2111[0m   [32m0.7287[0m        [35m0.2617[0m       0.9226        [94m0.2675[0m     +  13.3500
     27   [36m0.2146[0m   [32m0.7323[0m        0.2618       0.9226        [94m0.2670[0m     +  13.4468
     28   [36m0.2208[0m   [32m0.7361[0m        0.2621       0.9226        [94m0.2665[0m     +  13.8049
     29   [36m0.2255[0m   [32m0.7395[0m        [35m0.2612[0m       0.9226        [94m0.2660[0m     +  13.6720
     30   [36m0.2287[0m   [32m0.7421[0m        [35m0.2610[0m       0.9226        [94m0.2656[0m     +  13.6044
     31   [36m0.2305[0m   [32m0.7448[0m        [35m0.2604[0m       0.9226        [94m0.2651[0m     +  13.3521
     32   [36m0.2328[0m   [32m0.7471[0m        [35m0.2599[0m       0.9226        [94m0.2647[0m     +  13.4905
     33   [36m0.2372[0m   [32m0.7499[0m        [35m0.2593[0m       0.9226        [94m0.2643[0m     +  14.0068
     34   [36m0.2395[0m   [32m0.7517[0m        0.2595       0.9226        [94m0.2639[0m     +  13.4484
     35   [36m0.2421[0m   [32m0.7534[0m        0.2593       0.9226        [94m0.2635[0m     +  13.7878
     36   [36m0.2440[0m   [32m0.7548[0m        [35m0.2593[0m       0.9226        [94m0.2633[0m     +  13.7064
     37   0.2439   [32m0.7567[0m        [35m0.2591[0m       0.9226        [94m0.2630[0m     +  13.5055
     38   [36m0.2463[0m   [32m0.7588[0m        [35m0.2583[0m       0.9226        [94m0.2626[0m     +  13.5882
     39   [36m0.2512[0m   [32m0.7604[0m        [35m0.2581[0m       0.9226        [94m0.2624[0m     +  13.6963
     40   [36m0.2519[0m   [32m0.7618[0m        [35m0.2578[0m       0.9226        [94m0.2621[0m     +  13.5323
     41   [36m0.2540[0m   [32m0.7632[0m        0.2580       0.9226        [94m0.2616[0m     +  14.0616
     42   [36m0.2553[0m   [32m0.7644[0m        [35m0.2568[0m       0.9226        [94m0.2612[0m     +  13.8177
     43   [36m0.2567[0m   [32m0.7658[0m        0.2575       0.9226        [94m0.2610[0m     +  13.6789
     44   [36m0.2579[0m   [32m0.7670[0m        0.2568       0.9226        [94m0.2606[0m     +  13.6675
     45   [36m0.2597[0m   [32m0.7685[0m        [35m0.2568[0m       0.9226        [94m0.2604[0m     +  13.7496
     46   [36m0.2604[0m   [32m0.7691[0m        [35m0.2560[0m       0.9226        [94m0.2601[0m     +  14.1498
     47   [36m0.2612[0m   [32m0.7699[0m        [35m0.2558[0m       0.9226        [94m0.2598[0m     +  13.8626
     48   [36m0.2618[0m   [32m0.7704[0m        [35m0.2558[0m       0.9226        [94m0.2596[0m     +  13.8788
     49   [36m0.2630[0m   [32m0.7714[0m        0.2558       0.9226        [94m0.2593[0m     +  13.7009
     50   [36m0.2632[0m   [32m0.7721[0m        [35m0.2550[0m       0.9226        [94m0.2592[0m     +  13.8392
[32m[I 2023-05-02 01:06:54,114][0m Trial 6 finished with value: 0.25917867107368653 and parameters: {'lr': 8.06184080114586e-08, 'dropout': 0.24900358878865098, 'd_model_multiplier': 32, 'num_layers': 1, 'n_heads': 32, 'dim_feedforward': 146, 'batch_size': 13, 'pos_encoding': 'learnable', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 118}. Best is trial 0 with value: 0.23786523179607208.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 121
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1019[0m   [32m0.3575[0m        [35m0.7122[0m       [31m0.3289[0m        [94m0.7038[0m     +  15.2926
      2   0.0908   0.3456        [35m0.6981[0m       [31m0.4861[0m        [94m0.6841[0m     +  15.8033
      3   0.0760   0.3402        [35m0.6793[0m       [31m0.7582[0m        [94m0.6596[0m     +  15.8852
      4   0.0719   0.3320        [35m0.6546[0m       [31m0.8863[0m        [94m0.6326[0m     +  15.9684
      5   0.0699   0.3261        [35m0.6312[0m       [31m0.9069[0m        [94m0.6044[0m     +  16.2086
      6   0.0691   0.3232        [35m0.6048[0m       0.9069        [94m0.5759[0m     +  16.2080
      7   0.0684   0.3209        [35m0.5789[0m       0.9069        [94m0.5478[0m     +  16.0141
      8   0.0677   0.3192        [35m0.5517[0m       0.9069        [94m0.5212[0m     +  15.6306
      9   0.0674   0.3185        [35m0.5276[0m       0.9069        [94m0.4964[0m     +  15.6829
     10   0.0675   0.3190        [35m0.5028[0m       0.9069        [94m0.4735[0m     +  15.7747
     11   0.0675   0.3195        [35m0.4807[0m       0.9069        [94m0.4528[0m     +  15.4850
     12   0.0676   0.3202        [35m0.4575[0m       0.9069        [94m0.4341[0m     +  15.2713
     13   0.0677   0.3218        [35m0.4385[0m       0.9069        [94m0.4173[0m     +  15.7789
     14   0.0680   0.3244        [35m0.4200[0m       0.9069        [94m0.4022[0m     +  15.6507
     15   0.0686   0.3318        [35m0.4039[0m       0.9069        [94m0.3885[0m     +  15.6655
     16   0.0695   0.3451        [35m0.3890[0m       0.9069        [94m0.3763[0m     +  16.0148
     17   0.0714   [32m0.3675[0m        [35m0.3748[0m       0.9069        [94m0.3654[0m     +  15.9784
     18   0.0738   [32m0.3933[0m        [35m0.3616[0m       0.9069        [94m0.3556[0m     +  15.6556
     19   0.0765   [32m0.4193[0m        [35m0.3502[0m       0.9069        [94m0.3467[0m     +  15.6651
     20   0.0793   [32m0.4443[0m        [35m0.3387[0m       0.9069        [94m0.3389[0m     +  15.7403
     21   0.0830   [32m0.4719[0m        [35m0.3301[0m       0.9069        [94m0.3319[0m     +  15.4445
     22   0.0876   [32m0.4996[0m        [35m0.3211[0m       0.9069        [94m0.3256[0m     +  15.2235
     23   0.0929   [32m0.5252[0m        [35m0.3129[0m       0.9069        [94m0.3201[0m     +  15.8320
     24   0.0992   [32m0.5505[0m        [35m0.3062[0m       0.9069        [94m0.3153[0m     +  15.6515
     25   [36m0.1075[0m   [32m0.5746[0m        [35m0.3001[0m       0.9069        [94m0.3110[0m     +  15.6589
     26   [36m0.1179[0m   [32m0.5940[0m        [35m0.2945[0m       0.9069        [94m0.3073[0m     +  15.6470
     27   [36m0.1362[0m   [32m0.6122[0m        [35m0.2909[0m       0.9069        [94m0.3040[0m     +  15.7393
     28   [36m0.1655[0m   [32m0.6298[0m        [35m0.2868[0m       0.9069        [94m0.3014[0m     +  15.8764
     29   [36m0.1761[0m   [32m0.6427[0m        [35m0.2837[0m       0.9069        [94m0.2989[0m     +  15.7190
     30   [36m0.1843[0m   [32m0.6539[0m        [35m0.2798[0m       0.9069        [94m0.2968[0m     +  15.2393
     31   [36m0.1947[0m   [32m0.6634[0m        [35m0.2764[0m       0.9069        [94m0.2949[0m     +  16.0569
     32   [36m0.2020[0m   [32m0.6721[0m        [35m0.2747[0m       0.9069        [94m0.2933[0m     +  15.8273
     33   [36m0.2048[0m   [32m0.6790[0m        [35m0.2716[0m       0.9069        [94m0.2918[0m     +  15.6644
     34   [36m0.2069[0m   [32m0.6847[0m        [35m0.2701[0m       0.9069        [94m0.2907[0m     +  16.0452
     35   [36m0.2111[0m   [32m0.6905[0m        [35m0.2666[0m       0.9069        [94m0.2895[0m     +  16.0189
     36   [36m0.2156[0m   [32m0.6966[0m        [35m0.2650[0m       0.9069        [94m0.2884[0m     +  15.8345
     37   [36m0.2200[0m   [32m0.7014[0m        0.2657       0.9069        [94m0.2874[0m     +  15.7730
     38   [36m0.2233[0m   [32m0.7053[0m        [35m0.2631[0m       0.9069        [94m0.2867[0m     +  15.8542
     39   [36m0.2315[0m   [32m0.7103[0m        [35m0.2615[0m       0.9069        [94m0.2859[0m     +  15.8459
     40   [36m0.2339[0m   [32m0.7139[0m        [35m0.2601[0m       0.9069        [94m0.2852[0m     +  15.5557
     41   [36m0.2356[0m   [32m0.7167[0m        [35m0.2591[0m       0.9069        [94m0.2846[0m     +  15.8870
     42   [36m0.2360[0m   [32m0.7192[0m        [35m0.2581[0m       0.9069        [94m0.2838[0m     +  15.8753
     43   [36m0.2384[0m   [32m0.7221[0m        [35m0.2563[0m       0.9069        [94m0.2832[0m     +  15.6211
     44   [36m0.2407[0m   [32m0.7246[0m        [35m0.2557[0m       0.9069        [94m0.2828[0m     +  15.9086
     45   [36m0.2415[0m   [32m0.7272[0m        [35m0.2543[0m       0.9069        [94m0.2821[0m     +  15.5862
     46   [36m0.2472[0m   [32m0.7283[0m        [35m0.2538[0m       0.9069        [94m0.2819[0m     +  15.9808
     47   [36m0.2485[0m   [32m0.7303[0m        [35m0.2531[0m       0.9069        [94m0.2817[0m     +  15.5875
     48   [36m0.2504[0m   [32m0.7328[0m        [35m0.2515[0m       0.9069        [94m0.2811[0m     +  15.8329
     49   [36m0.2534[0m   [32m0.7353[0m        [35m0.2510[0m       0.9069        [94m0.2804[0m     +  15.7722
     50   [36m0.2544[0m   [32m0.7372[0m        [35m0.2497[0m       0.9069        [94m0.2801[0m     +  16.0746
[32m[I 2023-05-02 01:20:11,816][0m Trial 7 finished with value: 0.2800745686207602 and parameters: {'lr': 5.636199973335043e-07, 'dropout': 0.4632210752118876, 'd_model_multiplier': 64, 'num_layers': 8, 'n_heads': 4, 'dim_feedforward': 324, 'batch_size': 94, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'LayerNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 121}. Best is trial 0 with value: 0.23786523179607208.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 239
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
Warning, assumed runtime error: CUDA out of memory. Tried to allocate 2.93 GiB (GPU 0; 23.70 GiB total capacity; 20.64 GiB already allocated; 107.25 MiB free; 22.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[32m[I 2023-05-02 01:20:17,836][0m Trial 8 finished with value: 100.0 and parameters: {'lr': 0.04882980854535755, 'dropout': 0.6673646014264424, 'd_model_multiplier': 2, 'num_layers': 8, 'n_heads': 64, 'dim_feedforward': 166, 'batch_size': 124, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'BatchNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 239}. Best is trial 0 with value: 0.23786523179607208.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 18
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
Warning, assumed runtime error: CUDA out of memory. Tried to allocate 2.22 GiB (GPU 0; 23.70 GiB total capacity; 18.86 GiB already allocated; 91.25 MiB free; 22.53 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[32m[I 2023-05-02 01:20:21,786][0m Trial 9 finished with value: 100.0 and parameters: {'lr': 0.0024716764119156764, 'dropout': 0.30058817699016865, 'd_model_multiplier': 16, 'num_layers': 15, 'n_heads': 64, 'dim_feedforward': 149, 'batch_size': 94, 'pos_encoding': 'learnable', 'activation': 'gelu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.01, 'top_n_features': 18}. Best is trial 0 with value: 0.23786523179607208.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 195
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0812[0m   [32m0.4509[0m        [35m0.6594[0m       [31m0.8549[0m        [94m0.6759[0m     +  18.6777
      2   0.0786   0.4418        [35m0.6549[0m       [31m0.8609[0m        0.6760        18.1127
      3   0.0787   0.4410        [35m0.6546[0m       [31m0.8622[0m        0.6761        18.6604
      4   0.0791   0.4413        0.6587       0.8597        0.6762        18.7798
      5   0.0785   0.4403        0.6584       [31m0.8634[0m        [94m0.6758[0m     +  19.6509
      6   0.0784   0.4403        0.6562       [31m0.8670[0m        [94m0.6756[0m     +  18.7120
      7   0.0781   0.4384        0.6564       0.8670        0.6756        19.1117
      8   0.0781   0.4379        0.6567       0.8646        [94m0.6756[0m     +  19.3101
      9   0.0777   0.4353        0.6561       0.8646        0.6757        19.8173
     10   0.0784   0.4386        [35m0.6543[0m       0.8646        [94m0.6756[0m     +  19.2764
     11   0.0767   0.4325        0.6558       0.8658        [94m0.6753[0m     +  19.2583
     12   0.0768   0.4326        0.6563       0.8658        0.6756        19.3931
     13   0.0769   0.4333        [35m0.6520[0m       0.8646        0.6756        19.3971
     14   0.0758   0.4304        0.6541       [31m0.8694[0m        [94m0.6750[0m     +  18.8888
     15   0.0765   0.4306        0.6563       0.8658        0.6752        18.4383
     16   0.0770   0.4329        0.6538       0.8694        [94m0.6750[0m     +  19.4966
     17   0.0759   0.4310        [35m0.6513[0m       [31m0.8706[0m        [94m0.6747[0m     +  19.0732
     18   0.0761   0.4301        [35m0.6504[0m       [31m0.8730[0m        [94m0.6746[0m     +  19.1159
     19   0.0764   0.4324        0.6525       0.8706        0.6751        18.9634
     20   0.0758   0.4274        0.6542       0.8694        0.6750        19.0737
     21   0.0750   0.4291        0.6532       0.8694        [94m0.6745[0m     +  19.0061
     22   0.0761   0.4300        0.6523       0.8694        0.6748        19.4506
     23   0.0753   0.4285        0.6515       [31m0.8779[0m        [94m0.6739[0m     +  18.5630
     24   0.0757   0.4287        0.6529       0.8730        0.6742        18.9531
     25   0.0748   0.4272        0.6510       0.8706        0.6746        19.3247
     26   0.0758   0.4286        0.6517       0.8767        0.6740        20.1090
     27   0.0750   0.4271        [35m0.6496[0m       0.8742        0.6740        19.2414
     28   0.0747   0.4254        0.6502       0.8718        0.6742        19.2473
     29   0.0742   0.4249        0.6504       [31m0.8803[0m        [94m0.6736[0m     +  19.2073
     30   0.0736   0.4236        0.6533       0.8767        0.6736        18.8644
     31   0.0734   0.4222        0.6513       0.8767        0.6738        19.6096
     32   0.0730   0.4217        0.6507       0.8755        0.6737        19.4744
     33   0.0728   0.4202        0.6532       0.8779        0.6737        19.2494
     34   0.0737   0.4242        0.6504       0.8791        [94m0.6734[0m     +  18.7258
     35   0.0741   0.4236        [35m0.6485[0m       0.8791        0.6736        18.7156
     36   0.0736   0.4211        0.6497       [31m0.8815[0m        [94m0.6731[0m     +  18.7027
     37   0.0724   0.4191        0.6506       0.8767        0.6736        19.1231
     38   0.0726   0.4196        0.6508       0.8791        0.6735        18.7853
     39   0.0728   0.4188        0.6494       0.8779        0.6736        19.2912
     40   0.0723   0.4182        [35m0.6475[0m       [31m0.8827[0m        [94m0.6730[0m     +  19.6461
     41   0.0716   0.4159        0.6509       0.8803        0.6730        19.1347
     42   0.0719   0.4167        0.6504       0.8803        0.6733        19.3279
     43   0.0719   0.4139        0.6476       0.8815        0.6732        19.3810
     44   0.0722   0.4177        0.6496       [31m0.8839[0m        [94m0.6730[0m     +  19.1057
     45   0.0719   0.4158        [35m0.6473[0m       0.8827        [94m0.6728[0m     +  18.3339
     46   0.0715   0.4155        0.6515       0.8803        0.6729        18.5534
     47   0.0710   0.4118        0.6489       [31m0.8888[0m        [94m0.6721[0m     +  19.1293
     48   0.0710   0.4110        0.6481       0.8875        0.6727        18.9833
     49   0.0707   0.4119        [35m0.6462[0m       0.8875        0.6725        19.1912
     50   0.0702   0.4094        [35m0.6456[0m       0.8888        0.6724        18.9643
[32m[I 2023-05-02 01:36:18,278][0m Trial 10 finished with value: 0.6721280657538768 and parameters: {'lr': 1.0437075065732811e-08, 'dropout': 0.5794483367723846, 'd_model_multiplier': 8, 'num_layers': 4, 'n_heads': 16, 'dim_feedforward': 406, 'batch_size': 246, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'BatchNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0, 'top_n_features': 195}. Best is trial 0 with value: 0.23786523179607208.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 180
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
Warning, assumed runtime error: CUDA out of memory. Tried to allocate 1.12 GiB (GPU 0; 23.70 GiB total capacity; 20.51 GiB already allocated; 99.25 MiB free; 22.52 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[32m[I 2023-05-02 01:36:24,456][0m Trial 11 finished with value: 100.0 and parameters: {'lr': 0.00014079593587714063, 'dropout': 0.5426283654901487, 'd_model_multiplier': 4, 'num_layers': 11, 'n_heads': 16, 'dim_feedforward': 510, 'batch_size': 190, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0, 'top_n_features': 180}. Best is trial 0 with value: 0.23786523179607208.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 163
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2084[0m   [32m0.7744[0m        [35m0.2834[0m       [31m0.9347[0m        [94m0.2083[0m     +  13.5689
      2   [36m0.2441[0m   [32m0.7807[0m        [35m0.2659[0m       0.9299        [94m0.2047[0m     +  13.8309
      3   0.2410   0.7787        [35m0.2577[0m       0.9347        0.2050        13.7485
      4   [36m0.2890[0m   0.7769        [35m0.2525[0m       0.9347        0.2147        13.6476
      5   [36m0.2961[0m   0.7640        [35m0.2504[0m       [31m0.9359[0m        0.2199        14.0498
      6   0.2877   0.7671        [35m0.2456[0m       0.9299        0.2309        13.7098
      7   0.2767   0.7748        0.2460       0.9323        0.2206        14.0188
      8   0.2902   0.7776        0.2473       0.9311        0.2179        13.6441
      9   [36m0.3318[0m   0.7770        [35m0.2418[0m       0.9347        0.2148        13.8344
     10   0.3090   0.7778        0.2426       0.9359        0.2246        13.3430
     11   0.2810   [32m0.7818[0m        0.2432       0.9347        0.2295        13.8615
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 01:39:10,103][0m Trial 12 finished with value: 0.20472501771481755 and parameters: {'lr': 0.0006421562791202884, 'dropout': 0.5404054976182464, 'd_model_multiplier': 8, 'num_layers': 4, 'n_heads': 16, 'dim_feedforward': 433, 'batch_size': 57, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.01, 'top_n_features': 163}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 203
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1807[0m   [32m0.7099[0m        [35m0.3424[0m       [31m0.9166[0m        [94m0.2476[0m     +  16.4277
      2   [36m0.1869[0m   0.7043        [35m0.2686[0m       0.9117        0.2496        17.3551
      3   [36m0.2000[0m   [32m0.7108[0m        [35m0.2619[0m       0.8863        0.2748        17.4788
      4   0.1849   0.6958        0.2684       0.8972        0.2665        16.9023
      5   0.1880   0.6960        [35m0.2591[0m       [31m0.9226[0m        0.2544        17.1736
      6   0.1895   0.6963        [35m0.2591[0m       0.9214        0.2623        17.1750
      7   0.1942   0.6978        0.2597       0.9226        0.2575        18.2060
      8   0.1911   0.6928        0.2597       0.9214        0.2638        17.3117
      9   0.1828   0.6889        [35m0.2583[0m       [31m0.9287[0m        0.2742        17.3130
     10   0.1897   0.6931        [35m0.2561[0m       0.9287        0.2770        18.0364
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 01:42:21,650][0m Trial 13 finished with value: 0.2476065330919072 and parameters: {'lr': 0.0030907563506036456, 'dropout': 0.6138464349326367, 'd_model_multiplier': 8, 'num_layers': 4, 'n_heads': 16, 'dim_feedforward': 426, 'batch_size': 188, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.001, 'top_n_features': 203}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 152
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0588[0m   [32m0.2871[0m        [35m0.5306[0m       [31m0.9202[0m        [94m0.3700[0m     +  14.0911
      2   [36m0.0824[0m   [32m0.4846[0m        [35m0.3366[0m       0.9202        [94m0.2985[0m     +  14.1702
      3   [36m0.1168[0m   [32m0.5877[0m        [35m0.2840[0m       0.9202        [94m0.2801[0m     +  14.2843
      4   [36m0.1476[0m   [32m0.6324[0m        [35m0.2693[0m       0.9202        [94m0.2721[0m     +  14.4005
      5   [36m0.1637[0m   [32m0.6550[0m        [35m0.2591[0m       0.9202        [94m0.2673[0m     +  14.4480
      6   [36m0.1712[0m   [32m0.6661[0m        [35m0.2578[0m       0.9202        [94m0.2649[0m     +  14.7128
      7   [36m0.1805[0m   [32m0.6750[0m        [35m0.2541[0m       0.9202        [94m0.2627[0m     +  14.7877
      8   [36m0.1877[0m   [32m0.6839[0m        [35m0.2513[0m       0.9202        [94m0.2609[0m     +  14.9232
      9   [36m0.1967[0m   [32m0.6930[0m        [35m0.2508[0m       0.9202        [94m0.2597[0m     +  14.8913
     10   [36m0.2038[0m   [32m0.7022[0m        [35m0.2483[0m       0.9202        [94m0.2582[0m     +  15.0810
     11   [36m0.2132[0m   [32m0.7074[0m        0.2495       0.9202        [94m0.2572[0m     +  14.6346
     12   [36m0.2190[0m   [32m0.7135[0m        0.2484       0.9202        [94m0.2559[0m     +  14.4052
     13   [36m0.2241[0m   [32m0.7186[0m        [35m0.2455[0m       0.9202        [94m0.2546[0m     +  14.4204
     14   [36m0.2318[0m   [32m0.7243[0m        0.2464       0.9202        [94m0.2539[0m     +  14.8783
     15   [36m0.2368[0m   [32m0.7292[0m        0.2464       0.9190        [94m0.2530[0m     +  14.8109
     16   [36m0.2412[0m   [32m0.7328[0m        [35m0.2446[0m       0.9190        [94m0.2517[0m     +  14.8159
     17   [36m0.2502[0m   [32m0.7376[0m        [35m0.2445[0m       0.9190        [94m0.2507[0m     +  14.1390
     18   [36m0.2517[0m   [32m0.7411[0m        [35m0.2435[0m       0.9190        [94m0.2502[0m     +  14.3283
     19   [36m0.2557[0m   [32m0.7451[0m        0.2441       0.9166        [94m0.2490[0m     +  14.5270
     20   [36m0.2590[0m   [32m0.7488[0m        [35m0.2428[0m       0.9166        [94m0.2489[0m     +  14.4807
     21   [36m0.2592[0m   [32m0.7513[0m        [35m0.2420[0m       0.9166        [94m0.2483[0m     +  15.0650
     22   [36m0.2615[0m   [32m0.7532[0m        [35m0.2412[0m       0.9154        [94m0.2482[0m     +  14.6316
     23   [36m0.2642[0m   [32m0.7567[0m        [35m0.2386[0m       0.9154        0.2486        15.1155
     24   [36m0.2650[0m   [32m0.7592[0m        0.2388       0.9154        [94m0.2469[0m     +  14.7558
     25   [36m0.2660[0m   [32m0.7603[0m        [35m0.2382[0m       0.9141        0.2478        14.6398
     26   [36m0.2683[0m   [32m0.7627[0m        [35m0.2381[0m       0.9154        0.2471        14.8285
     27   [36m0.2697[0m   [32m0.7646[0m        0.2381       0.9178        0.2473        14.5636
     28   [36m0.2709[0m   [32m0.7666[0m        0.2384       0.9166        0.2471        14.9440
     29   [36m0.2724[0m   [32m0.7687[0m        [35m0.2378[0m       0.9154        0.2473        15.0873
     30   [36m0.2735[0m   [32m0.7692[0m        [35m0.2357[0m       0.9141        0.2482        15.0074
     31   [36m0.2737[0m   [32m0.7708[0m        [35m0.2356[0m       0.9141        0.2480        14.8793
     32   [36m0.2750[0m   [32m0.7718[0m        [35m0.2343[0m       0.9141        0.2490        15.0392
     33   0.2726   [32m0.7724[0m        0.2346       0.9166        0.2493        14.4513
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 01:50:41,186][0m Trial 14 finished with value: 0.24686951351497308 and parameters: {'lr': 2.016117105172039e-05, 'dropout': 0.5470647742171013, 'd_model_multiplier': 8, 'num_layers': 3, 'n_heads': 16, 'dim_feedforward': 271, 'batch_size': 176, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.01, 'top_n_features': 152}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 72
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2656[0m   [32m0.7368[0m        [35m0.3105[0m       [31m0.9045[0m        [94m0.3994[0m     +  17.5142
      2   [36m0.2663[0m   0.7323        [35m0.2710[0m       0.9045        0.4172        17.3284
      3   0.2647   [32m0.7433[0m        [35m0.2665[0m       0.9045        [94m0.3649[0m     +  17.5422
      4   0.2602   0.7346        [35m0.2596[0m       0.9033        0.3990        17.6325
      5   [36m0.2669[0m   0.7372        [35m0.2578[0m       0.9033        0.3910        17.7616
      6   [36m0.2674[0m   0.7379        0.2627       0.9045        [94m0.3579[0m     +  17.8001
      7   0.2647   0.7331        [35m0.2540[0m       0.9033        0.3650        17.8075
      8   0.2606   0.7380        0.2557       0.9033        0.3731        17.5172
      9   0.2534   0.7375        0.2561       0.9033        0.3802        17.6418
     10   0.2595   [32m0.7435[0m        0.2574       0.9033        0.3697        17.4950
     11   0.2669   0.7402        0.2555       0.9045        0.3721        17.5207
     12   0.2528   0.7361        0.2561       0.9045        0.3857        17.5620
     13   0.2605   0.7398        0.2542       0.9045        [94m0.3574[0m     +  17.4585
     14   0.2496   0.7326        0.2541       0.9045        [94m0.3417[0m     +  17.7319
     15   0.2443   0.7368        [35m0.2538[0m       0.9045        0.3514        17.7157
     16   0.2585   0.7393        0.2540       0.9045        [94m0.3301[0m     +  17.8089
     17   0.2553   0.7372        [35m0.2519[0m       0.9045        [94m0.3066[0m     +  17.8309
     18   0.2527   0.7324        [35m0.2511[0m       0.9045        [94m0.3044[0m     +  17.6361
     19   0.2461   0.7402        0.2517       0.9045        [94m0.2923[0m     +  17.5368
     20   0.2352   0.7195        0.2515       0.9045        0.2964        17.6134
     21   0.2492   0.7342        [35m0.2507[0m       0.9045        0.3001        17.5754
     22   0.2567   0.7375        [35m0.2506[0m       0.9045        [94m0.2897[0m     +  17.5025
     23   0.2391   0.7362        0.2519       0.9045        0.3020        18.0050
     24   0.2644   0.7407        0.2513       0.9045        0.2987        18.0410
     25   0.2646   0.7390        0.2509       0.9045        [94m0.2885[0m     +  18.0221
     26   0.2594   0.7296        0.2510       0.9045        [94m0.2870[0m     +  17.8992
     27   0.2244   0.7293        0.2537       0.9045        0.2874        17.8801
     28   0.2318   0.7327        0.2528       0.9045        0.2896        18.0397
     29   [36m0.2752[0m   0.7400        0.2511       0.9045        0.2969        18.0926
     30   0.2536   0.7314        [35m0.2499[0m       0.9045        0.2890        17.9419
     31   0.2486   [32m0.7461[0m        0.2506       0.9045        [94m0.2848[0m     +  18.0620
     32   0.2529   0.7422        [35m0.2469[0m       0.9045        0.2863        17.9639
     33   0.2503   0.7422        0.2485       0.9045        0.2860        17.8501
     34   0.2702   [32m0.7493[0m        0.2490       0.9045        0.2872        17.7610
     35   0.2579   0.7419        0.2476       0.9045        [94m0.2830[0m     +  17.6717
     36   0.2648   [32m0.7496[0m        0.2496       0.9045        0.2900        18.0723
     37   0.2472   0.7435        0.2483       0.9045        0.2883        17.7073
     38   0.2520   0.7365        0.2498       0.9045        0.2838        17.6466
     39   0.2611   0.7373        0.2496       0.9045        0.2875        17.5666
     40   0.2371   0.7276        0.2497       0.9045        0.2913        17.7260
     41   0.2447   0.7226        0.2501       0.9045        0.2914        17.7762
     42   0.2539   0.7353        0.2481       0.9045        0.2884        17.5457
     43   0.2545   0.7414        0.2501       0.9045        0.2873        17.4688
     44   0.2480   0.7423        0.2517       0.9045        0.2945        18.0733
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 02:04:01,341][0m Trial 15 finished with value: 0.2829510908677471 and parameters: {'lr': 0.002217444069562466, 'dropout': 0.6822588247467288, 'd_model_multiplier': 8, 'num_layers': 6, 'n_heads': 16, 'dim_feedforward': 354, 'batch_size': 52, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0, 'top_n_features': 72}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 159
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0888[0m   [32m0.4989[0m        [35m0.8225[0m       [31m0.9117[0m        [94m0.3180[0m     +  11.3168
      2   [36m0.1689[0m   [32m0.6815[0m        [35m0.2794[0m       0.8851        0.3421        11.6834
      3   [36m0.1852[0m   [32m0.7051[0m        [35m0.2656[0m       0.9093        [94m0.3070[0m     +  11.8314
      4   0.1738   [32m0.7069[0m        [35m0.2556[0m       0.9093        [94m0.2925[0m     +  12.0395
      5   0.1734   [32m0.7079[0m        [35m0.2517[0m       0.9021        0.3011        11.7735
      6   0.1717   0.7024        0.2530       0.8851        0.3067        11.9581
      7   0.1706   0.6986        0.2520       0.8839        0.3130        11.9779
      8   0.1733   0.6955        [35m0.2495[0m       0.9057        0.3101        11.5480
      9   0.1740   0.7028        0.2518       0.9069        0.3043        11.5663
     10   0.1724   0.7031        0.2495       0.9081        0.3039        11.4408
     11   0.1739   0.7051        0.2496       0.9117        0.3005        11.7911
     12   0.1798   0.7059        [35m0.2486[0m       0.9081        0.3069        11.4468
     13   0.1796   0.7051        0.2492       0.9117        0.2975        11.5724
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 02:06:45,506][0m Trial 16 finished with value: 0.29251852894636565 and parameters: {'lr': 0.08881251249820135, 'dropout': 0.5000101853581926, 'd_model_multiplier': 8, 'num_layers': 2, 'n_heads': 4, 'dim_feedforward': 429, 'batch_size': 222, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0, 'top_n_features': 159}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 207
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1706[0m   [32m0.5827[0m        [35m0.3262[0m       [31m0.9117[0m        [94m0.3654[0m     +  24.1529
      2   0.1611   [32m0.6021[0m        [35m0.2744[0m       0.9117        [94m0.3479[0m     +  23.7286
      3   [36m0.2120[0m   [32m0.6271[0m        [35m0.2632[0m       0.9117        [94m0.3422[0m     +  24.0900
      4   0.1893   0.6163        [35m0.2618[0m       0.9117        0.3478        24.1915
      5   0.1779   0.6172        0.2624       0.9117        [94m0.3314[0m     +  24.0494
      6   0.1895   0.6087        0.2623       0.9117        0.4278        24.3552
      7   0.2056   [32m0.6565[0m        [35m0.2604[0m       0.9117        0.3764        24.1429
      8   0.2037   [32m0.6657[0m        [35m0.2575[0m       0.9117        0.3831        24.0848
      9   0.2044   0.6442        0.2588       0.9117        0.3983        24.0395
     10   0.2074   [32m0.6682[0m        [35m0.2563[0m       0.9117        0.4242        24.0821
     11   [36m0.2162[0m   [32m0.6927[0m        [35m0.2549[0m       0.9117        0.3845        24.2661
     12   0.2037   0.6586        [35m0.2503[0m       0.9117        0.3988        24.0715
     13   [36m0.2244[0m   [32m0.6961[0m        0.2524       0.9117        0.4007        25.2124
     14   0.2115   0.6709        [35m0.2486[0m       0.9117        0.3923        24.0620
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 02:12:49,544][0m Trial 17 finished with value: 0.33138255463257665 and parameters: {'lr': 0.0007861778115561296, 'dropout': 0.6221736444427615, 'd_model_multiplier': 64, 'num_layers': 6, 'n_heads': 8, 'dim_feedforward': 387, 'batch_size': 163, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.01, 'top_n_features': 207}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 235
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1747[0m   [32m0.7203[0m        [35m0.2954[0m       [31m0.9105[0m        [94m0.2984[0m     +  14.6478
      2   0.1679   0.6397        [35m0.2519[0m       [31m0.9274[0m        [94m0.2806[0m     +  15.0559
      3   [36m0.2106[0m   [32m0.7515[0m        0.2520       0.8573        0.4553        14.9830
      4   0.2080   0.6653        0.2584       0.9274        [94m0.2726[0m     +  15.5074
      5   [36m0.2404[0m   0.7457        0.2542       0.9129        [94m0.2596[0m     +  15.4247
      6   0.2131   [32m0.7519[0m        [35m0.2502[0m       0.9202        [94m0.2394[0m     +  15.5312
      7   [36m0.2436[0m   [32m0.7645[0m        [35m0.2486[0m       0.8670        0.3446        15.6398
      8   [36m0.2510[0m   0.7526        [35m0.2470[0m       0.9226        [94m0.2305[0m     +  15.8179
      9   0.2331   [32m0.7649[0m        [35m0.2394[0m       0.9154        0.2546        15.4973
     10   0.2350   [32m0.7677[0m        [35m0.2358[0m       0.9166        0.2521        15.5412
     11   [36m0.2542[0m   [32m0.7726[0m        [35m0.2322[0m       0.9202        0.2366        15.8619
     12   0.2431   0.7645        [35m0.2321[0m       0.9045        0.2724        15.7249
     13   [36m0.2784[0m   [32m0.7746[0m        0.2336       0.9166        0.2567        16.0309
     14   0.2684   0.7651        0.2327       0.9154        0.2695        16.1057
     15   0.2652   0.7718        [35m0.2320[0m       0.9262        0.2366        16.1191
     16   0.2578   0.7716        [35m0.2305[0m       0.9033        0.2710        15.8238
     17   0.2507   [32m0.7777[0m        [35m0.2271[0m       0.8755        0.3217        15.2681
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 02:17:30,052][0m Trial 18 finished with value: 0.23047267046487893 and parameters: {'lr': 0.01627065005986551, 'dropout': 0.40013903800066314, 'd_model_multiplier': 2, 'num_layers': 3, 'n_heads': 16, 'dim_feedforward': 249, 'batch_size': 216, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.001, 'top_n_features': 235}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 89
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1930[0m   [32m0.7823[0m        [35m0.2906[0m       [31m0.8948[0m        [94m0.2678[0m     +  16.1266
      2   0.1491   0.6970        [35m0.2796[0m       [31m0.9081[0m        0.2818        16.2494
      3   0.1727   0.6861        0.2873       [31m0.9262[0m        [94m0.2462[0m     +  16.3736
      4   0.1543   0.6687        [35m0.2760[0m       0.9166        0.2802        16.4145
      5   0.1465   0.6754        [35m0.2645[0m       0.9154        0.2586        16.8728
      6   0.1514   0.6764        [35m0.2641[0m       0.9093        0.2707        17.0473
      7   0.1534   0.6714        0.2654       0.9141        0.2821        17.0447
      8   0.1560   0.6698        [35m0.2620[0m       0.9202        0.2564        16.9550
      9   0.1531   0.6592        [35m0.2615[0m       0.9057        0.2666        16.5712
     10   0.1484   0.6801        [35m0.2595[0m       0.9141        0.2591        16.7197
     11   0.1489   0.6713        [35m0.2593[0m       0.8912        0.2816        16.5599
     12   0.1475   0.7019        0.2614       0.9250        0.2564        16.8888
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 02:21:06,903][0m Trial 19 finished with value: 0.24618992713947827 and parameters: {'lr': 0.018486860030670495, 'dropout': 0.36264648826192974, 'd_model_multiplier': 2, 'num_layers': 6, 'n_heads': 16, 'dim_feedforward': 225, 'batch_size': 52, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.001, 'top_n_features': 89}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 141
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1971[0m   [32m0.6402[0m        [35m0.3102[0m       [31m0.9190[0m        [94m0.2688[0m     +  24.4085
      2   0.1967   [32m0.6867[0m        [35m0.2663[0m       0.9190        0.3499        24.6729
      3   [36m0.2273[0m   0.6825        [35m0.2609[0m       0.9154        0.3060        24.6211
      4   0.2117   0.6459        0.2689       0.9190        [94m0.2642[0m     +  24.6701
      5   0.2002   0.6454        0.2805       0.9190        [94m0.2640[0m     +  24.7562
      6   0.1952   0.6458        0.2614       0.9190        0.2660        24.8699
      7   0.1922   0.6533        0.2706       0.9190        0.2645        24.7478
      8   0.1916   0.6580        0.2868       0.9190        [94m0.2622[0m     +  24.8792
      9   0.1868   0.6511        0.2632       0.9141        0.2673        24.7842
     10   0.1864   0.6551        0.2622       0.9190        0.2707        24.9947
     11   0.1791   0.6593        0.3282       0.9033        0.2847        25.1449
     12   0.1810   0.6544        0.2743       0.9141        0.2655        25.0960
     13   0.1830   0.6493        0.2689       0.9190        0.2810        25.2140
     14   0.1890   0.6593        0.3075       0.9190        0.2630        25.5230
     15   0.2003   0.6580        0.2770       0.9190        0.2648        25.3651
     16   0.1929   0.6567        0.2815       0.9190        0.2683        25.2211
     17   0.1925   0.6581        0.2777       0.9178        0.2674        25.0475
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 02:28:36,555][0m Trial 20 finished with value: 0.26216345924564255 and parameters: {'lr': 0.013596599980764141, 'dropout': 0.4300687928798599, 'd_model_multiplier': 2, 'num_layers': 10, 'n_heads': 16, 'dim_feedforward': 256, 'batch_size': 96, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.001, 'top_n_features': 141}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 231
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2308[0m   [32m0.6692[0m        [35m0.3147[0m       [31m0.8803[0m        [94m0.3760[0m     +  23.3104
      2   0.2076   [32m0.7149[0m        [35m0.2720[0m       0.1729        1.0061        22.9195
      3   0.1433   0.5870        0.2804       [31m0.9238[0m        [94m0.3056[0m     +  23.0540
      4   0.1057   0.5431        0.2933       0.6820        0.6498        22.0516
      5   0.0571   0.3242        0.3311       [31m0.9250[0m        0.3997        22.6793
      6   0.1498   0.6986        0.5447       0.9250        [94m0.2884[0m     +  22.7415
      7   0.1764   [32m0.7271[0m        0.5448       0.8452        0.6752        22.8243
      8   0.1685   0.7065        0.5414       0.8537        0.6212        23.1986
      9   0.0742   0.4262        0.4705       0.9105        0.5085        22.9604
     10   0.0730   0.3688        0.4244       0.5889        0.6796        22.9101
     11   0.2092   0.6772        0.4482       0.9238        0.3147        22.5732
     12   0.2029   0.6872        0.3887       0.9250        [94m0.2611[0m     +  23.0435
     13   0.2089   0.7174        0.3652       0.9250        [94m0.2522[0m     +  22.7135
     14   0.1834   0.7050        0.3485       0.9226        [94m0.2511[0m     +  22.6387
     15   0.1759   0.7076        0.3344       0.8476        0.4165        23.1185
     16   0.1593   0.6748        0.3058       0.1052        1.0596        22.9847
     17   0.1848   0.6615        0.3159       0.5103        0.7768        23.2370
     18   0.1970   0.7141        0.2969       0.7086        0.5776        22.8587
     19   0.1848   [32m0.7303[0m        0.2912       0.9238        0.2524        22.7020
     20   0.1714   0.7144        0.3032       0.9045        0.2587        22.9609
     21   0.1692   0.7206        0.2798       0.9250        [94m0.2453[0m     +  23.1719
     22   0.2067   0.7048        0.2794       0.9250        0.2472        23.1174
     23   0.1828   0.6970        [35m0.2690[0m       0.9250        0.2518        23.0544
     24   0.1717   0.7051        [35m0.2674[0m       0.9154        0.2555        22.9936
     25   0.1804   0.6961        0.2803       0.9250        0.2571        22.8480
     26   0.1756   0.7195        0.2787       0.9250        0.2466        23.1454
     27   0.1945   0.7162        0.2697       0.9250        [94m0.2429[0m     +  22.2329
     28   0.1947   0.6988        [35m0.2601[0m       0.9250        0.2454        23.1889
     29   0.1813   0.6904        0.2641       0.9250        0.2459        22.1866
     30   0.1745   0.7231        [35m0.2597[0m       0.9250        0.2486        23.1229
     31   0.1863   0.7019        [35m0.2590[0m       0.9250        0.2460        22.8854
     32   0.1694   0.6818        [35m0.2573[0m       0.9250        0.2550        22.7566
     33   0.1737   0.7156        0.2665       0.9226        0.2450        22.9010
     34   0.1774   0.7136        0.2606       0.9226        0.2478        22.5203
     35   0.1963   0.7070        0.2578       0.9250        0.2453        22.6590
     36   0.1776   0.7163        0.2620       0.9250        0.2437        23.1338
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 02:42:44,549][0m Trial 21 finished with value: 0.24287627120260172 and parameters: {'lr': 0.008452383531706811, 'dropout': 0.5141145997759081, 'd_model_multiplier': 32, 'num_layers': 3, 'n_heads': 16, 'dim_feedforward': 317, 'batch_size': 210, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.001, 'top_n_features': 231}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 187
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0669[0m   [32m0.4096[0m        [35m0.6242[0m       [31m0.9299[0m        [94m0.6248[0m     +  14.4766
      2   [36m0.0811[0m   [32m0.4582[0m        [35m0.4182[0m       [31m0.9311[0m        [94m0.4849[0m     +  14.7018
      3   [36m0.1827[0m   [32m0.6488[0m        [35m0.3153[0m       [31m0.9323[0m        [94m0.4058[0m     +  14.8761
      4   0.1779   [32m0.6947[0m        [35m0.2814[0m       0.9311        [94m0.3660[0m     +  14.5776
      5   0.1806   [32m0.7166[0m        [35m0.2712[0m       0.9311        [94m0.3408[0m     +  14.3106
      6   [36m0.1877[0m   [32m0.7271[0m        [35m0.2709[0m       0.9311        [94m0.3239[0m     +  14.2165
      7   [36m0.1981[0m   [32m0.7404[0m        [35m0.2673[0m       0.9323        [94m0.3107[0m     +  14.1261
      8   [36m0.2063[0m   [32m0.7511[0m        [35m0.2646[0m       0.9323        [94m0.3037[0m     +  14.6406
      9   [36m0.2186[0m   [32m0.7575[0m        [35m0.2591[0m       0.9323        [94m0.2980[0m     +  14.7739
     10   0.2185   [32m0.7619[0m        [35m0.2575[0m       0.9323        [94m0.2939[0m     +  14.2311
     11   0.2104   [32m0.7648[0m        0.2604       0.9323        [94m0.2854[0m     +  14.4263
     12   [36m0.2191[0m   [32m0.7711[0m        [35m0.2549[0m       0.9323        [94m0.2724[0m     +  14.2684
     13   0.2037   0.7648        [35m0.2513[0m       0.9323        [94m0.2639[0m     +  15.0166
     14   0.2008   0.7681        0.2548       0.9323        [94m0.2596[0m     +  14.7278
     15   0.2149   0.7692        0.2520       0.9323        [94m0.2478[0m     +  14.2484
     16   0.2073   0.7691        [35m0.2505[0m       0.9323        [94m0.2393[0m     +  14.4087
     17   0.2132   [32m0.7754[0m        [35m0.2475[0m       0.9323        0.2448        14.4798
     18   0.2083   0.7736        0.2515       0.9311        0.2434        14.3853
     19   0.2089   0.7737        [35m0.2467[0m       0.9311        [94m0.2383[0m     +  14.6987
     20   0.2158   [32m0.7786[0m        0.2475       0.9311        [94m0.2383[0m     +  14.4027
     21   0.2059   0.7642        [35m0.2442[0m       0.9311        [94m0.2359[0m     +  14.2096
     22   0.1985   0.7550        0.2458       0.9311        [94m0.2324[0m     +  14.6326
     23   0.2045   0.7577        0.2451       0.9311        0.2338        14.4631
     24   0.1907   0.7541        [35m0.2429[0m       0.9311        [94m0.2294[0m     +  14.4623
     25   0.1837   0.7509        [35m0.2410[0m       0.9311        0.2324        14.3910
     26   0.1889   0.7581        0.2435       0.9299        0.2336        14.1128
     27   0.1986   0.7711        [35m0.2374[0m       0.9274        0.2300        14.4905
     28   0.1935   0.7669        0.2422       0.9311        [94m0.2256[0m     +  14.7285
     29   0.1975   0.7671        [35m0.2369[0m       0.9299        0.2260        14.9028
     30   0.1927   0.7634        0.2401       0.9274        0.2259        14.5273
     31   0.1874   0.7613        [35m0.2366[0m       0.9311        [94m0.2218[0m     +  14.3619
     32   0.1895   0.7574        0.2426       0.9323        0.2232        14.2319
     33   0.1843   0.7530        [35m0.2361[0m       0.9323        0.2230        14.5961
     34   0.1850   0.7577        0.2386       0.9323        0.2225        14.7090
     35   0.1912   0.7694        [35m0.2345[0m       0.9311        [94m0.2211[0m     +  14.6114
     36   0.1893   0.7641        0.2389       0.9311        0.2220        14.4888
     37   0.1952   0.7647        0.2356       0.9323        0.2219        14.2803
     38   0.2048   0.7736        [35m0.2340[0m       0.9311        [94m0.2196[0m     +  14.5948
     39   0.2067   0.7710        0.2375       0.9311        0.2207        14.4052
     40   0.2008   0.7649        0.2366       0.9311        0.2230        14.2238
     41   0.1980   0.7605        0.2341       0.9323        0.2256        14.6703
     42   0.2018   0.7664        [35m0.2313[0m       0.9299        0.2227        14.6004
     43   0.2171   [32m0.7805[0m        [35m0.2288[0m       0.9299        [94m0.2175[0m     +  14.0659
     44   0.2062   0.7713        0.2333       0.9299        0.2208        14.5977
     45   [36m0.2271[0m   0.7800        0.2330       0.9274        0.2177        14.7491
     46   [36m0.2278[0m   [32m0.7844[0m        0.2304       0.9287        [94m0.2173[0m     +  14.7525
     47   0.2211   0.7793        0.2305       0.9262        0.2191        14.3179
     48   0.2162   0.7794        0.2295       0.9274        0.2191        14.4805
     49   0.2206   0.7797        [35m0.2251[0m       0.9287        0.2194        14.7909
     50   0.2096   0.7816        0.2288       0.9311        0.2249        14.5950
[32m[I 2023-05-02 02:54:50,734][0m Trial 22 finished with value: 0.21725659121048638 and parameters: {'lr': 0.0007189055373413652, 'dropout': 0.5769599877296447, 'd_model_multiplier': 2, 'num_layers': 2, 'n_heads': 16, 'dim_feedforward': 456, 'batch_size': 255, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.01, 'top_n_features': 187}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 179
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0914[0m   [32m0.3683[0m        [35m0.6334[0m       [31m0.9262[0m        [94m0.6220[0m     +  17.5988
      2   0.0788   [32m0.3885[0m        [35m0.4464[0m       0.9262        [94m0.5082[0m     +  19.1727
      3   [36m0.1161[0m   [32m0.5850[0m        [35m0.3299[0m       0.9262        [94m0.4246[0m     +  18.4476
      4   [36m0.1528[0m   [32m0.6647[0m        [35m0.2953[0m       0.9238        [94m0.3696[0m     +  17.7829
      5   [36m0.1648[0m   [32m0.6795[0m        [35m0.2759[0m       0.9250        [94m0.3331[0m     +  18.2941
      6   [36m0.1751[0m   [32m0.6931[0m        [35m0.2717[0m       0.9250        [94m0.3065[0m     +  18.2273
      7   [36m0.1877[0m   [32m0.6948[0m        [35m0.2683[0m       0.9250        [94m0.3022[0m     +  18.0449
      8   [36m0.1881[0m   0.6873        [35m0.2650[0m       0.9262        [94m0.2763[0m     +  18.7438
      9   [36m0.1974[0m   0.6947        [35m0.2618[0m       0.9250        [94m0.2648[0m     +  18.0172
     10   [36m0.2081[0m   [32m0.6994[0m        [35m0.2603[0m       0.9238        0.2661        17.7604
     11   [36m0.2113[0m   0.6986        0.2620       0.9238        0.2713        18.2052
     12   0.2045   0.6988        0.2638       0.9250        [94m0.2576[0m     +  17.7141
     13   [36m0.2135[0m   [32m0.7049[0m        [35m0.2539[0m       0.9238        [94m0.2533[0m     +  18.8469
     14   [36m0.2202[0m   0.7033        [35m0.2529[0m       0.9250        [94m0.2501[0m     +  18.2377
     15   [36m0.2246[0m   0.7047        0.2592       0.9250        0.2510        18.0763
     16   0.2193   0.6996        [35m0.2519[0m       0.9250        [94m0.2482[0m     +  18.4068
     17   0.2137   0.6988        0.2549       0.9238        [94m0.2472[0m     +  17.7750
     18   [36m0.2273[0m   0.7040        0.2530       0.9238        [94m0.2466[0m     +  18.2518
     19   [36m0.2370[0m   [32m0.7119[0m        [35m0.2500[0m       0.9226        0.2566        18.1969
     20   0.2271   0.7100        0.2527       0.9226        0.2553        18.2738
     21   0.2274   0.7095        0.2538       0.9250        0.2481        18.0391
     22   0.2318   0.7053        [35m0.2492[0m       0.9238        [94m0.2460[0m     +  17.6496
     23   0.2273   0.7056        [35m0.2460[0m       0.9226        0.2468        17.7642
     24   0.2235   0.7044        0.2522       0.9214        0.2503        18.1500
     25   0.2244   0.7026        0.2533       0.9250        0.2469        18.8492
     26   0.2232   0.7040        0.2481       0.9238        0.2463        18.2417
     27   0.2227   0.7074        0.2462       0.9250        0.2470        17.9308
     28   0.2165   0.7019        0.2493       0.9238        0.2464        17.7876
     29   0.2168   0.7006        [35m0.2441[0m       0.9250        0.2482        18.3072
     30   0.2217   0.7001        0.2485       0.9250        0.2465        18.5988
     31   0.2204   0.7000        0.2468       0.9238        [94m0.2437[0m     +  18.0836
     32   0.2328   0.7072        [35m0.2419[0m       0.9250        [94m0.2421[0m     +  18.1709
     33   0.2315   0.7112        0.2453       0.9238        [94m0.2416[0m     +  17.7600
     34   0.2345   [32m0.7131[0m        [35m0.2418[0m       0.9238        [94m0.2400[0m     +  18.2402
     35   0.2284   0.7125        [35m0.2408[0m       0.9262        0.2401        18.7612
     36   0.2236   0.7113        [35m0.2368[0m       0.9250        0.2421        18.0950
     37   0.2250   0.7126        0.2397       0.9250        0.2409        18.0551
     38   0.2302   [32m0.7135[0m        0.2391       [31m0.9274[0m        0.2408        18.9149
     39   0.2222   0.7104        0.2397       0.9262        0.2441        18.0167
     40   0.2328   [32m0.7156[0m        0.2369       [31m0.9287[0m        0.2411        18.3821
     41   0.2189   0.7121        [35m0.2359[0m       0.9287        0.2437        17.4912
     42   0.2313   [32m0.7161[0m        0.2362       0.9262        0.2487        18.4649
     43   0.2230   0.7132        0.2359       0.9262        0.2476        19.3069
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 03:08:13,034][0m Trial 23 finished with value: 0.24000221161124496 and parameters: {'lr': 0.0007632216352060004, 'dropout': 0.5852676063820843, 'd_model_multiplier': 2, 'num_layers': 4, 'n_heads': 16, 'dim_feedforward': 462, 'batch_size': 255, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.01, 'top_n_features': 179}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 193
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2226[0m   [32m0.6893[0m        [35m0.3459[0m       [31m0.9105[0m        [94m0.3570[0m     +  11.6406
      2   [36m0.2387[0m   [32m0.7013[0m        [35m0.2670[0m       0.9105        0.3631        11.1327
      3   0.2385   [32m0.7073[0m        [35m0.2610[0m       0.9105        [94m0.3000[0m     +  13.2167
      4   [36m0.2453[0m   [32m0.7121[0m        [35m0.2592[0m       0.9105        [94m0.2778[0m     +  11.2603
      5   [36m0.2698[0m   [32m0.7215[0m        [35m0.2546[0m       0.9105        [94m0.2714[0m     +  12.5275
      6   0.2527   [32m0.7239[0m        0.2554       0.9105        0.2738        11.9123
      7   0.2657   [32m0.7330[0m        0.2561       0.9105        0.2792        12.0733
      8   0.2553   [32m0.7387[0m        [35m0.2537[0m       0.9105        0.2885        12.1306
      9   [36m0.2810[0m   0.7371        [35m0.2490[0m       0.9105        0.2715        12.3605
     10   [36m0.2856[0m   [32m0.7457[0m        [35m0.2450[0m       0.9105        0.2742        12.9630
     11   [36m0.2911[0m   0.7437        0.2458       [31m0.9117[0m        [94m0.2662[0m     +  12.6057
     12   [36m0.2938[0m   0.7453        0.2467       0.9105        [94m0.2641[0m     +  12.1718
     13   0.2837   0.7301        [35m0.2432[0m       0.9105        0.2788        12.4156
     14   0.2543   0.7074        [35m0.2420[0m       0.9105        0.2795        13.2930
     15   0.2568   0.7164        0.2429       0.9105        0.2921        12.2968
     16   0.2873   0.7339        [35m0.2410[0m       0.9105        0.2741        12.4017
     17   [36m0.3008[0m   0.7446        [35m0.2404[0m       [31m0.9129[0m        0.2698        12.5751
     18   0.2954   0.7369        [35m0.2362[0m       0.9105        0.2828        12.0865
     19   [36m0.3124[0m   [32m0.7595[0m        [35m0.2360[0m       0.9093        [94m0.2632[0m     +  12.3519
     20   0.3029   0.7513        0.2381       0.9117        0.2830        12.2432
     21   [36m0.3150[0m   0.7571        [35m0.2357[0m       0.9093        [94m0.2625[0m     +  12.2697
     22   0.2998   0.7536        0.2361       0.9129        0.2714        12.3550
     23   [36m0.3176[0m   0.7560        0.2369       0.9093        0.2678        12.7246
     24   0.2955   0.7355        0.2361       0.9117        0.2847        12.6558
     25   0.2997   [32m0.7610[0m        0.2384       0.9093        0.2637        13.0893
     26   0.2927   0.7437        [35m0.2342[0m       0.9105        0.2765        12.8502
     27   0.2502   0.7143        [35m0.2311[0m       0.9105        0.3050        12.4795
     28   0.2669   0.7314        0.2350       0.9105        0.2803        12.5881
     29   0.2720   0.7220        0.2379       0.9105        0.3060        12.2690
     30   0.2861   0.7360        0.2373       0.9117        0.2792        12.6722
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 03:14:37,828][0m Trial 24 finished with value: 0.2625477636703701 and parameters: {'lr': 0.02350766774629553, 'dropout': 0.6466654847694363, 'd_model_multiplier': 2, 'num_layers': 2, 'n_heads': 8, 'dim_feedforward': 206, 'batch_size': 231, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.01, 'top_n_features': 193}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 216
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2143[0m   [32m0.6897[0m        [35m0.4201[0m       [31m0.9057[0m        [94m0.5087[0m     +  10.4494
      2   [36m0.2550[0m   [32m0.7103[0m        [35m0.2735[0m       0.9057        [94m0.2854[0m     +  10.7855
      3   [36m0.2814[0m   [32m0.7203[0m        [35m0.2671[0m       0.9057        0.2984        11.2033
      4   0.2629   [32m0.7209[0m        [35m0.2596[0m       0.9057        0.2883        11.4491
      5   0.2649   0.7093        [35m0.2594[0m       0.9057        0.2922        11.3174
      6   0.2636   0.7162        [35m0.2525[0m       0.9057        [94m0.2819[0m     +  11.4178
      7   0.2632   [32m0.7273[0m        0.2559       0.9057        0.2828        11.5488
      8   0.2702   0.7236        0.2555       0.9057        [94m0.2802[0m     +  11.0104
      9   0.2782   0.7227        0.2590       0.9057        0.2818        10.8967
     10   0.2567   0.7263        0.2531       0.9057        0.2819        11.0284
     11   0.2750   [32m0.7273[0m        0.2601       0.9057        0.2872        11.1290
     12   0.2726   [32m0.7328[0m        0.2541       0.9057        0.2806        12.1404
     13   0.2735   [32m0.7383[0m        0.2541       0.9057        0.2806        11.4444
     14   0.2701   [32m0.7396[0m        0.2537       0.9057        [94m0.2798[0m     +  11.3671
     15   0.2766   0.7386        0.2539       0.9057        0.2892        11.4539
     16   [36m0.2868[0m   [32m0.7411[0m        0.2549       0.9057        0.2944        11.0358
     17   [36m0.2919[0m   [32m0.7433[0m        0.2593       0.9057        0.2801        11.4416
     18   0.2738   0.7329        0.2547       0.9057        0.2909        11.2788
     19   0.2797   0.7301        0.2551       0.9057        0.2911        11.2241
     20   0.2723   0.7289        0.2567       0.9057        0.2906        11.4611
     21   0.2736   0.7386        0.2562       0.9057        0.2870        11.2291
     22   0.2736   [32m0.7444[0m        0.2525       0.9057        0.2845        11.4514
     23   0.2603   0.7366        0.2563       0.9057        0.2920        11.9125
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 03:19:09,111][0m Trial 25 finished with value: 0.27979212087616234 and parameters: {'lr': 0.08843718230229852, 'dropout': 0.6873505274799138, 'd_model_multiplier': 2, 'num_layers': 5, 'n_heads': 4, 'dim_feedforward': 462, 'batch_size': 155, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.01, 'top_n_features': 216}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 166
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1648[0m   [32m0.6676[0m        [35m0.3068[0m       [31m0.9250[0m        [94m0.2822[0m     +  10.1623
      2   [36m0.2000[0m   [32m0.7115[0m        [35m0.2575[0m       0.9250        [94m0.2764[0m     +  11.3449
      3   0.1974   [32m0.7407[0m        0.2584       0.9202        0.2937        10.9603
      4   [36m0.2279[0m   [32m0.7614[0m        [35m0.2533[0m       0.9226        0.3004        11.6661
      5   0.2107   0.7251        0.2575       0.9202        0.2991        11.5513
      6   0.2272   [32m0.7662[0m        [35m0.2486[0m       0.9178        0.3002        11.3376
      7   [36m0.2363[0m   0.7177        0.2533       [31m0.9262[0m        [94m0.2614[0m     +  11.6656
      8   [36m0.2664[0m   0.7625        0.2565       0.9262        [94m0.2434[0m     +  11.7114
      9   0.2520   [32m0.7788[0m        0.2538       0.9262        [94m0.2362[0m     +  11.5608
     10   [36m0.2668[0m   0.7701        [35m0.2461[0m       0.9250        [94m0.2344[0m     +  11.6579
     11   [36m0.2767[0m   [32m0.7867[0m        0.2514       0.9262        0.2372        11.4517
     12   0.2670   0.7753        0.2491       0.9250        [94m0.2331[0m     +  11.3846
     13   0.2500   0.7572        [35m0.2459[0m       0.9250        0.2380        12.0304
     14   0.2620   [32m0.7874[0m        0.2465       0.9250        [94m0.2318[0m     +  11.5002
     15   0.2638   [32m0.7964[0m        [35m0.2450[0m       0.9262        [94m0.2291[0m     +  11.7101
     16   0.2526   [32m0.7975[0m        0.2455       0.9250        [94m0.2274[0m     +  11.2681
     17   0.2539   [32m0.8063[0m        0.2479       0.9250        [94m0.2267[0m     +  11.5088
     18   0.2671   [32m0.8098[0m        0.2495       0.9262        0.2274        11.0645
     19   0.2591   0.8075        [35m0.2429[0m       0.9250        0.2294        11.3383
     20   0.2703   [32m0.8175[0m        [35m0.2426[0m       0.9250        0.2369        11.5220
     21   0.2766   0.8174        0.2441       0.9250        0.2392        11.4776
     22   [36m0.2779[0m   [32m0.8245[0m        0.2437       0.9238        [94m0.2247[0m     +  11.6201
     23   0.2776   [32m0.8325[0m        [35m0.2425[0m       0.9238        [94m0.2216[0m     +  11.2575
     24   0.2732   [32m0.8346[0m        [35m0.2420[0m       0.9262        0.2224        11.5783
     25   0.2762   0.8235        [35m0.2416[0m       0.9250        0.2227        12.0270
     26   [36m0.2873[0m   0.8315        [35m0.2411[0m       0.9262        0.2220        11.7543
     27   0.2841   0.8329        [35m0.2396[0m       0.9190        [94m0.2193[0m     +  11.8229
     28   0.2857   0.8327        [35m0.2370[0m       0.9178        [94m0.2179[0m     +  11.9360
     29   0.2783   0.8337        0.2383       0.9238        0.2208        11.2760
     30   [36m0.2949[0m   [32m0.8362[0m        0.2396       0.9262        0.2188        11.4552
     31   0.2900   0.8217        0.2399       0.9262        0.2287        11.8153
     32   [36m0.2958[0m   0.8252        0.2386       0.9262        0.2313        11.7375
     33   [36m0.3111[0m   0.8306        0.2399       0.9262        0.2311        11.1313
     34   0.2981   0.8326        0.2424       0.9250        0.2201        11.6228
     35   0.2932   0.8302        0.2412       0.9250        0.2193        11.4703
     36   0.2994   0.8312        0.2385       0.9250        0.2216        11.7635
     37   0.2948   0.8303        [35m0.2363[0m       0.9214        0.2202        12.2285
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 03:26:27,809][0m Trial 26 finished with value: 0.21785039462806813 and parameters: {'lr': 0.006739523547877635, 'dropout': 0.5653890116508902, 'd_model_multiplier': 2, 'num_layers': 1, 'n_heads': 16, 'dim_feedforward': 302, 'batch_size': 202, 'pos_encoding': 'learnable', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.001, 'top_n_features': 166}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 163
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1568[0m   [32m0.5720[0m        [35m0.6990[0m       [31m0.9117[0m        [94m0.6678[0m     +  12.4443
      2   0.1291   0.4964        [35m0.6448[0m       [31m0.9335[0m        [94m0.6558[0m     +  13.0675
      3   0.1037   0.4430        [35m0.5869[0m       [31m0.9371[0m        [94m0.6394[0m     +  12.9590
      4   0.0778   0.4225        [35m0.5229[0m       0.9371        [94m0.6228[0m     +  12.7049
      5   0.0739   0.4287        [35m0.4647[0m       0.9371        [94m0.6001[0m     +  13.2377
      6   0.0703   0.4475        [35m0.4177[0m       0.9371        [94m0.5780[0m     +  13.1648
      7   0.0795   0.4821        [35m0.3829[0m       0.9371        [94m0.5615[0m     +  12.8377
      8   0.0875   0.5153        [35m0.3565[0m       0.9371        [94m0.5357[0m     +  13.6005
      9   0.0996   0.5574        [35m0.3365[0m       0.9371        [94m0.5158[0m     +  13.8209
     10   0.1121   [32m0.5972[0m        [35m0.3210[0m       0.9359        [94m0.4986[0m     +  13.4915
     11   0.1283   [32m0.6267[0m        [35m0.3114[0m       0.9359        [94m0.4909[0m     +  13.5907
     12   0.1438   [32m0.6494[0m        [35m0.3035[0m       0.9359        [94m0.4846[0m     +  13.1949
     13   [36m0.1590[0m   [32m0.6718[0m        [35m0.2977[0m       0.9347        [94m0.4654[0m     +  13.2643
     14   [36m0.1698[0m   [32m0.6877[0m        0.2985       0.9359        [94m0.4589[0m     +  13.2136
     15   [36m0.1825[0m   [32m0.6980[0m        [35m0.2864[0m       0.9359        [94m0.4477[0m     +  13.9213
     16   [36m0.1968[0m   [32m0.7082[0m        [35m0.2844[0m       0.9359        [94m0.4297[0m     +  13.4272
     17   [36m0.2059[0m   [32m0.7172[0m        [35m0.2816[0m       0.9359        [94m0.4250[0m     +  13.2555
     18   [36m0.2110[0m   [32m0.7218[0m        [35m0.2801[0m       0.9371        [94m0.4243[0m     +  13.3884
     19   [36m0.2139[0m   [32m0.7251[0m        [35m0.2795[0m       0.9371        [94m0.4129[0m     +  13.2411
     20   [36m0.2191[0m   [32m0.7299[0m        [35m0.2792[0m       0.9371        [94m0.4112[0m     +  13.0700
     21   [36m0.2236[0m   [32m0.7343[0m        [35m0.2756[0m       0.9371        [94m0.4015[0m     +  13.0035
     22   [36m0.2251[0m   [32m0.7380[0m        [35m0.2731[0m       0.9371        0.4025        13.1949
     23   0.2204   [32m0.7400[0m        0.2779       0.9371        0.4037        13.8693
     24   [36m0.2289[0m   [32m0.7436[0m        0.2769       0.9371        [94m0.3993[0m     +  13.5581
     25   0.2287   0.7433        0.2754       0.9371        [94m0.3889[0m     +  13.7489
     26   [36m0.2326[0m   [32m0.7458[0m        [35m0.2720[0m       0.9371        [94m0.3856[0m     +  13.0724
     27   [36m0.2330[0m   [32m0.7472[0m        [35m0.2704[0m       0.9371        [94m0.3843[0m     +  13.4876
     28   [36m0.2346[0m   [32m0.7501[0m        0.2733       0.9371        0.3849        13.5514
     29   [36m0.2351[0m   0.7498        [35m0.2686[0m       0.9371        [94m0.3780[0m     +  13.3507
     30   0.2277   [32m0.7509[0m        [35m0.2668[0m       0.9371        0.3783        13.4201
     31   0.2287   [32m0.7535[0m        [35m0.2667[0m       0.9371        [94m0.3759[0m     +  13.1952
     32   0.2305   [32m0.7539[0m        [35m0.2648[0m       0.9371        [94m0.3660[0m     +  13.0066
     33   0.2295   [32m0.7543[0m        0.2679       0.9371        0.3716        13.1001
     34   0.2314   0.7515        [35m0.2639[0m       0.9371        0.3715        13.0094
     35   0.2301   [32m0.7547[0m        0.2669       0.9371        [94m0.3631[0m     +  13.0712
     36   [36m0.2429[0m   0.7538        0.2672       0.9371        0.3737        13.1739
     37   [36m0.2449[0m   [32m0.7553[0m        0.2639       0.9371        [94m0.3595[0m     +  13.1428
     38   [36m0.2464[0m   0.7548        0.2647       0.9371        0.3603        13.3244
     39   [36m0.2476[0m   [32m0.7575[0m        [35m0.2628[0m       0.9371        [94m0.3561[0m     +  13.5535
     40   0.2409   0.7567        [35m0.2618[0m       0.9371        0.3646        13.5566
     41   0.2461   [32m0.7589[0m        0.2649       0.9371        [94m0.3522[0m     +  13.3354
     42   [36m0.2478[0m   [32m0.7597[0m        [35m0.2573[0m       0.9371        [94m0.3477[0m     +  13.3627
     43   0.2318   [32m0.7600[0m        0.2615       0.9371        0.3536        13.3674
     44   0.2421   0.7577        0.2638       0.9371        [94m0.3445[0m     +  13.1531
     45   0.2404   0.7560        0.2591       [31m0.9383[0m        [94m0.3413[0m     +  13.2881
     46   0.2393   0.7587        0.2593       0.9371        0.3440        13.5874
     47   0.2415   0.7596        0.2586       0.9371        [94m0.3343[0m     +  13.0528
     48   0.2454   0.7598        0.2632       0.9383        [94m0.3326[0m     +  13.4399
     49   0.2343   0.7595        [35m0.2557[0m       0.9371        0.3334        13.4819
     50   [36m0.2490[0m   [32m0.7600[0m        0.2573       0.9383        [94m0.3291[0m     +  13.2665
[32m[I 2023-05-02 03:37:33,969][0m Trial 27 finished with value: 0.3291066536519951 and parameters: {'lr': 5.843309885011662e-05, 'dropout': 0.5797566478272832, 'd_model_multiplier': 2, 'num_layers': 2, 'n_heads': 16, 'dim_feedforward': 313, 'batch_size': 205, 'pos_encoding': 'learnable', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.01, 'top_n_features': 163}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 145
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1820[0m   [32m0.6696[0m        [35m0.3131[0m       [31m0.9190[0m        [94m0.3437[0m     +  12.3774
      2   [36m0.2020[0m   [32m0.6820[0m        [35m0.2600[0m       0.9190        0.3543        13.0367
      3   [36m0.2652[0m   [32m0.7177[0m        [35m0.2567[0m       [31m0.9214[0m        0.3616        13.0952
      4   [36m0.2697[0m   [32m0.7183[0m        [35m0.2537[0m       0.9214        0.3698        12.8507
      5   [36m0.2844[0m   0.7086        [35m0.2523[0m       0.9202        0.3481        12.7854
      6   0.2779   0.7170        0.2572       0.9190        [94m0.3277[0m     +  11.4387
      7   [36m0.2925[0m   [32m0.7309[0m        0.2529       0.9166        [94m0.3105[0m     +  11.8040
      8   0.2886   [32m0.7493[0m        [35m0.2498[0m       0.9154        [94m0.2948[0m     +  11.8551
      9   [36m0.2958[0m   [32m0.7535[0m        [35m0.2495[0m       0.9166        0.3022        12.2835
     10   [36m0.3131[0m   0.7526        0.2516       0.9190        [94m0.2577[0m     +  12.3055
     11   [36m0.3305[0m   [32m0.7747[0m        [35m0.2483[0m       0.9178        0.2661        12.8254
     12   0.2930   0.7559        0.2506       0.9190        [94m0.2567[0m     +  12.7430
     13   0.2909   0.7511        [35m0.2460[0m       0.9178        [94m0.2484[0m     +  12.5226
     14   0.2857   0.7426        0.2469       0.9166        0.2489        12.3041
     15   0.2986   0.7584        0.2524       0.9166        [94m0.2419[0m     +  12.1283
     16   0.3002   0.7576        0.2469       0.9190        [94m0.2419[0m     +  12.1506
     17   0.2902   0.7644        [35m0.2454[0m       0.9166        [94m0.2405[0m     +  12.2154
     18   0.2908   0.7668        0.2458       0.9178        [94m0.2399[0m     +  12.3651
     19   0.2908   0.7655        [35m0.2415[0m       0.9166        0.2401        12.2932
     20   0.2904   0.7704        [35m0.2403[0m       0.9154        0.2437        12.4805
     21   0.3028   0.7724        0.2420       0.9166        [94m0.2371[0m     +  12.4597
     22   0.2817   0.7710        [35m0.2381[0m       0.9190        0.2500        12.3040
     23   0.2958   0.7732        0.2414       0.9202        0.2454        12.4519
     24   0.3031   [32m0.7767[0m        0.2398       0.9166        [94m0.2365[0m     +  12.2114
     25   0.3165   [32m0.7818[0m        [35m0.2378[0m       0.9166        [94m0.2341[0m     +  12.1819
     26   0.3040   0.7791        [35m0.2366[0m       0.9166        0.2358        12.2239
     27   0.3099   0.7800        0.2390       0.9178        0.2411        12.4613
     28   0.3150   [32m0.7831[0m        0.2382       0.9190        0.2351        12.5954
     29   0.3086   0.7760        0.2405       0.9178        0.2465        12.9341
     30   0.2999   0.7817        0.2418       0.9154        0.2388        12.5528
     31   0.2884   0.7713        [35m0.2365[0m       0.9057        0.2654        12.6713
     32   0.3012   0.7751        0.2404       0.9166        0.2428        12.5729
     33   0.3182   0.7813        0.2384       [31m0.9226[0m        0.2362        12.2924
     34   0.3038   0.7739        0.2382       0.9141        0.2405        12.4169
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 03:44:49,193][0m Trial 28 finished with value: 0.23410637092921868 and parameters: {'lr': 0.005512298729874975, 'dropout': 0.6316133532377362, 'd_model_multiplier': 4, 'num_layers': 1, 'n_heads': 16, 'dim_feedforward': 435, 'batch_size': 241, 'pos_encoding': 'learnable', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.01, 'top_n_features': 145}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 180
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1997[0m   [32m0.6750[0m        [35m0.3345[0m       [31m0.9287[0m        [94m0.3291[0m     +  11.5181
      2   [36m0.2060[0m   [32m0.6911[0m        [35m0.2511[0m       0.9262        0.3408        11.8144
      3   [36m0.2136[0m   [32m0.6964[0m        [35m0.2466[0m       0.9214        0.3416        11.8946
      4   0.2041   0.6927        [35m0.2408[0m       0.9214        0.3533        12.2367
      5   0.1756   0.6518        0.2424       0.9274        [94m0.3230[0m     +  12.3814
      6   0.1772   0.6570        0.2417       0.9262        0.3262        12.7529
      7   0.1940   0.6915        [35m0.2373[0m       0.9202        0.4282        12.4867
      8   0.1819   0.6787        0.2399       0.9250        0.3861        12.4814
      9   0.1552   0.6278        [35m0.2361[0m       0.9250        0.3519        12.4323
     10   0.1841   0.6720        0.2372       0.9238        0.3617        12.5880
     11   0.1569   0.6519        0.2368       0.9226        0.3491        12.0424
     12   0.1396   0.5819        [35m0.2359[0m       0.9274        0.3434        12.4879
     13   0.1259   0.5659        0.2417       0.9262        0.3632        12.9426
     14   0.1472   0.5760        0.2437       0.9045        0.4330        11.9414
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 03:47:53,306][0m Trial 29 finished with value: 0.3229550559979108 and parameters: {'lr': 0.0014017585834250946, 'dropout': 0.5385033503757009, 'd_model_multiplier': 16, 'num_layers': 1, 'n_heads': 16, 'dim_feedforward': 353, 'batch_size': 194, 'pos_encoding': 'learnable', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.001, 'top_n_features': 180}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 115
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2048[0m   [32m0.7402[0m        [35m0.3117[0m       [31m0.9093[0m        [94m0.4246[0m     +  29.2607
      2   [36m0.2388[0m   [32m0.7512[0m        [35m0.2534[0m       [31m0.9117[0m        [94m0.3297[0m     +  29.6622
      3   [36m0.2396[0m   0.7489        [35m0.2502[0m       [31m0.9166[0m        0.3542        30.0211
      4   0.2299   0.7447        [35m0.2477[0m       0.9166        0.3615        29.7619
      5   0.2186   0.7208        0.2566       0.9105        0.3977        29.8256
      6   0.1713   0.7013        0.2620       [31m0.9190[0m        0.3381        29.8959
      7   0.2078   0.7222        0.2742       [31m0.9202[0m        [94m0.2580[0m     +  29.7500
      8   0.0545   0.3420        0.2676       0.3954        1.1220        29.6332
      9   [36m0.2462[0m   [32m0.7799[0m        0.2537       0.9081        [94m0.2483[0m     +  29.6313
     10   0.2432   0.7667        0.2550       0.9190        [94m0.2390[0m     +  29.8105
     11   [36m0.2547[0m   [32m0.7908[0m        0.2560       [31m0.9214[0m        [94m0.2349[0m     +  30.1212
     12   0.2481   0.7655        0.2812       0.8924        0.2777        29.7957
     13   0.2531   0.7651        0.3098       0.8888        0.2862        29.5608
     14   0.2297   0.7503        0.3257       0.8900        0.2937        30.0628
     15   0.2139   0.7404        0.3425       0.9033        0.2639        29.8355
     16   0.2058   0.7402        0.3623       0.8936        0.2833        29.6071
     17   0.2011   0.6876        0.4195       0.8791        0.3289        29.5139
     18   0.1739   0.6242        0.4069       [31m0.9250[0m        0.2732        29.5303
     19   0.1831   0.6867        0.4014       0.8924        0.2970        30.0314
     20   0.1966   0.6931        0.4211       0.9202        0.2569        29.5599
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 03:58:20,957][0m Trial 30 finished with value: 0.23490162386571337 and parameters: {'lr': 0.0005301343531487531, 'dropout': 0.5997027049272217, 'd_model_multiplier': 64, 'num_layers': 3, 'n_heads': 16, 'dim_feedforward': 401, 'batch_size': 72, 'pos_encoding': 'learnable', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.01, 'top_n_features': 115}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 187
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1978[0m   [32m0.7154[0m        [35m0.3110[0m       [31m0.9190[0m        [94m0.2934[0m     +  15.0919
      2   [36m0.2125[0m   [32m0.7297[0m        [35m0.2573[0m       0.9141        0.3007        14.6050
      3   [36m0.2538[0m   0.7194        [35m0.2463[0m       [31m0.9214[0m        [94m0.2586[0m     +  15.2934
      4   [36m0.2569[0m   [32m0.7399[0m        [35m0.2463[0m       0.9178        0.2786        15.6563
      5   0.2545   [32m0.7507[0m        0.2485       0.9141        0.2991        15.3015
      6   0.2408   0.7485        [35m0.2462[0m       0.9081        0.3128        15.5379
      7   [36m0.2668[0m   0.7297        0.2488       0.9214        [94m0.2584[0m     +  15.7191
      8   0.2645   0.7302        [35m0.2413[0m       [31m0.9226[0m        [94m0.2502[0m     +  15.5721
      9   [36m0.2711[0m   0.7340        0.2432       0.9214        [94m0.2461[0m     +  15.6268
     10   [36m0.2744[0m   0.7486        [35m0.2374[0m       0.9202        0.2504        16.0601
     11   0.2608   0.7438        [35m0.2364[0m       0.9154        0.2506        15.6448
     12   0.2351   [32m0.7585[0m        0.2371       0.8888        0.3092        15.8144
     13   0.2558   0.7550        [35m0.2346[0m       0.9129        0.2565        15.9797
     14   0.2728   0.7483        [35m0.2287[0m       0.9202        0.2477        15.8957
     15   0.2583   0.7535        [35m0.2282[0m       0.9178        0.2498        15.4176
     16   [36m0.2752[0m   0.7553        0.2320       0.9190        0.2468        15.7437
     17   0.2693   [32m0.7614[0m        [35m0.2259[0m       0.9190        0.2465        15.7781
     18   [36m0.2826[0m   0.7569        [35m0.2241[0m       0.9190        0.2481        15.6830
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 04:03:17,655][0m Trial 31 finished with value: 0.24605336507834144 and parameters: {'lr': 0.0074130575915819065, 'dropout': 0.4808325438282488, 'd_model_multiplier': 2, 'num_layers': 3, 'n_heads': 16, 'dim_feedforward': 253, 'batch_size': 221, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.001, 'top_n_features': 187}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 205
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2124[0m   [32m0.7276[0m        [35m0.3162[0m       [31m0.9214[0m        [94m0.3118[0m     +  12.6143
      2   [36m0.2555[0m   [32m0.7285[0m        [35m0.2623[0m       [31m0.9238[0m        0.3302        13.1361
      3   [36m0.2719[0m   [32m0.7652[0m        [35m0.2603[0m       0.9214        0.3432        11.8291
      4   [36m0.2881[0m   [32m0.7861[0m        [35m0.2575[0m       0.9166        0.3484        12.6365
      5   [36m0.2926[0m   [32m0.7938[0m        [35m0.2565[0m       0.9166        0.3297        12.5090
      6   [36m0.2997[0m   0.7793        [35m0.2549[0m       0.9154        [94m0.3101[0m     +  13.0988
      7   [36m0.3261[0m   0.7678        [35m0.2522[0m       0.9238        0.3203        12.5631
      8   0.3244   0.7736        0.2577       0.9178        0.3252        13.1449
      9   0.3075   0.7526        0.2534       0.9226        [94m0.3053[0m     +  12.9847
     10   0.3032   0.7798        [35m0.2510[0m       0.9117        0.3222        13.2227
     11   0.2813   0.7433        [35m0.2503[0m       0.9214        [94m0.2724[0m     +  13.2591
     12   [36m0.3330[0m   0.7583        [35m0.2480[0m       [31m0.9250[0m        [94m0.2483[0m     +  12.8529
     13   0.3328   [32m0.7993[0m        [35m0.2436[0m       0.9190        0.2670        13.2923
     14   0.3215   0.7897        0.2473       0.9226        [94m0.2393[0m     +  12.6939
     15   [36m0.3390[0m   0.7612        [35m0.2433[0m       0.9226        [94m0.2341[0m     +  12.9895
     16   [36m0.3606[0m   0.7876        0.2492       0.9214        [94m0.2299[0m     +  12.9043
     17   0.3605   [32m0.8043[0m        0.2511       0.9202        [94m0.2276[0m     +  13.0519
     18   0.3507   0.7974        0.2511       0.9238        [94m0.2239[0m     +  12.9392
     19   0.3497   0.7948        0.2456       0.9226        [94m0.2233[0m     +  12.9481
     20   0.3493   0.8005        0.2460       0.9190        [94m0.2219[0m     +  13.5395
     21   0.3487   0.7840        0.2484       0.9190        0.2255        12.8984
     22   [36m0.3626[0m   0.7911        0.2472       0.9214        0.2235        13.1502
     23   0.3262   0.7997        0.2451       0.9129        0.2364        12.7431
     24   0.3429   0.7921        0.2464       0.9190        0.2248        12.8540
     25   0.3509   0.7926        0.2434       0.9190        0.2250        12.8116
     26   0.3504   0.8018        0.2458       0.9190        0.2262        12.7896
     27   0.3595   0.7800        [35m0.2407[0m       0.9250        0.2266        12.9114
     28   0.3403   0.8025        0.2435       0.9166        0.2219        12.6503
     29   0.3493   0.7870        0.2439       0.9238        0.2252        12.6098
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 04:09:44,472][0m Trial 32 finished with value: 0.22186393590763373 and parameters: {'lr': 0.005621324583450517, 'dropout': 0.5533402793785837, 'd_model_multiplier': 2, 'num_layers': 1, 'n_heads': 16, 'dim_feedforward': 289, 'batch_size': 256, 'pos_encoding': 'learnable', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.001, 'top_n_features': 205}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 209
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0812[0m   [32m0.4515[0m        [35m0.5241[0m       [31m0.9287[0m        [94m0.5092[0m     +  11.7133
      2   [36m0.1304[0m   [32m0.5825[0m        [35m0.3522[0m       0.9287        [94m0.3935[0m     +  12.5788
      3   [36m0.1596[0m   [32m0.6706[0m        [35m0.2871[0m       0.9287        [94m0.3497[0m     +  12.6278
      4   [36m0.1639[0m   [32m0.6953[0m        [35m0.2720[0m       0.9287        [94m0.3412[0m     +  12.7900
      5   [36m0.1783[0m   [32m0.7095[0m        [35m0.2623[0m       0.9287        [94m0.3293[0m     +  12.8238
      6   [36m0.1818[0m   [32m0.7183[0m        0.2631       0.9287        [94m0.3280[0m     +  12.5476
      7   [36m0.1847[0m   [32m0.7218[0m        [35m0.2573[0m       0.9287        [94m0.3250[0m     +  12.9428
      8   0.1824   [32m0.7224[0m        [35m0.2566[0m       0.9287        [94m0.3197[0m     +  13.0682
      9   [36m0.1936[0m   0.7224        [35m0.2551[0m       0.9274        0.3230        13.1304
     10   [36m0.1953[0m   [32m0.7242[0m        [35m0.2500[0m       0.9287        [94m0.3167[0m     +  13.2304
     11   0.1939   0.7242        0.2535       [31m0.9299[0m        0.3177        12.5643
     12   [36m0.1976[0m   0.7226        0.2512       0.9287        0.3177        12.1907
     13   [36m0.2011[0m   0.7207        0.2505       0.9287        0.3181        12.6088
     14   0.1989   0.7158        [35m0.2497[0m       0.9274        [94m0.3084[0m     +  13.0753
     15   [36m0.2092[0m   0.7112        0.2500       0.9274        0.3103        12.0746
     16   0.2070   0.7036        [35m0.2478[0m       0.9274        [94m0.3022[0m     +  12.7088
     17   0.2009   0.7064        [35m0.2469[0m       0.9274        [94m0.2998[0m     +  12.2846
     18   0.1959   0.7016        [35m0.2453[0m       0.9238        0.3054        13.0673
     19   0.2034   0.6911        [35m0.2453[0m       0.9274        [94m0.2992[0m     +  13.0466
     20   0.2044   0.6860        [35m0.2430[0m       0.9274        0.2997        12.8312
     21   0.1931   0.6915        0.2448       0.9262        0.3014        12.4760
     22   0.1913   0.6862        0.2439       0.9262        0.3037        12.6227
     23   0.1987   0.6852        [35m0.2416[0m       0.9226        0.3171        12.7850
     24   0.1972   0.6884        [35m0.2385[0m       0.9226        0.3081        12.4644
     25   0.1837   0.6816        [35m0.2367[0m       0.9238        0.3064        13.3347
     26   0.1879   0.6629        [35m0.2361[0m       0.9274        [94m0.2924[0m     +  12.4691
     27   0.1896   0.6689        0.2368       0.9238        0.2947        12.5088
     28   0.1913   0.6662        0.2384       0.9226        0.3027        12.7009
     29   0.1966   0.6808        0.2372       0.9226        0.3031        12.7181
     30   0.1958   0.6622        [35m0.2355[0m       0.9262        0.2954        12.9339
     31   0.2022   0.6627        [35m0.2352[0m       0.9250        0.2972        12.7593
     32   0.1973   0.6656        [35m0.2330[0m       0.9226        0.3012        13.0089
     33   0.2060   0.6715        0.2343       0.9238        0.2988        12.5043
     34   0.2038   0.6695        0.2343       0.9226        [94m0.2831[0m     +  12.8272
     35   0.2063   0.6759        0.2349       0.9238        [94m0.2806[0m     +  12.4912
     36   [36m0.2119[0m   0.6699        [35m0.2305[0m       0.9226        0.2823        12.4137
     37   [36m0.2215[0m   0.6738        0.2333       0.9226        0.2824        12.3853
     38   0.2079   0.6677        0.2320       0.9226        0.2936        12.3429
     39   0.2077   0.6695        0.2318       0.9226        0.2847        12.2997
     40   0.2020   0.6804        [35m0.2297[0m       0.9250        0.2935        12.7403
     41   0.2067   0.6769        0.2300       0.9238        0.2870        12.4699
     42   0.2011   0.6662        0.2334       0.9250        0.2905        12.5713
     43   0.2146   0.6648        0.2301       0.9250        [94m0.2754[0m     +  12.4064
     44   0.2120   0.6641        0.2315       0.9262        0.2922        12.7818
     45   0.2075   0.6719        [35m0.2280[0m       0.9250        0.2870        12.7849
     46   0.2116   0.6599        [35m0.2272[0m       0.9250        0.2806        12.7220
     47   0.2152   0.6669        0.2311       0.9238        0.2812        13.2959
     48   0.2115   0.6558        [35m0.2267[0m       0.9250        0.2780        12.6583
     49   0.2137   0.6660        [35m0.2260[0m       0.9226        0.2817        13.0179
     50   0.2101   0.6606        0.2270       0.9238        0.2909        13.2049
[32m[I 2023-05-02 04:20:19,835][0m Trial 33 finished with value: 0.2753639612549594 and parameters: {'lr': 0.0005659612070197857, 'dropout': 0.5518594629815186, 'd_model_multiplier': 2, 'num_layers': 1, 'n_heads': 16, 'dim_feedforward': 289, 'batch_size': 250, 'pos_encoding': 'learnable', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.001, 'top_n_features': 209}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 169
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1879[0m   [32m0.7134[0m        [35m0.3030[0m       [31m0.9190[0m        [94m0.5211[0m     +  12.8242
      2   [36m0.2211[0m   [32m0.7541[0m        [35m0.2814[0m       0.9154        [94m0.4384[0m     +  12.4462
      3   [36m0.2251[0m   [32m0.7797[0m        [35m0.2741[0m       0.5780        0.7227        12.8472
      4   [36m0.2329[0m   [32m0.7810[0m        0.2762       0.9166        [94m0.2713[0m     +  12.7465
      5   0.1946   0.7512        [35m0.2718[0m       0.9190        0.3134        12.4595
      6   0.2228   0.7751        [35m0.2715[0m       0.9057        0.3450        12.6445
      7   0.2228   0.7772        [35m0.2657[0m       0.9045        [94m0.2693[0m     +  12.6556
      8   0.2221   0.7714        0.2670       0.9190        [94m0.2365[0m     +  12.6816
      9   0.2025   0.7762        [35m0.2559[0m       0.9154        0.2478        12.8728
     10   0.2087   0.7700        0.2594       0.9105        0.2509        12.6611
     11   0.2141   [32m0.7829[0m        0.2611       0.8138        0.4922        12.7782
     12   0.2137   [32m0.7868[0m        0.2652       0.9021        0.3390        12.6762
     13   0.2284   [32m0.7893[0m        0.2590       0.8742        0.4205        12.8197
     14   0.2098   0.7754        0.2649       0.9105        0.2894        12.4844
     15   0.2115   0.7767        [35m0.2518[0m       0.9093        0.3147        12.5921
     16   [36m0.2361[0m   0.7794        0.2542       [31m0.9202[0m        0.2805        12.5297
     17   0.2084   0.7722        0.2533       0.9129        0.2754        12.5989
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 04:24:08,217][0m Trial 34 finished with value: 0.23649444840983583 and parameters: {'lr': 0.0031973536451836196, 'dropout': 0.639587354494216, 'd_model_multiplier': 2, 'num_layers': 2, 'n_heads': 32, 'dim_feedforward': 482, 'batch_size': 37, 'pos_encoding': 'learnable', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.001, 'top_n_features': 169}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 223
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2083[0m   [32m0.7001[0m        [35m0.3356[0m       [31m0.9166[0m        [94m0.3417[0m     +  34.8526
      2   [36m0.2287[0m   [32m0.7363[0m        [35m0.2511[0m       0.9057        0.3761        34.6948
      3   [36m0.2289[0m   0.7331        [35m0.2454[0m       0.8996        0.3959        34.7706
      4   [36m0.2443[0m   [32m0.7379[0m        [35m0.2424[0m       0.8815        0.4470        34.8454
      5   [36m0.2464[0m   [32m0.7443[0m        [35m0.2382[0m       0.8489        0.4973        34.6571
      6   [36m0.2477[0m   0.7327        0.2387       0.8138        0.5278        35.1672
      7   0.2459   0.7283        0.2418       0.7811        0.5536        34.6145
      8   [36m0.2547[0m   0.7352        [35m0.2381[0m       0.7594        0.5712        35.5324
      9   0.2520   0.7342        [35m0.2378[0m       0.7170        0.6204        35.2510
     10   0.2534   0.7180        [35m0.2352[0m       0.7751        0.5566        34.4886
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 04:30:33,444][0m Trial 35 finished with value: 0.34167783656028045 and parameters: {'lr': 0.00031485595070626215, 'dropout': 0.5091223745017656, 'd_model_multiplier': 32, 'num_layers': 1, 'n_heads': 64, 'dim_feedforward': 299, 'batch_size': 139, 'pos_encoding': 'learnable', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.001, 'top_n_features': 223}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 135
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0560[0m   [32m0.3110[0m        [35m0.6488[0m       [31m0.9033[0m        [94m0.6545[0m     +  13.6172
      2   0.0556   0.3087        [35m0.4784[0m       [31m0.9178[0m        [94m0.4853[0m     +  14.8902
      3   [36m0.1675[0m   [32m0.6273[0m        [35m0.3176[0m       0.9166        [94m0.3497[0m     +  14.6729
      4   [36m0.2434[0m   [32m0.6903[0m        [35m0.2770[0m       0.9154        [94m0.3124[0m     +  13.9112
      5   [36m0.2786[0m   [32m0.7091[0m        [35m0.2695[0m       0.9166        [94m0.2912[0m     +  13.6247
      6   [36m0.2968[0m   [32m0.7182[0m        [35m0.2631[0m       [31m0.9190[0m        [94m0.2789[0m     +  14.3340
      7   [36m0.3189[0m   [32m0.7320[0m        [35m0.2595[0m       [31m0.9214[0m        0.2815        14.0882
      8   [36m0.3304[0m   0.7319        [35m0.2593[0m       0.9202        [94m0.2598[0m     +  14.3883
      9   [36m0.3360[0m   0.7296        [35m0.2520[0m       0.9214        [94m0.2510[0m     +  14.1640
     10   [36m0.3477[0m   0.7309        [35m0.2511[0m       0.9202        [94m0.2508[0m     +  14.0932
     11   [36m0.3563[0m   [32m0.7401[0m        [35m0.2477[0m       0.9214        0.2512        14.1666
     12   [36m0.3607[0m   0.7383        0.2492       0.9202        [94m0.2479[0m     +  14.2416
     13   0.3588   [32m0.7439[0m        [35m0.2470[0m       [31m0.9238[0m        0.2584        14.6353
     14   0.3599   [32m0.7440[0m        0.2476       0.9238        0.2519        14.2726
     15   [36m0.3706[0m   [32m0.7499[0m        [35m0.2412[0m       0.9238        0.2500        14.0101
     16   [36m0.3747[0m   [32m0.7540[0m        [35m0.2404[0m       0.9202        [94m0.2403[0m     +  14.0751
     17   0.3701   0.7530        0.2426       0.9214        0.2415        14.2325
     18   [36m0.3779[0m   [32m0.7588[0m        [35m0.2334[0m       0.9214        [94m0.2397[0m     +  14.0983
     19   0.3707   [32m0.7596[0m        0.2343       0.9202        [94m0.2386[0m     +  14.2289
     20   0.3525   0.7588        [35m0.2334[0m       0.9226        0.2413        13.8573
     21   0.3474   0.7591        0.2349       0.9190        0.2408        14.1145
     22   0.3408   [32m0.7640[0m        [35m0.2330[0m       0.9190        0.2428        14.2500
     23   0.3507   [32m0.7700[0m        [35m0.2325[0m       0.9214        0.2393        14.3822
     24   0.3486   [32m0.7721[0m        0.2326       0.9226        0.2572        14.4681
     25   0.3478   0.7674        [35m0.2287[0m       0.9202        0.2432        13.8992
     26   0.3534   0.7638        0.2301       0.9226        0.2425        14.2101
     27   0.3405   0.7639        0.2307       0.9202        0.2431        14.3155
     28   0.3370   0.7655        [35m0.2282[0m       0.9190        0.2390        14.2044
     29   0.3544   [32m0.7724[0m        0.2317       0.9178        [94m0.2374[0m     +  14.2578
     30   0.3405   0.7691        [35m0.2268[0m       0.9202        0.2425        14.2479
     31   0.3359   0.7706        0.2269       0.9214        0.2381        14.5138
     32   0.3398   0.7663        [35m0.2228[0m       0.9202        0.2414        14.2870
     33   0.3550   0.7723        0.2269       0.9202        0.2417        14.3028
     34   0.3533   0.7710        0.2281       0.9166        0.2533        14.1495
     35   0.3489   0.7690        [35m0.2225[0m       0.9190        0.2411        14.1814
     36   0.3461   0.7646        0.2234       0.9178        0.2546        14.2103
     37   0.3292   0.7682        0.2259       0.9166        0.2468        14.5525
     38   0.3444   0.7666        [35m0.2214[0m       0.9202        0.2427        14.5713
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 04:39:49,344][0m Trial 36 finished with value: 0.23743451627234882 and parameters: {'lr': 0.0013300532935259638, 'dropout': 0.47658677505708585, 'd_model_multiplier': 1, 'num_layers': 5, 'n_heads': 8, 'dim_feedforward': 343, 'batch_size': 231, 'pos_encoding': 'learnable', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.01, 'top_n_features': 135}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 151
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2170[0m   [32m0.6840[0m        [35m0.3038[0m       [31m0.9154[0m        [94m0.2729[0m     +  71.3481
      2   [36m0.2257[0m   [32m0.6940[0m        [35m0.2733[0m       0.9154        0.2900        71.4014
      3   [36m0.2520[0m   [32m0.7056[0m        [35m0.2671[0m       0.9141        [94m0.2615[0m     +  71.5440
      4   [36m0.3009[0m   [32m0.7733[0m        [35m0.2654[0m       0.9021        0.2859        71.3635
      5   0.3004   0.7467        [35m0.2499[0m       0.9141        0.2680        71.4072
      6   0.2729   0.7439        0.2530       0.9057        0.2840        71.5575
      7   0.2879   0.7598        [35m0.2498[0m       0.8936        0.3023        71.5628
      8   0.2729   0.7449        [35m0.2478[0m       0.8900        0.3103        71.5778
      9   0.2687   0.7517        [35m0.2448[0m       0.8815        0.3594        71.4053
     10   0.2556   0.7371        [35m0.2438[0m       0.8888        0.3284        71.9313
     11   0.2640   0.7544        [35m0.2400[0m       0.9045        0.2792        71.5644
     12   0.2946   0.7564        0.2407       0.8984        0.2923        71.3764
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 04:55:20,237][0m Trial 37 finished with value: 0.2615431935820355 and parameters: {'lr': 7.45275061914794e-05, 'dropout': 0.5739702584078199, 'd_model_multiplier': 8, 'num_layers': 15, 'n_heads': 32, 'dim_feedforward': 487, 'batch_size': 27, 'pos_encoding': 'learnable', 'activation': 'relu', 'norm': 'BatchNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 151}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 167
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2759[0m   [32m0.7478[0m        [35m0.2885[0m       [31m0.9238[0m        [94m0.2849[0m     +  13.1989
      2   0.2497   0.7274        [35m0.2455[0m       [31m0.9250[0m        0.2893        14.9937
      3   [36m0.2997[0m   [32m0.7809[0m        0.2524       0.9226        [94m0.2711[0m     +  14.2688
      4   [36m0.3077[0m   0.7531        0.2564       0.9226        0.2856        14.3262
      5   [36m0.3254[0m   [32m0.8341[0m        0.2661       0.9202        [94m0.2359[0m     +  13.9610
      6   0.3094   0.7899        0.2835       0.9238        0.2454        14.3386
      7   0.2403   0.7086        0.2649       0.9226        0.3187        14.6278
      8   0.2972   0.7950        0.2506       0.9226        0.2778        14.5649
      9   0.2942   0.7947        [35m0.2449[0m       0.9226        0.2797        14.3224
     10   0.3075   0.8124        [35m0.2440[0m       0.9226        0.2634        14.0886
     11   0.3204   0.8155        0.2472       0.9250        0.2594        14.8474
     12   0.3108   0.8134        0.2505       0.9250        0.2412        14.1985
     13   [36m0.3389[0m   0.8237        [35m0.2422[0m       0.9226        [94m0.2335[0m     +  14.5170
     14   0.3375   0.8272        0.2439       0.9214        0.2360        14.4690
     15   [36m0.3453[0m   0.8250        0.2429       0.9238        0.2364        14.6163
     16   [36m0.3474[0m   0.8246        [35m0.2415[0m       [31m0.9274[0m        0.2423        14.3600
     17   [36m0.3557[0m   0.8292        [35m0.2387[0m       0.9226        0.2560        14.4611
     18   0.3474   0.8320        0.2402       0.9226        0.2339        14.3455
     19   0.3516   [32m0.8345[0m        [35m0.2383[0m       0.9190        0.2416        14.3696
     20   0.3357   0.8240        0.2434       0.9226        0.2752        14.4603
     21   0.3477   0.8239        0.2392       0.9226        0.2715        14.4858
     22   0.3104   0.8230        0.2435       0.9166        0.2359        14.3226
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 05:00:51,486][0m Trial 38 finished with value: 0.2335057026529831 and parameters: {'lr': 0.004520999328259488, 'dropout': 0.5234681539408138, 'd_model_multiplier': 16, 'num_layers': 2, 'n_heads': 16, 'dim_feedforward': 388, 'batch_size': 201, 'pos_encoding': 'learnable', 'activation': 'gelu', 'norm': 'LayerNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.001, 'top_n_features': 167}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 193
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.0646[0m   [32m0.4101[0m        [35m0.7115[0m       [31m0.8428[0m        [94m0.6867[0m     +  9.3437
      2   [36m0.0668[0m   [32m0.4187[0m        [35m0.6829[0m       [31m0.9202[0m        [94m0.6715[0m     +  9.4785
      3   [36m0.0717[0m   [32m0.4427[0m        [35m0.6394[0m       0.9202        [94m0.6519[0m     +  10.0938
      4   0.0705   0.4416        [35m0.5936[0m       0.9202        [94m0.6303[0m     +  10.2299
      5   [36m0.0730[0m   [32m0.4549[0m        [35m0.5314[0m       0.9202        [94m0.6075[0m     +  10.3332
      6   [36m0.0777[0m   [32m0.4761[0m        [35m0.4663[0m       0.9202        [94m0.5892[0m     +  10.1976
      7   [36m0.1031[0m   [32m0.5099[0m        [35m0.4109[0m       0.9202        [94m0.5491[0m     +  10.5419
      8   [36m0.1081[0m   [32m0.5347[0m        [35m0.3736[0m       0.9202        [94m0.5165[0m     +  10.3907
      9   [36m0.1215[0m   [32m0.5554[0m        [35m0.3400[0m       0.9202        [94m0.4683[0m     +  10.2372
     10   [36m0.1292[0m   [32m0.5870[0m        [35m0.3208[0m       0.9202        [94m0.4280[0m     +  10.3683
     11   [36m0.1423[0m   [32m0.6286[0m        [35m0.3062[0m       0.9202        [94m0.3891[0m     +  10.4761
     12   [36m0.1560[0m   [32m0.6577[0m        [35m0.2974[0m       0.9202        [94m0.3627[0m     +  10.4194
     13   [36m0.1649[0m   [32m0.6808[0m        [35m0.2876[0m       0.9202        [94m0.3499[0m     +  10.5606
     14   [36m0.1704[0m   [32m0.6934[0m        [35m0.2832[0m       0.9202        [94m0.3400[0m     +  10.4472
     15   [36m0.1791[0m   [32m0.7112[0m        0.2837       0.9202        [94m0.3246[0m     +  10.3025
     16   [36m0.1874[0m   [32m0.7224[0m        [35m0.2789[0m       0.9202        [94m0.3159[0m     +  10.4091
     17   [36m0.1911[0m   [32m0.7256[0m        [35m0.2716[0m       0.9202        [94m0.3075[0m     +  10.4475
     18   [36m0.2016[0m   [32m0.7342[0m        0.2734       0.9202        [94m0.2990[0m     +  10.4221
     19   [36m0.2085[0m   [32m0.7412[0m        [35m0.2706[0m       0.9202        [94m0.2961[0m     +  10.5197
     20   [36m0.2102[0m   [32m0.7440[0m        0.2720       0.9202        [94m0.2883[0m     +  10.5313
     21   [36m0.2153[0m   [32m0.7479[0m        [35m0.2667[0m       0.9202        0.2916        10.3554
     22   [36m0.2172[0m   [32m0.7486[0m        0.2670       0.9202        0.2893        10.3903
     23   [36m0.2178[0m   [32m0.7503[0m        0.2703       0.9202        [94m0.2773[0m     +  10.5930
     24   [36m0.2181[0m   [32m0.7507[0m        0.2670       0.9202        0.2817        10.5462
     25   0.2101   0.7443        [35m0.2631[0m       0.9202        0.2828        10.7640
     26   0.2137   0.7471        0.2635       0.9202        0.2813        10.6331
     27   0.2148   0.7472        0.2689       0.9202        0.2832        10.4152
     28   0.2147   0.7469        0.2637       0.9202        0.2793        10.4136
     29   0.2116   0.7363        0.2632       0.9202        0.2820        10.4574
     30   [36m0.2193[0m   0.7356        [35m0.2624[0m       0.9202        0.2830        10.7110
     31   0.2171   0.7343        [35m0.2618[0m       0.9202        0.2864        10.9892
     32   [36m0.2219[0m   0.7360        [35m0.2577[0m       0.9202        0.2813        10.3716
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 05:06:35,560][0m Trial 39 finished with value: 0.27734491820058627 and parameters: {'lr': 0.0002219552691611951, 'dropout': 0.6075528207237512, 'd_model_multiplier': 1, 'num_layers': 4, 'n_heads': 4, 'dim_feedforward': 335, 'batch_size': 112, 'pos_encoding': 'learnable', 'activation': 'relu', 'norm': 'BatchNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.01, 'top_n_features': 193}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 106
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0592[0m   [32m0.3388[0m        [35m0.7381[0m       [31m0.9238[0m        [94m0.3508[0m     +  20.5649
      2   [36m0.1017[0m   [32m0.5171[0m        [35m0.2749[0m       0.9238        [94m0.3228[0m     +  20.9443
      3   [36m0.1866[0m   [32m0.7277[0m        [35m0.2711[0m       0.9238        [94m0.2478[0m     +  20.6974
      4   [36m0.2057[0m   [32m0.7376[0m        [35m0.2645[0m       0.9093        0.2493        20.8589
      5   [36m0.2375[0m   0.7273        0.2657       0.9238        [94m0.2476[0m     +  20.9774
      6   0.2159   0.7314        [35m0.2612[0m       0.9238        0.2512        21.0375
      7   0.2006   0.7323        [35m0.2593[0m       0.9238        [94m0.2469[0m     +  21.4110
      8   [36m0.2408[0m   0.7354        [35m0.2585[0m       0.9226        [94m0.2399[0m     +  21.0434
      9   0.2279   0.7368        0.2586       [31m0.9250[0m        0.2426        21.1403
     10   0.2333   0.7278        0.2599       0.9238        0.2429        21.0196
     11   0.2251   0.7315        0.2597       0.9238        0.2453        20.8185
     12   [36m0.2458[0m   0.7338        0.2588       0.9238        0.2478        21.4673
     13   0.2160   0.7215        [35m0.2559[0m       0.9238        0.2454        21.0414
     14   0.2230   0.7314        0.2579       0.9238        0.2450        21.5365
     15   0.2163   0.7287        0.2583       0.9238        0.2438        21.2766
     16   0.2379   [32m0.7418[0m        0.2564       0.9238        [94m0.2381[0m     +  21.0276
     17   0.2359   0.7334        [35m0.2536[0m       0.9238        0.2438        21.3733
     18   [36m0.2768[0m   [32m0.7428[0m        0.2584       0.9238        [94m0.2369[0m     +  21.3649
     19   0.2021   0.7104        0.2586       0.9238        0.2484        21.2048
     20   0.1959   0.7334        0.2545       0.9238        0.2421        21.1323
     21   0.2015   0.7194        0.2547       0.9238        0.2462        20.9469
     22   0.2159   0.7323        0.2573       0.9238        0.2428        20.9851
     23   0.1930   0.7262        0.2559       0.9238        0.2496        21.0074
     24   0.1938   0.7066        0.2574       0.9238        0.2488        21.5773
     25   0.1779   0.6921        0.2566       0.9238        0.2514        20.9619
     26   0.1885   0.7089        0.2594       0.9238        0.2490        21.1695
     27   0.2297   0.7300        0.2560       0.9238        0.2416        21.1770
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 05:16:27,257][0m Trial 40 finished with value: 0.23692039428267542 and parameters: {'lr': 0.0353264312405006, 'dropout': 0.45462291794380383, 'd_model_multiplier': 2, 'num_layers': 7, 'n_heads': 16, 'dim_feedforward': 375, 'batch_size': 171, 'pos_encoding': 'learnable', 'activation': 'gelu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 106}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 232
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1629[0m   [32m0.6759[0m        [35m0.3129[0m       [31m0.9274[0m        [94m0.3276[0m     +  15.1711
      2   [36m0.2339[0m   [32m0.7275[0m        [35m0.2646[0m       0.9262        [94m0.3171[0m     +  16.0071
      3   [36m0.2368[0m   [32m0.7372[0m        [35m0.2638[0m       0.9238        [94m0.2973[0m     +  15.5978
      4   0.2319   0.7256        0.2702       0.9238        [94m0.2572[0m     +  15.6230
      5   0.2089   0.6907        [35m0.2577[0m       0.9250        [94m0.2545[0m     +  16.1658
      6   0.2351   [32m0.7484[0m        [35m0.2566[0m       0.9190        [94m0.2334[0m     +  15.7244
      7   0.2155   0.7176        0.2599       0.9274        0.2391        16.1907
      8   [36m0.2413[0m   [32m0.7646[0m        0.2584       0.9262        0.2432        15.9548
      9   [36m0.2675[0m   [32m0.7823[0m        [35m0.2560[0m       0.9274        [94m0.2308[0m     +  16.5114
     10   0.2421   [32m0.7827[0m        [35m0.2446[0m       0.8972        0.2582        16.2526
     11   0.2367   0.7783        0.2549       0.8827        0.2721        15.8764
     12   0.2516   [32m0.7844[0m        0.2495       0.9117        0.2455        15.2439
     13   0.2621   [32m0.7954[0m        0.2487       0.8815        0.2835        15.7829
     14   0.2289   0.7792        0.2501       0.9166        0.2394        15.8945
     15   0.2375   0.7885        0.2497       0.9141        0.2340        16.1242
     16   0.2275   0.7837        0.2451       0.7872        0.4340        15.7627
     17   0.2534   [32m0.8062[0m        [35m0.2442[0m       0.9081        0.2493        16.3727
     18   0.2478   0.7988        0.2483       0.9129        0.2400        15.6011
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 05:21:29,519][0m Trial 41 finished with value: 0.23083136867655232 and parameters: {'lr': 0.011448969054469637, 'dropout': 0.5638170287072277, 'd_model_multiplier': 2, 'num_layers': 3, 'n_heads': 16, 'dim_feedforward': 239, 'batch_size': 220, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.001, 'top_n_features': 232}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 216
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1646[0m   [32m0.6596[0m        [35m0.3009[0m       [31m0.9190[0m        [94m0.2697[0m     +  11.9568
      2   [36m0.2976[0m   [32m0.7651[0m        [35m0.2524[0m       0.9166        0.2973        12.3711
      3   0.1445   0.5500        0.2604       0.9190        0.3660        12.6834
      4   0.2442   0.7207        0.2626       0.9178        [94m0.2592[0m     +  12.8891
      5   0.2330   0.7101        0.2613       0.9190        0.2645        12.6779
      6   0.2848   [32m0.7654[0m        0.2616       [31m0.9202[0m        [94m0.2500[0m     +  12.4707
      7   [36m0.3077[0m   [32m0.7706[0m        0.2527       0.9202        [94m0.2443[0m     +  12.6334
      8   0.2923   0.7449        0.2534       [31m0.9214[0m        0.2478        12.6662
      9   0.2882   0.7689        [35m0.2504[0m       0.9202        [94m0.2429[0m     +  12.2615
     10   0.2890   [32m0.7737[0m        [35m0.2472[0m       0.9117        [94m0.2410[0m     +  12.6815
     11   0.2842   [32m0.7818[0m        0.2475       0.9069        0.2796        12.5883
     12   0.2802   0.7622        0.2492       0.9178        0.2457        13.2955
     13   [36m0.3143[0m   [32m0.7925[0m        [35m0.2429[0m       0.9178        [94m0.2345[0m     +  13.2833
     14   0.3035   0.7894        0.2430       0.9202        [94m0.2342[0m     +  12.8911
     15   [36m0.3175[0m   0.7902        0.2461       0.9141        0.2402        12.9148
     16   [36m0.3213[0m   0.7852        [35m0.2428[0m       0.9190        0.2358        12.9884
     17   [36m0.3306[0m   [32m0.7947[0m        [35m0.2417[0m       0.9129        0.2367        12.7157
     18   0.3102   0.7875        0.2435       0.9154        0.2458        12.8300
     19   0.3031   0.7836        0.2421       0.9141        0.2364        12.7623
     20   0.3054   0.7901        [35m0.2400[0m       0.9178        0.2393        12.7639
     21   0.3134   0.7935        0.2413       0.9117        0.2426        12.8831
     22   0.3031   0.7923        0.2429       0.9154        0.2348        12.9419
     23   0.3050   [32m0.7973[0m        0.2425       0.9141        [94m0.2340[0m     +  13.0130
     24   0.3136   [32m0.8012[0m        [35m0.2399[0m       0.9105        0.2396        12.7806
     25   0.3166   [32m0.8051[0m        0.2426       0.9166        0.2362        12.6602
     26   0.2937   0.7927        0.2418       0.9154        0.2382        13.2205
     27   0.3128   0.7915        [35m0.2393[0m       0.9190        0.2387        12.9066
     28   0.3107   0.7906        0.2396       0.9154        0.2454        12.7273
     29   0.3141   0.7919        [35m0.2386[0m       0.9117        0.2367        13.2918
     30   0.3145   0.7888        0.2406       0.9141        0.2542        12.8281
     31   0.3256   0.7833        [35m0.2363[0m       0.9141        0.2432        13.1560
     32   [36m0.3336[0m   0.7924        [35m0.2360[0m       0.9178        0.2346        13.3685
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 05:28:32,897][0m Trial 42 finished with value: 0.23403713826040548 and parameters: {'lr': 0.022840697163858666, 'dropout': 0.41995972214396793, 'd_model_multiplier': 2, 'num_layers': 1, 'n_heads': 16, 'dim_feedforward': 216, 'batch_size': 236, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.001, 'top_n_features': 216}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 238
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
Warning, assumed runtime error: CUDA out of memory. Tried to allocate 6.06 GiB (GPU 0; 23.70 GiB total capacity; 14.55 GiB already allocated; 4.04 GiB free; 18.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[32m[I 2023-05-02 05:28:39,953][0m Trial 43 finished with value: 100.0 and parameters: {'lr': 0.005876432316600084, 'dropout': 0.5241791757416588, 'd_model_multiplier': 2, 'num_layers': 3, 'n_heads': 64, 'dim_feedforward': 273, 'batch_size': 256, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.001, 'top_n_features': 238}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 204
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2563[0m   [32m0.7632[0m        [35m0.3480[0m       [31m0.9323[0m        [94m0.2733[0m     +  11.3347
      2   [36m0.2895[0m   [32m0.7647[0m        [35m0.2608[0m       [31m0.9335[0m        [94m0.2693[0m     +  11.6189
      3   [36m0.2897[0m   0.7619        [35m0.2535[0m       0.9323        0.2888        12.4049
      4   0.2737   0.7601        [35m0.2475[0m       0.9335        0.2802        12.0660
      5   0.2747   0.7594        [35m0.2443[0m       [31m0.9347[0m        0.3099        12.3249
      6   0.2623   0.7561        0.2451       0.9299        0.3123        11.8138
      7   0.2469   0.7265        [35m0.2402[0m       0.9323        0.2814        11.7059
      8   0.2382   0.7260        0.2434       0.9323        0.3340        12.2749
      9   0.2347   0.7260        [35m0.2381[0m       0.9166        0.3486        12.3949
     10   0.2779   0.7400        0.2389       0.9238        0.3253        12.5197
     11   [36m0.3077[0m   0.7562        [35m0.2370[0m       0.9311        0.3110        11.8704
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 05:31:04,458][0m Trial 44 finished with value: 0.2692508073715085 and parameters: {'lr': 0.0015622491107921048, 'dropout': 0.48190631367216336, 'd_model_multiplier': 4, 'num_layers': 2, 'n_heads': 16, 'dim_feedforward': 197, 'batch_size': 150, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.001, 'top_n_features': 204}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 176
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
Warning, assumed runtime error: CUDA out of memory. Tried to allocate 1.42 GiB (GPU 0; 23.70 GiB total capacity; 20.93 GiB already allocated; 1.21 GiB free; 21.40 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[32m[I 2023-05-02 05:31:11,089][0m Trial 45 finished with value: 100.0 and parameters: {'lr': 0.04126989767826542, 'dropout': 0.6635973577272782, 'd_model_multiplier': 2, 'num_layers': 13, 'n_heads': 16, 'dim_feedforward': 301, 'batch_size': 240, 'pos_encoding': 'learnable', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.001, 'top_n_features': 176}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 26
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
Warning, assumed runtime error: CUDA out of memory. Tried to allocate 2.14 GiB (GPU 0; 23.70 GiB total capacity; 16.80 GiB already allocated; 1.19 GiB free; 21.41 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[32m[I 2023-05-02 05:31:16,985][0m Trial 46 finished with value: 100.0 and parameters: {'lr': 0.010793061592010562, 'dropout': 0.5532711986680964, 'd_model_multiplier': 8, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 451, 'batch_size': 181, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.01, 'top_n_features': 26}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 218
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1653[0m   [32m0.6444[0m        [35m1.1712[0m       [31m0.9274[0m        [94m0.2800[0m     +  38.5404
      2   0.0916   0.5835        [35m0.9821[0m       0.9262        [94m0.2798[0m     +  38.4720
      3   [36m0.2185[0m   [32m0.7405[0m        [35m0.7577[0m       0.9250        [94m0.2363[0m     +  38.8164
      4   0.2097   0.7372        0.7980       0.9274        0.2381        39.1310
      5   0.2130   [32m0.7610[0m        [35m0.7372[0m       0.9238        0.2438        39.1425
      6   [36m0.2217[0m   [32m0.7671[0m        0.9006       0.9093        0.2484        38.8376
      7   [36m0.2325[0m   0.7391        0.7537       0.9274        0.2388        39.2213
      8   0.0547   0.3284        0.9934       0.9274        0.3709        39.3100
      9   [36m0.2548[0m   0.7631        0.8920       0.9178        [94m0.2302[0m     +  39.2031
     10   0.2420   0.7390        0.7395       0.9274        0.2357        39.2614
     11   0.1975   0.6853        0.7784       0.9238        0.3129        39.1288
     12   0.2246   0.6966        0.7677       0.9250        0.2455        38.8100
     13   0.1814   0.6639        1.1023       0.9129        0.3800        39.4894
     14   0.1165   0.6027        1.1951       0.0726        4.9445        39.1192
     15   0.1772   0.7278        1.7481       0.8767        0.3336        38.9035
     16   0.1634   0.6643        1.0250       0.9274        0.2722        39.5561
     17   0.2545   0.7173        1.1007       0.6675        0.6609        39.3468
     18   0.1904   0.6775        0.9509       0.9033        0.4606        39.4351
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 05:43:41,559][0m Trial 47 finished with value: 0.2301832255800373 and parameters: {'lr': 0.004777946938609365, 'dropout': 0.6016193669895465, 'd_model_multiplier': 32, 'num_layers': 9, 'n_heads': 16, 'dim_feedforward': 257, 'batch_size': 81, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 218}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 189
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0936[0m   [32m0.4139[0m        [35m0.5014[0m       [31m0.9178[0m        [94m0.4443[0m     +  35.6519
      2   [36m0.2226[0m   [32m0.6895[0m        [35m0.3151[0m       0.9178        [94m0.3521[0m     +  35.6385
      3   0.1862   0.6895        [35m0.2865[0m       0.9178        0.3682        36.1400
      4   [36m0.2349[0m   [32m0.7123[0m        [35m0.2733[0m       0.9178        [94m0.3062[0m     +  35.8423
      5   0.2263   [32m0.7221[0m        [35m0.2650[0m       0.9178        0.3106        35.9583
      6   0.1964   0.7040        [35m0.2640[0m       0.9166        0.3818        36.0784
      7   0.2012   0.7089        [35m0.2570[0m       0.9178        0.3567        35.9004
      8   0.2288   0.7065        0.2577       0.9178        0.3464        36.1327
      9   [36m0.2524[0m   0.7217        0.2598       0.9178        0.3335        35.7282
     10   0.2146   0.7048        0.2572       0.9166        0.3250        35.7423
     11   0.1539   0.6641        [35m0.2564[0m       0.9166        0.3884        35.6185
     12   [36m0.2563[0m   0.7064        [35m0.2552[0m       0.9166        0.3231        35.9525
     13   [36m0.2684[0m   0.7148        0.2576       0.9178        0.3145        35.9759
     14   0.2605   0.7117        [35m0.2526[0m       0.9178        [94m0.2945[0m     +  36.1573
     15   0.2531   0.7061        0.2550       0.9166        0.3164        36.0097
     16   0.2612   0.7175        [35m0.2515[0m       0.9178        [94m0.2800[0m     +  36.0139
     17   0.2651   0.7147        0.2521       0.9178        [94m0.2712[0m     +  35.9530
     18   0.2629   0.7114        0.2550       0.9166        0.2959        35.9163
     19   0.2645   0.7136        0.2558       0.9178        0.2808        36.1569
     20   0.2636   0.7015        0.2519       0.9178        [94m0.2682[0m     +  35.8679
     21   [36m0.2765[0m   0.7186        0.2527       0.9178        [94m0.2597[0m     +  35.5865
     22   0.2636   0.7066        0.2548       0.9166        0.2649        36.0381
     23   0.2510   0.7142        0.2524       0.9178        0.2604        36.0024
     24   0.2534   [32m0.7287[0m        [35m0.2512[0m       0.9178        [94m0.2597[0m     +  35.7153
     25   0.2371   0.7199        [35m0.2511[0m       0.9178        0.2643        35.7655
     26   0.2448   0.7176        0.2520       0.9166        0.2608        35.9210
     27   0.2523   0.7248        0.2516       0.9166        0.2634        35.9908
     28   0.2439   0.7217        0.2516       0.9166        0.2651        35.9374
     29   0.2635   0.7261        [35m0.2504[0m       0.9178        0.2724        35.6989
     30   0.2447   0.7164        0.2516       0.9166        0.2753        35.9883
     31   0.2560   0.7122        0.2516       0.9166        0.2745        35.6062
     32   0.1800   0.6773        0.2505       0.9166        0.2959        35.5890
     33   0.2163   0.7086        [35m0.2481[0m       0.9166        0.2801        35.9094
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 06:04:05,680][0m Trial 48 finished with value: 0.25968308586451566 and parameters: {'lr': 0.0022164731513471423, 'dropout': 0.5960538789021466, 'd_model_multiplier': 32, 'num_layers': 9, 'n_heads': 16, 'dim_feedforward': 278, 'batch_size': 69, 'pos_encoding': 'learnable', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 189}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 128
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2869[0m   [32m0.7495[0m        [35m0.3415[0m       [31m0.9214[0m        [94m0.3639[0m     +  20.9874
      2   [36m0.3211[0m   [32m0.7804[0m        [35m0.3135[0m       0.9178        [94m0.3120[0m     +  21.6326
      3   0.2951   0.7763        [35m0.3005[0m       0.9178        [94m0.2751[0m     +  21.2153
      4   [36m0.3400[0m   [32m0.7831[0m        [35m0.2972[0m       0.9214        [94m0.2719[0m     +  20.8749
      5   [36m0.3562[0m   [32m0.7955[0m        [35m0.2880[0m       [31m0.9226[0m        [94m0.2566[0m     +  21.1098
      6   0.3417   0.7903        0.2887       0.9190        [94m0.2524[0m     +  21.3145
      7   [36m0.3564[0m   [32m0.7986[0m        0.2895       0.9105        0.3284        21.8128
      8   0.3446   0.7552        [35m0.2772[0m       0.9190        0.2789        21.4066
      9   [36m0.3793[0m   0.7809        [35m0.2680[0m       0.9202        0.2909        21.4082
     10   [36m0.3995[0m   0.7907        0.2775       0.9166        0.3792        21.1417
     11   0.3727   0.7859        0.2695       0.9202        0.3704        21.4889
     12   [36m0.4063[0m   0.7909        0.2725       0.9202        0.3051        21.4345
     13   [36m0.4227[0m   [32m0.8048[0m        [35m0.2627[0m       [31m0.9250[0m        0.2735        21.2742
     14   0.4213   0.8030        0.2750       0.9214        0.2898        21.1418
     15   [36m0.4278[0m   [32m0.8075[0m        0.2660       0.9250        0.2664        20.9882
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 06:09:47,545][0m Trial 49 finished with value: 0.2524306776301604 and parameters: {'lr': 0.0003357178107347635, 'dropout': 0.616951143286173, 'd_model_multiplier': 32, 'num_layers': 9, 'n_heads': 8, 'dim_feedforward': 333, 'batch_size': 80, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'BatchNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 128}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 203
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
Warning, assumed runtime error: CUDA out of memory. Tried to allocate 656.00 MiB (GPU 0; 23.70 GiB total capacity; 21.73 GiB already allocated; 577.25 MiB free; 22.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[32m[I 2023-05-02 06:09:50,543][0m Trial 50 finished with value: 100.0 and parameters: {'lr': 0.0031385300850930086, 'dropout': 0.6574838285546138, 'd_model_multiplier': 32, 'num_layers': 11, 'n_heads': 64, 'dim_feedforward': 170, 'batch_size': 27, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 203}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 221
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0733[0m   [32m0.3628[0m        [35m9.1465[0m       [31m0.9202[0m        [94m8.5719[0m     +  72.8723
      2   0.0721   [32m0.3655[0m        [35m6.5106[0m       0.9202        8.8407        72.7737
      3   0.0659   0.3300        [35m3.8284[0m       0.9202        [94m1.9159[0m     +  73.0172
      4   [36m0.1217[0m   [32m0.4954[0m        [35m1.6777[0m       0.9129        [94m0.6293[0m     +  73.2174
      5   0.0935   0.4480        2.7627       0.9154        2.3936        72.8557
      6   [36m0.1240[0m   [32m0.5311[0m        2.2665       0.9129        1.6187        73.1693
      7   0.0933   0.5089        1.8499       0.9202        1.1144        72.9755
      8   [36m0.1663[0m   [32m0.5666[0m        [35m1.5619[0m       0.9202        0.8145        73.3597
      9   0.1618   [32m0.5928[0m        [35m1.4560[0m       0.9190        0.7430        73.0661
     10   0.0858   0.4197        2.1400       0.9202        [94m0.4498[0m     +  72.8019
     11   0.1076   0.5479        2.4103       0.9202        0.6919        72.9825
     12   [36m0.1894[0m   [32m0.6762[0m        2.1309       0.8984        1.2093        73.1937
     13   0.1266   0.5087        2.7154       0.5429        0.9112        72.7758
     14   0.0571   0.2906        3.5729       0.8924        1.1270        73.1661
     15   0.1349   0.5598        3.9740       0.9117        0.6457        72.6043
     16   0.0725   0.3832        5.9639       0.9141        1.0590        72.5640
     17   0.1106   0.5391        6.8417       0.1608        7.5210        72.7265
     18   0.0837   0.4977        7.2329       0.9141        2.1776        73.1531
     19   0.0645   0.3332        7.2866       0.9154        2.7413        72.5734
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 06:34:14,653][0m Trial 51 finished with value: 0.4498369267802717 and parameters: {'lr': 0.005683690037493059, 'dropout': 0.5695813513862762, 'd_model_multiplier': 64, 'num_layers': 9, 'n_heads': 16, 'dim_feedforward': 262, 'batch_size': 60, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0, 'top_n_features': 221}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 212
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.2310[0m   [32m0.6917[0m        [35m0.2891[0m       [31m0.8017[0m        [94m0.6183[0m     +  9.2162
      2   0.1619   0.5309        [35m0.2781[0m       0.7715        0.6452        9.0011
      3   0.0677   0.3800        [35m0.2760[0m       0.7956        0.6329        9.0262
      4   [36m0.2343[0m   [32m0.7162[0m        0.2845       [31m0.8646[0m        [94m0.5815[0m     +  9.4560
      5   0.1040   0.5079        0.2794       [31m0.9274[0m        [94m0.3355[0m     +  9.0845
      6   0.1254   0.5581        0.2831       0.9274        [94m0.2658[0m     +  9.4462
      7   [36m0.2566[0m   0.7127        0.2776       0.9274        [94m0.2374[0m     +  9.2288
      8   0.2434   [32m0.7609[0m        [35m0.2682[0m       0.9274        [94m0.2354[0m     +  9.0821
      9   [36m0.2641[0m   [32m0.7839[0m        [35m0.2644[0m       0.9262        [94m0.2220[0m     +  9.3122
     10   [36m0.2835[0m   0.7582        [35m0.2597[0m       [31m0.9287[0m        0.2274        9.1786
     11   [36m0.2960[0m   0.7789        0.2615       0.9287        0.2234        9.3610
     12   0.2787   0.7767        [35m0.2558[0m       0.9287        0.2230        9.4394
     13   [36m0.3172[0m   [32m0.7982[0m        [35m0.2523[0m       [31m0.9299[0m        [94m0.2148[0m     +  9.0465
     14   0.3132   0.7700        [35m0.2470[0m       0.9287        0.2202        9.2982
     15   [36m0.3290[0m   0.7969        [35m0.2410[0m       0.9299        [94m0.2144[0m     +  9.1807
     16   0.3100   0.7830        [35m0.2403[0m       [31m0.9311[0m        0.2178        9.1296
     17   0.3010   0.7751        0.2403       0.9287        0.2238        9.3232
     18   0.2962   0.7852        [35m0.2373[0m       0.9299        0.2190        9.0630
     19   0.2952   0.7749        0.2403       0.9287        0.2228        9.3345
     20   0.3022   [32m0.8028[0m        0.2392       0.9299        0.2156        8.9189
     21   0.3163   0.7965        [35m0.2369[0m       [31m0.9323[0m        0.2159        9.1539
     22   0.3144   [32m0.8056[0m        [35m0.2364[0m       0.9299        0.2149        9.2191
     23   0.3276   [32m0.8123[0m        0.2370       0.9323        [94m0.2124[0m     +  9.4910
     24   [36m0.3368[0m   0.8080        0.2366       0.9323        0.2126        9.2010
     25   0.3167   0.8109        [35m0.2341[0m       0.9311        0.2152        9.2194
     26   0.3183   0.8081        0.2358       0.9299        0.2173        9.1439
     27   0.3212   [32m0.8157[0m        0.2347       0.9311        0.2144        9.5990
     28   0.3138   0.8015        [35m0.2335[0m       0.9262        0.2167        9.3747
     29   0.3218   0.7987        0.2371       0.9323        0.2166        9.1578
     30   0.2969   0.7794        0.2339       0.9262        0.2225        9.4378
     31   0.3099   0.8020        [35m0.2324[0m       0.9287        0.2170        9.1897
     32   0.3027   0.8001        0.2350       0.9287        0.2182        9.2829
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 06:39:20,263][0m Trial 52 finished with value: 0.2124404691907486 and parameters: {'lr': 0.0010686938688129585, 'dropout': 0.5257726747179655, 'd_model_multiplier': 8, 'num_layers': 1, 'n_heads': 16, 'dim_feedforward': 243, 'batch_size': 42, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 212}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 200
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.1128[0m   [32m0.3951[0m        [35m0.2833[0m       [31m0.1415[0m        [94m1.0385[0m     +  9.7542
      2   0.1009   [32m0.4264[0m        [35m0.2637[0m       [31m0.2140[0m        [94m0.8007[0m     +  9.7902
      3   0.0922   0.3634        0.2638       0.1971        0.8667        10.2120
      4   [36m0.1365[0m   [32m0.5438[0m        0.2695       0.1294        0.9682        10.7967
      5   0.0965   0.3877        [35m0.2622[0m       0.1898        0.8408        10.3498
      6   [36m0.2264[0m   [32m0.6561[0m        [35m0.2597[0m       [31m0.5949[0m        [94m0.6713[0m     +  10.3529
      7   0.1609   0.6062        0.2688       [31m0.9021[0m        [94m0.5168[0m     +  10.3794
      8   0.2193   0.6130        0.2743       [31m0.9105[0m        [94m0.4204[0m     +  10.2430
      9   [36m0.2565[0m   [32m0.7221[0m        0.2611       0.9081        [94m0.3709[0m     +  10.2917
     10   [36m0.2821[0m   [32m0.7408[0m        0.2628       0.9069        [94m0.3585[0m     +  10.4319
     11   0.2789   [32m0.7492[0m        0.2620       0.9105        [94m0.2771[0m     +  10.3239
     12   0.2673   [32m0.7621[0m        [35m0.2582[0m       0.9033        0.2827        10.2767
     13   0.2818   0.7573        0.2650       0.9093        [94m0.2692[0m     +  10.1141
     14   [36m0.2884[0m   [32m0.7654[0m        [35m0.2549[0m       0.9057        0.2728        9.9341
     15   0.2871   0.7550        [35m0.2511[0m       0.9093        0.2984        10.3466
     16   0.2852   [32m0.7673[0m        0.2537       0.9081        0.2779        10.4088
     17   0.2845   0.7606        0.2527       0.9105        0.2707        10.6142
     18   0.2747   [32m0.7723[0m        [35m0.2447[0m       0.9033        0.2762        10.5160
     19   0.2674   0.7717        0.2491       0.9045        0.2692        10.2260
     20   0.2788   0.7657        [35m0.2417[0m       0.9081        0.2693        10.2736
     21   [36m0.2901[0m   0.7682        0.2432       0.9081        [94m0.2687[0m     +  10.1912
     22   [36m0.3038[0m   0.7723        [35m0.2408[0m       0.9069        [94m0.2650[0m     +  10.1423
     23   0.2896   0.7698        0.2433       0.9105        0.2666        10.3449
     24   0.3027   [32m0.7744[0m        [35m0.2403[0m       0.9093        [94m0.2632[0m     +  10.4501
     25   0.3016   [32m0.7766[0m        [35m0.2362[0m       0.9057        0.2657        10.1690
     26   [36m0.3072[0m   [32m0.7794[0m        [35m0.2337[0m       0.9093        0.2639        10.1510
     27   [36m0.3161[0m   [32m0.7832[0m        0.2369       [31m0.9129[0m        [94m0.2602[0m     +  10.4010
     28   0.2855   0.7600        [35m0.2333[0m       0.9069        0.2666        10.4747
     29   0.3125   0.7755        0.2338       [31m0.9154[0m        0.2613        10.7810
     30   0.3160   0.7827        0.2339       0.9117        0.2624        10.5130
     31   [36m0.3325[0m   [32m0.7908[0m        0.2369       0.9117        [94m0.2565[0m     +  10.4735
     32   [36m0.3345[0m   0.7869        [35m0.2310[0m       [31m0.9178[0m        0.2580        10.4058
     33   [36m0.3403[0m   0.7903        [35m0.2287[0m       0.9154        0.2619        10.4083
     34   0.3360   0.7905        0.2305       0.9154        0.2606        10.5954
     35   0.3371   0.7893        0.2299       0.9093        0.2582        10.6611
     36   0.3380   0.7898        0.2319       0.9154        0.2595        10.5327
     37   0.3366   0.7888        0.2305       0.9154        0.2591        10.2340
     38   [36m0.3468[0m   0.7904        0.2322       0.9141        0.2582        10.5105
     39   0.3377   0.7901        0.2306       0.9093        [94m0.2560[0m     +  10.3285
     40   [36m0.3477[0m   0.7865        [35m0.2287[0m       0.9117        0.2586        10.2328
     41   0.3380   0.7843        0.2331       0.9166        0.2601        10.2653
     42   [36m0.3516[0m   0.7857        [35m0.2272[0m       0.9154        0.2578        10.5024
     43   0.3425   0.7877        0.2315       0.9154        0.2584        10.1448
     44   [36m0.3586[0m   0.7868        0.2305       0.9117        [94m0.2554[0m     +  10.3095
     45   0.3466   0.7880        0.2294       0.9166        [94m0.2542[0m     +  10.2571
     46   0.3537   0.7908        0.2274       0.9154        0.2602        10.2106
     47   0.3512   0.7842        0.2285       0.9129        0.2598        10.3156
     48   0.3538   0.7871        0.2292       0.9166        0.2588        10.7174
     49   0.3551   0.7870        0.2303       0.9117        0.2583        10.9623
     50   [36m0.3644[0m   [32m0.7915[0m        0.2289       0.9117        [94m0.2542[0m     +  11.1863
[32m[I 2023-05-02 06:47:59,840][0m Trial 53 finished with value: 0.2542142234517412 and parameters: {'lr': 0.0016236003598109822, 'dropout': 0.5333527930415983, 'd_model_multiplier': 8, 'num_layers': 1, 'n_heads': 16, 'dim_feedforward': 235, 'batch_size': 113, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 200}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 213
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.2576[0m   [32m0.7119[0m        [35m0.2877[0m       [31m0.9238[0m        [94m0.3661[0m     +  9.5619
      2   0.2501   [32m0.7321[0m        [35m0.2558[0m       0.9190        [94m0.3507[0m     +  8.6374
      3   0.2467   0.7281        [35m0.2510[0m       0.9226        [94m0.3506[0m     +  9.2355
      4   0.2342   0.7261        [35m0.2471[0m       0.9202        [94m0.3488[0m     +  9.0704
      5   0.2262   0.7245        [35m0.2454[0m       0.9202        [94m0.3433[0m     +  9.3455
      6   0.2258   0.7182        [35m0.2439[0m       0.9178        [94m0.3378[0m     +  9.3594
      7   0.2250   0.7130        0.2445       0.9226        [94m0.3330[0m     +  9.5164
      8   0.2238   0.7208        [35m0.2437[0m       0.9154        0.3605        9.4335
      9   0.2121   0.7110        [35m0.2429[0m       0.9154        0.3456        9.3521
     10   0.2192   0.7246        [35m0.2413[0m       0.9166        0.3530        9.4034
     11   0.2099   0.7099        [35m0.2379[0m       0.9166        0.3463        9.5915
     12   0.2091   0.7028        0.2403       0.9178        0.3393        9.7264
     13   0.2175   0.7055        [35m0.2375[0m       0.9178        0.3511        9.4292
     14   0.2159   0.6949        0.2388       0.9166        0.3438        9.5251
     15   0.2110   0.6939        [35m0.2366[0m       0.9154        0.3545        9.5648
     16   0.1992   0.6723        0.2392       0.9190        0.3495        9.5148
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 06:50:40,041][0m Trial 54 finished with value: 0.33300036451404297 and parameters: {'lr': 0.00012734176610861684, 'dropout': 0.5973291487378216, 'd_model_multiplier': 8, 'num_layers': 1, 'n_heads': 16, 'dim_feedforward': 291, 'batch_size': 46, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 213}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 155
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1794[0m   [32m0.5731[0m        [35m0.3055[0m       [31m0.9008[0m        [94m0.3453[0m     +  20.7274
      2   [36m0.2548[0m   [32m0.7829[0m        [35m0.2766[0m       0.8912        0.3699        20.8119
      3   [36m0.2680[0m   0.7377        [35m0.2672[0m       [31m0.9226[0m        [94m0.2737[0m     +  21.0885
      4   [36m0.2771[0m   0.7683        0.2769       0.8295        0.4229        21.3446
      5   0.2383   0.7692        [35m0.2614[0m       0.9226        0.2737        21.1152
      6   0.2194   0.7520        0.2706       0.9226        [94m0.2487[0m     +  21.3287
      7   0.2223   0.7505        0.2641       0.9226        0.2586        21.1313
      8   0.2317   0.7459        0.2835       0.9129        [94m0.2469[0m     +  21.1719
      9   0.2002   0.7445        0.2861       0.9226        [94m0.2444[0m     +  21.4389
     10   0.2280   0.7316        0.2699       0.9226        0.2510        21.4404
     11   0.2250   0.7391        0.2727       0.9226        [94m0.2430[0m     +  21.1451
     12   0.2162   0.7434        0.2749       0.9214        0.2549        20.9827
     13   0.2111   0.7469        0.2848       0.9226        0.2461        21.3882
     14   0.2229   0.7493        0.2695       0.9226        [94m0.2415[0m     +  21.4525
     15   0.2245   0.7520        0.2928       0.9226        0.2463        21.3881
     16   0.2298   0.7464        0.2800       0.9226        [94m0.2413[0m     +  21.1753
     17   0.2104   0.7428        0.2772       0.9226        0.2544        21.1554
     18   0.2082   0.7423        0.2835       0.9226        [94m0.2411[0m     +  21.0524
     19   0.2301   0.7524        0.2754       0.9226        [94m0.2410[0m     +  21.1724
     20   0.2204   0.7349        0.2754       0.9226        0.2447        21.3030
     21   0.2175   0.7447        0.2615       0.9226        0.2415        21.2148
     22   0.2252   0.7515        0.2623       0.9226        [94m0.2396[0m     +  21.1648
     23   0.2364   0.7527        0.2675       0.9154        0.2488        21.0066
     24   0.2125   0.7469        0.2767       0.9226        0.2426        21.4997
     25   0.2038   0.7297        0.2838       0.9226        0.2452        21.6699
     26   0.2324   0.7480        0.2633       0.9226        0.2418        21.3576
     27   0.2185   0.7344        0.2821       0.9226        0.2508        20.9958
     28   0.2226   0.7507        0.2667       0.9214        0.2409        20.8741
     29   0.2380   0.7507        0.2730       0.9226        0.2401        21.0173
     30   0.2397   0.7463        0.2806       0.9226        0.2447        21.2319
     31   0.2051   0.7354        0.2864       0.9226        0.2460        21.3761
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 07:02:00,199][0m Trial 55 finished with value: 0.23957668174567218 and parameters: {'lr': 0.0011827782066901006, 'dropout': 0.5026514375381643, 'd_model_multiplier': 8, 'num_layers': 7, 'n_heads': 16, 'dim_feedforward': 415, 'batch_size': 86, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 155}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 227
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.2428[0m   [32m0.7618[0m        [35m0.2809[0m       [31m0.9250[0m        [94m0.2255[0m     +  8.7122
      2   0.2393   0.7512        [35m0.2674[0m       0.9154        0.2475        8.8703
      3   [36m0.2560[0m   [32m0.7692[0m        [35m0.2624[0m       0.9069        0.2683        8.9711
      4   [36m0.2776[0m   [32m0.7782[0m        [35m0.2592[0m       0.8912        0.2846        9.0555
      5   [36m0.3019[0m   [32m0.7975[0m        [35m0.2579[0m       0.9178        0.2510        9.0671
      6   0.2792   0.7833        [35m0.2546[0m       0.8815        0.3140        9.3345
      7   [36m0.3203[0m   [32m0.8035[0m        [35m0.2504[0m       [31m0.9299[0m        [94m0.2252[0m     +  9.1203
      8   [36m0.3261[0m   [32m0.8057[0m        0.2525       [31m0.9359[0m        [94m0.2173[0m     +  9.3739
      9   [36m0.3268[0m   [32m0.8067[0m        [35m0.2457[0m       0.9323        0.2181        9.2306
     10   0.2977   0.7967        0.2476       0.9214        0.2238        9.3428
     11   0.3003   [32m0.8090[0m        [35m0.2420[0m       0.9141        0.2335        9.2343
     12   0.3155   0.8078        [35m0.2386[0m       0.9081        0.2474        9.1019
     13   0.2991   [32m0.8226[0m        0.2417       0.9226        0.2179        9.1348
     14   0.2870   0.8158        [35m0.2384[0m       0.9226        0.2177        9.6709
     15   0.2880   0.8156        [35m0.2363[0m       0.9335        [94m0.2115[0m     +  9.9644
     16   0.2801   0.8170        0.2372       0.9287        0.2136        9.9057
     17   0.2870   0.8011        [35m0.2353[0m       0.9299        0.2140        9.6710
     18   0.2952   0.7904        [35m0.2329[0m       0.9154        0.2360        9.2233
     19   0.2985   0.7924        [35m0.2325[0m       0.9081        0.2449        9.1377
     20   0.3023   0.7984        [35m0.2304[0m       0.9045        0.2537        9.3104
     21   0.3031   0.8083        0.2314       0.9057        0.2542        9.1316
     22   0.2601   0.8006        [35m0.2287[0m       0.9129        0.2420        9.2456
     23   0.2778   0.8112        [35m0.2275[0m       0.9178        0.2319        9.2607
     24   0.2869   0.8053        0.2304       0.9117        0.2393        9.2893
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 07:05:52,405][0m Trial 56 finished with value: 0.2114639981720763 and parameters: {'lr': 0.0005851615486802006, 'dropout': 0.5575651994199265, 'd_model_multiplier': 8, 'num_layers': 2, 'n_heads': 4, 'dim_feedforward': 238, 'batch_size': 19, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 227}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 228
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1652[0m   [32m0.6870[0m        [35m0.2782[0m       [31m0.9299[0m        [94m0.2301[0m     +  11.3345
      2   [36m0.2112[0m   [32m0.6916[0m        [35m0.2630[0m       0.9299        0.2324        11.1059
      3   0.1864   0.6914        [35m0.2617[0m       [31m0.9311[0m        [94m0.2284[0m     +  11.3376
      4   0.1999   [32m0.7070[0m        [35m0.2590[0m       0.9274        0.2299        11.1248
      5   [36m0.2533[0m   [32m0.7485[0m        [35m0.2512[0m       0.9262        0.2292        11.4302
      6   0.2451   [32m0.7492[0m        [35m0.2484[0m       0.9166        0.2581        11.2166
      7   [36m0.2602[0m   [32m0.7620[0m        [35m0.2467[0m       0.9214        0.2381        11.3132
      8   [36m0.3003[0m   [32m0.7675[0m        [35m0.2408[0m       0.9274        [94m0.2270[0m     +  11.4454
      9   0.2913   [32m0.7741[0m        0.2417       [31m0.9323[0m        [94m0.2169[0m     +  10.8718
     10   0.2924   [32m0.7770[0m        [35m0.2382[0m       0.9323        0.2209        11.2247
     11   [36m0.3027[0m   0.7760        0.2398       0.9311        0.2231        11.1164
     12   [36m0.3094[0m   [32m0.7839[0m        [35m0.2340[0m       [31m0.9347[0m        [94m0.2158[0m     +  11.3521
     13   0.3072   0.7766        [35m0.2313[0m       [31m0.9383[0m        0.2206        11.3074
     14   0.3054   0.7792        0.2323       [31m0.9407[0m        0.2207        11.5441
     15   [36m0.3136[0m   0.7722        0.2339       0.9383        [94m0.2134[0m     +  11.2762
     16   [36m0.3175[0m   0.7692        [35m0.2303[0m       0.9395        0.2179        11.1859
     17   0.2555   0.7481        [35m0.2278[0m       0.9395        0.2300        11.3638
     18   0.2609   0.7803        [35m0.2266[0m       [31m0.9420[0m        0.2238        11.1901
     19   0.2908   0.7787        0.2298       0.9383        0.2227        11.4125
     20   0.2898   0.7775        [35m0.2253[0m       0.9395        0.2203        11.0719
     21   0.2891   [32m0.7844[0m        0.2273       0.9407        0.2167        11.3415
     22   0.2598   0.7386        0.2267       [31m0.9444[0m        0.2194        11.1811
     23   0.2733   0.7495        [35m0.2240[0m       0.9420        0.2207        11.1399
     24   0.2772   0.7572        [35m0.2239[0m       0.9420        0.2158        11.4078
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 07:10:35,206][0m Trial 57 finished with value: 0.21339567550125801 and parameters: {'lr': 0.0005275289547560971, 'dropout': 0.560796520703218, 'd_model_multiplier': 8, 'num_layers': 2, 'n_heads': 4, 'dim_feedforward': 492, 'batch_size': 10, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 228}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 227
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1860[0m   [32m0.7198[0m        [35m0.2729[0m       [31m0.9335[0m        [94m0.2312[0m     +  11.4684
      2   [36m0.2014[0m   [32m0.7259[0m        [35m0.2589[0m       0.9335        [94m0.2280[0m     +  11.6598
      3   [36m0.2272[0m   [32m0.7394[0m        [35m0.2584[0m       0.9335        [94m0.2228[0m     +  11.6982
      4   [36m0.2346[0m   [32m0.7493[0m        [35m0.2557[0m       0.9323        [94m0.2222[0m     +  11.5341
      5   [36m0.2397[0m   0.7477        [35m0.2501[0m       0.9311        0.2304        11.8800
      6   0.2339   [32m0.7521[0m        [35m0.2447[0m       0.9335        0.2330        11.6305
      7   0.2312   0.7476        [35m0.2441[0m       0.9274        0.2287        11.6361
      8   0.2366   0.7389        [35m0.2422[0m       0.9323        0.2397        11.8210
      9   0.2081   0.7359        [35m0.2365[0m       0.9311        0.2511        11.1497
     10   0.2193   0.7473        [35m0.2356[0m       0.9287        0.2334        11.5745
     11   [36m0.2541[0m   0.7499        0.2358       [31m0.9359[0m        0.2346        11.3012
     12   0.2303   0.7462        [35m0.2346[0m       0.9311        0.2457        11.3821
     13   0.2221   0.7411        [35m0.2344[0m       0.9335        0.2437        11.7089
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 07:13:17,598][0m Trial 58 finished with value: 0.22222139162913487 and parameters: {'lr': 0.00038646999473549343, 'dropout': 0.5304992808312475, 'd_model_multiplier': 8, 'num_layers': 4, 'n_heads': 4, 'dim_feedforward': 512, 'batch_size': 16, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 227}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 187
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2084[0m   [32m0.7268[0m        [35m0.2783[0m       [31m0.8851[0m        [94m0.2994[0m     +  10.5987
      2   [36m0.2108[0m   [32m0.7323[0m        [35m0.2653[0m       [31m0.8972[0m        [94m0.2910[0m     +  10.2009
      3   [36m0.2137[0m   0.7248        [35m0.2582[0m       0.8912        0.3050        10.0563
      4   0.2114   0.7194        [35m0.2573[0m       [31m0.9081[0m        [94m0.2883[0m     +  10.2708
      5   0.2104   0.7173        [35m0.2566[0m       0.8960        0.2925        10.3478
      6   0.2119   0.7246        [35m0.2551[0m       [31m0.9105[0m        [94m0.2808[0m     +  10.5661
      7   0.2132   0.7265        0.2562       0.9057        0.2808        10.8116
      8   0.2130   0.7317        [35m0.2533[0m       0.9057        0.2832        10.6342
      9   0.2121   0.7283        0.2543       0.9069        [94m0.2795[0m     +  10.6779
     10   0.2100   0.7248        0.2537       0.9081        0.2821        10.5908
     11   0.2118   0.7292        [35m0.2530[0m       0.9069        [94m0.2785[0m     +  10.8565
     12   0.2090   0.7272        [35m0.2523[0m       0.9045        0.2839        10.7516
     13   0.2080   0.7278        0.2527       0.9069        0.2799        10.7523
     14   0.2066   0.7277        0.2531       [31m0.9129[0m        [94m0.2724[0m     +  10.7217
     15   0.2112   0.7250        [35m0.2523[0m       0.9105        0.2732        10.6178
     16   0.2125   0.7279        0.2542       0.9117        0.2736        11.5017
     17   0.2103   0.7274        [35m0.2511[0m       0.9105        0.2729        10.7894
     18   0.2103   0.7265        0.2530       0.9081        0.2725        10.6084
     19   0.2074   0.7218        0.2515       0.9129        [94m0.2670[0m     +  10.8146
     20   0.2117   0.7300        0.2540       0.9105        [94m0.2660[0m     +  10.7039
     21   0.2133   0.7290        0.2523       0.9105        [94m0.2624[0m     +  10.7669
     22   [36m0.2279[0m   [32m0.7622[0m        [35m0.2506[0m       0.9081        [94m0.2583[0m     +  10.2263
     23   [36m0.2496[0m   [32m0.7733[0m        [35m0.2439[0m       0.9069        [94m0.2496[0m     +  10.3889
     24   [36m0.2722[0m   [32m0.7859[0m        [35m0.2438[0m       [31m0.9141[0m        [94m0.2425[0m     +  10.8413
     25   [36m0.2780[0m   [32m0.7899[0m        [35m0.2400[0m       0.9129        [94m0.2397[0m     +  10.4533
     26   0.2683   0.7875        [35m0.2383[0m       0.9081        0.2466        10.6784
     27   [36m0.2852[0m   [32m0.7899[0m        0.2405       [31m0.9190[0m        [94m0.2331[0m     +  10.7799
     28   [36m0.3158[0m   [32m0.7983[0m        0.2394       [31m0.9226[0m        [94m0.2266[0m     +  10.7857
     29   0.2954   0.7922        [35m0.2373[0m       0.9178        0.2357        10.9203
     30   0.2940   0.7935        [35m0.2355[0m       0.9178        0.2336        10.6887
     31   0.2997   0.7791        [35m0.2339[0m       0.9178        0.2334        10.6329
     32   [36m0.3170[0m   0.7881        0.2345       0.9226        0.2333        10.5307
     33   0.2954   0.7909        0.2349       0.9166        0.2341        10.5037
     34   0.3021   0.7903        0.2343       0.9214        0.2360        10.5743
     35   0.2875   0.7854        0.2342       0.9105        0.2416        10.7402
     36   [36m0.3235[0m   0.7897        [35m0.2320[0m       [31m0.9250[0m        0.2312        10.5205
     37   0.3091   0.7896        0.2328       0.9226        0.2340        10.7739
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 07:20:04,586][0m Trial 59 finished with value: 0.2265723713345943 and parameters: {'lr': 0.0008549572143764238, 'dropout': 0.569839852705504, 'd_model_multiplier': 8, 'num_layers': 2, 'n_heads': 4, 'dim_feedforward': 453, 'batch_size': 11, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 187}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 174
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.1684[0m   [32m0.6778[0m        [35m0.2916[0m       [31m0.9287[0m        [94m0.2447[0m     +  9.9378
      2   [36m0.1835[0m   [32m0.6970[0m        [35m0.2613[0m       0.9250        [94m0.2388[0m     +  10.2195
      3   [36m0.2102[0m   [32m0.7222[0m        [35m0.2566[0m       0.9250        [94m0.2357[0m     +  9.9024
      4   [36m0.2540[0m   [32m0.7549[0m        [35m0.2526[0m       0.9250        [94m0.2290[0m     +  9.8737
      5   [36m0.2791[0m   [32m0.7556[0m        [35m0.2493[0m       0.9214        0.2300        9.9047
      6   [36m0.2820[0m   [32m0.7568[0m        [35m0.2461[0m       0.9250        0.2304        9.8393
      7   0.2678   [32m0.7585[0m        [35m0.2449[0m       0.9274        0.2337        9.8874
      8   0.2805   [32m0.7606[0m        [35m0.2404[0m       0.9274        0.2366        10.1660
      9   [36m0.2837[0m   [32m0.7617[0m        0.2418       0.9274        0.2328        9.7368
     10   0.2745   0.7594        [35m0.2388[0m       [31m0.9323[0m        0.2389        10.0094
     11   [36m0.2861[0m   [32m0.7660[0m        0.2401       0.9287        0.2304        9.9158
     12   0.2777   0.7589        [35m0.2366[0m       0.9287        0.2409        10.1047
     13   0.2749   0.7611        [35m0.2349[0m       0.9299        0.2399        10.1497
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 07:22:24,885][0m Trial 60 finished with value: 0.22903964807929195 and parameters: {'lr': 0.0001853974922040157, 'dropout': 0.633365720444478, 'd_model_multiplier': 8, 'num_layers': 4, 'n_heads': 4, 'dim_feedforward': 481, 'batch_size': 23, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 174}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 228
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.2047[0m   [32m0.6846[0m        [35m0.2766[0m       [31m0.9190[0m        [94m0.2608[0m     +  8.3387
      2   [36m0.2257[0m   [32m0.6937[0m        [35m0.2607[0m       0.9190        0.2664        8.2115
      3   [36m0.2583[0m   [32m0.7199[0m        [35m0.2551[0m       0.9190        [94m0.2560[0m     +  8.9586
      4   [36m0.2626[0m   [32m0.7326[0m        [35m0.2520[0m       [31m0.9202[0m        0.2673        8.9917
      5   [36m0.2773[0m   [32m0.7610[0m        [35m0.2490[0m       0.9202        0.2640        8.9463
      6   [36m0.2776[0m   [32m0.7642[0m        [35m0.2479[0m       0.9202        0.2710        9.0254
      7   [36m0.2913[0m   [32m0.7747[0m        [35m0.2444[0m       0.9190        0.2715        9.7609
      8   [36m0.3028[0m   [32m0.7873[0m        [35m0.2423[0m       [31m0.9226[0m        0.2678        9.6228
      9   [36m0.3033[0m   0.7858        0.2447       0.9202        0.2811        9.3543
     10   [36m0.3145[0m   [32m0.7945[0m        [35m0.2397[0m       0.9202        0.2870        9.5250
     11   0.3139   0.7926        [35m0.2394[0m       [31m0.9262[0m        0.2810        9.8085
     12   [36m0.3232[0m   [32m0.7994[0m        0.2409       0.9226        0.2756        9.2620
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 07:24:24,041][0m Trial 61 finished with value: 0.25597321897510894 and parameters: {'lr': 0.0006998345121288106, 'dropout': 0.5494840879039065, 'd_model_multiplier': 8, 'num_layers': 2, 'n_heads': 4, 'dim_feedforward': 236, 'batch_size': 37, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0, 'top_n_features': 228}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 210
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.1110[0m   [32m0.6529[0m        [35m0.2784[0m       [31m0.8888[0m        [94m0.2619[0m     +  8.2062
      2   [36m0.1474[0m   [32m0.6557[0m        [35m0.2599[0m       [31m0.9323[0m        [94m0.2299[0m     +  8.7849
      3   0.1318   [32m0.6616[0m        [35m0.2573[0m       0.9141        0.2425        8.8481
      4   [36m0.1524[0m   [32m0.6900[0m        [35m0.2571[0m       0.9311        [94m0.2292[0m     +  8.7488
      5   [36m0.1574[0m   [32m0.7010[0m        [35m0.2521[0m       0.9226        0.2308        8.8135
      6   [36m0.1642[0m   [32m0.7138[0m        [35m0.2514[0m       0.9105        0.2399        8.8329
      7   [36m0.1771[0m   [32m0.7365[0m        [35m0.2490[0m       0.8984        0.2603        8.6735
      8   [36m0.1808[0m   0.7301        [35m0.2447[0m       0.9178        0.2372        9.2012
      9   [36m0.1885[0m   [32m0.7473[0m        0.2474       0.8996        0.2661        9.2094
     10   [36m0.1934[0m   0.7460        0.2466       0.9287        [94m0.2210[0m     +  9.5178
     11   [36m0.2023[0m   [32m0.7535[0m        [35m0.2429[0m       0.9166        0.2363        9.5938
     12   [36m0.2029[0m   0.7450        0.2433       0.9214        0.2347        9.2186
     13   [36m0.2031[0m   0.7512        [35m0.2421[0m       0.9274        0.2274        9.0606
     14   [36m0.2094[0m   0.7504        [35m0.2415[0m       0.9250        0.2299        9.1026
     15   [36m0.2333[0m   [32m0.7579[0m        [35m0.2374[0m       0.9299        0.2247        8.7849
     16   0.2160   [32m0.7596[0m        0.2402       0.9226        0.2317        9.0774
     17   0.2078   [32m0.7622[0m        0.2375       0.9178        0.2359        9.1489
     18   0.2146   [32m0.7652[0m        [35m0.2370[0m       0.9117        0.2554        9.7282
     19   0.2328   [32m0.7674[0m        [35m0.2348[0m       0.9274        0.2311        9.0309
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 07:27:25,245][0m Trial 62 finished with value: 0.220975230293666 and parameters: {'lr': 0.0004368380491771148, 'dropout': 0.4951404053989128, 'd_model_multiplier': 8, 'num_layers': 1, 'n_heads': 4, 'dim_feedforward': 498, 'batch_size': 18, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.01, 'top_n_features': 210}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 197
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1393[0m   [32m0.7051[0m        [35m0.2760[0m       [31m0.9238[0m        [94m0.2652[0m     +  14.8989
      2   0.1379   0.6995        [35m0.2636[0m       0.9190        0.2767        15.1859
      3   [36m0.1404[0m   [32m0.7052[0m        [35m0.2612[0m       0.9226        0.2816        14.8941
      4   0.1382   [32m0.7067[0m        [35m0.2593[0m       0.9214        0.2795        14.9987
      5   0.1384   [32m0.7096[0m        [35m0.2551[0m       0.9226        [94m0.2594[0m     +  14.7965
      6   0.1387   [32m0.7097[0m        [35m0.2549[0m       0.9226        0.2640        14.6872
      7   [36m0.1440[0m   [32m0.7157[0m        0.2575       0.9238        0.2615        15.3096
      8   0.1435   0.7108        0.2585       0.9226        0.2682        14.8921
      9   0.1429   0.7118        0.2554       0.9238        0.2614        15.1524
     10   [36m0.1516[0m   [32m0.7365[0m        [35m0.2499[0m       0.9226        [94m0.2509[0m     +  15.3256
     11   0.1368   0.7147        [35m0.2485[0m       0.8803        0.2752        15.7486
     12   0.1370   0.7146        [35m0.2476[0m       0.9129        0.2553        14.9953
     13   0.1492   [32m0.7484[0m        [35m0.2445[0m       0.9008        0.2752        15.0050
     14   [36m0.1536[0m   0.7444        [35m0.2390[0m       0.8972        0.2665        14.7428
     15   [36m0.1544[0m   [32m0.7503[0m        [35m0.2369[0m       0.9008        0.2826        14.8193
     16   0.1505   0.7317        0.2394       0.8924        0.2715        15.4434
     17   0.1502   0.7361        0.2396       0.8863        0.2779        15.8351
     18   [36m0.1602[0m   [32m0.7513[0m        0.2386       0.9069        0.2631        15.6327
     19   [36m0.1633[0m   [32m0.7543[0m        0.2393       0.9057        0.2697        15.4152
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 07:32:29,352][0m Trial 63 finished with value: 0.2508879147172695 and parameters: {'lr': 0.00047734115025328203, 'dropout': 0.4968556804002685, 'd_model_multiplier': 8, 'num_layers': 3, 'n_heads': 4, 'dim_feedforward': 497, 'batch_size': 8, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.01, 'top_n_features': 197}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 209
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.2698[0m   [32m0.7457[0m        [35m0.2681[0m       [31m0.9190[0m        [94m0.2733[0m     +  8.8426
      2   [36m0.2953[0m   [32m0.7581[0m        [35m0.2603[0m       0.9190        0.2868        8.0746
      3   [36m0.3256[0m   [32m0.7765[0m        [35m0.2536[0m       0.9190        0.2847        8.3167
      4   [36m0.3475[0m   [32m0.7876[0m        [35m0.2501[0m       [31m0.9214[0m        0.2852        9.0770
      5   [36m0.3608[0m   [32m0.7892[0m        0.2514       0.9214        [94m0.2692[0m     +  8.1735
      6   [36m0.3976[0m   [32m0.8047[0m        [35m0.2455[0m       [31m0.9250[0m        0.2703        8.2722
      7   0.3722   0.7994        [35m0.2432[0m       0.9226        0.2991        9.2717
      8   [36m0.4026[0m   [32m0.8130[0m        [35m0.2424[0m       [31m0.9299[0m        [94m0.2563[0m     +  8.3319
      9   0.3793   0.8067        [35m0.2415[0m       0.9287        0.2645        9.1612
     10   0.3780   [32m0.8162[0m        [35m0.2385[0m       0.9262        0.2575        9.5574
     11   0.3979   0.8132        0.2389       0.9287        0.2661        9.3534
     12   0.3786   0.8112        [35m0.2377[0m       0.9250        0.2632        9.6981
     13   0.3779   [32m0.8180[0m        [35m0.2360[0m       0.9166        [94m0.2545[0m     +  9.8129
     14   0.3546   0.8149        0.2399       0.9008        0.2640        9.3027
     15   0.3884   0.8177        0.2361       0.9166        0.2589        9.1600
     16   0.3886   0.8172        0.2367       0.9274        0.2578        9.2249
     17   0.3698   0.8089        [35m0.2347[0m       0.9166        0.2701        8.9374
     18   0.3695   [32m0.8188[0m        0.2353       0.9190        [94m0.2528[0m     +  9.2671
     19   0.3492   0.8006        [35m0.2337[0m       0.9129        0.2739        9.3369
     20   0.3411   0.8097        [35m0.2313[0m       0.9178        0.2667        9.1155
     21   0.3201   0.7980        0.2333       0.9202        0.2765        9.1835
     22   0.3946   0.8157        0.2331       0.9190        [94m0.2518[0m     +  9.0386
     23   0.3254   0.7989        0.2321       0.9190        0.2903        9.2962
     24   0.3242   0.8032        [35m0.2310[0m       0.9202        0.2737        9.2265
     25   0.3450   0.8071        0.2321       0.9214        0.2710        9.2871
     26   0.3467   0.8052        [35m0.2292[0m       0.9214        0.2716        9.0978
     27   0.3644   0.8155        [35m0.2272[0m       0.9214        0.2640        9.0942
     28   0.3689   [32m0.8203[0m        0.2277       0.9190        0.2543        9.1634
     29   0.4006   [32m0.8268[0m        0.2325       0.9214        [94m0.2428[0m     +  9.2476
     30   [36m0.4076[0m   [32m0.8330[0m        0.2276       0.9250        [94m0.2401[0m     +  9.0504
     31   0.4010   [32m0.8369[0m        [35m0.2262[0m       0.9141        0.2454        9.4058
     32   [36m0.4272[0m   [32m0.8385[0m        0.2275       0.9105        0.2516        9.3439
     33   0.4131   0.8316        0.2279       0.9045        0.2487        9.3141
     34   0.3790   0.8262        [35m0.2255[0m       0.9021        0.2619        9.1514
     35   0.4038   0.8311        [35m0.2254[0m       0.9214        [94m0.2334[0m     +  9.1067
     36   0.3999   0.8281        0.2269       0.9178        0.2396        9.1882
     37   0.3968   0.8276        [35m0.2235[0m       0.9214        0.2451        9.2611
     38   0.4126   0.8345        0.2275       0.9154        0.2354        8.9989
     39   [36m0.4292[0m   0.8312        0.2264       0.9214        0.2398        9.3614
     40   0.4165   0.8349        0.2253       0.9250        0.2466        9.0083
     41   0.3852   0.8232        0.2249       0.9238        0.2653        9.0821
     42   0.3622   0.8166        0.2252       0.9202        0.2685        9.0902
     43   0.3854   0.8234        0.2288       0.9214        0.2522        9.1929
     44   0.3706   0.8190        0.2245       0.9166        0.2613        9.2245
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 07:39:20,185][0m Trial 64 finished with value: 0.2334133966892345 and parameters: {'lr': 0.0008753766298624107, 'dropout': 0.5217965873235016, 'd_model_multiplier': 8, 'num_layers': 2, 'n_heads': 4, 'dim_feedforward': 440, 'batch_size': 39, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.01, 'top_n_features': 209}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 184
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.2236[0m   [32m0.6834[0m        [35m0.2934[0m       [31m0.9226[0m        [94m0.2493[0m     +  8.3071
      2   [36m0.2495[0m   [32m0.7244[0m        [35m0.2516[0m       0.9226        [94m0.2429[0m     +  8.5666
      3   [36m0.2821[0m   [32m0.7369[0m        [35m0.2483[0m       [31m0.9250[0m        [94m0.2403[0m     +  9.1446
      4   [36m0.2938[0m   [32m0.7431[0m        [35m0.2439[0m       0.9202        0.2406        8.9086
      5   [36m0.2954[0m   0.7407        [35m0.2416[0m       0.9129        0.2445        8.9122
      6   [36m0.3030[0m   0.7327        [35m0.2393[0m       0.9117        0.2500        9.4939
      7   [36m0.3070[0m   [32m0.7437[0m        [35m0.2375[0m       0.9154        0.2460        9.8148
      8   0.3052   0.7418        [35m0.2350[0m       0.9105        0.2512        9.2729
      9   0.2950   0.7394        [35m0.2339[0m       0.9105        0.2591        9.6544
     10   0.2865   0.7314        0.2354       0.9081        0.2584        9.6550
     11   0.2930   0.7353        [35m0.2325[0m       0.9105        0.2615        9.2047
     12   0.2862   0.7247        0.2332       0.9117        0.2663        9.2212
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 07:41:19,521][0m Trial 65 finished with value: 0.24028419651892916 and parameters: {'lr': 0.00012262436073907652, 'dropout': 0.49223535210362634, 'd_model_multiplier': 8, 'num_layers': 1, 'n_heads': 4, 'dim_feedforward': 504, 'batch_size': 20, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.01, 'top_n_features': 184}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 239
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.2364[0m   [32m0.7255[0m        [35m0.2852[0m       [31m0.9262[0m        [94m0.2419[0m     +  8.0828
      2   [36m0.2382[0m   [32m0.7338[0m        [35m0.2520[0m       0.9190        0.2471        9.2270
      3   0.2275   0.7289        [35m0.2506[0m       0.9214        0.2491        9.1246
      4   [36m0.2415[0m   0.7199        [35m0.2452[0m       0.9190        0.2601        9.2096
      5   [36m0.2472[0m   0.7326        [35m0.2451[0m       0.9202        0.2640        9.2183
      6   0.2400   [32m0.7448[0m        [35m0.2421[0m       0.9141        0.2584        9.1394
      7   0.2402   0.7418        0.2430       0.9214        0.2620        8.9249
      8   [36m0.2523[0m   [32m0.7451[0m        [35m0.2392[0m       0.9250        0.2721        9.8838
      9   0.2411   0.7407        [35m0.2386[0m       0.9154        0.2771        9.7902
     10   0.2408   0.7386        [35m0.2374[0m       0.9238        0.2866        9.5026
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 07:43:01,426][0m Trial 66 finished with value: 0.24192649844083416 and parameters: {'lr': 0.000279167561436317, 'dropout': 0.5854754785332105, 'd_model_multiplier': 8, 'num_layers': 2, 'n_heads': 4, 'dim_feedforward': 492, 'batch_size': 34, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.01, 'top_n_features': 239}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 160
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.2401[0m   [32m0.7557[0m        [35m0.2816[0m       [31m0.8924[0m        [94m0.2770[0m     +  8.4425
      2   0.2346   0.7530        [35m0.2668[0m       [31m0.8984[0m        [94m0.2713[0m     +  8.9813
      3   [36m0.2439[0m   0.7499        [35m0.2608[0m       [31m0.9154[0m        0.2729        8.6847
      4   [36m0.2443[0m   0.7497        [35m0.2566[0m       0.9117        0.2744        9.0958
      5   [36m0.2452[0m   0.7498        0.2599       0.9154        0.2806        9.3100
      6   0.2383   0.7455        0.2578       0.9154        0.2795        9.3105
      7   0.2386   0.7472        0.2572       0.9117        0.2752        9.3462
      8   0.2443   0.7467        [35m0.2553[0m       0.9154        0.2850        9.5453
      9   0.2307   0.7416        [35m0.2548[0m       0.9154        0.2959        9.1472
     10   [36m0.2477[0m   0.7488        [35m0.2541[0m       0.9154        0.2942        9.3701
     11   0.2362   0.7452        [35m0.2538[0m       0.9154        0.2915        9.2856
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 07:44:51,640][0m Trial 67 finished with value: 0.27127403950604734 and parameters: {'lr': 0.0021279527477548036, 'dropout': 0.5362180682278371, 'd_model_multiplier': 8, 'num_layers': 3, 'n_heads': 4, 'dim_feedforward': 445, 'batch_size': 46, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.01, 'top_n_features': 160}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 225
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.2760[0m   [32m0.7013[0m        [35m0.2688[0m       [31m0.9214[0m        [94m0.3080[0m     +  8.1934
      2   0.2408   0.6543        [35m0.2524[0m       0.9214        0.3214        8.8216
      3   0.2295   0.6412        [35m0.2490[0m       0.9214        0.3175        9.0170
      4   0.1924   0.6300        [35m0.2486[0m       0.9214        0.3400        8.8941
      5   0.1709   0.5872        [35m0.2439[0m       0.9214        0.3797        9.1068
      6   0.1683   0.5943        0.2447       0.9214        0.3685        8.9564
      7   0.1666   0.6242        [35m0.2439[0m       0.9214        0.3695        9.1239
      8   0.1819   0.6397        [35m0.2400[0m       0.9214        0.3629        9.3291
      9   0.2333   0.6661        [35m0.2368[0m       [31m0.9226[0m        0.3373        9.1229
     10   0.2258   0.6404        [35m0.2348[0m       0.9214        0.3444        9.0732
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 07:46:30,534][0m Trial 68 finished with value: 0.3080188526153709 and parameters: {'lr': 0.0004590088054253706, 'dropout': 0.47180516822424895, 'd_model_multiplier': 16, 'num_layers': 1, 'n_heads': 4, 'dim_feedforward': 475, 'batch_size': 28, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 225}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 197
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.1178[0m   [32m0.6496[0m        [35m0.6599[0m       [31m0.9154[0m        [94m0.6010[0m     +  8.9928
      2   0.0574   0.3358        [35m0.5574[0m       [31m0.9274[0m        [94m0.4866[0m     +  9.4547
      3   0.0584   0.3162        [35m0.4610[0m       0.9274        [94m0.3980[0m     +  9.6092
      4   0.0548   0.3186        [35m0.3875[0m       0.9274        [94m0.3278[0m     +  9.8133
      5   0.0612   0.4013        [35m0.3377[0m       0.9274        [94m0.2952[0m     +  9.4160
      6   0.0853   0.4667        [35m0.3105[0m       0.9274        [94m0.2789[0m     +  9.3881
      7   0.0946   0.5119        [35m0.2913[0m       0.9274        [94m0.2701[0m     +  9.4602
      8   0.1043   0.5506        [35m0.2814[0m       0.9274        [94m0.2648[0m     +  9.4080
      9   0.1120   0.5758        [35m0.2736[0m       0.9274        [94m0.2614[0m     +  9.4011
     10   [36m0.1208[0m   0.5972        [35m0.2703[0m       0.9274        [94m0.2593[0m     +  9.4900
     11   [36m0.1229[0m   0.6096        [35m0.2636[0m       0.9274        [94m0.2577[0m     +  9.7173
     12   [36m0.1258[0m   0.6233        [35m0.2626[0m       0.9274        [94m0.2563[0m     +  9.7165
     13   [36m0.1259[0m   0.6298        [35m0.2598[0m       0.9274        [94m0.2551[0m     +  9.7069
     14   [36m0.1272[0m   0.6334        [35m0.2585[0m       0.9274        [94m0.2543[0m     +  9.6950
     15   [36m0.1299[0m   0.6412        0.2596       0.9274        [94m0.2535[0m     +  10.3126
     16   [36m0.1333[0m   0.6475        [35m0.2569[0m       0.9274        [94m0.2530[0m     +  9.8554
     17   [36m0.1353[0m   [32m0.6566[0m        0.2579       0.9274        [94m0.2524[0m     +  10.2084
     18   [36m0.1373[0m   [32m0.6608[0m        0.2578       0.9274        [94m0.2522[0m     +  9.8081
     19   [36m0.1395[0m   [32m0.6656[0m        [35m0.2540[0m       0.9274        [94m0.2519[0m     +  9.7077
     20   [36m0.1406[0m   [32m0.6693[0m        0.2549       0.9274        [94m0.2513[0m     +  9.6491
     21   [36m0.1407[0m   [32m0.6720[0m        0.2557       0.9274        [94m0.2507[0m     +  9.7042
     22   [36m0.1437[0m   [32m0.6776[0m        [35m0.2533[0m       0.9274        [94m0.2500[0m     +  9.5058
     23   [36m0.1457[0m   [32m0.6797[0m        0.2560       0.9274        [94m0.2497[0m     +  9.4367
     24   [36m0.1477[0m   [32m0.6854[0m        0.2547       0.9274        [94m0.2488[0m     +  9.7042
     25   [36m0.1509[0m   [32m0.6954[0m        0.2572       0.9274        [94m0.2469[0m     +  9.3432
     26   [36m0.1570[0m   [32m0.7038[0m        [35m0.2522[0m       0.9274        [94m0.2459[0m     +  9.5411
     27   [36m0.1716[0m   [32m0.7172[0m        [35m0.2511[0m       0.9274        [94m0.2429[0m     +  9.5766
     28   [36m0.1940[0m   [32m0.7328[0m        [35m0.2471[0m       0.9274        [94m0.2386[0m     +  9.9973
     29   [36m0.2055[0m   [32m0.7421[0m        0.2484       0.9226        [94m0.2362[0m     +  9.9905
     30   [36m0.2124[0m   0.7411        [35m0.2446[0m       0.9226        [94m0.2362[0m     +  10.1934
     31   [36m0.2163[0m   [32m0.7464[0m        [35m0.2422[0m       0.9226        [94m0.2354[0m     +  9.6397
     32   [36m0.2264[0m   [32m0.7476[0m        0.2428       0.9250        [94m0.2342[0m     +  9.5983
     33   [36m0.2334[0m   [32m0.7507[0m        [35m0.2402[0m       0.9262        0.2342        9.9473
     34   0.2323   [32m0.7553[0m        0.2421       0.9274        [94m0.2338[0m     +  9.2199
     35   [36m0.2344[0m   [32m0.7578[0m        [35m0.2379[0m       0.9274        0.2343        9.3840
     36   0.2317   [32m0.7587[0m        0.2404       0.9262        0.2340        9.5177
     37   [36m0.2361[0m   [32m0.7592[0m        0.2380       0.9250        0.2339        9.3945
     38   [36m0.2401[0m   [32m0.7614[0m        0.2400       0.9250        [94m0.2335[0m     +  9.6412
     39   0.2354   [32m0.7626[0m        [35m0.2378[0m       0.9262        [94m0.2332[0m     +  9.6843
     40   0.2380   0.7619        0.2383       0.9262        0.2332        9.8328
     41   [36m0.2411[0m   [32m0.7651[0m        [35m0.2360[0m       0.9262        [94m0.2330[0m     +  9.9386
     42   [36m0.2437[0m   0.7649        [35m0.2346[0m       0.9250        [94m0.2328[0m     +  9.5960
     43   0.2410   [32m0.7683[0m        0.2350       0.9250        0.2330        9.5174
     44   0.2385   [32m0.7687[0m        [35m0.2343[0m       0.9250        0.2334        9.1923
     45   0.2385   0.7669        [35m0.2334[0m       0.9250        0.2346        9.5095
     46   0.2349   0.7668        [35m0.2332[0m       0.9250        0.2355        9.5589
     47   0.2317   0.7657        0.2341       0.9250        0.2368        9.6713
     48   0.2344   0.7638        [35m0.2310[0m       0.9262        0.2373        9.6409
     49   0.2364   0.7634        0.2344       0.9274        0.2365        9.4734
     50   0.2357   0.7618        [35m0.2303[0m       0.9274        0.2367        9.6678
[32m[I 2023-05-02 07:54:34,174][0m Trial 69 finished with value: 0.23276533390042858 and parameters: {'lr': 4.608827453242271e-05, 'dropout': 0.5112407754582766, 'd_model_multiplier': 4, 'num_layers': 4, 'n_heads': 4, 'dim_feedforward': 469, 'batch_size': 65, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.01, 'top_n_features': 197}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 214
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.2346[0m   [32m0.7083[0m        [35m0.2899[0m       [31m0.9178[0m        [94m0.2649[0m     +  8.9628
      2   0.2069   [32m0.7125[0m        [35m0.2580[0m       0.9178        0.2684        8.9866
      3   0.2060   [32m0.7170[0m        0.2590       0.9178        [94m0.2609[0m     +  9.1715
      4   0.2074   0.7159        0.2597       0.9178        0.2615        9.3258
      5   0.2132   0.7156        0.2600       0.9178        [94m0.2587[0m     +  9.5224
      6   0.2162   0.7155        [35m0.2572[0m       0.9178        0.2612        9.3646
      7   0.2229   0.7164        [35m0.2556[0m       0.9178        0.2599        9.4175
      8   0.2069   0.7118        0.2584       0.9178        0.2616        9.3145
      9   0.2136   0.7141        [35m0.2548[0m       0.9178        0.2611        9.2166
     10   0.2078   0.7160        0.2551       0.9178        0.2606        9.4631
     11   0.2038   0.7148        0.2548       0.9178        0.2659        9.4159
     12   0.2043   0.7144        [35m0.2522[0m       0.9178        0.2655        9.4012
     13   0.2085   0.7163        0.2539       0.9178        0.2629        9.4775
     14   0.2130   [32m0.7194[0m        0.2537       0.9178        0.2614        9.2400
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 07:56:54,305][0m Trial 70 finished with value: 0.25867271085045646 and parameters: {'lr': 0.0009348282911888275, 'dropout': 0.6186693177422892, 'd_model_multiplier': 1, 'num_layers': 2, 'n_heads': 8, 'dim_feedforward': 421, 'batch_size': 16, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 214}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 208
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.1841[0m   [32m0.6016[0m        [35m0.3910[0m       [31m0.9117[0m        [94m0.4241[0m     +  9.5345
      2   [36m0.2113[0m   [32m0.6787[0m        [35m0.2790[0m       0.9117        [94m0.3544[0m     +  9.8398
      3   [36m0.2513[0m   [32m0.7209[0m        [35m0.2690[0m       0.9117        [94m0.3395[0m     +  10.3259
      4   [36m0.2669[0m   0.7087        [35m0.2618[0m       0.9117        [94m0.3371[0m     +  9.9878
      5   0.2557   0.7006        [35m0.2604[0m       0.9117        [94m0.3098[0m     +  9.7494
      6   0.2633   0.7133        [35m0.2572[0m       0.9105        [94m0.2830[0m     +  10.1710
      7   0.2577   0.7163        [35m0.2546[0m       0.9117        [94m0.2802[0m     +  10.0387
      8   0.2653   0.7157        0.2557       0.9105        [94m0.2677[0m     +  9.6628
      9   [36m0.3030[0m   0.7119        0.2569       0.9117        [94m0.2671[0m     +  9.5579
     10   0.2518   0.7171        0.2547       0.9105        0.2681        9.8002
     11   0.2439   0.7159        0.2556       0.9081        0.2850        9.8662
     12   0.2866   0.7169        [35m0.2538[0m       0.9105        0.2787        9.7180
     13   0.2710   0.7172        [35m0.2532[0m       0.9117        0.2922        9.5702
     14   0.2738   0.7150        0.2543       0.9117        0.3355        9.8113
     15   0.2717   0.7017        [35m0.2515[0m       [31m0.9141[0m        0.2973        10.2265
     16   0.2796   0.7131        0.2532       0.9117        0.3228        10.1573
     17   0.2816   0.7170        0.2517       0.9117        0.3339        10.1301
     18   0.2751   [32m0.7220[0m        0.2516       0.9117        0.3424        9.8426
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 08:00:03,110][0m Trial 71 finished with value: 0.267074747270079 and parameters: {'lr': 0.002382209901729479, 'dropout': 0.5601367629823852, 'd_model_multiplier': 8, 'num_layers': 1, 'n_heads': 32, 'dim_feedforward': 308, 'batch_size': 31, 'pos_encoding': 'learnable', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.01, 'top_n_features': 208}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 221
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.2272[0m   [32m0.6984[0m        [35m0.2930[0m       [31m0.9226[0m        [94m0.2671[0m     +  8.5293
      2   0.2086   0.6973        [35m0.2637[0m       0.9226        0.2847        8.9988
      3   0.2126   [32m0.7014[0m        0.2654       0.9226        0.2720        8.7333
      4   0.1977   0.6989        [35m0.2595[0m       0.9226        0.2790        9.2987
      5   0.1909   0.6943        0.2602       0.9226        0.2801        9.0714
      6   0.1906   [32m0.7031[0m        [35m0.2576[0m       0.9214        0.2762        9.1379
      7   0.1912   0.6941        [35m0.2568[0m       0.9226        0.2965        9.2158
      8   0.1823   0.6899        0.2584       0.9226        0.2926        9.1871
      9   0.1834   0.6954        0.2572       0.9226        0.2908        9.1146
     10   0.1917   0.6993        0.2588       0.9226        0.2885        9.3322
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 08:01:43,218][0m Trial 72 finished with value: 0.26705948067006924 and parameters: {'lr': 0.00342106468710189, 'dropout': 0.547482156342165, 'd_model_multiplier': 8, 'num_layers': 1, 'n_heads': 4, 'dim_feedforward': 460, 'batch_size': 45, 'pos_encoding': 'learnable', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0, 'top_n_features': 221}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 182
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2341[0m   [32m0.7888[0m        [35m0.2884[0m       [31m0.9274[0m        [94m0.2228[0m     +  12.4841
      2   [36m0.2437[0m   0.7871        [35m0.2712[0m       0.8912        0.2437        12.7471
      3   0.2435   [32m0.7897[0m        [35m0.2648[0m       0.8851        0.2421        12.7642
      4   [36m0.2553[0m   [32m0.7951[0m        [35m0.2585[0m       0.8755        0.2815        12.6860
      5   [36m0.2810[0m   [32m0.8106[0m        [35m0.2571[0m       0.9045        0.2399        13.6762
      6   [36m0.2899[0m   [32m0.8121[0m        [35m0.2531[0m       0.9081        0.2377        12.6839
      7   0.2816   0.8082        [35m0.2469[0m       0.8863        0.2818        12.9133
      8   0.2844   0.7936        [35m0.2461[0m       0.9057        0.2348        12.5017
      9   0.2879   0.8027        [35m0.2439[0m       0.8827        0.2911        14.0914
     10   [36m0.3113[0m   0.8080        [35m0.2411[0m       0.9093        0.2350        12.8831
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 08:04:05,680][0m Trial 73 finished with value: 0.22284145740636746 and parameters: {'lr': 0.0005929258380783476, 'dropout': 0.5828601107843165, 'd_model_multiplier': 2, 'num_layers': 2, 'n_heads': 16, 'dim_feedforward': 322, 'batch_size': 8, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 182}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 171
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2410[0m   [32m0.7020[0m        [35m0.3307[0m       [31m0.9057[0m        [94m0.3080[0m     +  16.8260
      2   [36m0.3036[0m   [32m0.7119[0m        [35m0.2554[0m       [31m0.9081[0m        0.3222        18.0535
      3   [36m0.3104[0m   [32m0.7330[0m        [35m0.2490[0m       [31m0.9105[0m        [94m0.2861[0m     +  17.2570
      4   [36m0.3185[0m   [32m0.7642[0m        0.2517       0.8924        0.3094        17.5021
      5   0.2647   0.6825        [35m0.2475[0m       0.8755        0.3409        17.3876
      6   0.2472   0.6726        0.2873       0.9093        0.3205        17.8058
      7   0.2386   0.6193        0.2944       0.9093        0.3889        17.6898
      8   [36m0.3292[0m   [32m0.7748[0m        0.2855       0.8960        0.2984        17.8149
      9   0.0674   0.3556        0.3252       0.9081        0.5439        17.7060
     10   0.2718   0.6825        0.2874       0.9069        0.3345        17.5786
     11   [36m0.3295[0m   0.7635        0.2716       0.9069        [94m0.2815[0m     +  18.1535
     12   [36m0.3423[0m   [32m0.7842[0m        0.2664       0.9105        [94m0.2655[0m     +  17.7282
     13   0.0677   0.3541        0.2865       0.9081        0.6235        17.6252
     14   0.3107   0.7641        0.2561       0.9081        0.2871        17.7931
     15   0.3339   0.7644        0.2720       0.9081        0.3080        17.0639
     16   0.3170   0.7597        [35m0.2444[0m       0.9081        0.3060        17.8699
     17   [36m0.3538[0m   [32m0.7955[0m        0.2483       [31m0.9117[0m        [94m0.2578[0m     +  17.7328
     18   0.3513   0.7925        0.2537       0.9105        0.3054        17.7707
     19   [36m0.3561[0m   0.7952        [35m0.2372[0m       0.9069        0.2633        17.8634
     20   0.2212   0.6413        0.2822       0.9081        0.4113        18.2872
     21   0.3130   0.7607        0.2500       0.9105        0.3146        17.9856
     22   0.3511   0.7826        [35m0.2369[0m       0.9093        0.2972        17.5994
     23   0.3016   0.7403        0.2400       [31m0.9129[0m        0.3172        18.3466
     24   [36m0.3790[0m   [32m0.8052[0m        0.2393       0.9117        0.2738        17.6588
     25   [36m0.3820[0m   0.7989        0.2420       0.9081        0.2892        17.7712
     26   0.3478   0.7920        [35m0.2353[0m       [31m0.9141[0m        0.2910        18.0446
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 08:12:05,410][0m Trial 74 finished with value: 0.25783540672445815 and parameters: {'lr': 0.001292817439769928, 'dropout': 0.5446785766769426, 'd_model_multiplier': 64, 'num_layers': 1, 'n_heads': 16, 'dim_feedforward': 499, 'batch_size': 245, 'pos_encoding': 'learnable', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.01, 'top_n_features': 171}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 192
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
Warning, assumed runtime error: CUDA out of memory. Tried to allocate 5.42 GiB (GPU 0; 23.70 GiB total capacity; 19.90 GiB already allocated; 575.25 MiB free; 22.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[32m[I 2023-05-02 08:12:12,939][0m Trial 75 finished with value: 100.0 and parameters: {'lr': 0.0002371647997190961, 'dropout': 0.5642163960127589, 'd_model_multiplier': 8, 'num_layers': 2, 'n_heads': 64, 'dim_feedforward': 208, 'batch_size': 229, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.001, 'top_n_features': 192}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 146
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.2222[0m   [32m0.7255[0m        [35m0.3034[0m       [31m0.9238[0m        [94m0.3276[0m     +  9.1134
      2   [36m0.2914[0m   0.7042        [35m0.2692[0m       [31m0.9250[0m        [94m0.2466[0m     +  9.0195
      3   [36m0.3332[0m   [32m0.7545[0m        [35m0.2605[0m       [31m0.9274[0m        [94m0.2369[0m     +  9.1143
      4   0.3217   [32m0.7640[0m        [35m0.2509[0m       0.9262        [94m0.2291[0m     +  9.0312
      5   0.3172   0.7584        [35m0.2452[0m       0.9262        0.2322        9.3954
      6   [36m0.3368[0m   [32m0.7747[0m        [35m0.2417[0m       [31m0.9287[0m        [94m0.2281[0m     +  8.9467
      7   0.3350   0.7671        [35m0.2375[0m       [31m0.9311[0m        [94m0.2270[0m     +  9.3097
      8   0.3219   [32m0.7759[0m        [35m0.2332[0m       0.9287        0.2315        9.1582
      9   [36m0.3596[0m   [32m0.7857[0m        0.2342       0.9299        [94m0.2206[0m     +  9.2838
     10   0.3350   0.7843        0.2338       0.9299        0.2246        9.5016
     11   0.3290   0.7700        0.2365       0.9299        0.2261        9.4969
     12   0.3248   0.7689        0.2346       0.9008        0.2886        9.3375
     13   0.3500   0.7794        0.2341       0.9287        0.2229        9.1007
     14   0.3316   0.7777        [35m0.2283[0m       0.9238        0.2283        9.0182
     15   0.3362   0.7844        [35m0.2275[0m       0.9287        0.2211        9.0824
     16   0.3476   [32m0.7954[0m        [35m0.2259[0m       0.9226        0.2237        9.4715
     17   0.3327   0.7846        [35m0.2259[0m       0.9250        0.2289        9.4880
     18   0.3206   0.7833        0.2294       0.9129        0.2398        9.1435
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 08:15:08,802][0m Trial 76 finished with value: 0.22059940673780268 and parameters: {'lr': 0.008886941823250097, 'dropout': 0.5182677363890442, 'd_model_multiplier': 2, 'num_layers': 3, 'n_heads': 4, 'dim_feedforward': 187, 'batch_size': 54, 'pos_encoding': 'learnable', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.01, 'top_n_features': 146}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 145
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.2271[0m   [32m0.6790[0m        [35m0.3544[0m       [31m0.9226[0m        [94m0.2520[0m     +  9.1423
      2   0.1884   [32m0.7132[0m        [35m0.2656[0m       0.9226        [94m0.2422[0m     +  8.8488
      3   0.2085   [32m0.7494[0m        [35m0.2578[0m       0.9226        [94m0.2374[0m     +  9.2604
      4   [36m0.2716[0m   [32m0.7932[0m        [35m0.2527[0m       0.9202        [94m0.2301[0m     +  8.9042
      5   [36m0.2931[0m   [32m0.7977[0m        [35m0.2455[0m       0.9214        [94m0.2264[0m     +  9.3951
      6   [36m0.3139[0m   [32m0.8053[0m        [35m0.2375[0m       0.9226        [94m0.2237[0m     +  9.1263
      7   [36m0.3301[0m   [32m0.8126[0m        0.2392       0.9226        [94m0.2231[0m     +  9.1805
      8   [36m0.3338[0m   [32m0.8133[0m        [35m0.2357[0m       [31m0.9238[0m        [94m0.2223[0m     +  9.4093
      9   [36m0.3444[0m   0.8127        [35m0.2352[0m       [31m0.9250[0m        0.2264        9.4679
     10   [36m0.3572[0m   [32m0.8158[0m        [35m0.2331[0m       [31m0.9262[0m        0.2236        9.3070
     11   [36m0.3648[0m   0.8142        [35m0.2328[0m       0.9250        [94m0.2204[0m     +  9.2684
     12   [36m0.3653[0m   0.8118        [35m0.2315[0m       0.9250        0.2213        11.0325
     13   [36m0.3749[0m   [32m0.8165[0m        [35m0.2291[0m       0.9202        0.2248        10.4330
     14   0.3727   0.8157        [35m0.2271[0m       0.9214        0.2234        9.6385
     15   0.3726   0.8148        [35m0.2264[0m       0.9226        0.2247        9.4751
     16   0.3609   [32m0.8174[0m        [35m0.2249[0m       0.9250        0.2209        9.5581
     17   0.3691   0.8069        [35m0.2248[0m       0.9226        0.2300        9.4708
     18   0.3515   0.8131        [35m0.2220[0m       0.9238        0.2263        9.3831
     19   0.3512   0.8144        0.2241       0.9238        0.2292        9.6185
     20   0.3540   [32m0.8259[0m        0.2241       0.9250        0.2237        9.4211
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 08:18:28,022][0m Trial 77 finished with value: 0.2203884096467077 and parameters: {'lr': 0.0003637563724270303, 'dropout': 0.45624459652883975, 'd_model_multiplier': 2, 'num_layers': 3, 'n_heads': 4, 'dim_feedforward': 136, 'batch_size': 55, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.01, 'top_n_features': 145}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 141
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.1948[0m   [32m0.6999[0m        [35m0.3906[0m       [31m0.9250[0m        [94m0.2771[0m     +  8.9218
      2   [36m0.2172[0m   [32m0.7397[0m        [35m0.2715[0m       0.9250        [94m0.2499[0m     +  9.7074
      3   [36m0.2279[0m   [32m0.7517[0m        [35m0.2590[0m       0.9250        [94m0.2459[0m     +  9.5870
      4   0.2255   [32m0.7564[0m        [35m0.2585[0m       0.9250        0.2478        9.4957
      5   [36m0.2373[0m   0.7542        [35m0.2537[0m       0.9250        0.2468        9.2861
      6   [36m0.2656[0m   [32m0.7593[0m        [35m0.2515[0m       0.9250        [94m0.2360[0m     +  9.6652
      7   [36m0.2671[0m   [32m0.7635[0m        [35m0.2506[0m       0.9250        [94m0.2358[0m     +  9.6678
      8   0.2540   0.7620        [35m0.2453[0m       0.9250        [94m0.2335[0m     +  9.7446
      9   0.2470   [32m0.7684[0m        [35m0.2437[0m       0.9250        [94m0.2332[0m     +  9.5655
     10   0.2627   [32m0.7713[0m        [35m0.2374[0m       0.9202        [94m0.2307[0m     +  9.9051
     11   0.2602   [32m0.7738[0m        [35m0.2369[0m       0.9250        0.2307        9.7229
     12   0.2645   [32m0.7794[0m        0.2380       0.9226        0.2327        9.7977
     13   0.2619   [32m0.7841[0m        [35m0.2361[0m       0.9250        0.2315        9.7526
     14   [36m0.2754[0m   0.7803        [35m0.2336[0m       0.9238        [94m0.2299[0m     +  9.9291
     15   [36m0.2933[0m   [32m0.7908[0m        [35m0.2306[0m       0.9238        [94m0.2256[0m     +  9.9735
     16   0.2889   [32m0.7920[0m        [35m0.2290[0m       0.9226        0.2351        9.9571
     17   [36m0.3008[0m   0.7890        [35m0.2279[0m       [31m0.9262[0m        0.2282        9.9547
     18   [36m0.3137[0m   [32m0.7974[0m        0.2285       0.9238        [94m0.2237[0m     +  9.8002
     19   [36m0.3154[0m   0.7945        0.2280       0.9250        0.2242        9.7537
     20   [36m0.3251[0m   [32m0.8016[0m        [35m0.2274[0m       0.9117        0.2506        9.4969
     21   [36m0.3264[0m   0.7938        [35m0.2231[0m       0.9178        0.2337        9.7622
     22   0.3247   0.7962        0.2262       0.9226        0.2280        9.6688
     23   0.3164   0.7977        0.2236       0.9154        0.2372        9.8480
     24   [36m0.3311[0m   0.7871        0.2251       [31m0.9299[0m        [94m0.2214[0m     +  10.0160
     25   0.3280   0.7847        [35m0.2219[0m       0.9262        0.2242        9.8750
     26   [36m0.3371[0m   0.7967        [35m0.2217[0m       [31m0.9311[0m        0.2275        10.0128
     27   0.3347   0.7923        0.2218       0.9274        0.2224        10.1117
     28   0.3316   0.7940        [35m0.2200[0m       0.9274        0.2248        10.0753
     29   0.3094   0.7861        [35m0.2192[0m       0.9202        0.2309        10.0800
     30   0.3179   0.7864        [35m0.2174[0m       0.9250        0.2310        9.5320
     31   [36m0.3371[0m   0.7992        0.2193       0.9287        0.2224        9.9003
     32   [36m0.3404[0m   0.7930        [35m0.2169[0m       0.9274        0.2260        9.8712
     33   0.3259   0.7902        0.2176       0.9274        0.2228        9.8137
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 08:24:00,961][0m Trial 78 finished with value: 0.22142240484363368 and parameters: {'lr': 0.0017944589616882955, 'dropout': 0.5184164851893082, 'd_model_multiplier': 2, 'num_layers': 3, 'n_heads': 4, 'dim_feedforward': 138, 'batch_size': 59, 'pos_encoding': 'learnable', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.01, 'top_n_features': 141}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 151
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.2101[0m   [32m0.6217[0m        [35m0.7334[0m       [31m0.2624[0m        [94m0.6969[0m     +  9.5442
      2   0.1190   0.4443        [35m0.6919[0m       [31m0.6784[0m        [94m0.6874[0m     +  10.5054
      3   0.0568   0.3079        [35m0.6416[0m       [31m0.9069[0m        [94m0.6722[0m     +  10.5857
      4   0.0552   0.2859        [35m0.5772[0m       [31m0.9154[0m        [94m0.6458[0m     +  10.3561
      5   0.0561   0.2937        [35m0.4975[0m       0.9154        [94m0.5807[0m     +  10.6091
      6   0.0592   0.3340        [35m0.4140[0m       0.9154        [94m0.5007[0m     +  10.8686
      7   0.0706   0.4352        [35m0.3521[0m       0.9154        [94m0.4349[0m     +  10.8931
      8   0.1397   0.5533        [35m0.3182[0m       0.9154        [94m0.3922[0m     +  11.1882
      9   0.2069   [32m0.6386[0m        [35m0.2989[0m       0.9154        [94m0.3620[0m     +  10.4888
     10   [36m0.2499[0m   [32m0.6907[0m        [35m0.2881[0m       0.9154        [94m0.3418[0m     +  10.4070
     11   [36m0.2733[0m   [32m0.7208[0m        [35m0.2804[0m       0.9154        [94m0.3259[0m     +  10.6385
     12   [36m0.2882[0m   [32m0.7400[0m        [35m0.2776[0m       0.9154        [94m0.3139[0m     +  10.7290
     13   [36m0.3002[0m   [32m0.7533[0m        [35m0.2695[0m       0.9154        [94m0.3024[0m     +  10.4488
     14   0.2981   [32m0.7620[0m        0.2704       0.9154        [94m0.2991[0m     +  10.7209
     15   [36m0.3046[0m   [32m0.7713[0m        [35m0.2686[0m       0.9141        [94m0.2918[0m     +  10.5983
     16   0.3014   [32m0.7752[0m        [35m0.2641[0m       0.9141        [94m0.2857[0m     +  10.5891
     17   [36m0.3060[0m   [32m0.7781[0m        [35m0.2593[0m       0.9154        [94m0.2834[0m     +  10.5242
     18   [36m0.3138[0m   0.7775        0.2617       0.9154        [94m0.2809[0m     +  10.7841
     19   [36m0.3230[0m   [32m0.7849[0m        0.2594       0.9141        [94m0.2764[0m     +  11.0752
     20   [36m0.3265[0m   0.7843        0.2602       0.9141        0.2771        10.8675
     21   [36m0.3278[0m   [32m0.7885[0m        [35m0.2559[0m       0.9141        [94m0.2735[0m     +  10.7337
     22   [36m0.3317[0m   [32m0.7916[0m        0.2564       0.9141        [94m0.2722[0m     +  10.5603
     23   0.3275   [32m0.7932[0m        0.2566       0.9141        0.2741        10.5620
     24   0.3292   [32m0.7942[0m        0.2572       0.9141        0.2723        10.7051
     25   0.3283   [32m0.7949[0m        0.2560       0.9154        [94m0.2699[0m     +  10.5748
     26   [36m0.3330[0m   [32m0.7950[0m        [35m0.2546[0m       0.9154        [94m0.2683[0m     +  10.7409
     27   0.3325   [32m0.8003[0m        [35m0.2532[0m       0.9154        [94m0.2667[0m     +  10.6417
     28   [36m0.3336[0m   [32m0.8043[0m        0.2536       0.9154        [94m0.2665[0m     +  10.7260
     29   [36m0.3353[0m   [32m0.8081[0m        0.2534       0.9154        [94m0.2650[0m     +  10.3565
     30   [36m0.3357[0m   [32m0.8091[0m        [35m0.2531[0m       0.9154        0.2673        10.6888
     31   [36m0.3385[0m   [32m0.8097[0m        [35m0.2496[0m       [31m0.9166[0m        0.2652        10.6563
     32   0.3364   [32m0.8106[0m        0.2510       0.9166        [94m0.2629[0m     +  10.7243
     33   [36m0.3415[0m   [32m0.8124[0m        0.2509       [31m0.9178[0m        0.2630        10.2855
     34   [36m0.3451[0m   [32m0.8126[0m        0.2516       0.9166        [94m0.2612[0m     +  10.3608
     35   [36m0.3473[0m   [32m0.8150[0m        0.2512       0.9166        [94m0.2590[0m     +  10.5344
     36   [36m0.3473[0m   [32m0.8152[0m        [35m0.2491[0m       0.9166        [94m0.2569[0m     +  10.7903
     37   [36m0.3474[0m   [32m0.8160[0m        [35m0.2459[0m       0.9166        0.2575        10.8466
     38   [36m0.3488[0m   [32m0.8170[0m        0.2484       0.9166        0.2573        10.6699
     39   [36m0.3516[0m   0.8138        0.2466       0.9166        [94m0.2569[0m     +  10.5857
     40   0.3503   0.8106        0.2502       0.9166        0.2583        10.5753
     41   [36m0.3550[0m   0.8123        0.2470       0.9166        [94m0.2569[0m     +  10.4279
     42   [36m0.3563[0m   0.8159        [35m0.2458[0m       0.9178        0.2569        10.6391
     43   0.3543   0.8162        [35m0.2437[0m       0.9178        [94m0.2557[0m     +  10.9802
     44   [36m0.3610[0m   [32m0.8171[0m        0.2473       0.9178        [94m0.2549[0m     +  10.4925
     45   [36m0.3631[0m   0.8159        0.2481       [31m0.9190[0m        [94m0.2536[0m     +  10.7088
     46   [36m0.3635[0m   0.8164        0.2485       0.9190        0.2540        10.5181
     47   [36m0.3659[0m   0.8152        0.2441       0.9166        [94m0.2525[0m     +  11.0609
     48   [36m0.3682[0m   [32m0.8174[0m        0.2440       0.9178        [94m0.2522[0m     +  10.6466
     49   [36m0.3755[0m   0.8153        0.2448       0.9178        0.2524        10.6673
     50   0.3720   0.8157        [35m0.2432[0m       0.9190        0.2523        10.5655
[32m[I 2023-05-02 08:32:54,631][0m Trial 79 finished with value: 0.2521966115234841 and parameters: {'lr': 0.00010822365965513408, 'dropout': 0.4443593562987422, 'd_model_multiplier': 2, 'num_layers': 5, 'n_heads': 4, 'dim_feedforward': 180, 'batch_size': 107, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.01, 'top_n_features': 151}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 147
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.1578[0m   [32m0.6762[0m        [35m0.3001[0m       [31m0.9371[0m        [94m0.2456[0m     +  8.8913
      2   0.1359   0.6490        [35m0.2743[0m       0.9371        [94m0.2443[0m     +  8.4291
      3   0.1198   0.6391        0.2774       0.9371        [94m0.2380[0m     +  9.1997
      4   0.1085   0.6261        [35m0.2650[0m       0.9371        0.2432        8.9676
      5   0.1163   0.6675        [35m0.2611[0m       0.9371        [94m0.2371[0m     +  9.3831
      6   0.1143   0.6668        0.2619       0.9371        [94m0.2315[0m     +  9.3586
      7   0.1138   0.6690        [35m0.2588[0m       0.9371        0.2366        9.5227
      8   0.1208   0.6709        [35m0.2587[0m       0.9371        [94m0.2304[0m     +  9.0316
      9   0.1163   0.6730        [35m0.2583[0m       0.9371        [94m0.2278[0m     +  9.1567
     10   0.1177   0.6704        0.2591       0.9371        0.2280        9.3473
     11   0.1183   0.6721        [35m0.2568[0m       0.9371        0.2324        9.3845
     12   0.1163   0.6709        [35m0.2558[0m       0.9371        [94m0.2275[0m     +  9.1434
     13   0.1150   0.6703        [35m0.2547[0m       0.9371        0.2279        9.2807
     14   0.1181   0.6742        0.2558       0.9371        0.2302        9.3972
     15   0.1153   0.6715        0.2565       0.9371        [94m0.2270[0m     +  9.2696
     16   0.1153   0.6708        [35m0.2545[0m       0.9371        0.2281        8.8503
     17   0.1221   0.6722        0.2558       0.9371        0.2300        9.2698
     18   0.1190   0.6726        0.2550       0.9371        0.2295        9.1855
     19   0.1175   0.6732        0.2555       0.9371        0.2308        9.3830
     20   0.1338   0.6731        [35m0.2544[0m       0.9371        0.2323        9.3789
     21   0.1173   0.6739        0.2555       0.9371        0.2305        9.4100
     22   0.1277   [32m0.6770[0m        [35m0.2544[0m       0.9371        0.2321        9.2156
     23   0.1365   0.6690        0.2544       [31m0.9383[0m        0.2377        9.4787
     24   0.1193   [32m0.6771[0m        [35m0.2532[0m       0.9371        0.2360        9.1742
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 08:36:45,707][0m Trial 80 finished with value: 0.2270017885743779 and parameters: {'lr': 0.008875795915962307, 'dropout': 0.5830929373937257, 'd_model_multiplier': 2, 'num_layers': 3, 'n_heads': 4, 'dim_feedforward': 152, 'batch_size': 52, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.01, 'top_n_features': 147}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 134
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.1879[0m   [32m0.7034[0m        [35m0.3070[0m       [31m0.9214[0m        [94m0.2557[0m     +  8.5469
      2   [36m0.2236[0m   [32m0.7531[0m        [35m0.2486[0m       0.9129        [94m0.2494[0m     +  8.7059
      3   [36m0.2488[0m   [32m0.7647[0m        [35m0.2462[0m       0.9141        [94m0.2425[0m     +  9.2043
      4   [36m0.2632[0m   [32m0.7709[0m        [35m0.2392[0m       0.9190        [94m0.2421[0m     +  9.4248
      5   0.2544   [32m0.7794[0m        0.2397       0.9214        [94m0.2393[0m     +  9.1296
      6   [36m0.2634[0m   0.7778        [35m0.2390[0m       [31m0.9226[0m        0.2414        9.2742
      7   [36m0.2671[0m   [32m0.7926[0m        [35m0.2386[0m       0.9214        [94m0.2338[0m     +  9.2492
      8   [36m0.2735[0m   [32m0.8003[0m        [35m0.2341[0m       0.9226        [94m0.2322[0m     +  9.4545
      9   [36m0.2941[0m   [32m0.8088[0m        [35m0.2333[0m       0.9202        [94m0.2290[0m     +  9.3420
     10   0.2767   0.8087        [35m0.2306[0m       0.9226        0.2317        9.2883
     11   0.2810   [32m0.8102[0m        0.2324       0.9202        [94m0.2284[0m     +  9.1154
     12   0.2791   [32m0.8166[0m        [35m0.2259[0m       0.9214        0.2300        9.4462
     13   0.2864   [32m0.8198[0m        [35m0.2259[0m       0.9214        [94m0.2258[0m     +  9.2790
     14   0.2723   [32m0.8210[0m        0.2264       0.9190        0.2273        9.3975
     15   0.2843   0.8148        [35m0.2228[0m       0.9214        0.2303        9.2378
     16   0.2876   0.8165        0.2238       0.9214        0.2284        9.4814
     17   0.2879   0.8173        [35m0.2220[0m       0.9214        0.2310        9.2524
     18   0.2875   0.8154        0.2238       0.9214        0.2306        9.2382
     19   0.2815   0.8121        0.2226       0.9226        0.2311        9.1649
     20   0.2880   0.8173        [35m0.2204[0m       0.9226        0.2287        9.2775
     21   0.2870   0.8140        [35m0.2200[0m       [31m0.9238[0m        0.2315        9.2540
     22   0.2836   [32m0.8215[0m        [35m0.2199[0m       0.9202        0.2260        9.1619
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 08:40:19,057][0m Trial 81 finished with value: 0.22582951431334955 and parameters: {'lr': 0.00033877973317954685, 'dropout': 0.4915538525026508, 'd_model_multiplier': 2, 'num_layers': 2, 'n_heads': 4, 'dim_feedforward': 156, 'batch_size': 19, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.01, 'top_n_features': 134}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 156
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.1797[0m   [32m0.7381[0m        [35m0.2837[0m       [31m0.9359[0m        [94m0.2192[0m     +  8.5135
      2   [36m0.2029[0m   [32m0.7781[0m        [35m0.2489[0m       0.9178        0.2219        9.4676
      3   [36m0.2194[0m   [32m0.7904[0m        [35m0.2412[0m       0.9202        [94m0.2162[0m     +  9.2837
      4   0.2187   0.7874        [35m0.2377[0m       0.9190        0.2199        9.2103
      5   [36m0.2278[0m   [32m0.7948[0m        [35m0.2361[0m       0.9166        0.2199        9.3474
      6   0.2123   0.7929        0.2371       0.9129        0.2354        9.5644
      7   [36m0.2347[0m   0.7833        [35m0.2317[0m       0.9105        0.2362        9.5050
      8   0.2210   0.7817        0.2328       0.9129        0.2391        9.7629
      9   0.2176   [32m0.7955[0m        [35m0.2298[0m       0.9166        0.2431        9.5554
     10   0.2217   0.7897        0.2305       0.9190        0.2392        9.3015
     11   0.2176   0.7867        [35m0.2265[0m       0.9190        0.2277        9.5626
     12   0.2098   0.7841        [35m0.2261[0m       0.9190        0.2302        9.4480
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 08:42:21,804][0m Trial 82 finished with value: 0.21620399053540743 and parameters: {'lr': 0.00018035372028989924, 'dropout': 0.4644537301079474, 'd_model_multiplier': 16, 'num_layers': 4, 'n_heads': 4, 'dim_feedforward': 131, 'batch_size': 42, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.01, 'top_n_features': 156}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 164
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.2813[0m   [32m0.7467[0m        [35m0.2789[0m       [31m0.9093[0m        [94m0.2754[0m     +  9.2355
      2   [36m0.2841[0m   [32m0.7736[0m        [35m0.2434[0m       0.9069        [94m0.2706[0m     +  9.2198
      3   [36m0.2900[0m   [32m0.7769[0m        [35m0.2383[0m       0.8984        0.2797        9.2356
      4   [36m0.2967[0m   [32m0.7806[0m        [35m0.2353[0m       0.8996        0.2732        9.2918
      5   0.2911   0.7747        [35m0.2342[0m       0.8984        0.2791        9.1705
      6   [36m0.3080[0m   0.7797        [35m0.2306[0m       0.8863        0.2901        9.4216
      7   [36m0.3188[0m   0.7775        [35m0.2275[0m       0.8912        0.2824        9.3938
      8   [36m0.3202[0m   0.7790        [35m0.2273[0m       0.8936        0.2804        9.2440
      9   [36m0.3259[0m   0.7712        [35m0.2245[0m       0.8815        0.2960        9.5468
     10   [36m0.3350[0m   0.7776        [35m0.2238[0m       0.8936        0.2838        9.3234
     11   0.2915   0.7750        0.2276       0.8839        0.2888        9.3393
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 08:44:13,876][0m Trial 83 finished with value: 0.2705514370458158 and parameters: {'lr': 0.00017966129700450474, 'dropout': 0.4671047259766067, 'd_model_multiplier': 16, 'num_layers': 4, 'n_heads': 4, 'dim_feedforward': 136, 'batch_size': 41, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.01, 'top_n_features': 164}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 123
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.2789[0m   [32m0.7910[0m        [35m0.3047[0m       [31m0.9335[0m        [94m0.2082[0m     +  8.3891
      2   0.2577   0.7855        [35m0.2589[0m       0.9274        0.2129        8.3560
      3   0.2413   [32m0.8037[0m        [35m0.2462[0m       0.9274        0.2118        9.1190
      4   0.2546   0.7981        [35m0.2448[0m       0.9250        0.2173        9.0257
      5   0.2562   0.8035        [35m0.2406[0m       0.9311        0.2104        9.2691
      6   0.2482   0.8010        [35m0.2381[0m       0.9262        0.2132        9.2257
      7   0.2654   [32m0.8072[0m        [35m0.2370[0m       0.9287        0.2150        9.1020
      8   0.2369   0.8025        [35m0.2325[0m       0.9262        0.2250        9.1739
      9   0.2399   0.8054        0.2339       0.9262        0.2251        9.5655
     10   0.2438   0.7991        [35m0.2311[0m       0.9287        0.2206        9.3463
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 08:45:53,805][0m Trial 84 finished with value: 0.20818104376924082 and parameters: {'lr': 0.001040644795444706, 'dropout': 0.4509004619481119, 'd_model_multiplier': 2, 'num_layers': 3, 'n_heads': 4, 'dim_feedforward': 130, 'batch_size': 54, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.01, 'top_n_features': 123}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 118
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.2689[0m   [32m0.7401[0m        [35m0.2685[0m       [31m0.9166[0m        [94m0.2612[0m     +  8.6305
      2   [36m0.3272[0m   [32m0.7771[0m        [35m0.2454[0m       0.9166        [94m0.2476[0m     +  9.3656
      3   [36m0.3346[0m   0.7741        [35m0.2371[0m       [31m0.9178[0m        0.2546        9.4662
      4   0.3345   0.7674        0.2380       0.9154        0.2713        9.7114
      5   0.3262   0.7735        [35m0.2328[0m       0.9154        0.2686        9.6376
      6   0.3243   [32m0.7981[0m        [35m0.2296[0m       [31m0.9190[0m        0.2536        9.5268
      7   0.2877   0.7724        [35m0.2291[0m       0.9154        0.2883        9.7065
      8   0.2951   0.7853        [35m0.2254[0m       0.9154        0.2839        9.4919
      9   0.3042   0.7848        [35m0.2248[0m       0.9117        0.2793        9.7521
     10   0.3145   0.7828        [35m0.2236[0m       0.9141        0.2754        9.6968
     11   0.3139   0.7912        [35m0.2233[0m       0.9166        0.2646        9.8248
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 08:47:48,453][0m Trial 85 finished with value: 0.2475891887709192 and parameters: {'lr': 0.0008712008290199911, 'dropout': 0.43589452845313653, 'd_model_multiplier': 16, 'num_layers': 4, 'n_heads': 4, 'dim_feedforward': 131, 'batch_size': 67, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.01, 'top_n_features': 118}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 100
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1574[0m   [32m0.6127[0m        [35m0.3238[0m       [31m0.8416[0m        [94m0.4267[0m     +  19.7282
      2   0.1505   0.6100        [35m0.2874[0m       [31m0.8634[0m        0.4470        19.5889
      3   [36m0.1765[0m   [32m0.6875[0m        [35m0.2659[0m       [31m0.8815[0m        [94m0.3695[0m     +  19.8310
      4   0.1418   0.6169        [35m0.2650[0m       0.8392        0.4381        19.8990
      5   0.1397   0.6100        [35m0.2643[0m       0.8537        0.4001        19.8613
      6   0.1527   0.6257        [35m0.2617[0m       0.8670        0.3907        19.8197
      7   0.1518   0.6305        [35m0.2578[0m       0.8742        0.3828        19.7974
      8   0.1510   0.6236        [35m0.2560[0m       [31m0.8863[0m        0.3803        19.9834
      9   0.1512   0.6333        [35m0.2542[0m       [31m0.9045[0m        [94m0.3587[0m     +  19.7013
     10   0.1547   0.6316        [35m0.2530[0m       [31m0.9057[0m        [94m0.3579[0m     +  19.8211
     11   0.1515   0.6276        [35m0.2501[0m       0.9057        0.3613        19.8251
     12   0.1486   0.6253        0.2520       [31m0.9069[0m        [94m0.3562[0m     +  19.7371
     13   0.1519   0.6304        0.2506       0.9033        0.3655        20.0219
     14   0.1561   0.6403        0.2512       0.9033        0.3650        19.9084
     15   0.1559   0.6412        0.2506       [31m0.9081[0m        0.3628        19.8810
     16   0.1556   0.6436        [35m0.2485[0m       0.9081        0.3661        19.9050
     17   0.1540   0.6400        0.2511       [31m0.9093[0m        0.3643        19.7252
     18   0.1519   0.6316        [35m0.2478[0m       0.9081        0.3688        20.1498
     19   0.1531   0.6360        [35m0.2476[0m       0.9093        0.3634        19.9977
     20   0.1589   0.6425        0.2479       0.9093        0.3656        19.9794
     21   0.1541   0.6360        0.2496       0.9057        0.3695        19.9141
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 08:55:06,557][0m Trial 86 finished with value: 0.35616946169946617 and parameters: {'lr': 0.0006534965225718603, 'dropout': 0.45128046963575724, 'd_model_multiplier': 16, 'num_layers': 3, 'n_heads': 32, 'dim_feedforward': 162, 'batch_size': 44, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 100}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 126
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2627[0m   [32m0.7406[0m        [35m0.2692[0m       [31m0.9033[0m        [94m0.2835[0m     +  11.3236
      2   [36m0.2926[0m   [32m0.7763[0m        [35m0.2478[0m       [31m0.9093[0m        [94m0.2691[0m     +  11.3822
      3   [36m0.3371[0m   [32m0.8074[0m        [35m0.2410[0m       0.9093        [94m0.2547[0m     +  11.5251
      4   0.3288   [32m0.8149[0m        [35m0.2339[0m       0.9057        0.2598        11.7376
      5   [36m0.3544[0m   [32m0.8207[0m        [35m0.2336[0m       0.9057        0.2555        11.5913
      6   0.3427   [32m0.8237[0m        [35m0.2302[0m       0.9093        [94m0.2547[0m     +  11.6217
      7   0.3307   0.8153        0.2312       0.8996        0.2579        11.7521
      8   0.3316   0.8193        [35m0.2277[0m       0.9021        0.2600        11.6575
      9   0.3449   0.8196        [35m0.2266[0m       0.9069        0.2552        11.5516
     10   0.3360   0.8225        [35m0.2226[0m       0.9033        0.2583        11.5789
     11   0.3322   0.8190        0.2234       0.9057        0.2579        11.6800
     12   0.3463   [32m0.8250[0m        [35m0.2182[0m       0.9093        0.2583        11.6980
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 08:57:37,777][0m Trial 87 finished with value: 0.2546677276618247 and parameters: {'lr': 0.00023270298157781338, 'dropout': 0.4605689626447939, 'd_model_multiplier': 16, 'num_layers': 5, 'n_heads': 8, 'dim_feedforward': 138, 'batch_size': 52, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.01, 'top_n_features': 126}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 81
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.0685[0m   [32m0.3414[0m        [35m0.6753[0m       [31m0.9238[0m        [94m0.5749[0m     +  9.4453
      2   0.0610   0.3320        [35m0.4950[0m       0.9238        [94m0.4014[0m     +  9.4954
      3   0.0617   [32m0.3508[0m        [35m0.3960[0m       0.9238        [94m0.3507[0m     +  9.6841
      4   [36m0.1076[0m   [32m0.5252[0m        [35m0.3473[0m       0.9238        [94m0.2982[0m     +  9.8854
      5   [36m0.1521[0m   [32m0.6689[0m        [35m0.3073[0m       0.9238        [94m0.2685[0m     +  9.9347
      6   [36m0.1900[0m   [32m0.7039[0m        [35m0.2857[0m       0.9238        [94m0.2561[0m     +  9.9230
      7   [36m0.2114[0m   [32m0.7154[0m        [35m0.2767[0m       0.9238        [94m0.2500[0m     +  9.8805
      8   [36m0.2145[0m   [32m0.7193[0m        [35m0.2718[0m       0.9238        [94m0.2468[0m     +  10.1060
      9   [36m0.2181[0m   [32m0.7232[0m        [35m0.2675[0m       0.9238        [94m0.2448[0m     +  10.1853
     10   [36m0.2222[0m   0.7228        0.2676       0.9238        [94m0.2435[0m     +  9.8857
     11   0.2168   [32m0.7247[0m        0.2701       0.9238        [94m0.2428[0m     +  10.1060
     12   0.2200   [32m0.7249[0m        0.2682       0.9238        [94m0.2422[0m     +  9.9756
     13   0.2165   [32m0.7253[0m        [35m0.2646[0m       0.9238        [94m0.2420[0m     +  10.1169
     14   0.2176   0.7250        0.2661       0.9238        [94m0.2416[0m     +  9.8841
     15   0.2161   0.7251        0.2685       0.9238        [94m0.2416[0m     +  9.8928
     16   0.2183   0.7250        [35m0.2635[0m       0.9238        [94m0.2414[0m     +  10.0402
     17   0.2211   0.7242        [35m0.2624[0m       0.9238        [94m0.2413[0m     +  10.0222
     18   [36m0.2223[0m   0.7240        0.2656       0.9238        0.2414        10.0076
     19   0.2216   0.7234        0.2625       0.9238        [94m0.2413[0m     +  9.8206
     20   0.2205   0.7236        0.2645       0.9238        [94m0.2412[0m     +  9.9786
     21   0.2189   0.7226        [35m0.2610[0m       0.9238        0.2412        9.9102
     22   0.2166   0.7222        0.2622       0.9238        0.2413        10.0002
     23   0.2156   0.7220        [35m0.2599[0m       0.9238        0.2413        9.9278
     24   0.2152   0.7196        0.2625       0.9238        0.2413        9.9853
     25   0.2153   0.7197        [35m0.2573[0m       0.9238        0.2414        9.8738
     26   0.2152   0.7201        0.2592       0.9238        0.2414        10.0982
     27   0.2129   0.7197        0.2603       0.9238        0.2416        10.1626
     28   0.2128   0.7197        0.2596       0.9238        0.2416        9.9767
     29   0.2131   0.7202        0.2597       0.9238        0.2415        9.6656
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 09:02:36,530][0m Trial 88 finished with value: 0.24122136481727913 and parameters: {'lr': 8.133867206657515e-05, 'dropout': 0.48351151701354556, 'd_model_multiplier': 1, 'num_layers': 6, 'n_heads': 4, 'dim_feedforward': 144, 'batch_size': 76, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 81}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 157
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1840[0m   [32m0.7126[0m        [35m0.2771[0m       [31m0.9238[0m        [94m0.2450[0m     +  11.5743
      2   0.1740   [32m0.7238[0m        [35m0.2575[0m       [31m0.9262[0m        [94m0.2431[0m     +  11.6624
      3   [36m0.1881[0m   [32m0.7284[0m        [35m0.2548[0m       0.9262        0.2497        11.9873
      4   [36m0.1955[0m   [32m0.7287[0m        [35m0.2547[0m       0.9250        0.2487        12.0142
      5   [36m0.2223[0m   [32m0.7596[0m        [35m0.2476[0m       0.9202        0.2477        11.6559
      6   [36m0.2304[0m   [32m0.7758[0m        [35m0.2440[0m       0.9141        0.2448        12.0152
      7   [36m0.2725[0m   [32m0.7904[0m        [35m0.2419[0m       0.9190        0.2465        12.1366
      8   0.2722   0.7891        [35m0.2384[0m       0.9202        [94m0.2419[0m     +  12.0091
      9   0.2724   [32m0.7960[0m        0.2390       0.9214        0.2498        11.9582
     10   [36m0.2738[0m   [32m0.8073[0m        0.2412       0.9226        0.2453        12.1960
     11   [36m0.2846[0m   0.8000        [35m0.2346[0m       0.9202        0.2468        12.2780
     12   0.2716   0.8014        [35m0.2335[0m       0.9202        0.2470        11.7939
     13   0.2741   0.8037        0.2336       0.9190        [94m0.2385[0m     +  12.3206
     14   [36m0.2943[0m   [32m0.8094[0m        [35m0.2324[0m       0.9202        0.2503        12.0379
     15   0.2583   0.8008        0.2343       0.9214        0.2595        12.2069
     16   0.2735   0.8050        0.2335       0.9238        0.2460        12.1662
     17   0.2835   [32m0.8109[0m        0.2325       0.9202        0.2416        12.2033
     18   0.2873   [32m0.8142[0m        [35m0.2294[0m       0.9190        [94m0.2377[0m     +  12.2748
     19   0.2889   [32m0.8165[0m        0.2307       0.9226        0.2467        12.2675
     20   0.2868   0.8149        [35m0.2285[0m       0.9190        0.2448        12.0625
     21   0.2934   0.8143        0.2292       0.9178        0.2448        12.5401
     22   0.2891   0.8122        0.2302       0.9214        [94m0.2366[0m     +  12.1495
     23   0.2844   [32m0.8182[0m        [35m0.2284[0m       0.9202        0.2401        12.2713
     24   0.2934   [32m0.8206[0m        0.2292       0.9214        [94m0.2348[0m     +  12.0953
     25   0.2866   0.8122        [35m0.2283[0m       0.9214        0.2384        12.0854
     26   0.2625   0.8046        [35m0.2268[0m       0.9190        0.2411        12.3342
     27   0.2610   0.8010        0.2284       0.9154        0.2356        12.4105
     28   0.2543   0.7832        0.2287       0.9117        0.2493        12.3466
     29   0.2720   0.7951        [35m0.2267[0m       0.9190        0.2439        12.2828
     30   0.2657   0.7948        0.2283       0.9202        0.2407        12.0822
     31   0.2630   0.8021        0.2275       0.9202        0.2416        11.9393
     32   0.2787   0.8016        0.2288       0.9202        0.2360        12.3538
     33   0.2691   0.7969        [35m0.2259[0m       0.9178        0.2451        12.4666
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 09:09:29,179][0m Trial 89 finished with value: 0.23479994282829142 and parameters: {'lr': 0.001237115072277193, 'dropout': 0.41642404405239397, 'd_model_multiplier': 4, 'num_layers': 3, 'n_heads': 16, 'dim_feedforward': 178, 'batch_size': 91, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.01, 'top_n_features': 157}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 166
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1537[0m   [32m0.6423[0m        [35m0.2755[0m       [31m0.9274[0m        [94m0.2676[0m     +  95.8790
      2   [36m0.1644[0m   [32m0.6811[0m        [35m0.2641[0m       0.9274        0.2680        95.7121
      3   0.1593   [32m0.6838[0m        [35m0.2617[0m       0.9274        0.2742        95.6368
      4   [36m0.1899[0m   [32m0.6864[0m        0.2618       0.9274        0.2682        95.9206
      5   0.1862   [32m0.6880[0m        [35m0.2576[0m       0.9274        0.2700        95.8674
      6   [36m0.1931[0m   [32m0.6882[0m        0.2583       0.9274        0.2687        95.8648
      7   [36m0.2134[0m   [32m0.6888[0m        0.2583       0.9274        0.2689        95.8851
      8   0.2001   [32m0.6917[0m        [35m0.2572[0m       0.9274        0.2762        96.1160
      9   0.2053   [32m0.6957[0m        [35m0.2562[0m       0.9274        0.2750        95.6927
     10   0.2059   0.6953        [35m0.2527[0m       0.9274        0.2740        95.8396
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 09:27:03,820][0m Trial 90 finished with value: 0.26764552639358136 and parameters: {'lr': 0.0003106909097807889, 'dropout': 0.5355597844814264, 'd_model_multiplier': 2, 'num_layers': 13, 'n_heads': 64, 'dim_feedforward': 193, 'batch_size': 24, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0, 'top_n_features': 166}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 140
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.1425[0m   [32m0.6949[0m        [35m0.3507[0m       [31m0.9214[0m        [94m0.3215[0m     +  8.8698
      2   [36m0.1780[0m   [32m0.7521[0m        [35m0.2687[0m       0.9178        [94m0.3028[0m     +  8.9504
      3   [36m0.2060[0m   [32m0.7740[0m        [35m0.2605[0m       0.9190        [94m0.2509[0m     +  9.1411
      4   0.2010   0.7572        [35m0.2574[0m       [31m0.9238[0m        [94m0.2315[0m     +  9.3097
      5   0.2005   [32m0.7743[0m        [35m0.2506[0m       [31m0.9299[0m        [94m0.2218[0m     +  9.1066
      6   [36m0.2153[0m   [32m0.7931[0m        [35m0.2496[0m       0.9262        [94m0.2181[0m     +  9.1788
      7   [36m0.2243[0m   0.7853        [35m0.2452[0m       0.9250        [94m0.2174[0m     +  9.1425
      8   [36m0.2363[0m   0.7743        [35m0.2390[0m       0.9250        [94m0.2168[0m     +  9.6403
      9   0.2313   0.7738        0.2397       [31m0.9311[0m        0.2179        9.2515
     10   0.2328   0.7878        [35m0.2316[0m       0.9238        [94m0.2136[0m     +  9.1700
     11   [36m0.2381[0m   0.7883        0.2327       0.9141        0.2219        9.3848
     12   0.2227   0.7928        [35m0.2302[0m       0.9129        0.2237        9.4675
     13   0.2239   0.7871        0.2312       0.9117        0.2414        9.2960
     14   0.2265   [32m0.7938[0m        [35m0.2276[0m       0.9202        0.2172        9.3049
     15   0.2266   0.7903        0.2311       0.9117        0.2293        9.2712
     16   0.2214   0.7873        [35m0.2253[0m       0.9057        0.2405        9.2524
     17   [36m0.2462[0m   [32m0.8030[0m        0.2287       0.9238        0.2144        9.2732
     18   0.2425   0.8008        0.2275       0.9154        0.2163        9.4821
     19   0.2320   0.7887        0.2261       0.9214        0.2173        9.3547
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 09:30:09,781][0m Trial 91 finished with value: 0.21362207231743593 and parameters: {'lr': 0.003942247381693, 'dropout': 0.5113248205627465, 'd_model_multiplier': 2, 'num_layers': 3, 'n_heads': 4, 'dim_feedforward': 128, 'batch_size': 60, 'pos_encoding': 'learnable', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.01, 'top_n_features': 140}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 111
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.2391[0m   [32m0.7625[0m        [35m0.2936[0m       [31m0.9178[0m        [94m0.2518[0m     +  8.7928
      2   [36m0.2581[0m   [32m0.7683[0m        [35m0.2562[0m       0.9178        [94m0.2361[0m     +  9.1536
      3   [36m0.2836[0m   0.7673        [35m0.2454[0m       [31m0.9214[0m        [94m0.2359[0m     +  9.1154
      4   0.2529   0.7561        [35m0.2431[0m       0.9069        0.2500        9.3578
      5   0.2816   0.7658        [35m0.2409[0m       0.9141        0.2415        8.9083
      6   [36m0.2868[0m   [32m0.7717[0m        [35m0.2361[0m       [31m0.9238[0m        [94m0.2334[0m     +  9.0928
      7   [36m0.3121[0m   [32m0.7768[0m        [35m0.2343[0m       [31m0.9250[0m        [94m0.2314[0m     +  9.2486
      8   [36m0.3187[0m   0.7715        [35m0.2337[0m       0.9214        [94m0.2304[0m     +  9.1412
      9   [36m0.3273[0m   0.7739        [35m0.2312[0m       0.9250        [94m0.2288[0m     +  9.1913
     10   0.3024   0.7713        0.2316       0.9202        0.2370        9.3076
     11   0.3182   [32m0.7788[0m        [35m0.2295[0m       0.9238        0.2302        9.1076
     12   0.3180   0.7686        [35m0.2278[0m       0.9250        0.2360        8.9490
     13   0.3096   0.7719        [35m0.2276[0m       0.9166        0.2361        9.0736
     14   0.3150   0.7751        [35m0.2244[0m       0.9250        0.2296        9.5181
     15   0.3226   0.7783        0.2274       0.9250        0.2293        9.1253
     16   0.3164   0.7744        [35m0.2227[0m       0.9226        0.2295        9.2649
     17   0.3213   [32m0.7820[0m        0.2288       [31m0.9262[0m        [94m0.2279[0m     +  9.1823
     18   0.3177   0.7773        0.2266       0.9262        0.2292        8.9554
     19   0.3187   0.7773        0.2263       0.9226        0.2307        8.9987
     20   0.3242   [32m0.7836[0m        0.2287       0.9250        [94m0.2276[0m     +  9.2444
     21   0.3129   0.7783        0.2245       0.9238        0.2304        9.3305
     22   0.3107   0.7761        0.2268       0.9226        0.2337        9.5807
     23   0.3167   0.7688        [35m0.2221[0m       0.9202        0.2342        9.2543
     24   0.3210   0.7823        [35m0.2217[0m       0.9214        0.2290        9.4070
     25   0.3015   0.7688        0.2225       0.9250        0.2349        9.0329
     26   0.3064   0.7821        0.2240       0.9250        0.2307        8.9498
     27   0.3077   0.7806        [35m0.2215[0m       0.9214        0.2321        9.1914
     28   0.3136   0.7768        0.2244       0.9214        0.2331        9.3187
     29   0.3170   0.7766        0.2227       0.9250        0.2317        8.9449
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 09:34:45,430][0m Trial 92 finished with value: 0.22758288382402786 and parameters: {'lr': 0.0037212478206439777, 'dropout': 0.4763174090658809, 'd_model_multiplier': 2, 'num_layers': 3, 'n_heads': 4, 'dim_feedforward': 169, 'batch_size': 65, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.01, 'top_n_features': 111}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 139
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.1528[0m   [32m0.6165[0m        [35m0.3929[0m       [31m0.9190[0m        [94m0.2733[0m     +  8.9995
      2   [36m0.1938[0m   [32m0.7129[0m        [35m0.2655[0m       0.9190        [94m0.2571[0m     +  8.6820
      3   [36m0.2106[0m   [32m0.7444[0m        [35m0.2605[0m       0.9190        [94m0.2531[0m     +  9.0971
      4   [36m0.2152[0m   [32m0.7486[0m        [35m0.2600[0m       0.9190        0.2533        9.2991
      5   [36m0.2386[0m   [32m0.7509[0m        [35m0.2579[0m       0.9190        0.2544        9.3961
      6   0.2198   [32m0.7641[0m        [35m0.2579[0m       0.9154        0.2552        9.3277
      7   0.2324   0.7636        [35m0.2522[0m       0.9081        0.2605        9.5292
      8   0.2182   0.7473        [35m0.2486[0m       0.9129        0.2567        9.4795
      9   [36m0.2464[0m   0.7641        [35m0.2483[0m       0.9008        0.2698        9.5805
     10   [36m0.2616[0m   [32m0.7816[0m        [35m0.2452[0m       0.9178        [94m0.2480[0m     +  9.3465
     11   [36m0.2773[0m   0.7801        [35m0.2411[0m       0.9178        0.2554        9.3612
     12   0.2644   [32m0.7845[0m        0.2461       0.9154        0.2482        9.3779
     13   [36m0.2998[0m   [32m0.7851[0m        [35m0.2403[0m       0.9190        [94m0.2433[0m     +  9.5101
     14   [36m0.3032[0m   0.7842        [35m0.2397[0m       [31m0.9238[0m        0.2439        9.3551
     15   0.2984   0.7770        0.2408       0.9178        0.2511        9.9443
     16   [36m0.3166[0m   [32m0.7925[0m        [35m0.2359[0m       0.9226        [94m0.2419[0m     +  9.2774
     17   0.2822   0.7681        0.2386       0.9129        0.2535        9.5446
     18   0.2988   0.7771        [35m0.2349[0m       0.9166        0.2461        9.4435
     19   0.3017   0.7793        [35m0.2343[0m       0.9178        0.2464        9.4376
     20   0.3057   0.7815        [35m0.2323[0m       0.9190        0.2448        9.1527
     21   0.2921   0.7798        0.2353       0.9141        0.2439        9.0794
     22   [36m0.3176[0m   0.7775        [35m0.2316[0m       0.9202        0.2442        9.2159
     23   [36m0.3402[0m   0.7854        [35m0.2306[0m       0.9238        0.2443        9.1607
     24   0.3100   0.7751        [35m0.2286[0m       0.9202        0.2505        8.9213
     25   0.3116   0.7847        0.2302       0.9166        0.2435        9.2864
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 09:38:48,010][0m Trial 93 finished with value: 0.24186051857658858 and parameters: {'lr': 0.0024934024210643054, 'dropout': 0.5048927973219371, 'd_model_multiplier': 2, 'num_layers': 2, 'n_heads': 4, 'dim_feedforward': 130, 'batch_size': 58, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.01, 'top_n_features': 139}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 122
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.1225[0m   [32m0.6394[0m        [35m0.3494[0m       [31m0.9287[0m        [94m0.2746[0m     +  8.9087
      2   [36m0.1295[0m   [32m0.6711[0m        [35m0.2769[0m       0.9287        [94m0.2605[0m     +  8.7149
      3   [36m0.1320[0m   [32m0.6806[0m        [35m0.2649[0m       0.9274        [94m0.2539[0m     +  9.2943
      4   [36m0.1357[0m   [32m0.6894[0m        0.2653       0.9274        [94m0.2531[0m     +  9.2949
      5   [36m0.1460[0m   [32m0.7004[0m        [35m0.2572[0m       0.9287        0.2542        9.6650
      6   [36m0.1521[0m   [32m0.7065[0m        [35m0.2539[0m       0.9274        [94m0.2514[0m     +  9.5741
      7   [36m0.1551[0m   0.7049        [35m0.2514[0m       0.9274        [94m0.2484[0m     +  9.4214
      8   [36m0.1604[0m   [32m0.7079[0m        [35m0.2469[0m       0.9287        [94m0.2455[0m     +  9.6744
      9   [36m0.1667[0m   [32m0.7084[0m        [35m0.2466[0m       0.9287        [94m0.2445[0m     +  9.5357
     10   [36m0.1686[0m   [32m0.7111[0m        [35m0.2461[0m       0.9274        0.2446        9.6862
     11   0.1658   [32m0.7130[0m        [35m0.2412[0m       0.9274        [94m0.2438[0m     +  9.6235
     12   [36m0.1721[0m   [32m0.7184[0m        0.2419       0.9274        [94m0.2413[0m     +  9.5009
     13   [36m0.1831[0m   [32m0.7220[0m        [35m0.2396[0m       0.9287        [94m0.2391[0m     +  9.5443
     14   0.1812   0.7219        [35m0.2362[0m       0.9238        0.2401        9.7078
     15   0.1810   [32m0.7244[0m        0.2375       0.9287        [94m0.2382[0m     +  9.7257
     16   0.1825   0.7242        [35m0.2346[0m       0.9274        0.2394        9.5601
     17   [36m0.1967[0m   0.7241        0.2357       0.9274        [94m0.2381[0m     +  9.8708
     18   [36m0.1991[0m   [32m0.7293[0m        [35m0.2342[0m       0.9202        0.2383        9.5739
     19   [36m0.2004[0m   [32m0.7368[0m        0.2351       0.9202        [94m0.2372[0m     +  9.5648
     20   [36m0.2207[0m   [32m0.7403[0m        [35m0.2304[0m       0.9226        [94m0.2336[0m     +  9.6770
     21   0.2005   0.7385        0.2318       0.9202        0.2374        9.4725
     22   [36m0.2252[0m   [32m0.7425[0m        [35m0.2284[0m       0.9214        0.2337        9.5929
     23   0.2251   0.7413        [35m0.2280[0m       0.9226        [94m0.2329[0m     +  9.5852
     24   [36m0.2352[0m   [32m0.7478[0m        [35m0.2259[0m       0.9262        [94m0.2308[0m     +  9.6144
     25   [36m0.2382[0m   [32m0.7530[0m        [35m0.2249[0m       0.9250        [94m0.2301[0m     +  9.4575
     26   0.2321   [32m0.7532[0m        [35m0.2234[0m       0.9262        [94m0.2300[0m     +  9.6317
     27   0.2223   0.7449        0.2249       0.9250        0.2341        9.4514
     28   0.2379   0.7512        [35m0.2223[0m       0.9214        0.2330        9.3083
     29   [36m0.2399[0m   [32m0.7537[0m        0.2235       0.9262        0.2312        9.5986
     30   [36m0.2436[0m   0.7513        [35m0.2213[0m       0.9274        0.2312        9.5561
     31   0.2414   [32m0.7546[0m        0.2227       0.9262        [94m0.2292[0m     +  9.5366
     32   [36m0.2477[0m   [32m0.7581[0m        [35m0.2206[0m       0.9274        [94m0.2264[0m     +  9.6627
     33   [36m0.2503[0m   0.7568        0.2227       0.9262        0.2278        9.4087
     34   0.2427   0.7575        [35m0.2200[0m       0.9262        0.2289        9.4725
     35   0.2440   [32m0.7593[0m        0.2203       0.9250        0.2283        9.4030
     36   0.2428   [32m0.7611[0m        0.2201       0.9238        0.2285        9.5019
     37   [36m0.2539[0m   0.7607        [35m0.2195[0m       0.9238        0.2291        9.7728
     38   [36m0.2610[0m   0.7598        [35m0.2184[0m       0.9262        [94m0.2264[0m     +  9.3065
     39   0.2562   0.7580        [35m0.2178[0m       0.9274        0.2265        9.6201
     40   0.2457   0.7542        0.2207       0.9274        0.2289        9.6106
     41   0.2428   0.7519        0.2182       0.9274        0.2292        9.6343
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 09:45:29,530][0m Trial 94 finished with value: 0.2264311472370662 and parameters: {'lr': 0.0004919267744855441, 'dropout': 0.5090385266495, 'd_model_multiplier': 2, 'num_layers': 4, 'n_heads': 4, 'dim_feedforward': 149, 'batch_size': 73, 'pos_encoding': 'learnable', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.01, 'top_n_features': 122}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 131
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.1994[0m   [32m0.7186[0m        [35m0.2842[0m       [31m0.9117[0m        [94m0.2569[0m     +  7.8564
      2   0.1974   0.7109        [35m0.2621[0m       0.8984        0.2822        8.9475
      3   [36m0.2384[0m   0.6877        0.2702       [31m0.9190[0m        0.2607        9.0502
      4   0.2172   [32m0.7309[0m        0.2729       0.8972        0.2972        8.7969
      5   [36m0.2628[0m   [32m0.7516[0m        0.2626       0.9166        [94m0.2447[0m     +  9.1312
      6   [36m0.2874[0m   [32m0.7854[0m        [35m0.2532[0m       0.9129        [94m0.2403[0m     +  8.8603
      7   0.2755   0.7852        [35m0.2495[0m       [31m0.9226[0m        0.2482        9.1915
      8   0.2854   [32m0.7915[0m        0.2514       0.9226        0.2646        8.9607
      9   [36m0.3001[0m   [32m0.7995[0m        [35m0.2487[0m       [31m0.9250[0m        0.2670        9.1464
     10   0.2960   [32m0.8005[0m        [35m0.2453[0m       0.9190        0.2507        9.2624
     11   [36m0.3109[0m   [32m0.8036[0m        0.2459       0.9226        0.2736        9.1270
     12   0.2809   [32m0.8059[0m        [35m0.2425[0m       0.9190        0.2496        8.9948
     13   0.2879   0.7983        0.2459       0.9238        0.2541        9.1269
     14   0.2226   0.7408        0.2462       0.8791        0.3089        9.2434
     15   0.2302   0.6869        0.2446       0.8960        0.2710        9.5558
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 09:47:54,675][0m Trial 95 finished with value: 0.24031668284315585 and parameters: {'lr': 0.0010301024172583168, 'dropout': 0.5597171757935968, 'd_model_multiplier': 64, 'num_layers': 2, 'n_heads': 4, 'dim_feedforward': 369, 'batch_size': 49, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.01, 'top_n_features': 131}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 139
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0699[0m   [32m0.3938[0m        [35m0.4941[0m       [31m0.9117[0m        [94m0.4714[0m     +  13.4054
      2   [36m0.1927[0m   [32m0.7025[0m        [35m0.2924[0m       0.9057        [94m0.3803[0m     +  14.4088
      3   [36m0.2239[0m   [32m0.7392[0m        [35m0.2622[0m       0.9033        [94m0.3597[0m     +  14.2376
      4   [36m0.2490[0m   [32m0.7448[0m        [35m0.2533[0m       0.9033        [94m0.3506[0m     +  14.2207
      5   0.2437   [32m0.7457[0m        0.2556       0.9057        [94m0.3201[0m     +  14.4188
      6   [36m0.2552[0m   [32m0.7480[0m        [35m0.2478[0m       0.9045        [94m0.3090[0m     +  14.7544
      7   0.2522   0.7395        [35m0.2435[0m       0.9069        [94m0.2956[0m     +  14.6906
      8   [36m0.2689[0m   [32m0.7537[0m        [35m0.2420[0m       0.9021        0.3321        14.9252
      9   0.2655   0.7396        0.2425       0.9045        0.3076        14.7839
     10   [36m0.2847[0m   0.7517        [35m0.2390[0m       0.9045        [94m0.2915[0m     +  14.4453
     11   [36m0.2902[0m   0.7510        [35m0.2367[0m       0.9045        0.2985        14.6535
     12   0.2875   0.7523        [35m0.2339[0m       0.9069        [94m0.2867[0m     +  14.5502
     13   [36m0.2996[0m   [32m0.7591[0m        0.2347       0.9033        [94m0.2838[0m     +  14.5900
     14   0.2979   0.7532        [35m0.2325[0m       0.9093        [94m0.2828[0m     +  15.8036
     15   0.2958   [32m0.7617[0m        0.2342       0.9069        0.2926        15.5494
     16   [36m0.3016[0m   [32m0.7640[0m        [35m0.2322[0m       0.9081        0.2829        14.8313
     17   [36m0.3124[0m   0.7597        [35m0.2268[0m       0.9081        0.2836        15.2890
     18   [36m0.3198[0m   [32m0.7680[0m        [35m0.2240[0m       0.9105        0.2929        15.4168
     19   [36m0.3303[0m   [32m0.7746[0m        0.2265       0.9033        0.2895        14.7250
     20   0.3205   0.7684        0.2279       0.9057        0.3047        14.8338
     21   0.3182   0.7611        0.2256       0.9081        0.2938        14.9076
     22   0.3297   0.7707        0.2276       [31m0.9129[0m        0.3441        15.0070
     23   0.3267   [32m0.7756[0m        0.2262       0.9129        0.2858        14.6155
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 09:53:49,828][0m Trial 96 finished with value: 0.282786247411691 and parameters: {'lr': 0.00165469613745949, 'dropout': 0.5268593980486902, 'd_model_multiplier': 2, 'num_layers': 3, 'n_heads': 16, 'dim_feedforward': 157, 'batch_size': 196, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 139}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 151
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.2251[0m   [32m0.6895[0m        [35m0.2958[0m       [31m0.9178[0m        [94m0.2693[0m     +  8.1025
      2   0.2175   [32m0.7197[0m        [35m0.2594[0m       0.9178        [94m0.2660[0m     +  8.9978
      3   0.2128   [32m0.7349[0m        [35m0.2584[0m       0.9178        [94m0.2653[0m     +  9.8522
      4   [36m0.2411[0m   [32m0.7460[0m        [35m0.2534[0m       0.9178        [94m0.2613[0m     +  9.8432
      5   [36m0.2509[0m   [32m0.7672[0m        [35m0.2475[0m       0.9045        [94m0.2599[0m     +  9.2776
      6   [36m0.2534[0m   0.7580        [35m0.2450[0m       0.9093        0.2653        9.2212
      7   [36m0.2586[0m   0.7645        [35m0.2411[0m       0.9069        [94m0.2567[0m     +  9.0258
      8   0.2480   0.7636        [35m0.2382[0m       0.9154        0.2586        9.1863
      9   [36m0.2707[0m   0.7646        [35m0.2369[0m       0.9166        0.2578        9.4039
     10   0.2579   0.7469        0.2379       [31m0.9190[0m        0.2770        9.1589
     11   0.2544   0.7404        [35m0.2340[0m       0.9057        0.2774        9.0138
     12   0.2578   0.7353        0.2379       0.9129        0.2826        9.0283
     13   0.2511   0.7307        [35m0.2306[0m       0.9117        0.2794        8.9317
     14   [36m0.2715[0m   0.7330        0.2334       0.9178        0.2826        9.5564
     15   [36m0.2794[0m   0.7470        [35m0.2294[0m       0.9141        0.2681        9.2561
     16   0.2590   0.7322        0.2323       0.9105        0.2743        9.1327
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 09:56:26,349][0m Trial 97 finished with value: 0.25672180577662146 and parameters: {'lr': 0.0007151119598759718, 'dropout': 0.5422061962092869, 'd_model_multiplier': 2, 'num_layers': 2, 'n_heads': 4, 'dim_feedforward': 225, 'batch_size': 32, 'pos_encoding': 'learnable', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.01, 'top_n_features': 151}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 175
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1718[0m   [32m0.6953[0m        [35m0.2935[0m       [31m0.9166[0m        [94m0.3994[0m     +  19.4599
      2   [36m0.1821[0m   0.6886        [35m0.2446[0m       [31m0.9238[0m        0.4209        20.2818
      3   0.1779   [32m0.7112[0m        [35m0.2408[0m       0.9154        0.4444        20.3192
      4   [36m0.1879[0m   [32m0.7114[0m        [35m0.2361[0m       0.9226        [94m0.3840[0m     +  19.4580
      5   [36m0.1893[0m   0.7100        [35m0.2340[0m       0.9154        [94m0.3717[0m     +  20.3185
      6   [36m0.1916[0m   0.7096        [35m0.2335[0m       0.9105        0.4196        20.2179
      7   0.1858   0.7095        [35m0.2276[0m       0.9202        [94m0.3650[0m     +  20.2563
      8   [36m0.2007[0m   0.6973        0.2299       0.9238        [94m0.3573[0m     +  20.0604
      9   0.1952   0.7021        [35m0.2240[0m       0.9105        0.3691        19.6982
     10   [36m0.2052[0m   [32m0.7134[0m        [35m0.2220[0m       0.9202        [94m0.3184[0m     +  20.1850
     11   0.2004   [32m0.7210[0m        [35m0.2195[0m       0.9202        [94m0.3092[0m     +  20.2856
     12   [36m0.2074[0m   0.7148        0.2223       [31m0.9274[0m        0.3202        19.9523
     13   [36m0.2201[0m   0.7192        [35m0.2181[0m       0.9238        0.3167        19.5420
     14   0.2087   0.7133        [35m0.2174[0m       0.9190        0.3464        19.8464
     15   0.2002   0.6972        [35m0.2148[0m       0.9190        0.3636        20.3496
     16   0.2087   0.7178        [35m0.2128[0m       0.9274        0.3096        20.0598
     17   0.1958   0.6974        0.2136       0.9262        [94m0.3022[0m     +  19.7599
     18   0.1953   0.7064        [35m0.2104[0m       0.9214        0.3141        20.8690
     19   0.1954   0.7037        [35m0.2077[0m       0.9238        0.3134        19.8226
     20   0.1949   0.7072        0.2080       0.9250        0.3158        19.5684
     21   0.1957   0.7114        [35m0.2061[0m       0.9250        0.3149        20.4122
     22   0.1870   0.6991        [35m0.2053[0m       0.9274        0.3029        20.5844
     23   0.2021   0.7120        [35m0.2042[0m       [31m0.9287[0m        0.3130        19.4917
     24   0.1948   0.7145        [35m0.2011[0m       0.9274        [94m0.2838[0m     +  19.9948
     25   0.2063   0.7155        0.2027       [31m0.9299[0m        0.2988        19.6878
     26   0.1921   0.6986        [35m0.1967[0m       0.9287        0.2916        19.7916
     27   0.1958   0.6983        0.1967       0.9287        [94m0.2831[0m     +  20.3932
     28   0.2087   0.7067        [35m0.1962[0m       0.9274        0.3021        20.7685
     29   0.2039   0.7119        [35m0.1945[0m       0.9287        0.2968        19.7230
     30   0.1852   0.7100        0.1949       0.9250        [94m0.2787[0m     +  19.4031
     31   0.1994   0.7151        [35m0.1916[0m       0.9238        0.2968        19.2919
     32   0.2047   0.7066        [35m0.1907[0m       0.9226        [94m0.2677[0m     +  19.4397
     33   0.1898   0.7051        [35m0.1868[0m       0.9274        0.2887        19.6091
     34   0.2028   0.7078        0.1883       0.9262        [94m0.2659[0m     +  19.5129
     35   0.2148   0.7200        0.1886       [31m0.9311[0m        0.2726        19.6826
     36   0.1755   0.7002        [35m0.1854[0m       0.9287        0.2728        20.0036
     37   0.1785   0.6950        [35m0.1849[0m       0.9287        0.2784        19.7375
     38   0.1863   0.6946        [35m0.1809[0m       0.9262        [94m0.2595[0m     +  19.8380
     39   0.1938   0.6846        [35m0.1800[0m       0.9238        0.2750        19.4666
     40   0.2056   0.7064        [35m0.1789[0m       0.9262        0.2640        19.9181
     41   0.2022   0.7073        [35m0.1779[0m       [31m0.9335[0m        0.2875        19.2992
     42   [36m0.2282[0m   0.7161        [35m0.1744[0m       0.9274        0.2692        19.4804
     43   0.2054   0.7177        [35m0.1718[0m       0.9238        [94m0.2562[0m     +  19.6231
     44   0.1990   0.7116        0.1730       0.9274        0.2581        19.5765
     45   0.2064   0.7130        0.1719       0.9274        [94m0.2515[0m     +  19.4807
     46   0.2276   [32m0.7407[0m        [35m0.1647[0m       0.9311        [94m0.2498[0m     +  19.6251
     47   0.2219   0.7042        0.1685       0.9323        0.2579        19.6795
     48   0.1755   0.6915        0.1675       0.9214        0.2910        19.8511
     49   0.1877   0.7065        0.1668       0.9238        0.2681        19.6780
     50   0.1882   0.6812        [35m0.1610[0m       0.9250        0.2904        20.1964
[32m[I 2023-05-02 10:13:02,319][0m Trial 98 finished with value: 0.2497668860541001 and parameters: {'lr': 0.0001876312409971585, 'dropout': 0.4620986586020618, 'd_model_multiplier': 16, 'num_layers': 4, 'n_heads': 16, 'dim_feedforward': 145, 'batch_size': 211, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 175}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 231
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.0544[0m   [32m0.3208[0m        [35m0.4753[0m       [31m0.9214[0m        [94m0.3350[0m     +  9.3686
      2   [36m0.2237[0m   [32m0.6614[0m        [35m0.2955[0m       0.9214        [94m0.2587[0m     +  9.2990
      3   [36m0.2246[0m   [32m0.6938[0m        [35m0.2658[0m       0.9214        [94m0.2496[0m     +  9.2979
      4   [36m0.2611[0m   [32m0.7173[0m        [35m0.2619[0m       0.9214        [94m0.2462[0m     +  9.7802
      5   0.2462   [32m0.7222[0m        [35m0.2608[0m       0.9214        0.2469        9.8154
      6   [36m0.2946[0m   [32m0.7367[0m        [35m0.2593[0m       0.9214        [94m0.2438[0m     +  9.8020
      7   [36m0.3112[0m   [32m0.7374[0m        [35m0.2569[0m       0.9214        0.2447        9.9008
      8   0.3031   [32m0.7404[0m        0.2586       0.9214        0.2456        9.8920
      9   0.2761   [32m0.7431[0m        0.2584       0.9214        0.2467        9.8815
     10   0.2787   0.7409        0.2584       0.9214        0.2484        9.7103
     11   0.2803   0.7422        0.2587       0.9214        0.2452        9.7150
     12   0.2970   [32m0.7482[0m        0.2573       0.9202        [94m0.2339[0m     +  9.9887
     13   [36m0.3151[0m   [32m0.7560[0m        [35m0.2556[0m       0.9202        0.2464        10.1998
     14   0.2858   0.7504        [35m0.2533[0m       0.9202        0.2543        10.3553
     15   0.2926   0.7482        [35m0.2491[0m       0.9214        0.2573        10.2055
     16   0.3065   0.7526        [35m0.2458[0m       0.9202        0.2524        9.5364
     17   0.2978   [32m0.7713[0m        0.2480       0.9202        0.2388        9.7009
     18   0.2792   0.7648        0.2483       0.9190        0.2408        9.5163
     19   0.2676   0.7572        0.2463       0.9202        0.2423        9.7851
     20   0.3039   0.7586        0.2465       0.9202        0.2435        9.7518
     21   0.2952   0.7654        [35m0.2425[0m       0.9202        0.2404        9.5935
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 10:16:37,620][0m Trial 99 finished with value: 0.23391489596291926 and parameters: {'lr': 0.0011706622963417394, 'dropout': 0.5697794996517608, 'd_model_multiplier': 2, 'num_layers': 4, 'n_heads': 4, 'dim_feedforward': 246, 'batch_size': 62, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.01, 'top_n_features': 231}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 161
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2607[0m   [32m0.7428[0m        [35m0.2838[0m       [31m0.9141[0m        [94m0.3907[0m     +  15.6446
      2   0.2574   0.7377        [35m0.2683[0m       [31m0.9190[0m        0.4131        15.4074
      3   0.2421   0.7212        [35m0.2661[0m       0.9117        [94m0.3234[0m     +  15.5907
      4   0.2177   0.6954        [35m0.2615[0m       0.9141        [94m0.2854[0m     +  15.8380
      5   [36m0.2769[0m   [32m0.7647[0m        [35m0.2600[0m       0.9154        0.3045        15.8335
      6   [36m0.2979[0m   [32m0.7777[0m        [35m0.2559[0m       [31m0.9202[0m        [94m0.2407[0m     +  15.9938
      7   0.2971   0.7722        [35m0.2514[0m       0.9154        [94m0.2326[0m     +  15.9894
      8   [36m0.3064[0m   [32m0.7916[0m        [35m0.2496[0m       0.9190        [94m0.2271[0m     +  16.1512
      9   0.3007   0.7730        [35m0.2476[0m       [31m0.9226[0m        0.2295        16.0589
     10   [36m0.3094[0m   [32m0.7938[0m        [35m0.2407[0m       0.9214        0.2295        16.0115
     11   [36m0.3330[0m   [32m0.8088[0m        [35m0.2406[0m       [31m0.9250[0m        [94m0.2200[0m     +  15.8953
     12   0.3264   [32m0.8113[0m        [35m0.2355[0m       0.9214        0.2399        16.0459
     13   0.3194   [32m0.8121[0m        [35m0.2353[0m       0.9154        0.2235        16.1460
     14   [36m0.3505[0m   0.8107        [35m0.2312[0m       0.9238        0.2236        15.7950
     15   0.3418   [32m0.8133[0m        0.2330       0.9214        0.2396        15.5826
     16   [36m0.3521[0m   [32m0.8151[0m        0.2335       [31m0.9274[0m        0.2334        15.6406
     17   0.3408   0.8117        [35m0.2311[0m       0.9141        0.2558        15.8080
     18   0.3433   0.8149        [35m0.2273[0m       0.9226        0.2434        15.8559
     19   0.3216   [32m0.8165[0m        0.2278       0.9202        0.2217        16.0517
     20   0.3509   0.8149        0.2288       0.9274        0.2221        15.6006
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 10:22:11,599][0m Trial 100 finished with value: 0.22003879040443625 and parameters: {'lr': 0.0005886282827996577, 'dropout': 0.44215785077198827, 'd_model_multiplier': 8, 'num_layers': 5, 'n_heads': 16, 'dim_feedforward': 129, 'batch_size': 36, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.01, 'top_n_features': 161}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 162
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1811[0m   [32m0.6742[0m        [35m0.2813[0m       [31m0.9214[0m        [94m0.4081[0m     +  16.9258
      2   [36m0.2122[0m   [32m0.7363[0m        [35m0.2499[0m       0.9081        [94m0.3711[0m     +  17.9274
      3   [36m0.2357[0m   0.7348        [35m0.2475[0m       0.9141        [94m0.3305[0m     +  17.6701
      4   0.2191   0.7276        [35m0.2444[0m       0.9166        0.3433        17.7011
      5   0.1808   0.7284        [35m0.2424[0m       [31m0.9274[0m        0.3396        17.7394
      6   0.1686   0.6638        0.2426       [31m0.9347[0m        [94m0.3241[0m     +  17.7629
      7   0.2017   0.7096        [35m0.2411[0m       0.9262        0.3421        17.9687
      8   0.1971   0.7352        [35m0.2389[0m       0.9250        [94m0.3171[0m     +  17.9945
      9   0.1104   0.6224        [35m0.2364[0m       0.9274        [94m0.2949[0m     +  18.0860
     10   0.1843   0.7208        0.2370       0.9238        0.3077        17.6527
     11   [36m0.2414[0m   [32m0.7710[0m        [35m0.2363[0m       0.9202        0.3081        17.7725
     12   0.2056   0.7383        [35m0.2315[0m       0.9214        [94m0.2854[0m     +  17.9622
     13   0.2278   0.7633        0.2338       0.9238        [94m0.2592[0m     +  17.6610
     14   0.2123   0.7538        [35m0.2313[0m       0.9262        0.2879        18.0695
     15   0.2099   0.7527        [35m0.2304[0m       0.9214        0.2857        17.5558
     16   0.1902   0.7352        [35m0.2289[0m       0.9262        0.2709        17.8977
     17   0.2138   [32m0.7810[0m        0.2297       0.9226        [94m0.2536[0m     +  17.5236
     18   0.2072   0.7796        [35m0.2247[0m       0.9214        0.2790        18.2112
     19   0.2372   0.7692        0.2280       0.9250        [94m0.2534[0m     +  17.9749
     20   0.2149   0.7588        0.2289       0.9238        0.2634        18.3460
     21   0.2319   0.7807        [35m0.2227[0m       0.9214        0.2646        17.9304
     22   0.2215   0.7737        0.2245       0.9202        0.2610        18.2587
     23   0.2412   [32m0.7812[0m        [35m0.2190[0m       0.9226        0.2670        17.9123
     24   0.2239   0.7730        0.2214       0.9226        0.2802        18.0096
     25   0.2018   0.7369        0.2240       0.9178        0.2717        18.2982
     26   0.2276   0.7401        0.2238       0.9238        0.2729        18.0822
     27   [36m0.2559[0m   [32m0.7871[0m        [35m0.2183[0m       0.9262        [94m0.2487[0m     +  18.0368
     28   0.2404   0.7838        0.2219       0.9190        0.2524        17.6705
     29   0.2379   0.7796        0.2189       0.9226        0.2640        17.8352
     30   0.2407   [32m0.7990[0m        [35m0.2155[0m       0.9154        0.2703        17.9540
     31   0.2328   0.7816        0.2180       0.9178        0.2564        17.6032
     32   [36m0.2574[0m   [32m0.8070[0m        [35m0.2155[0m       0.9202        [94m0.2401[0m     +  17.8671
     33   [36m0.2611[0m   [32m0.8109[0m        0.2180       0.9250        0.2474        17.8577
     34   0.2362   0.7975        0.2179       0.9226        0.2556        18.2524
     35   0.2539   0.8027        [35m0.2106[0m       0.9202        0.2432        17.8223
     36   0.2531   0.7973        0.2119       0.9262        [94m0.2371[0m     +  17.9413
     37   [36m0.2611[0m   0.8081        0.2140       0.9274        [94m0.2125[0m     +  18.0892
     38   [36m0.2659[0m   0.8105        0.2161       0.9238        0.2218        17.9866
     39   0.2374   0.7964        0.2121       0.9274        0.2450        17.8242
     40   0.2477   0.8078        0.2115       0.9202        0.2474        17.7530
     41   0.2613   0.8031        0.2126       0.9250        0.2395        17.9973
     42   0.2600   0.8003        0.2108       0.9214        0.2310        17.5465
     43   0.2382   0.8037        0.2128       0.9226        0.2411        18.0244
     44   0.2427   0.7944        [35m0.2069[0m       0.9226        0.2529        18.8276
     45   0.2399   0.8092        0.2108       0.9202        0.2561        18.0570
     46   0.2531   0.7965        0.2088       0.9214        0.2493        18.0130
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 10:36:15,171][0m Trial 101 finished with value: 0.21249047754177014 and parameters: {'lr': 0.0005734528505978846, 'dropout': 0.4419204571600452, 'd_model_multiplier': 8, 'num_layers': 5, 'n_heads': 16, 'dim_feedforward': 129, 'batch_size': 135, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.01, 'top_n_features': 162}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 170
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1948[0m   [32m0.6401[0m        [35m0.2862[0m       [31m0.9214[0m        [94m0.3105[0m     +  18.1235
      2   [36m0.2528[0m   0.6395        [35m0.2528[0m       [31m0.9274[0m        [94m0.2540[0m     +  17.7536
      3   0.2286   [32m0.6917[0m        [35m0.2497[0m       0.9250        0.2795        18.1309
      4   [36m0.2805[0m   [32m0.7159[0m        [35m0.2483[0m       0.9238        0.2756        17.7289
      5   0.2612   [32m0.7232[0m        [35m0.2478[0m       0.9250        [94m0.2403[0m     +  17.6022
      6   [36m0.2886[0m   [32m0.7767[0m        [35m0.2391[0m       0.9226        0.2469        18.3639
      7   [36m0.2944[0m   0.6949        0.2461       [31m0.9311[0m        0.2448        18.0745
      8   [36m0.3345[0m   0.7688        [35m0.2388[0m       0.9274        0.2702        18.0920
      9   0.2835   0.7650        [35m0.2381[0m       0.9069        0.2855        18.2542
     10   0.2944   0.7540        0.2410       0.9274        0.2664        17.7829
     11   0.3012   0.7447        [35m0.2379[0m       0.9262        0.2776        18.1911
     12   0.3051   0.6591        [35m0.2358[0m       0.9311        0.2500        17.8799
     13   0.2955   0.7168        [35m0.2356[0m       0.9274        0.2739        18.2123
     14   0.2835   0.6711        [35m0.2297[0m       0.9287        0.2487        18.3690
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 10:40:46,607][0m Trial 102 finished with value: 0.24025381912343302 and parameters: {'lr': 0.0006378784873502885, 'dropout': 0.44501196409116495, 'd_model_multiplier': 8, 'num_layers': 5, 'n_heads': 16, 'dim_feedforward': 396, 'batch_size': 127, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.01, 'top_n_features': 170}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 161
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1589[0m   [32m0.6918[0m        [35m0.3214[0m       [31m0.9250[0m        [94m0.2693[0m     +  20.1948
      2   [36m0.2453[0m   [32m0.7535[0m        [35m0.2764[0m       0.9093        [94m0.2519[0m     +  20.2478
      3   0.2240   0.7430        0.2777       0.9250        [94m0.2507[0m     +  20.5898
      4   0.2143   0.7317        [35m0.2669[0m       0.9202        [94m0.2470[0m     +  21.1708
      5   0.2000   0.7181        0.2720       0.9226        0.2560        20.7043
      6   0.1905   0.7160        0.2795       0.9250        [94m0.2416[0m     +  20.8027
      7   0.2046   0.7287        0.3047       0.9238        0.2446        20.6571
      8   0.1879   0.7312        0.2941       0.9250        0.2436        20.7116
      9   0.1845   0.7265        0.2825       0.9250        0.2423        20.6428
     10   0.2071   0.7388        0.2822       0.9238        0.2444        20.9949
     11   0.1703   0.6757        0.2840       0.9250        0.2513        20.8940
     12   0.1928   0.7259        0.2835       0.9250        0.2500        21.2416
     13   0.1925   0.7330        0.2738       0.9214        0.2451        20.9136
     14   0.1854   0.7337        0.2786       0.9093        0.2629        20.8149
     15   0.1897   0.7332        0.2737       0.9238        0.2508        20.7369
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 10:46:19,123][0m Trial 103 finished with value: 0.24161250518123645 and parameters: {'lr': 0.002564942833312291, 'dropout': 0.4048889281985573, 'd_model_multiplier': 8, 'num_layers': 6, 'n_heads': 16, 'dim_feedforward': 130, 'batch_size': 169, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.01, 'top_n_features': 161}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 156
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2538[0m   [32m0.7638[0m        [35m0.3142[0m       [31m0.8356[0m        [94m0.4637[0m     +  15.5232
      2   [36m0.2605[0m   0.7401        [35m0.2673[0m       [31m0.9190[0m        [94m0.3713[0m     +  15.5830
      3   [36m0.2613[0m   [32m0.7825[0m        [35m0.2641[0m       0.8984        0.3991        15.8288
      4   [36m0.2769[0m   0.7702        0.2649       [31m0.9274[0m        [94m0.3323[0m     +  16.5129
      5   0.2656   0.7738        0.2646       0.9190        [94m0.2682[0m     +  15.8655
      6   0.2373   0.7556        [35m0.2577[0m       0.9250        [94m0.2303[0m     +  16.5688
      7   0.2381   0.7716        [35m0.2518[0m       0.9262        [94m0.2273[0m     +  16.4903
      8   0.2595   0.7806        0.2545       0.9250        [94m0.2258[0m     +  16.5172
      9   0.2395   0.7710        0.2537       0.9250        0.2287        16.1003
     10   [36m0.2827[0m   0.7802        0.2526       0.9262        [94m0.2241[0m     +  16.3056
     11   0.2706   0.7784        0.2523       0.9250        [94m0.2238[0m     +  16.3582
     12   0.2729   0.7817        0.2539       0.9202        0.2345        17.1269
     13   0.2815   [32m0.7892[0m        0.2565       0.9274        [94m0.2218[0m     +  16.3835
     14   0.2686   [32m0.7905[0m        0.2520       0.9262        0.2227        16.5111
     15   0.2713   0.7875        0.2558       0.9250        0.2241        16.3600
     16   0.2520   0.7830        0.2688       0.9262        0.2328        15.8869
     17   [36m0.3107[0m   0.7829        0.2592       [31m0.9299[0m        0.2342        15.9822
     18   0.2538   0.7862        0.2599       0.9262        0.2275        16.4592
     19   0.2477   0.7828        0.2820       0.9226        0.2290        16.2014
     20   0.2786   0.7751        0.2833       0.9262        0.2322        16.7309
     21   0.2177   0.7615        0.2913       0.9262        0.2388        16.7335
     22   0.2192   0.7466        0.3013       0.9250        0.2417        16.3117
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 10:52:34,574][0m Trial 104 finished with value: 0.2218333433912628 and parameters: {'lr': 0.0016148240477205074, 'dropout': 0.4827800200043715, 'd_model_multiplier': 8, 'num_layers': 4, 'n_heads': 16, 'dim_feedforward': 145, 'batch_size': 157, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.01, 'top_n_features': 156}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 176
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2373[0m   [32m0.7654[0m        [35m0.2847[0m       [31m0.9238[0m        [94m0.4136[0m     +  17.7672
      2   0.2124   0.7335        [35m0.2562[0m       [31m0.9262[0m        [94m0.4037[0m     +  17.5810
      3   0.2148   0.7611        [35m0.2467[0m       0.9250        [94m0.3700[0m     +  18.1744
      4   0.2048   0.7545        0.2472       [31m0.9274[0m        [94m0.3186[0m     +  17.7254
      5   [36m0.2609[0m   0.7561        [35m0.2420[0m       0.9202        0.3274        18.5262
      6   0.2070   0.7519        [35m0.2397[0m       0.9262        [94m0.2599[0m     +  18.4976
      7   0.2119   0.7545        [35m0.2378[0m       0.9166        0.3068        18.2163
      8   0.1966   0.7584        0.2394       0.9166        0.2876        18.5412
      9   0.2036   0.7529        0.2393       0.9190        0.2839        18.0113
     10   0.1982   0.7597        0.2387       0.9250        [94m0.2426[0m     +  17.7551
     11   0.1860   0.7158        [35m0.2356[0m       0.9238        0.2530        18.1070
     12   0.1805   0.7461        0.2364       0.9226        0.2593        18.0490
     13   0.2271   0.7252        [35m0.2295[0m       0.9238        0.2701        18.0403
     14   0.2099   0.6887        0.2314       0.9274        0.2612        17.7363
     15   0.2189   [32m0.7849[0m        [35m0.2283[0m       0.9238        0.2702        17.6846
     16   0.2109   0.7349        0.2307       0.9238        [94m0.2422[0m     +  17.8938
     17   0.2135   0.7468        [35m0.2257[0m       0.9226        0.2757        17.7152
     18   0.2049   0.7633        [35m0.2249[0m       0.9238        0.2476        17.8727
     19   0.1883   0.7623        [35m0.2246[0m       0.9238        0.2694        17.6708
     20   0.2168   0.7583        [35m0.2236[0m       0.9214        0.2463        18.0479
     21   0.2233   0.7664        0.2238       0.9190        0.2595        17.8485
     22   0.2080   [32m0.7850[0m        0.2242       0.9190        [94m0.2421[0m     +  18.2648
     23   0.2175   0.7702        [35m0.2196[0m       0.9226        0.2629        17.7854
     24   0.2262   0.7759        0.2282       0.9226        0.2679        17.9905
     25   0.2223   0.7388        [35m0.2190[0m       0.9226        0.2443        18.1230
     26   0.2609   0.7617        [35m0.2148[0m       0.9262        [94m0.2344[0m     +  17.5979
     27   0.2375   0.7653        0.2172       0.9226        0.2492        17.5434
     28   0.2431   0.7582        0.2179       0.9214        0.2494        18.2014
     29   0.2265   0.7708        [35m0.2132[0m       0.9238        0.2599        17.7448
     30   0.2022   0.7617        0.2155       0.9214        0.2545        18.0558
     31   0.2220   0.7546        0.2150       0.9226        0.2532        17.6750
     32   0.2273   0.7343        0.2143       0.9214        0.2482        17.9180
     33   [36m0.2613[0m   [32m0.7850[0m        [35m0.2063[0m       0.9190        0.2448        17.9951
     34   0.2067   0.7286        0.2141       0.9202        0.2506        17.9348
     35   0.2371   0.7534        [35m0.2060[0m       0.9214        0.2582        18.1143
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 11:03:21,887][0m Trial 105 finished with value: 0.2343785780264648 and parameters: {'lr': 0.00052741018190341, 'dropout': 0.4340330361134908, 'd_model_multiplier': 8, 'num_layers': 5, 'n_heads': 16, 'dim_feedforward': 162, 'batch_size': 139, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 176}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 161
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1913[0m   [32m0.6869[0m        [35m0.2789[0m       [31m0.9166[0m        [94m0.3400[0m     +  17.8515
      2   [36m0.2011[0m   [32m0.6985[0m        [35m0.2578[0m       0.9093        [94m0.3141[0m     +  18.1110
      3   [36m0.2110[0m   [32m0.7007[0m        [35m0.2548[0m       [31m0.9178[0m        [94m0.3002[0m     +  18.1907
      4   0.2034   0.6934        [35m0.2486[0m       [31m0.9226[0m        [94m0.2773[0m     +  18.0785
      5   0.1963   0.6975        [35m0.2465[0m       0.9141        [94m0.2684[0m     +  18.0247
      6   [36m0.2223[0m   [32m0.7119[0m        0.2483       0.9202        [94m0.2538[0m     +  18.2010
      7   0.2062   0.7108        [35m0.2427[0m       0.9166        0.2653        18.1885
      8   0.1811   0.6492        [35m0.2403[0m       0.9178        0.2834        18.0739
      9   [36m0.2271[0m   0.7081        0.2433       0.9166        0.2773        18.2426
     10   [36m0.2352[0m   [32m0.7435[0m        [35m0.2386[0m       0.9154        0.2635        18.0218
     11   0.2161   0.7182        0.2432       0.9154        0.2648        18.0606
     12   0.1941   0.6918        0.2410       0.9154        0.2753        18.1755
     13   0.2077   0.6947        0.2453       0.9141        0.2811        18.0705
     14   0.2164   0.6956        0.2392       0.9178        0.2687        18.1105
     15   0.1788   0.6276        0.2400       0.9154        0.2836        18.0035
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 11:08:12,099][0m Trial 106 finished with value: 0.25382204828592425 and parameters: {'lr': 0.00027541486228096633, 'dropout': 0.5032750860705415, 'd_model_multiplier': 8, 'num_layers': 6, 'n_heads': 16, 'dim_feedforward': 174, 'batch_size': 37, 'pos_encoding': 'learnable', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.01, 'top_n_features': 161}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 154
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1105[0m   [32m0.6386[0m        [35m0.3068[0m       [31m0.9287[0m        [94m0.2788[0m     +  16.3859
      2   [36m0.2173[0m   [32m0.7589[0m        0.3105       0.9190        [94m0.2671[0m     +  16.2948
      3   0.1623   0.6793        0.3334       [31m0.9335[0m        [94m0.2330[0m     +  16.3019
      4   0.1996   0.7452        0.3486       0.9323        [94m0.2256[0m     +  16.2289
      5   0.1911   0.7241        0.3564       0.9311        0.2991        16.3700
      6   0.1768   0.7123        0.3408       0.9335        0.2365        16.1953
      7   0.1837   0.7382        0.3248       0.9287        0.2417        16.5806
      8   0.0590   0.3798        [35m0.3055[0m       0.9335        0.3330        16.5009
      9   0.1923   [32m0.7610[0m        [35m0.2964[0m       0.9311        [94m0.2181[0m     +  16.4550
     10   0.1611   0.7020        0.2979       0.9323        0.3112        16.2933
     11   0.1635   0.7238        [35m0.2913[0m       0.9311        0.2305        16.3875
     12   0.1942   0.7420        [35m0.2785[0m       0.9311        0.2253        16.4543
     13   [36m0.2210[0m   0.7508        [35m0.2738[0m       0.9335        0.2294        16.4457
     14   0.2096   0.7509        [35m0.2733[0m       0.9335        0.2301        16.3881
     15   0.1750   0.7521        0.2845       0.9335        0.2247        16.4206
     16   0.1789   0.7296        0.2736       0.9335        0.2306        16.4728
     17   0.2081   0.7516        0.2741       0.9335        [94m0.2178[0m     +  16.3641
     18   0.1691   0.7445        [35m0.2728[0m       0.9335        0.2188        16.3050
     19   0.1803   0.7497        0.2736       0.9335        0.2187        16.4381
     20   0.1585   0.7404        0.2917       0.9311        0.2350        16.2439
     21   0.1534   0.7339        [35m0.2680[0m       0.9335        0.2371        16.3248
     22   0.1208   0.6465        0.2685       0.9335        0.2389        16.5293
     23   0.0688   0.4673        0.2728       0.9335        0.2930        16.4867
     24   0.0648   0.4360        0.2886       0.9335        0.2596        16.4584
     25   0.1801   0.7479        0.2808       0.9226        0.2399        16.3173
     26   0.1792   0.7491        0.2778       0.9335        0.2216        16.4382
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 11:15:35,818][0m Trial 107 finished with value: 0.21776859277573288 and parameters: {'lr': 0.0044805895218438365, 'dropout': 0.38086777257156706, 'd_model_multiplier': 8, 'num_layers': 5, 'n_heads': 16, 'dim_feedforward': 351, 'batch_size': 31, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.01, 'top_n_features': 154}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 168
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.3208[0m   [32m0.8062[0m        [35m0.2886[0m       [31m0.9226[0m        [94m0.2528[0m     +  10.5106
      2   [36m0.3685[0m   0.7924        [35m0.2415[0m       [31m0.9238[0m        [94m0.2392[0m     +  10.9446
      3   0.3504   0.7794        [35m0.2376[0m       0.9226        0.2476        11.0329
      4   0.3375   0.7674        0.2411       0.9238        0.2552        10.9038
      5   0.3019   0.7212        0.2389       0.9226        0.2826        11.0175
      6   0.2768   0.7105        0.2417       0.9238        0.3062        11.1341
      7   0.2764   0.7194        0.2409       0.9141        0.3683        11.2584
      8   0.3030   0.7315        0.2378       0.9238        0.2650        12.0064
      9   0.3606   0.7810        0.2395       [31m0.9274[0m        0.2604        12.0563
     10   0.1905   0.6766        0.2401       0.9214        0.2626        11.2873
     11   0.3284   0.7702        [35m0.2369[0m       0.9214        [94m0.2373[0m     +  11.3942
     12   0.3671   0.7723        0.2473       0.9238        0.2513        10.9409
     13   0.2629   0.7025        0.2379       0.9214        0.2603        11.2585
     14   [36m0.4075[0m   [32m0.8274[0m        0.2412       [31m0.9287[0m        [94m0.2311[0m     +  11.2589
     15   0.3852   0.8028        0.2405       0.9274        0.2467        11.2113
     16   0.3721   [32m0.8289[0m        0.2385       0.9238        [94m0.2247[0m     +  11.1058
     17   0.4068   [32m0.8387[0m        0.2387       0.9262        [94m0.2162[0m     +  11.5110
     18   0.3917   [32m0.8425[0m        0.2390       0.9238        0.2216        11.0840
     19   0.3669   0.8345        [35m0.2361[0m       0.9250        0.2283        11.6637
     20   0.3794   [32m0.8512[0m        0.2366       0.9226        0.2292        11.1949
     21   0.3938   0.8405        [35m0.2348[0m       0.9262        0.2222        11.4271
     22   0.3768   0.8423        0.2369       0.9202        0.2188        11.5319
     23   0.4047   [32m0.8560[0m        [35m0.2339[0m       0.9262        0.2172        11.2806
     24   [36m0.4406[0m   0.8502        [35m0.2313[0m       0.9287        0.2175        11.1751
     25   0.3961   0.8449        0.2327       [31m0.9299[0m        [94m0.2118[0m     +  11.5351
     26   0.4093   0.8557        [35m0.2299[0m       0.9238        0.2206        11.3653
     27   0.4363   [32m0.8564[0m        0.2318       0.9274        0.2150        11.2333
     28   0.3532   0.8451        0.2323       0.9250        0.2168        11.3980
     29   0.3788   0.8521        0.2310       0.9202        0.2138        11.3716
     30   0.3797   [32m0.8579[0m        [35m0.2286[0m       0.9190        0.2293        11.5181
     31   0.3642   0.8325        0.2326       0.9226        0.2188        11.3430
     32   0.3956   0.8489        0.2312       0.9226        0.2128        11.5549
     33   0.3590   0.8310        0.2289       0.9190        0.2294        11.3655
     34   0.3700   0.8414        0.2309       0.9214        0.2209        11.6844
     35   0.3902   0.8497        [35m0.2252[0m       0.9226        [94m0.2115[0m     +  11.2215
     36   0.3663   0.8434        0.2302       0.9262        0.2175        11.2655
     37   0.4195   0.8429        0.2292       0.9238        0.2200        11.4882
     38   0.3791   0.8433        0.2280       0.9250        0.2136        11.0142
     39   0.4004   0.8451        0.2270       0.9274        0.2132        11.1833
     40   0.3979   0.8508        0.2278       0.9287        [94m0.2106[0m     +  11.4078
     41   0.3952   0.8524        0.2270       0.9274        [94m0.2099[0m     +  11.3188
     42   0.3958   0.8490        0.2276       0.9287        0.2116        11.3872
     43   0.3979   0.8431        0.2275       0.9262        0.2130        11.1354
     44   0.3850   0.8394        0.2288       0.9274        0.2160        11.1073
     45   0.3682   0.8335        0.2270       0.9202        0.2239        11.3510
     46   0.3753   0.8510        0.2258       0.9226        0.2110        11.3423
     47   0.3750   0.8467        0.2257       0.9226        0.2145        11.4042
     48   0.3394   0.8331        0.2276       0.9214        0.2230        11.1568
     49   0.3807   0.8508        0.2264       0.9274        0.2106        11.6717
     50   0.3481   0.8253        0.2254       0.9057        0.2722        11.3682
[32m[I 2023-05-02 11:25:01,995][0m Trial 108 finished with value: 0.2099154568496898 and parameters: {'lr': 0.003455818569724779, 'dropout': 0.37220394008548335, 'd_model_multiplier': 8, 'num_layers': 1, 'n_heads': 16, 'dim_feedforward': 414, 'batch_size': 182, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 168}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 234
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1897[0m   [32m0.7228[0m        [35m0.3536[0m       [31m0.6155[0m        [94m0.8439[0m     +  22.6578
      2   [36m0.2214[0m   0.6664        0.3746       [31m0.9262[0m        [94m0.2457[0m     +  22.7501
      3   0.1852   [32m0.7259[0m        0.3660       0.9045        0.3194        23.2906
      4   0.1856   0.7173        0.3609       0.9129        0.2781        22.6957
      5   0.2101   0.6682        [35m0.3163[0m       0.9262        [94m0.2454[0m     +  22.5174
      6   [36m0.2604[0m   0.6908        [35m0.3081[0m       0.9262        [94m0.2411[0m     +  22.9792
      7   0.1980   [32m0.7347[0m        [35m0.2721[0m       0.9129        0.2497        23.1356
      8   0.2075   0.7287        [35m0.2675[0m       0.9262        [94m0.2374[0m     +  23.0633
      9   0.1990   [32m0.7392[0m        [35m0.2644[0m       0.9166        0.2419        22.8579
     10   0.2048   [32m0.7499[0m        [35m0.2602[0m       0.9238        [94m0.2350[0m     +  23.0533
     11   0.2073   0.7387        0.2617       0.9226        0.2381        22.9139
     12   0.2094   0.7377        0.2620       0.9226        0.2375        23.0206
     13   0.2001   0.7247        0.2630       0.9226        0.2392        22.9591
     14   0.2003   0.7387        0.2619       0.9214        0.2390        22.8366
     15   0.2028   0.7398        0.2624       0.9214        0.2371        22.9099
     16   0.1987   0.7332        0.2621       0.9214        0.2374        22.8262
     17   0.1992   0.7342        0.2608       0.9190        0.2383        22.9531
     18   0.1950   0.7346        0.2617       0.9262        0.2373        22.9463
     19   0.2098   0.7343        0.2631       0.9202        0.2382        23.0232
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 11:32:41,798][0m Trial 109 finished with value: 0.23498444608271915 and parameters: {'lr': 0.0035944445723254097, 'dropout': 0.37165386068192197, 'd_model_multiplier': 8, 'num_layers': 7, 'n_heads': 16, 'dim_feedforward': 427, 'batch_size': 13, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 234}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 154
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.3228[0m   [32m0.7876[0m        [35m0.3174[0m       [31m0.9226[0m        [94m0.2896[0m     +  10.9534
      2   [36m0.3406[0m   [32m0.7965[0m        [35m0.2470[0m       0.9226        0.3096        10.9144
      3   [36m0.3581[0m   [32m0.8094[0m        [35m0.2388[0m       0.9190        0.2911        11.4350
      4   0.3349   0.8033        [35m0.2360[0m       [31m0.9250[0m        [94m0.2731[0m     +  11.4025
      5   [36m0.3720[0m   [32m0.8180[0m        [35m0.2304[0m       0.9202        0.2931        11.4223
      6   [36m0.3742[0m   0.8031        0.2357       0.9214        0.3179        11.3409
      7   [36m0.3756[0m   [32m0.8250[0m        0.2335       0.9117        0.3479        11.1371
      8   0.3446   0.8117        0.2312       0.9154        0.2809        11.2979
      9   0.3450   0.8127        0.2346       0.9129        0.3370        11.1114
     10   0.3466   0.7861        [35m0.2274[0m       0.9178        0.3407        11.2112
     11   0.3084   0.7756        0.2298       0.9093        0.3595        11.3692
     12   0.3740   0.7968        0.2299       0.9202        0.2942        11.2781
     13   0.3507   0.8050        0.2276       0.9129        0.3081        11.2474
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 11:35:19,556][0m Trial 110 finished with value: 0.2731120150351611 and parameters: {'lr': 0.0010478999312065742, 'dropout': 0.4238518162705608, 'd_model_multiplier': 8, 'num_layers': 2, 'n_heads': 16, 'dim_feedforward': 381, 'batch_size': 104, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 154}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 57
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.3208[0m   [32m0.7445[0m        [35m0.2924[0m       [31m0.9274[0m        [94m0.3322[0m     +  9.4399
      2   0.2979   [32m0.7587[0m        [35m0.2471[0m       0.9069        0.3501        10.0949
      3   0.2987   [32m0.7596[0m        0.2492       0.8996        0.3787        10.0723
      4   0.3038   0.7223        0.2555       0.9190        0.3457        10.6682
      5   [36m0.3311[0m   0.7082        0.2503       0.9154        0.3983        10.3598
      6   0.3247   0.6841        0.2522       [31m0.9347[0m        [94m0.3267[0m     +  10.2852
      7   0.2598   0.7312        0.2553       0.9238        [94m0.2812[0m     +  10.7262
      8   0.2968   0.7556        0.2546       0.9274        [94m0.2415[0m     +  10.3594
      9   0.3205   [32m0.7623[0m        [35m0.2469[0m       0.9129        [94m0.2390[0m     +  10.5587
     10   0.2961   [32m0.7810[0m        [35m0.2461[0m       0.9274        [94m0.2319[0m     +  10.2969
     11   0.3080   0.7758        [35m0.2453[0m       0.9214        0.2336        10.0776
     12   0.2774   0.7720        [35m0.2437[0m       0.9250        [94m0.2310[0m     +  10.3336
     13   0.3003   0.7692        [35m0.2414[0m       0.9238        0.2375        10.2300
     14   0.3118   0.7759        [35m0.2390[0m       0.9287        0.2330        10.4751
     15   0.2893   0.7737        [35m0.2358[0m       0.9274        [94m0.2272[0m     +  10.9263
     16   0.2892   0.7768        0.2368       0.9274        0.2339        10.4916
     17   0.3013   0.7802        [35m0.2345[0m       0.9287        0.2273        10.2653
     18   0.2880   0.7726        [35m0.2321[0m       0.9262        0.2313        10.6408
     19   0.2865   0.7673        0.2342       0.9274        0.2296        10.5104
     20   0.2870   0.7645        0.2362       0.9262        0.2391        10.6878
     21   0.2945   0.7689        0.2333       0.9274        0.2328        10.3029
     22   0.2787   0.7733        0.2334       0.9262        0.2305        10.4154
     23   0.2933   0.7729        0.2337       0.9262        0.2309        10.5687
     24   0.3088   0.7779        0.2322       0.9274        0.2289        11.2183
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 11:39:40,993][0m Trial 111 finished with value: 0.22717249514185964 and parameters: {'lr': 0.006675374600583172, 'dropout': 0.5546056690164308, 'd_model_multiplier': 8, 'num_layers': 1, 'n_heads': 16, 'dim_feedforward': 360, 'batch_size': 152, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 57}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 166
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2019[0m   [32m0.6768[0m        [35m0.2815[0m       [31m0.9226[0m        [94m0.2722[0m     +  11.2756
      2   0.1920   0.6556        [35m0.2460[0m       [31m0.9238[0m        0.3088        11.0818
      3   [36m0.2243[0m   [32m0.6971[0m        [35m0.2404[0m       0.9141        0.3646        11.4597
      4   0.2199   0.6307        0.2413       0.9214        0.3780        11.4963
      5   0.1650   0.6344        [35m0.2394[0m       0.9214        0.3475        11.3242
      6   0.1861   0.6407        0.2399       0.9154        0.3733        11.4863
      7   0.2043   0.6591        0.2405       0.9141        0.3761        11.6504
      8   0.1480   0.5687        0.2423       0.9154        0.4149        11.5488
      9   0.2008   0.6757        0.2443       0.9045        0.4122        11.4280
     10   0.1619   0.5960        0.2456       0.9202        0.3658        11.4463
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 11:41:46,582][0m Trial 112 finished with value: 0.2721716194371935 and parameters: {'lr': 0.004461902749248374, 'dropout': 0.3925997888514623, 'd_model_multiplier': 8, 'num_layers': 1, 'n_heads': 16, 'dim_feedforward': 409, 'batch_size': 185, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 166}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 171
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.2093[0m   [32m0.6765[0m        [35m0.2872[0m       [31m0.8730[0m        [94m0.4583[0m     +  8.2559
      2   [36m0.2646[0m   0.6370        [35m0.2738[0m       0.8730        0.5041        9.3021
      3   0.1313   0.4921        0.2890       [31m0.9166[0m        [94m0.4102[0m     +  9.7050
      4   [36m0.2769[0m   [32m0.7275[0m        0.2981       0.9166        [94m0.2789[0m     +  9.5935
      5   0.2378   0.7111        0.2937       0.9166        0.3094        9.5294
      6   0.2593   [32m0.7461[0m        0.2903       [31m0.9178[0m        0.3119        9.8106
      7   0.2547   [32m0.7474[0m        0.2846       0.9141        0.2899        9.4533
      8   [36m0.2967[0m   0.7452        [35m0.2681[0m       0.9178        [94m0.2760[0m     +  9.3194
      9   0.2650   [32m0.7522[0m        0.2739       0.9166        0.3160        9.3599
     10   0.2954   [32m0.7782[0m        [35m0.2653[0m       0.9178        [94m0.2618[0m     +  9.8384
     11   [36m0.3122[0m   [32m0.7854[0m        [35m0.2562[0m       0.9178        [94m0.2421[0m     +  9.3611
     12   0.2858   0.7705        [35m0.2472[0m       0.9178        0.2440        9.4289
     13   0.2928   0.7759        0.2472       0.9117        0.2548        9.5976
     14   0.2870   [32m0.7871[0m        0.2506       0.9154        0.2427        9.4469
     15   0.3026   0.7469        [35m0.2454[0m       0.9178        0.2532        9.7834
     16   0.2796   0.7694        [35m0.2398[0m       0.9154        0.2499        9.8087
     17   0.3094   0.7745        [35m0.2391[0m       [31m0.9190[0m        0.2460        9.9322
     18   [36m0.3272[0m   0.7701        0.2404       0.9178        0.2437        9.9378
     19   0.2898   0.7676        0.2414       0.9190        0.2457        9.8515
     20   0.2781   0.7718        0.2412       0.9166        0.2451        9.6984
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 11:45:07,870][0m Trial 113 finished with value: 0.24206983682754682 and parameters: {'lr': 0.0020061479837393723, 'dropout': 0.5312857380161013, 'd_model_multiplier': 8, 'num_layers': 1, 'n_heads': 16, 'dim_feedforward': 472, 'batch_size': 24, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.001, 'top_n_features': 171}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 184
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2454[0m   [32m0.7298[0m        [35m0.3417[0m       [31m0.8888[0m        [94m0.3345[0m     +  10.1398
      2   0.2123   0.7096        0.3500       0.8791        [94m0.3242[0m     +  10.1118
      3   0.1942   0.7049        [35m0.2849[0m       [31m0.9154[0m        [94m0.2551[0m     +  9.7124
      4   0.2067   0.7103        [35m0.2771[0m       0.9117        0.2730        10.2073
      5   0.2026   0.7147        [35m0.2766[0m       0.8827        0.2818        10.2392
      6   0.2101   0.7077        0.2833       0.8900        0.2732        10.5411
      7   0.2199   0.7071        0.2817       0.9141        0.2597        10.2019
      8   0.1896   0.7014        0.2939       0.8888        0.2663        10.1126
      9   0.2017   0.7026        0.3055       0.8900        0.2682        10.0531
     10   0.2402   0.7115        0.3187       [31m0.9287[0m        [94m0.2460[0m     +  10.0040
     11   0.2376   0.7132        0.3153       0.9262        0.2525        9.9301
     12   0.2050   0.7067        0.3074       0.9141        0.2523        9.9896
     13   0.2008   0.7070        0.3052       0.8888        0.2647        10.1031
     14   0.2069   0.7016        0.3145       0.9129        0.2494        10.3378
     15   0.2187   0.7121        0.3226       0.8996        0.2584        10.0716
     16   0.1899   0.7099        0.3105       0.9178        0.2514        9.9718
     17   0.2277   0.7068        0.3227       0.9274        0.2583        9.9934
     18   0.2368   0.7062        0.3219       0.9274        [94m0.2445[0m     +  9.8393
     19   0.2013   0.7110        0.3186       0.9141        0.2569        10.2052
     20   0.2220   0.7112        0.3159       0.9250        0.2484        10.1219
     21   0.2076   0.7036        0.3230       0.8839        0.2847        10.0660
     22   0.1906   0.6919        0.3277       0.9226        0.2459        10.0716
     23   0.1915   0.7037        0.3257       0.9117        0.2546        10.1931
     24   0.2266   0.7148        0.3144       0.8912        0.2766        10.2184
     25   0.2095   0.7089        0.3185       0.9190        0.2453        10.1849
     26   0.2082   0.7099        0.3255       0.9250        [94m0.2411[0m     +  10.0039
     27   0.1993   0.7160        0.3143       0.9202        0.2476        10.0926
     28   0.2019   0.7095        0.3111       0.8912        0.2896        10.3255
     29   0.2262   0.7073        0.3259       0.8972        0.2568        10.4605
     30   0.2125   0.7131        0.3184       0.8912        0.2639        10.2314
     31   0.1894   0.7049        0.3208       0.8900        0.2908        10.0168
     32   0.2213   0.6989        0.3223       0.9274        0.2428        10.1263
     33   0.1935   0.7130        0.3308       0.8888        0.2884        10.3247
     34   0.2014   0.7073        0.3123       0.8984        0.2548        10.4241
     35   0.1909   0.7097        0.3163       0.9250        0.2478        10.2135
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 11:51:13,608][0m Trial 114 finished with value: 0.2411033125839758 and parameters: {'lr': 0.014226617265297968, 'dropout': 0.5947235973353052, 'd_model_multiplier': 8, 'num_layers': 2, 'n_heads': 16, 'dim_feedforward': 344, 'batch_size': 29, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 184}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 178
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.1835[0m   [32m0.6470[0m        [35m0.2749[0m       [31m0.9323[0m        [94m0.2342[0m     +  7.9286
      2   [36m0.1901[0m   [32m0.6748[0m        [35m0.2632[0m       0.9323        [94m0.2328[0m     +  8.7121
      3   [36m0.1978[0m   [32m0.7211[0m        [35m0.2520[0m       0.9323        0.2337        8.9540
      4   [36m0.2039[0m   [32m0.7232[0m        [35m0.2493[0m       0.9311        [94m0.2311[0m     +  8.9901
      5   0.1990   [32m0.7374[0m        [35m0.2485[0m       0.9311        0.2358        9.3637
      6   [36m0.2118[0m   [32m0.7544[0m        [35m0.2410[0m       0.9311        [94m0.2200[0m     +  9.3349
      7   0.2042   0.7505        0.2416       0.9311        0.2230        9.4086
      8   0.2063   [32m0.7597[0m        [35m0.2394[0m       0.9299        0.2203        9.5876
      9   0.2091   0.7546        [35m0.2349[0m       0.9311        0.2206        9.5075
     10   0.2078   0.7530        0.2360       0.9299        0.2211        9.4733
     11   [36m0.2205[0m   0.7538        0.2353       0.9311        0.2206        9.6824
     12   0.2191   0.7547        0.2350       0.9311        0.2266        9.4044
     13   0.2106   0.7402        0.2364       0.9311        0.2305        9.2744
     14   [36m0.2236[0m   0.7408        0.2359       0.9323        0.2283        9.3441
     15   [36m0.2290[0m   [32m0.7800[0m        [35m0.2313[0m       0.9299        [94m0.2164[0m     +  9.4247
     16   0.2250   0.7632        [35m0.2298[0m       0.9323        0.2213        9.4844
     17   0.2279   0.7466        0.2320       0.9311        0.2206        9.7816
     18   [36m0.2466[0m   0.7645        [35m0.2261[0m       0.9323        0.2204        9.4429
     19   0.2230   0.7733        0.2269       0.9287        0.2191        9.4804
     20   0.2275   0.7632        0.2280       0.9287        0.2193        9.3203
     21   0.2073   0.7661        0.2297       0.9250        0.2207        9.4431
     22   [36m0.2540[0m   0.7592        0.2286       [31m0.9347[0m        0.2188        9.4673
     23   0.2304   0.7647        0.2280       0.9299        0.2191        9.1855
     24   0.2090   0.6935        0.2280       0.9311        0.2285        9.3139
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 11:55:06,707][0m Trial 115 finished with value: 0.2164282140557394 and parameters: {'lr': 0.005028341904805781, 'dropout': 0.34419067250361035, 'd_model_multiplier': 1, 'num_layers': 1, 'n_heads': 16, 'dim_feedforward': 435, 'batch_size': 41, 'pos_encoding': 'learnable', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0, 'top_n_features': 178}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 180
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.2050[0m   [32m0.7356[0m        [35m0.2987[0m       [31m0.9141[0m        [94m0.2726[0m     +  8.1840
      2   [36m0.2339[0m   [32m0.7381[0m        [35m0.2534[0m       0.9093        0.2742        8.7384
      3   [36m0.2539[0m   [32m0.7424[0m        [35m0.2475[0m       0.9008        0.2804        8.7739
      4   [36m0.2712[0m   0.7343        [35m0.2469[0m       0.9057        0.2769        9.0109
      5   [36m0.2732[0m   0.7224        [35m0.2428[0m       0.9093        0.2764        9.2694
      6   [36m0.3147[0m   0.7412        [35m0.2351[0m       0.9069        0.2857        9.8380
      7   0.2658   [32m0.7611[0m        0.2393       0.9081        [94m0.2586[0m     +  9.1661
      8   0.2596   0.7343        0.2387       0.9021        0.2731        9.2158
      9   0.2978   [32m0.7617[0m        [35m0.2322[0m       0.9117        0.2592        9.2440
     10   0.2855   [32m0.7896[0m        [35m0.2316[0m       0.9117        [94m0.2529[0m     +  9.5616
     11   0.3004   [32m0.7944[0m        0.2320       0.9093        [94m0.2504[0m     +  9.3921
     12   0.3038   [32m0.7973[0m        [35m0.2293[0m       0.9093        0.2557        9.2370
     13   0.2896   0.7657        0.2316       0.9093        0.2652        9.4610
     14   0.3058   0.7784        [35m0.2279[0m       0.9045        0.2673        9.3408
     15   0.2850   0.7860        0.2280       0.9069        0.2660        9.2914
     16   0.2757   0.7468        [35m0.2278[0m       0.9057        0.2703        9.2051
     17   0.2889   [32m0.7974[0m        [35m0.2232[0m       0.8948        0.2732        9.3918
     18   0.3023   0.7894        0.2265       0.9045        0.2681        9.3077
     19   0.2970   [32m0.7976[0m        0.2268       0.8924        0.2896        9.0238
     20   0.3050   0.7845        [35m0.2198[0m       0.9033        0.2709        9.3605
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 11:58:20,672][0m Trial 116 finished with value: 0.25036641913933505 and parameters: {'lr': 0.0028044390849305736, 'dropout': 0.3509669462588617, 'd_model_multiplier': 1, 'num_layers': 1, 'n_heads': 8, 'dim_feedforward': 445, 'batch_size': 42, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0, 'top_n_features': 180}. Best is trial 12 with value: 0.20472501771481755.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 149
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2343[0m   [32m0.8058[0m        [35m0.2916[0m       [31m0.9274[0m        [94m0.2209[0m     +  10.9006
      2   0.1890   0.7905        [35m0.2581[0m       0.9202        0.3184        11.0039
      3   0.2056   [32m0.8110[0m        [35m0.2512[0m       0.9190        0.2559        11.0315
      4   [36m0.2432[0m   [32m0.8213[0m        [35m0.2490[0m       [31m0.9287[0m        0.2928        11.0355
      5   0.2376   0.7818        0.2494       [31m0.9323[0m        0.2576        11.0208
      6   [36m0.2943[0m   0.8162        [35m0.2479[0m       0.9287        [94m0.2175[0m     +  11.0710
      7   [36m0.3277[0m   0.8082        [35m0.2439[0m       [31m0.9347[0m        [94m0.2003[0m     +  10.8629
      8   0.2838   0.7800        [35m0.2431[0m       [31m0.9383[0m        0.2068        10.8929
      9   0.3046   0.8055        [35m0.2401[0m       0.9335        [94m0.1937[0m     +  11.1852
     10   0.2605   0.7892        [35m0.2380[0m       0.9287        0.2007        11.0142
     11   0.2503   0.7706        [35m0.2363[0m       0.9250        0.2057        11.0755
     12   0.2438   0.7622        0.2370       0.9214        0.2117        11.1096
     13   0.2780   0.8065        [35m0.2335[0m       0.9359        0.1958        11.2489
     14   0.2605   0.8181        0.2359       0.9262        0.2012        11.1983
     15   0.2903   [32m0.8292[0m        [35m0.2280[0m       0.9262        0.2033        11.1224
     16   0.2946   0.8177        0.2293       0.9262        0.2054        11.0094
     17   [36m0.3381[0m   0.8260        0.2300       0.9323        [94m0.1914[0m     +  11.1391
     18   [36m0.3483[0m   0.8283        0.2313       0.9323        0.1944        11.6407
     19   0.3428   0.8227        [35m0.2260[0m       0.9250        0.1961        11.0611
     20   0.2578   0.8069        [35m0.2260[0m       0.9238        0.2063        11.0701
     21   0.3333   0.8192        0.2286       0.9274        0.2036        11.0552
     22   0.3297   0.8275        0.2293       0.9214        0.2020        11.1076
     23   0.3194   0.8205        0.2269       0.9154        0.2129        11.2532
     24   0.2895   0.8085        [35m0.2259[0m       0.9129        0.2099        11.3347
     25   0.3078   0.8087        [35m0.2255[0m       0.9190        0.2103        11.2625
     26   0.3226   0.8138        0.2262       0.9141        0.2228        11.0353
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 12:03:20,950][0m Trial 117 finished with value: 0.191397856754288 and parameters: {'lr': 0.001629696511034259, 'dropout': 0.33317572685232183, 'd_model_multiplier': 1, 'num_layers': 2, 'n_heads': 32, 'dim_feedforward': 437, 'batch_size': 32, 'pos_encoding': 'learnable', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0, 'top_n_features': 149}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 218
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2735[0m   [32m0.7389[0m        [35m0.3146[0m       [31m0.9262[0m        [94m0.2623[0m     +  11.2794
      2   [36m0.2893[0m   [32m0.7551[0m        [35m0.2549[0m       0.9262        0.2715        11.0934
      3   0.2608   0.7471        [35m0.2502[0m       0.9226        [94m0.2573[0m     +  11.3121
      4   0.2658   0.7472        [35m0.2422[0m       0.9250        0.2608        11.4697
      5   [36m0.3136[0m   0.7528        [35m0.2417[0m       0.9262        [94m0.2452[0m     +  11.4049
      6   0.2694   0.7467        [35m0.2377[0m       0.9250        0.2483        11.4199
      7   0.2779   0.7328        [35m0.2356[0m       0.9262        0.2794        11.2754
      8   0.2664   0.7282        [35m0.2351[0m       [31m0.9274[0m        0.2458        11.3303
      9   0.2964   0.7204        [35m0.2343[0m       [31m0.9287[0m        0.2541        11.3063
     10   0.2746   0.7336        [35m0.2313[0m       0.9250        0.2630        11.2181
     11   0.2678   [32m0.7567[0m        [35m0.2278[0m       0.9262        0.2719        11.3662
     12   0.2691   0.7161        [35m0.2273[0m       0.9262        0.2475        11.4523
     13   0.2530   [32m0.7703[0m        [35m0.2225[0m       0.9262        [94m0.2442[0m     +  11.4027
     14   0.2524   0.7570        0.2270       0.9250        0.2463        11.4434
     15   0.2935   0.7661        0.2265       0.9274        [94m0.2425[0m     +  11.4029
     16   0.2469   [32m0.7745[0m        0.2240       0.9262        0.2454        11.4207
     17   0.2589   [32m0.7765[0m        0.2231       0.9287        [94m0.2366[0m     +  11.3590
     18   0.2687   0.7713        [35m0.2215[0m       0.9262        0.2367        11.4523
     19   0.2805   [32m0.7908[0m        [35m0.2179[0m       0.9274        [94m0.2314[0m     +  11.3864
     20   0.2680   0.7828        0.2203       [31m0.9299[0m        0.2322        11.6141
     21   0.2685   0.7827        0.2183       0.9226        0.2348        11.3543
     22   0.2805   0.7749        [35m0.2170[0m       0.9226        0.2404        11.2597
     23   0.2692   0.7683        [35m0.2156[0m       0.9226        0.2421        11.2791
     24   0.2676   0.7840        [35m0.2153[0m       0.9214        0.2397        11.5969
     25   0.2506   0.7717        [35m0.2144[0m       0.9202        0.2389        11.9110
     26   0.2588   0.7826        [35m0.2130[0m       0.9190        0.2383        11.5007
     27   0.2573   0.7780        [35m0.2127[0m       0.9226        0.2454        11.3560
     28   0.2775   0.7631        0.2127       0.9287        0.2349        11.3601
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 12:08:52,202][0m Trial 118 finished with value: 0.23137791775968805 and parameters: {'lr': 0.0008366330406028387, 'dropout': 0.3411969729267832, 'd_model_multiplier': 1, 'num_layers': 2, 'n_heads': 32, 'dim_feedforward': 438, 'batch_size': 48, 'pos_encoding': 'learnable', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0, 'top_n_features': 218}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 145
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1789[0m   [32m0.6678[0m        [35m0.2725[0m       [31m0.9250[0m        [94m0.2663[0m     +  11.1801
      2   [36m0.2315[0m   0.6486        [35m0.2545[0m       0.9250        [94m0.2620[0m     +  11.3259
      3   [36m0.2532[0m   [32m0.6771[0m        [35m0.2480[0m       0.9250        [94m0.2521[0m     +  11.1431
      4   [36m0.2888[0m   [32m0.7136[0m        [35m0.2412[0m       [31m0.9287[0m        [94m0.2422[0m     +  11.2750
      5   [36m0.3054[0m   [32m0.7173[0m        0.2429       0.9262        0.2435        11.4085
      6   [36m0.3180[0m   [32m0.7630[0m        [35m0.2383[0m       0.9250        [94m0.2362[0m     +  11.2141
      7   [36m0.3290[0m   [32m0.7787[0m        [35m0.2381[0m       0.9274        [94m0.2265[0m     +  11.1582
      8   [36m0.3386[0m   0.7714        0.2389       0.9262        0.2318        11.1235
      9   0.3333   0.7771        [35m0.2339[0m       0.9262        0.2270        11.3305
     10   [36m0.3457[0m   0.7637        [35m0.2311[0m       0.9287        [94m0.2249[0m     +  11.0921
     11   0.3370   0.7521        0.2312       [31m0.9311[0m        0.2285        11.1736
     12   [36m0.3652[0m   [32m0.7951[0m        [35m0.2285[0m       0.9274        [94m0.2184[0m     +  11.2186
     13   [36m0.3799[0m   [32m0.7964[0m        0.2288       0.9287        0.2199        11.3753
     14   0.3656   0.7954        [35m0.2259[0m       0.9299        0.2216        11.2434
     15   0.3496   0.7753        0.2261       0.9287        0.2287        11.1901
     16   0.3737   [32m0.7965[0m        0.2262       0.9287        [94m0.2179[0m     +  11.2443
     17   0.3768   [32m0.7977[0m        0.2261       0.9287        0.2236        11.3287
     18   [36m0.3826[0m   [32m0.8010[0m        [35m0.2223[0m       0.9311        [94m0.2157[0m     +  11.3944
     19   0.3630   0.7938        [35m0.2211[0m       0.9299        0.2217        11.3724
     20   0.3654   0.7954        0.2233       0.9299        0.2193        11.4319
     21   0.3789   0.7848        0.2212       0.9287        0.2263        11.1060
     22   [36m0.3992[0m   0.7932        [35m0.2206[0m       0.9299        0.2169        11.2684
     23   0.3922   [32m0.8025[0m        0.2230       0.9311        0.2212        11.2930
     24   0.3898   [32m0.8127[0m        [35m0.2201[0m       0.9299        [94m0.2152[0m     +  11.2776
     25   0.3664   0.8004        [35m0.2184[0m       0.9299        0.2187        11.3143
     26   0.3805   0.7868        [35m0.2177[0m       0.9299        0.2269        11.2860
     27   0.3973   0.8117        0.2178       0.9299        [94m0.2106[0m     +  11.2712
     28   0.3860   0.7996        0.2178       0.9299        0.2168        11.2833
     29   0.3813   0.7999        [35m0.2167[0m       0.9299        0.2230        11.3079
     30   0.3809   0.7996        [35m0.2162[0m       0.9311        0.2182        11.2034
     31   0.3666   0.7835        0.2184       0.9311        0.2260        11.3014
     32   0.3818   0.8061        [35m0.2153[0m       0.9287        0.2125        11.1197
     33   0.3869   0.8026        [35m0.2140[0m       [31m0.9323[0m        0.2163        11.0135
     34   0.3875   0.8001        0.2142       0.9323        0.2183        11.3416
     35   0.3893   0.8089        [35m0.2132[0m       0.9287        0.2134        11.2285
     36   0.3915   0.8036        0.2143       0.9311        0.2153        11.3014
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 12:15:50,104][0m Trial 119 finished with value: 0.2106102122153812 and parameters: {'lr': 0.0014131523918753702, 'dropout': 0.32127069195274566, 'd_model_multiplier': 1, 'num_layers': 2, 'n_heads': 32, 'dim_feedforward': 418, 'batch_size': 21, 'pos_encoding': 'learnable', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0, 'top_n_features': 145}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 146
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2338[0m   [32m0.7623[0m        [35m0.3659[0m       [31m0.9093[0m        [94m0.2547[0m     +  15.9210
      2   [36m0.2611[0m   [32m0.7892[0m        [35m0.2530[0m       [31m0.9154[0m        [94m0.2512[0m     +  16.5138
      3   [36m0.2781[0m   0.7825        [35m0.2455[0m       [31m0.9190[0m        [94m0.2479[0m     +  16.6665
      4   [36m0.2907[0m   0.7832        [35m0.2451[0m       0.9190        [94m0.2431[0m     +  16.3395
      5   [36m0.3360[0m   [32m0.7934[0m        [35m0.2392[0m       [31m0.9214[0m        0.2491        15.8411
      6   0.3032   0.7872        [35m0.2356[0m       0.9214        [94m0.2381[0m     +  15.8608
      7   [36m0.3597[0m   0.7915        [35m0.2328[0m       [31m0.9226[0m        [94m0.2345[0m     +  16.1499
      8   [36m0.3618[0m   [32m0.7979[0m        [35m0.2265[0m       0.9226        [94m0.2305[0m     +  16.4112
      9   [36m0.3874[0m   [32m0.8029[0m        0.2274       [31m0.9262[0m        [94m0.2254[0m     +  16.2848
     10   [36m0.3881[0m   [32m0.8079[0m        [35m0.2243[0m       0.9250        [94m0.2236[0m     +  16.1101
     11   [36m0.3989[0m   [32m0.8101[0m        [35m0.2239[0m       [31m0.9299[0m        [94m0.2214[0m     +  16.2363
     12   0.3826   [32m0.8123[0m        [35m0.2197[0m       0.9226        0.2240        16.5659
     13   0.3762   0.8091        [35m0.2188[0m       0.9202        0.2247        16.3760
     14   0.3635   0.7986        [35m0.2183[0m       0.9202        0.2280        15.6799
     15   0.3683   0.8085        [35m0.2147[0m       0.9190        0.2244        16.4421
     16   0.3917   0.8111        [35m0.2129[0m       0.9238        0.2221        16.1824
     17   0.3901   [32m0.8127[0m        0.2133       0.9226        0.2229        16.3408
     18   0.3648   0.8027        [35m0.2094[0m       0.9190        0.2264        16.3949
     19   0.3497   0.7571        [35m0.2065[0m       0.9226        0.2406        16.2632
     20   0.3698   0.7938        [35m0.2062[0m       0.9202        0.2283        16.3159
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 12:21:31,659][0m Trial 120 finished with value: 0.22137135858259005 and parameters: {'lr': 0.0014562308185067587, 'dropout': 0.31645137748759444, 'd_model_multiplier': 1, 'num_layers': 3, 'n_heads': 32, 'dim_feedforward': 419, 'batch_size': 144, 'pos_encoding': 'learnable', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0, 'top_n_features': 146}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 137
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2648[0m   [32m0.7141[0m        [35m0.3020[0m       [31m0.9371[0m        [94m0.2270[0m     +  11.0601
      2   [36m0.2879[0m   [32m0.7287[0m        [35m0.2598[0m       [31m0.9420[0m        [94m0.2214[0m     +  11.7879
      3   [36m0.3006[0m   0.7221        [35m0.2505[0m       0.9371        [94m0.2173[0m     +  11.1093
      4   [36m0.3013[0m   [32m0.7360[0m        [35m0.2462[0m       0.9395        0.2206        11.2604
      5   [36m0.3023[0m   [32m0.7456[0m        [35m0.2446[0m       [31m0.9432[0m        0.2183        11.1246
      6   0.2927   0.7421        [35m0.2424[0m       0.9407        0.2178        11.4930
      7   0.2820   0.7450        [35m0.2347[0m       0.9407        [94m0.2108[0m     +  11.4138
      8   0.2837   [32m0.7491[0m        0.2349       0.9407        0.2115        11.3088
      9   0.2782   [32m0.7513[0m        [35m0.2294[0m       0.9420        0.2195        11.8093
     10   0.2782   [32m0.7616[0m        [35m0.2275[0m       0.9359        0.2173        11.4023
     11   0.2609   [32m0.7779[0m        [35m0.2256[0m       0.9359        0.2137        11.6600
     12   0.2408   [32m0.7820[0m        [35m0.2223[0m       0.9299        0.2206        11.3501
     13   0.2706   [32m0.7938[0m        0.2252       0.9335        0.2154        11.6212
     14   0.2549   0.7720        [35m0.2215[0m       0.9359        0.2203        11.5290
     15   0.2908   0.7702        [35m0.2186[0m       0.9395        0.2108        11.4287
     16   [36m0.3066[0m   0.7512        [35m0.2160[0m       0.9407        [94m0.2054[0m     +  11.8063
     17   0.2543   0.7432        [35m0.2153[0m       0.9359        0.2189        11.3513
     18   0.2666   0.7350        [35m0.2122[0m       0.9383        0.2207        11.2576
     19   0.2961   0.7560        [35m0.2053[0m       0.9335        0.2170        11.4684
     20   0.2978   0.7378        0.2088       0.9432        0.2153        11.5102
     21   0.2739   0.7489        0.2062       0.9347        0.2272        11.4223
     22   0.2867   0.7487        0.2072       0.9323        0.2255        11.5286
     23   0.2943   0.7285        [35m0.2022[0m       0.9359        0.2286        11.3159
     24   0.2367   0.7460        0.2025       0.9335        0.2222        11.3817
     25   0.2564   0.7169        [35m0.2015[0m       0.9335        0.2296        11.5360
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 12:26:29,833][0m Trial 121 finished with value: 0.20536984351014284 and parameters: {'lr': 0.0004061803683819846, 'dropout': 0.3216638493285997, 'd_model_multiplier': 1, 'num_layers': 2, 'n_heads': 32, 'dim_feedforward': 410, 'batch_size': 22, 'pos_encoding': 'learnable', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0, 'top_n_features': 137}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 131
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2891[0m   [32m0.7081[0m        [35m0.2736[0m       [31m0.8984[0m        [94m0.3220[0m     +  11.2235
      2   0.2767   [32m0.7101[0m        [35m0.2555[0m       [31m0.9141[0m        [94m0.3063[0m     +  11.4391
      3   [36m0.3150[0m   [32m0.7543[0m        [35m0.2494[0m       0.9105        [94m0.2672[0m     +  11.3422
      4   [36m0.3244[0m   0.7243        [35m0.2433[0m       0.9008        0.2700        11.2030
      5   [36m0.3254[0m   0.7461        [35m0.2366[0m       0.9033        0.2692        11.3575
      6   [36m0.3404[0m   0.7503        [35m0.2320[0m       [31m0.9166[0m        [94m0.2632[0m     +  11.2403
      7   0.3280   0.7518        0.2324       0.9117        0.2705        11.2866
      8   [36m0.3657[0m   0.7409        [35m0.2287[0m       0.9105        0.2713        11.1839
      9   0.3620   0.7338        [35m0.2273[0m       0.9093        0.2738        11.2849
     10   0.3141   0.7490        [35m0.2247[0m       0.9105        0.2652        11.1600
     11   0.3629   [32m0.7635[0m        0.2284       0.9045        0.2691        11.2462
     12   0.3194   [32m0.7675[0m        0.2249       0.9093        0.3006        11.2524
     13   0.3072   0.7348        0.2263       0.9069        0.2887        11.2809
     14   0.3227   0.7643        [35m0.2221[0m       0.9081        0.3188        11.2778
     15   0.3415   0.7348        0.2234       0.9093        0.2693        11.0594
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 12:29:30,286][0m Trial 122 finished with value: 0.26318440329567266 and parameters: {'lr': 0.0019904352119823458, 'dropout': 0.31744134376442557, 'd_model_multiplier': 1, 'num_layers': 2, 'n_heads': 32, 'dim_feedforward': 432, 'batch_size': 23, 'pos_encoding': 'learnable', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0, 'top_n_features': 131}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 138
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.2113[0m   [32m0.6595[0m        [35m0.2986[0m       [31m0.9202[0m        [94m0.2616[0m     +  9.3763
      2   [36m0.2254[0m   0.6592        [35m0.2638[0m       0.9202        0.2643        9.4925
      3   [36m0.2294[0m   0.6382        [35m0.2530[0m       0.9202        0.2662        9.6408
      4   [36m0.2294[0m   0.6584        0.2548       [31m0.9226[0m        0.2654        9.2393
      5   0.1859   0.6246        [35m0.2486[0m       0.9202        0.2806        9.2991
      6   [36m0.2312[0m   0.6353        [35m0.2423[0m       0.9202        0.2716        9.4279
      7   0.2116   0.6038        0.2462       [31m0.9238[0m        0.2731        9.7216
      8   [36m0.2578[0m   [32m0.6896[0m        0.2436       0.9214        [94m0.2578[0m     +  9.3738
      9   0.2292   0.6545        [35m0.2371[0m       [31m0.9250[0m        0.2642        9.6034
     10   0.2253   0.6356        0.2403       0.9226        0.2697        9.6888
     11   0.2427   0.6612        [35m0.2345[0m       0.9250        0.2668        9.6756
     12   [36m0.2606[0m   0.6680        [35m0.2319[0m       0.9226        0.2585        9.5473
     13   [36m0.2726[0m   [32m0.7015[0m        0.2339       0.9250        [94m0.2512[0m     +  10.4290
     14   0.2608   [32m0.7133[0m        [35m0.2290[0m       0.9202        0.2598        9.6452
     15   0.2634   0.6901        [35m0.2281[0m       0.9238        0.2573        9.7500
     16   0.2694   0.6915        [35m0.2278[0m       0.9250        0.2717        9.9016
     17   0.2489   0.6770        [35m0.2266[0m       0.9214        0.2701        9.7745
     18   [36m0.2755[0m   0.7056        0.2269       0.9238        0.2651        9.3697
     19   [36m0.2937[0m   0.7124        [35m0.2218[0m       0.9226        0.2517        10.0854
     20   0.2750   0.6975        [35m0.2203[0m       0.9214        0.2582        9.5567
     21   0.2666   0.6833        [35m0.2202[0m       0.9214        0.2625        9.4394
     22   0.2784   0.6662        [35m0.2166[0m       0.9226        0.2654        9.3858
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 12:33:11,938][0m Trial 123 finished with value: 0.2511733742261884 and parameters: {'lr': 0.00046577787561240404, 'dropout': 0.349092674715513, 'd_model_multiplier': 1, 'num_layers': 1, 'n_heads': 32, 'dim_feedforward': 397, 'batch_size': 12, 'pos_encoding': 'learnable', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0, 'top_n_features': 138}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 127
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2596[0m   [32m0.7649[0m        [35m0.2772[0m       [31m0.9033[0m        [94m0.2743[0m     +  10.9315
      2   [36m0.2599[0m   [32m0.7758[0m        [35m0.2507[0m       [31m0.9045[0m        0.2763        11.1661
      3   [36m0.2887[0m   [32m0.8058[0m        [35m0.2475[0m       [31m0.9093[0m        [94m0.2647[0m     +  11.3223
      4   0.2694   0.7967        [35m0.2368[0m       0.9033        0.2724        11.1805
      5   [36m0.2889[0m   [32m0.8178[0m        0.2378       0.9057        [94m0.2608[0m     +  11.2347
      6   [36m0.2927[0m   [32m0.8187[0m        [35m0.2322[0m       0.9033        0.2733        11.0502
      7   [36m0.2948[0m   [32m0.8222[0m        [35m0.2300[0m       0.9033        0.2976        11.1001
      8   [36m0.3254[0m   [32m0.8300[0m        [35m0.2273[0m       0.9069        [94m0.2474[0m     +  11.2232
      9   0.2982   0.8231        [35m0.2245[0m       0.9021        0.2804        11.3121
     10   0.3135   0.8213        [35m0.2235[0m       0.9069        0.2522        11.3183
     11   0.3015   0.8251        [35m0.2232[0m       0.9069        0.3117        11.4070
     12   0.3111   0.8212        0.2240       0.9033        0.3341        11.3946
     13   [36m0.3268[0m   [32m0.8346[0m        [35m0.2197[0m       0.9045        0.3742        11.4618
     14   [36m0.3294[0m   [32m0.8417[0m        0.2216       0.9081        0.3909        11.8566
     15   [36m0.3317[0m   0.8380        0.2218       0.9057        0.4579        11.3943
     16   0.3315   0.8407        [35m0.2187[0m       0.9081        0.5156        11.4540
     17   0.3196   0.8361        0.2196       0.9057        0.7506        11.4347
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 12:36:36,363][0m Trial 124 finished with value: 0.24742402787688567 and parameters: {'lr': 0.0011068350231987717, 'dropout': 0.29969723347618127, 'd_model_multiplier': 1, 'num_layers': 2, 'n_heads': 32, 'dim_feedforward': 405, 'batch_size': 18, 'pos_encoding': 'learnable', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0, 'top_n_features': 127}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 142
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2568[0m   [32m0.7522[0m        [35m0.3260[0m       [31m0.9178[0m        [94m0.2591[0m     +  13.8764
      2   [36m0.2716[0m   [32m0.7849[0m        [35m0.2555[0m       0.9154        [94m0.2447[0m     +  14.0411
      3   [36m0.2792[0m   [32m0.7897[0m        [35m0.2477[0m       0.9166        [94m0.2434[0m     +  14.0653
      4   [36m0.3266[0m   [32m0.8042[0m        [35m0.2448[0m       [31m0.9214[0m        [94m0.2367[0m     +  14.0359
      5   0.2992   0.7985        [35m0.2396[0m       0.9129        0.2459        14.0350
      6   0.3134   0.7722        [35m0.2374[0m       0.9166        0.2475        14.0332
      7   [36m0.3336[0m   0.7872        [35m0.2348[0m       0.9190        0.2405        14.0976
      8   0.3302   0.7939        [35m0.2341[0m       0.9154        0.2372        13.9892
      9   0.3082   [32m0.8049[0m        [35m0.2298[0m       0.9117        [94m0.2360[0m     +  13.8206
     10   [36m0.3378[0m   [32m0.8070[0m        [35m0.2243[0m       0.9154        [94m0.2341[0m     +  13.9289
     11   0.2959   0.7949        0.2266       0.9117        0.2471        14.1480
     12   0.3233   0.8023        [35m0.2217[0m       0.9154        0.2376        14.0974
     13   [36m0.3418[0m   0.8015        [35m0.2201[0m       0.9166        0.2384        13.9740
     14   0.3279   [32m0.8103[0m        [35m0.2159[0m       0.9141        0.2380        13.9367
     15   0.3127   0.7990        [35m0.2145[0m       0.9105        0.2413        13.6888
     16   0.2962   0.7879        [35m0.2141[0m       0.9141        0.2454        14.0306
     17   0.2916   0.7824        [35m0.2099[0m       0.9141        0.2543        13.7952
     18   0.2974   0.7777        0.2104       0.9129        0.2588        13.8897
     19   0.2810   0.7809        [35m0.2067[0m       0.9129        0.2563        13.9664
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 12:41:16,293][0m Trial 125 finished with value: 0.2341121744450087 and parameters: {'lr': 0.00037924058627114034, 'dropout': 0.3289496742984887, 'd_model_multiplier': 1, 'num_layers': 3, 'n_heads': 32, 'dim_feedforward': 411, 'batch_size': 27, 'pos_encoding': 'learnable', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0, 'top_n_features': 142}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 135
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2238[0m   [32m0.7615[0m        [35m0.3187[0m       [31m0.9081[0m        [94m0.2805[0m     +  13.4671
      2   [36m0.2526[0m   [32m0.7804[0m        [35m0.2500[0m       [31m0.9166[0m        [94m0.2520[0m     +  13.6851
      3   0.2472   0.7795        [35m0.2418[0m       0.9129        [94m0.2517[0m     +  13.7514
      4   0.2493   0.7673        [35m0.2407[0m       [31m0.9178[0m        0.2548        13.7769
      5   [36m0.2812[0m   0.7751        [35m0.2353[0m       0.9166        [94m0.2450[0m     +  13.8058
      6   0.2789   [32m0.7909[0m        [35m0.2339[0m       0.9178        0.2454        13.8214
      7   0.2769   0.7735        [35m0.2297[0m       0.9166        0.2454        13.8966
      8   0.2712   0.7716        0.2320       [31m0.9190[0m        [94m0.2435[0m     +  13.9925
      9   [36m0.2868[0m   0.7728        [35m0.2246[0m       0.9154        [94m0.2425[0m     +  13.9850
     10   [36m0.2982[0m   [32m0.7942[0m        0.2250       [31m0.9202[0m        0.2452        13.7478
     11   0.2938   0.7681        [35m0.2216[0m       0.9202        0.2450        13.9808
     12   0.2699   0.7742        0.2223       0.9190        0.2440        13.7976
     13   0.2898   0.7739        [35m0.2201[0m       0.9178        0.2448        13.8797
     14   0.2859   0.7808        [35m0.2167[0m       0.9154        0.2498        13.7130
     15   [36m0.3086[0m   0.7860        0.2196       [31m0.9226[0m        [94m0.2411[0m     +  13.6826
     16   [36m0.3171[0m   [32m0.7946[0m        [35m0.2158[0m       0.9214        [94m0.2390[0m     +  13.5536
     17   0.3130   0.7913        0.2164       0.9202        0.2398        13.8235
     18   0.3090   0.7816        [35m0.2133[0m       0.9093        0.2475        13.8728
     19   0.2809   0.7839        [35m0.2120[0m       0.8839        0.2719        14.0863
     20   0.3061   0.7655        0.2123       0.9069        0.2575        13.9786
     21   0.3012   0.7741        [35m0.2119[0m       0.8996        0.2638        13.7257
     22   0.2988   0.7635        [35m0.2083[0m       0.9202        0.2507        13.7648
     23   0.2960   0.7728        0.2161       0.9202        0.2544        13.7652
     24   0.2923   0.7741        0.2140       0.9178        0.2620        13.9143
     25   0.2939   0.7776        0.2106       0.9178        0.2453        13.9036
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 12:47:16,215][0m Trial 126 finished with value: 0.23895787498142296 and parameters: {'lr': 0.0009126895304410177, 'dropout': 0.3679526930480695, 'd_model_multiplier': 1, 'num_layers': 3, 'n_heads': 32, 'dim_feedforward': 389, 'batch_size': 39, 'pos_encoding': 'learnable', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0, 'top_n_features': 135}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 148
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.2156[0m   [32m0.7328[0m        [35m0.2764[0m       [31m0.9117[0m        [94m0.2704[0m     +  8.9147
      2   [36m0.2458[0m   0.7288        [35m0.2527[0m       0.9081        [94m0.2675[0m     +  9.3360
      3   [36m0.2615[0m   [32m0.7419[0m        [35m0.2465[0m       0.9117        0.2691        8.9665
      4   0.2355   0.7172        0.2493       0.9105        0.2679        8.9325
      5   0.2400   0.7072        [35m0.2420[0m       0.9117        0.2848        9.2965
      6   [36m0.3013[0m   0.7237        [35m0.2406[0m       [31m0.9190[0m        0.2845        9.4286
      7   0.2292   0.6996        [35m0.2384[0m       0.9129        0.2739        9.3215
      8   [36m0.3103[0m   [32m0.7943[0m        [35m0.2325[0m       0.9178        [94m0.2590[0m     +  9.2968
      9   0.2700   0.7512        [35m0.2316[0m       0.9081        0.2696        8.9835
     10   0.2931   0.7615        [35m0.2294[0m       0.9117        0.2615        9.2538
     11   [36m0.3188[0m   [32m0.8002[0m        [35m0.2281[0m       0.9154        [94m0.2505[0m     +  9.3082
     12   [36m0.3239[0m   0.7821        0.2294       0.9166        0.2559        9.0690
     13   0.2961   0.7649        0.2295       0.9117        0.2604        9.3715
     14   0.3154   0.7917        0.2287       0.9081        0.2613        9.3551
     15   [36m0.3301[0m   0.7903        [35m0.2250[0m       [31m0.9202[0m        0.2551        9.5018
     16   0.3085   0.7854        0.2270       0.9154        0.2520        9.3654
     17   0.3180   0.7863        [35m0.2248[0m       0.9021        0.2580        9.2264
     18   0.2826   0.7518        0.2260       0.9129        0.2658        9.1474
     19   0.2603   0.7418        [35m0.2226[0m       0.9154        0.2748        9.4929
     20   0.3262   0.7933        [35m0.2212[0m       0.9166        0.2517        9.4459
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 12:50:30,889][0m Trial 127 finished with value: 0.25049476367855017 and parameters: {'lr': 0.0015804165613920542, 'dropout': 0.29527807112745735, 'd_model_multiplier': 1, 'num_layers': 1, 'n_heads': 32, 'dim_feedforward': 422, 'batch_size': 33, 'pos_encoding': 'learnable', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0, 'top_n_features': 148}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 151
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.3090[0m   [32m0.7988[0m        [35m0.2794[0m       [31m0.9238[0m        [94m0.3026[0m     +  11.0678
      2   [36m0.3701[0m   [32m0.8397[0m        [35m0.2699[0m       [31m0.9250[0m        [94m0.2359[0m     +  11.2258
      3   0.3483   [32m0.8548[0m        [35m0.2563[0m       0.9202        [94m0.2207[0m     +  11.2033
      4   0.3382   0.8455        [35m0.2545[0m       0.9214        [94m0.2168[0m     +  11.1605
      5   0.3572   0.8415        [35m0.2480[0m       0.9093        0.2314        11.1721
      6   0.3581   0.8413        [35m0.2465[0m       0.9166        0.2284        11.3118
      7   0.3488   0.8363        [35m0.2457[0m       0.9202        0.2211        11.0846
      8   0.3461   0.8409        [35m0.2419[0m       0.9214        0.2200        11.0090
      9   0.3532   0.8493        0.2452       0.9190        0.2205        11.0472
     10   0.3590   0.8462        [35m0.2411[0m       0.9190        0.2270        11.2573
     11   [36m0.3889[0m   0.8460        0.2417       0.9166        0.2195        11.1908
     12   0.3860   0.8495        [35m0.2392[0m       0.9202        0.2169        11.2747
     13   0.3240   0.8466        0.2419       0.9202        0.2255        11.0983
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 12:53:07,575][0m Trial 128 finished with value: 0.2168140946251161 and parameters: {'lr': 0.003068203851564654, 'dropout': 0.3788934763752146, 'd_model_multiplier': 1, 'num_layers': 2, 'n_heads': 32, 'dim_feedforward': 444, 'batch_size': 22, 'pos_encoding': 'learnable', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0, 'top_n_features': 151}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 112
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2667[0m   [32m0.7851[0m        [35m0.2684[0m       [31m0.9274[0m        [94m0.2235[0m     +  11.2578
      2   [36m0.2843[0m   [32m0.7897[0m        [35m0.2485[0m       0.9262        [94m0.2206[0m     +  11.4367
      3   [36m0.3203[0m   [32m0.7982[0m        [35m0.2440[0m       [31m0.9287[0m        [94m0.2174[0m     +  11.3986
      4   [36m0.3380[0m   [32m0.7992[0m        [35m0.2375[0m       [31m0.9359[0m        0.2236        11.4401
      5   [36m0.3383[0m   [32m0.8086[0m        [35m0.2362[0m       0.9299        0.2231        11.5866
      6   0.2846   0.7979        [35m0.2343[0m       0.9238        0.2261        11.4520
      7   0.2890   0.7957        [35m0.2329[0m       0.9262        0.2282        11.2676
      8   0.2865   0.7882        [35m0.2324[0m       0.9166        0.2486        11.6367
      9   0.2964   0.8079        [35m0.2316[0m       0.9299        0.2267        11.5859
     10   0.2716   0.7851        0.2319       0.9238        0.2470        11.4700
     11   0.2668   0.7824        [35m0.2294[0m       0.9154        0.2546        11.4675
     12   0.2801   0.7975        0.2306       0.9202        0.2567        11.5149
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 12:55:37,289][0m Trial 129 finished with value: 0.21735880580903036 and parameters: {'lr': 0.0006853213937095384, 'dropout': 0.4106621225402725, 'd_model_multiplier': 1, 'num_layers': 2, 'n_heads': 32, 'dim_feedforward': 452, 'batch_size': 13, 'pos_encoding': 'learnable', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0, 'top_n_features': 112}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 143
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1837[0m   [32m0.6833[0m        [35m0.3232[0m       [31m0.9287[0m        [94m0.2378[0m     +  10.9908
      2   [36m0.2275[0m   [32m0.6939[0m        [35m0.2779[0m       0.9274        [94m0.2344[0m     +  10.8510
      3   0.2169   0.6625        [35m0.2682[0m       0.9274        0.2427        10.4580
      4   [36m0.2381[0m   0.6556        [35m0.2659[0m       [31m0.9299[0m        0.2446        10.4623
      5   0.2300   0.6774        [35m0.2606[0m       0.9287        0.2402        10.6015
      6   0.2159   0.6588        [35m0.2591[0m       [31m0.9311[0m        0.2453        10.3989
      7   0.2344   0.6807        [35m0.2546[0m       0.9274        0.2438        10.5508
      8   [36m0.2393[0m   0.6525        [35m0.2516[0m       0.9274        0.2522        10.4177
      9   0.2354   0.6687        0.2548       0.9274        0.2458        10.7509
     10   [36m0.2423[0m   0.6828        [35m0.2475[0m       0.9311        0.2485        10.8816
     11   0.2253   0.6624        [35m0.2446[0m       0.9311        0.2488        10.9609
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 12:57:45,919][0m Trial 130 finished with value: 0.2344096940474758 and parameters: {'lr': 0.00018020517976373063, 'dropout': 0.3904771076966729, 'd_model_multiplier': 1, 'num_layers': 1, 'n_heads': 32, 'dim_feedforward': 431, 'batch_size': 8, 'pos_encoding': 'learnable', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0, 'top_n_features': 143}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 157
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2648[0m   [32m0.7411[0m        [35m0.2797[0m       [31m0.9117[0m        [94m0.2657[0m     +  11.1470
      2   [36m0.2813[0m   [32m0.7664[0m        [35m0.2623[0m       0.9105        [94m0.2614[0m     +  11.4349
      3   0.2772   [32m0.7764[0m        [35m0.2529[0m       [31m0.9129[0m        0.2645        11.1959
      4   [36m0.2947[0m   [32m0.7764[0m        [35m0.2477[0m       0.9105        0.2646        11.2182
      5   [36m0.3153[0m   0.7748        [35m0.2413[0m       0.9129        [94m0.2608[0m     +  11.3475
      6   0.3015   [32m0.7776[0m        [35m0.2392[0m       [31m0.9141[0m        [94m0.2595[0m     +  11.0402
      7   [36m0.3512[0m   [32m0.7840[0m        [35m0.2362[0m       [31m0.9154[0m        [94m0.2559[0m     +  11.3257
      8   0.3317   0.7830        [35m0.2349[0m       0.9141        0.2595        11.1063
      9   0.3151   0.7738        [35m0.2317[0m       0.9154        0.2670        11.4185
     10   0.3183   [32m0.7908[0m        [35m0.2300[0m       0.9141        0.2575        11.2515
     11   0.3208   0.7879        [35m0.2298[0m       0.9045        0.2650        11.3310
     12   0.3381   [32m0.7911[0m        0.2313       [31m0.9166[0m        [94m0.2525[0m     +  11.3828
     13   0.3293   [32m0.7926[0m        [35m0.2277[0m       0.9141        0.2564        11.4296
     14   0.3236   0.7916        0.2288       0.9154        [94m0.2522[0m     +  11.4285
     15   0.3090   0.7835        0.2288       0.9129        0.2551        11.1764
     16   0.3465   [32m0.7999[0m        [35m0.2254[0m       0.9057        0.2553        11.1686
     17   0.3285   0.7877        0.2283       0.9166        0.2536        11.1925
     18   0.3433   0.7884        0.2255       0.9166        0.2552        11.1365
     19   0.3129   0.7910        [35m0.2245[0m       0.9117        0.2540        10.9765
     20   0.2902   0.7856        [35m0.2242[0m       0.9117        0.2561        11.1085
     21   0.3288   0.7993        [35m0.2221[0m       0.9141        [94m0.2497[0m     +  11.1521
     22   0.3270   0.7937        0.2230       0.9166        0.2506        11.4229
     23   0.3465   0.7933        0.2240       0.9117        0.2509        11.2113
     24   0.3394   0.7941        0.2238       0.9117        0.2535        11.4445
     25   0.3140   0.7920        [35m0.2207[0m       0.9008        0.2671        11.4643
     26   0.3170   0.7853        0.2221       0.9117        0.2558        11.6381
     27   0.3303   [32m0.8038[0m        0.2237       0.9069        0.2609        11.2979
     28   0.3364   0.8005        0.2220       0.9141        [94m0.2491[0m     +  11.1837
     29   0.3391   0.7976        0.2237       0.9154        0.2507        11.3882
     30   0.2830   0.7768        [35m0.2207[0m       0.9117        0.2636        11.4906
     31   0.2999   0.7889        0.2215       0.9129        0.2552        11.2776
     32   0.2982   0.7786        0.2212       0.9129        0.2660        11.4826
     33   0.3084   0.7914        0.2223       0.9141        0.2521        11.1761
     34   0.3223   0.7981        0.2259       0.9081        0.2584        11.4313
     35   0.3224   0.7952        [35m0.2200[0m       0.9141        0.2584        11.3177
     36   0.3192   0.7992        [35m0.2200[0m       0.9154        0.2596        11.3804
     37   0.3307   0.7965        [35m0.2197[0m       0.9166        0.2495        11.1362
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 13:04:55,985][0m Trial 131 finished with value: 0.24910886837518634 and parameters: {'lr': 0.0029827419412485237, 'dropout': 0.35925705373258077, 'd_model_multiplier': 1, 'num_layers': 2, 'n_heads': 32, 'dim_feedforward': 447, 'batch_size': 21, 'pos_encoding': 'learnable', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0, 'top_n_features': 157}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 151
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.3582[0m   [32m0.7754[0m        [35m0.2739[0m       [31m0.9190[0m        [94m0.2471[0m     +  10.9877
      2   0.3463   0.7439        [35m0.2599[0m       0.9190        0.2554        11.1136
      3   0.3540   0.7754        [35m0.2565[0m       0.9178        0.3044        11.1247
      4   [36m0.3735[0m   [32m0.8058[0m        [35m0.2528[0m       0.9178        0.2482        11.1878
      5   0.3213   [32m0.8124[0m        [35m0.2450[0m       0.9129        0.2611        11.1523
      6   0.3430   0.7914        0.2460       0.9190        0.2493        11.1242
      7   0.3589   [32m0.8136[0m        0.2454       0.9178        [94m0.2391[0m     +  11.3736
      8   [36m0.3788[0m   [32m0.8354[0m        [35m0.2444[0m       [31m0.9274[0m        [94m0.2273[0m     +  11.1539
      9   [36m0.3841[0m   0.8289        [35m0.2361[0m       0.9226        [94m0.2262[0m     +  11.0818
     10   0.3703   0.8343        0.2375       0.9202        0.2437        11.4252
     11   0.3693   0.8324        [35m0.2331[0m       0.9238        0.2316        11.2514
     12   0.3658   [32m0.8363[0m        [35m0.2329[0m       0.9202        0.2304        11.3136
     13   [36m0.3875[0m   0.8286        0.2339       0.9238        [94m0.2228[0m     +  11.3298
     14   0.3563   0.8259        [35m0.2293[0m       0.9190        0.2271        10.9145
     15   [36m0.3884[0m   0.8191        0.2298       0.9214        0.2308        11.1779
     16   0.3564   0.8277        [35m0.2286[0m       0.9202        0.2245        11.1926
     17   0.3715   0.8282        [35m0.2275[0m       0.9226        0.2234        11.1741
     18   0.3539   0.8250        0.2282       0.9190        0.2330        11.1635
     19   0.3729   0.8286        [35m0.2243[0m       0.9141        0.2266        11.1825
     20   0.3510   0.8218        0.2259       0.9214        0.2278        11.0942
     21   0.3546   0.8168        [35m0.2223[0m       0.9214        0.2284        11.0481
     22   0.3529   0.8137        0.2232       0.9190        0.2364        11.1980
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 13:09:13,456][0m Trial 132 finished with value: 0.22278635370507927 and parameters: {'lr': 0.0020448449806795507, 'dropout': 0.3762145663817256, 'd_model_multiplier': 1, 'num_layers': 2, 'n_heads': 32, 'dim_feedforward': 437, 'batch_size': 26, 'pos_encoding': 'learnable', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0, 'top_n_features': 151}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 148
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2824[0m   [32m0.7730[0m        [35m0.2813[0m       [31m0.8888[0m        [94m0.2882[0m     +  13.8015
      2   0.2727   [32m0.7782[0m        [35m0.2574[0m       0.8742        0.3359        13.8118
      3   0.2785   0.7689        [35m0.2561[0m       [31m0.8984[0m        0.2884        13.8401
      4   0.2509   0.7175        [35m0.2500[0m       [31m0.9033[0m        [94m0.2828[0m     +  13.9488
      5   0.2690   [32m0.7810[0m        [35m0.2500[0m       0.8585        0.3224        13.9444
      6   [36m0.2846[0m   0.7761        [35m0.2473[0m       0.8549        0.3450        14.0962
      7   0.2674   0.7802        [35m0.2446[0m       0.8271        0.4001        14.2003
      8   [36m0.2939[0m   [32m0.7852[0m        0.2502       0.8235        0.4179        13.9933
      9   0.2566   0.7717        0.2488       0.8307        0.3941        14.0953
     10   0.2833   0.7806        0.2455       0.7316        0.5390        14.0162
     11   0.2377   0.7359        0.2498       0.8452        0.3850        14.1578
     12   0.2492   0.7603        0.2503       0.8634        0.3311        13.9200
     13   0.2504   0.7442        0.2460       0.8307        0.4063        13.8547
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 13:12:29,636][0m Trial 133 finished with value: 0.28283969926430497 and parameters: {'lr': 0.004884968526734332, 'dropout': 0.3992807288377115, 'd_model_multiplier': 1, 'num_layers': 3, 'n_heads': 32, 'dim_feedforward': 465, 'batch_size': 44, 'pos_encoding': 'learnable', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0, 'top_n_features': 148}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 134
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2099[0m   [32m0.7137[0m        [35m0.3019[0m       [31m0.9069[0m        [94m0.3128[0m     +  10.9797
      2   [36m0.2297[0m   [32m0.7344[0m        [35m0.2497[0m       0.8996        [94m0.3033[0m     +  11.4659
      3   [36m0.2451[0m   [32m0.7430[0m        [35m0.2469[0m       0.9008        0.3186        11.1702
      4   [36m0.2457[0m   [32m0.7443[0m        [35m0.2423[0m       0.9045        [94m0.2966[0m     +  11.3291
      5   [36m0.2594[0m   [32m0.7598[0m        [35m0.2401[0m       [31m0.9081[0m        [94m0.2785[0m     +  11.0172
      6   [36m0.2792[0m   [32m0.7640[0m        [35m0.2382[0m       [31m0.9129[0m        [94m0.2742[0m     +  11.1371
      7   [36m0.2826[0m   0.7597        [35m0.2312[0m       0.9045        [94m0.2731[0m     +  11.4040
      8   0.2780   0.7616        0.2322       0.9117        [94m0.2701[0m     +  11.2262
      9   0.2521   0.7404        [35m0.2286[0m       0.9081        0.2751        11.2432
     10   0.2789   [32m0.7734[0m        [35m0.2272[0m       0.9069        [94m0.2657[0m     +  11.1902
     11   0.2699   [32m0.7790[0m        0.2294       0.9069        0.2678        11.1908
     12   0.2721   0.7698        [35m0.2247[0m       0.9093        0.2705        11.0704
     13   0.2764   0.7770        0.2262       0.9057        0.2687        11.1067
     14   0.2575   0.7715        [35m0.2216[0m       0.9045        0.2720        11.3336
     15   0.2712   [32m0.7861[0m        0.2217       0.9033        [94m0.2652[0m     +  11.2088
     16   0.2689   [32m0.7900[0m        [35m0.2214[0m       0.9033        0.2681        11.0971
     17   0.2711   0.7856        0.2215       0.9033        0.2656        11.1290
     18   0.2580   0.7812        [35m0.2180[0m       0.9045        0.2685        11.2559
     19   0.2691   0.7868        0.2203       0.9021        0.2668        11.1729
     20   0.2556   0.7792        [35m0.2167[0m       0.9057        0.2687        11.1052
     21   0.2572   [32m0.7903[0m        0.2197       0.9069        0.2753        11.2935
     22   0.2625   0.7894        [35m0.2159[0m       0.9057        0.2699        11.1193
     23   0.2598   0.7899        0.2166       0.9057        0.2671        11.1042
     24   0.2634   0.7854        0.2162       0.9093        0.2732        11.1787
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 13:17:09,990][0m Trial 134 finished with value: 0.26518253877560777 and parameters: {'lr': 0.0013481702954555106, 'dropout': 0.42659824512162436, 'd_model_multiplier': 1, 'num_layers': 2, 'n_heads': 32, 'dim_feedforward': 428, 'batch_size': 50, 'pos_encoding': 'learnable', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0, 'top_n_features': 134}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 121
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2172[0m   [32m0.7038[0m        [35m0.3466[0m       [31m0.9202[0m        [94m0.3004[0m     +  27.6820
      2   [36m0.2176[0m   0.6969        [35m0.3332[0m       [31m0.9287[0m        [94m0.2366[0m     +  27.8468
      3   [36m0.2201[0m   [32m0.7296[0m        [35m0.2849[0m       0.9021        0.2891        27.9391
      4   0.1721   0.7139        [35m0.2752[0m       0.9262        0.2615        27.7728
      5   [36m0.2253[0m   0.7257        [35m0.2613[0m       0.9262        0.2412        27.6992
      6   [36m0.2564[0m   [32m0.7347[0m        0.2619       0.9262        0.2388        27.8266
      7   0.2112   0.7258        [35m0.2599[0m       0.9274        [94m0.2349[0m     +  27.8483
      8   0.2025   0.7199        [35m0.2568[0m       0.9274        [94m0.2332[0m     +  27.9155
      9   0.2132   0.7329        0.2577       0.9274        [94m0.2292[0m     +  27.7966
     10   0.2240   0.7301        [35m0.2544[0m       0.9287        0.2340        27.8134
     11   0.2129   0.7329        0.2546       0.9274        [94m0.2291[0m     +  27.9172
     12   0.2218   0.7277        [35m0.2542[0m       0.9274        0.2326        27.7572
     13   0.2346   0.7336        0.2553       0.9274        0.2348        27.8057
     14   0.2104   0.7255        0.2547       0.9274        0.2294        27.9023
     15   0.2227   [32m0.7351[0m        [35m0.2520[0m       0.9274        [94m0.2290[0m     +  27.7133
     16   0.2256   0.7319        0.2555       0.9274        0.2300        27.8008
     17   0.2254   0.7287        0.2532       0.9287        0.2309        27.8653
     18   0.2407   0.7316        0.2535       0.9274        0.2300        27.7686
     19   0.2162   0.7250        [35m0.2517[0m       0.9287        0.2309        27.8808
     20   0.2169   0.7242        0.2520       0.9287        0.2329        27.8604
     21   0.2231   0.7337        0.2525       0.9287        0.2299        27.9013
     22   0.2430   [32m0.7384[0m        [35m0.2512[0m       0.9287        0.2297        27.7362
     23   0.2310   [32m0.7403[0m        0.2514       0.9287        0.2306        27.8109
     24   0.2343   0.7329        [35m0.2508[0m       0.9287        0.2291        27.9551
     25   0.2444   0.7379        0.2515       0.9287        [94m0.2289[0m     +  27.7593
     26   0.2429   0.7354        0.2510       0.9287        [94m0.2284[0m     +  27.7972
     27   0.2466   [32m0.7406[0m        0.2665       0.9287        0.2291        27.7218
     28   0.2307   0.7347        0.2555       0.9287        0.2320        27.6388
     29   0.2439   0.7376        0.2514       0.9274        [94m0.2279[0m     +  27.6726
     30   0.2350   0.7373        0.2517       0.9287        0.2290        27.9538
     31   0.2335   0.7368        0.2527       0.9287        0.2293        27.9125
     32   0.2485   0.7361        0.2522       0.9287        0.2286        27.7265
     33   0.2432   0.7362        0.2509       0.9287        0.2294        27.6515
     34   0.2396   [32m0.7416[0m        0.2533       0.9274        [94m0.2275[0m     +  27.9290
     35   0.2320   0.7374        0.2572       0.9287        0.2311        27.7639
     36   0.2409   0.7377        [35m0.2507[0m       0.9274        0.2276        27.6808
     37   0.2543   0.7353        0.2561       0.9287        0.2315        27.7739
     38   0.2432   0.7403        0.2574       0.9287        [94m0.2271[0m     +  27.7748
     39   0.2283   0.7385        0.2512       0.9287        0.2301        27.7790
     40   0.2463   0.7402        [35m0.2498[0m       0.9274        0.2274        27.8567
     41   0.2454   0.7390        0.2519       0.9274        0.2278        27.7687
     42   0.2502   [32m0.7432[0m        0.2509       0.9274        [94m0.2263[0m     +  28.0731
     43   0.2467   0.7422        0.2507       0.9274        0.2276        27.9081
     44   0.2481   0.7404        0.2507       0.9287        0.2285        27.8371
     45   0.2465   0.7379        0.2512       0.9287        0.2309        27.8170
     46   0.2402   0.7386        0.2510       0.9274        0.2285        28.0373
     47   0.2448   0.7423        0.2509       0.9274        0.2290        27.9253
     48   0.2449   [32m0.7464[0m        0.2499       0.9287        0.2298        27.8411
     49   0.2523   0.7442        0.2508       0.9287        0.2297        27.8704
     50   0.2450   0.7397        0.2509       0.9287        0.2288        27.7086
[32m[I 2023-05-02 13:40:23,585][0m Trial 135 finished with value: 0.2262772197014187 and parameters: {'lr': 0.003188447953973845, 'dropout': 0.3605576496668373, 'd_model_multiplier': 1, 'num_layers': 4, 'n_heads': 64, 'dim_feedforward': 412, 'batch_size': 16, 'pos_encoding': 'learnable', 'activation': 'relu', 'norm': 'BatchNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0, 'top_n_features': 121}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 227
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.2035[0m   [32m0.6893[0m        [35m0.2890[0m       [31m0.9166[0m        [94m0.2801[0m     +  9.3697
      2   0.1887   0.6849        [35m0.2683[0m       0.9129        0.2927        9.1133
      3   0.1801   0.6803        [35m0.2621[0m       [31m0.9178[0m        0.2836        9.6053
      4   0.1817   0.6749        [35m0.2609[0m       0.9154        0.2926        9.1834
      5   0.1735   0.6738        [35m0.2585[0m       0.9069        0.3070        9.2083
      6   0.1816   0.6726        0.2629       0.9154        0.2932        9.6243
      7   0.2034   0.6723        0.2613       [31m0.9202[0m        [94m0.2786[0m     +  9.6752
      8   [36m0.2080[0m   0.6734        0.2613       0.9178        0.2892        9.6797
      9   0.1835   0.6718        0.2605       0.9105        0.2908        9.6921
     10   [36m0.2162[0m   0.6750        0.2586       0.9202        [94m0.2705[0m     +  9.7348
     11   0.1849   [32m0.6979[0m        [35m0.2582[0m       0.9190        [94m0.2685[0m     +  9.5894
     12   0.2058   0.6779        [35m0.2579[0m       0.9202        [94m0.2647[0m     +  9.4354
     13   0.1824   0.6744        [35m0.2574[0m       0.9081        0.2925        9.5500
     14   0.1814   0.6805        0.2582       0.9202        0.2677        9.4779
     15   [36m0.2204[0m   0.6846        [35m0.2569[0m       0.9202        0.2677        9.3395
     16   0.1865   0.6767        0.2590       0.9202        0.2688        9.3308
     17   0.1883   0.6892        [35m0.2562[0m       0.9141        0.2702        9.5580
     18   0.1843   0.6925        0.2567       0.9117        0.2771        9.4133
     19   0.2014   [32m0.7092[0m        [35m0.2530[0m       0.9008        0.2789        9.4493
     20   0.2188   0.6931        [35m0.2520[0m       [31m0.9214[0m        0.2711        9.2951
     21   0.2141   [32m0.7260[0m        0.2520       0.9081        0.2702        9.4071
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 13:43:52,206][0m Trial 136 finished with value: 0.26469792303107814 and parameters: {'lr': 0.007380085261040127, 'dropout': 0.38824384985369376, 'd_model_multiplier': 4, 'num_layers': 1, 'n_heads': 32, 'dim_feedforward': 417, 'batch_size': 34, 'pos_encoding': 'learnable', 'activation': 'gelu', 'norm': 'LayerNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0, 'top_n_features': 227}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 166
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1165[0m   [32m0.6537[0m        [35m0.2927[0m       [31m0.7110[0m        [94m0.5871[0m     +  21.0054
      2   0.1014   0.5970        [35m0.2650[0m       0.6796        0.6235        21.1560
      3   0.0986   0.5759        [35m0.2643[0m       0.3108        0.8718        21.2671
      4   0.0933   0.5513        0.2671       [31m0.8089[0m        [94m0.4715[0m     +  21.2115
      5   [36m0.1659[0m   [32m0.7112[0m        [35m0.2626[0m       [31m0.8222[0m        [94m0.4301[0m     +  21.1542
      6   [36m0.2300[0m   [32m0.7205[0m        [35m0.2599[0m       [31m0.8682[0m        [94m0.3525[0m     +  21.1924
      7   0.2220   [32m0.7468[0m        [35m0.2549[0m       [31m0.9202[0m        [94m0.2606[0m     +  21.9252
      8   0.2242   [32m0.7521[0m        0.2555       0.9190        0.2646        21.0718
      9   [36m0.2351[0m   0.7478        [35m0.2461[0m       0.9154        0.2647        21.2047
     10   [36m0.2560[0m   [32m0.7687[0m        0.2554       [31m0.9226[0m        [94m0.2524[0m     +  20.9527
     11   [36m0.2876[0m   0.7632        0.2469       0.9190        0.2604        21.3126
     12   0.2699   0.7657        0.2516       0.9202        [94m0.2523[0m     +  21.1531
     13   [36m0.3148[0m   0.7597        0.2541       [31m0.9250[0m        [94m0.2488[0m     +  21.0418
     14   [36m0.3199[0m   [32m0.7857[0m        [35m0.2431[0m       0.9226        [94m0.2413[0m     +  21.2423
     15   [36m0.3714[0m   [32m0.7919[0m        0.2439       [31m0.9287[0m        [94m0.2337[0m     +  21.1800
     16   0.3172   0.7842        [35m0.2416[0m       0.9202        0.2357        21.0487
     17   0.3472   0.7835        [35m0.2374[0m       0.9250        0.2383        21.2512
     18   0.2763   0.7746        0.2431       0.9178        0.2733        21.1849
     19   0.3065   [32m0.8226[0m        0.2409       0.9093        0.2704        21.2072
     20   0.3028   0.7676        0.2422       0.9250        0.2618        21.0817
     21   0.2524   0.7594        [35m0.2374[0m       0.9238        0.2532        21.2136
     22   0.3328   0.7972        [35m0.2342[0m       0.9226        0.2362        21.1601
     23   0.3623   0.7692        [35m0.2315[0m       0.9274        0.2613        21.1922
     24   0.2979   0.8065        [35m0.2299[0m       0.9033        0.2656        21.0793
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 13:52:44,355][0m Trial 137 finished with value: 0.23371332960412752 and parameters: {'lr': 0.0002512225712186015, 'dropout': 0.33480419205081946, 'd_model_multiplier': 16, 'num_layers': 3, 'n_heads': 32, 'dim_feedforward': 266, 'batch_size': 20, 'pos_encoding': 'learnable', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0, 'top_n_features': 166}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 157
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.3124[0m   [32m0.7931[0m        [35m0.2891[0m       [31m0.9166[0m        [94m0.2461[0m     +  8.9594
      2   0.3018   0.7477        [35m0.2485[0m       0.9141        0.2730        8.3671
      3   0.2889   0.7283        0.2505       0.9117        0.2859        8.7291
      4   0.2990   0.7364        [35m0.2435[0m       0.9154        0.2630        9.0259
      5   [36m0.3134[0m   0.7438        [35m0.2402[0m       0.9129        0.2601        8.8228
      6   [36m0.3339[0m   0.7749        [35m0.2361[0m       0.9166        0.2515        8.7623
      7   0.3206   0.7570        [35m0.2338[0m       [31m0.9202[0m        0.2607        8.7742
      8   [36m0.3441[0m   0.7501        0.2338       [31m0.9214[0m        0.2778        8.9534
      9   0.3339   0.7722        [35m0.2303[0m       0.9154        0.2511        8.9997
     10   [36m0.3516[0m   [32m0.7987[0m        0.2308       [31m0.9238[0m        [94m0.2433[0m     +  8.8719
     11   0.3419   0.7901        [35m0.2295[0m       0.9141        0.2514        8.9552
     12   0.3186   0.7729        [35m0.2289[0m       0.9141        0.2588        9.1971
     13   0.3328   [32m0.8116[0m        [35m0.2285[0m       0.9141        0.2698        9.2500
     14   0.2874   0.7706        [35m0.2269[0m       0.9129        0.3029        8.9947
     15   0.2915   0.7843        0.2332       0.9129        0.2947        9.2021
     16   0.3091   [32m0.8205[0m        0.2332       0.9141        0.2439        8.8855
     17   0.3110   0.7764        0.2270       0.9202        0.2720        9.0478
     18   0.3324   0.7880        [35m0.2268[0m       0.9141        0.2929        9.0451
     19   0.3437   0.8114        [35m0.2240[0m       0.9190        0.2436        8.9515
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 13:55:44,062][0m Trial 138 finished with value: 0.2432581794233103 and parameters: {'lr': 0.00045896663389847597, 'dropout': 0.34673269044945654, 'd_model_multiplier': 64, 'num_layers': 2, 'n_heads': 4, 'dim_feedforward': 456, 'batch_size': 40, 'pos_encoding': 'learnable', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0, 'top_n_features': 157}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 138
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.2423[0m   [32m0.7202[0m        [35m0.2933[0m       [31m0.9045[0m        [94m0.2942[0m     +  8.0422
      2   [36m0.2531[0m   [32m0.7282[0m        [35m0.2553[0m       0.9008        [94m0.2906[0m     +  8.4035
      3   [36m0.2776[0m   0.7224        [35m0.2504[0m       0.9008        [94m0.2849[0m     +  8.7695
      4   0.2772   0.7259        [35m0.2425[0m       0.9033        [94m0.2830[0m     +  8.8103
      5   0.2747   0.7032        [35m0.2403[0m       [31m0.9057[0m        0.2884        8.7093
      6   [36m0.2856[0m   0.6935        [35m0.2397[0m       [31m0.9069[0m        0.2894        8.7641
      7   [36m0.3242[0m   [32m0.7342[0m        [35m0.2358[0m       [31m0.9093[0m        [94m0.2764[0m     +  8.7376
      8   [36m0.3381[0m   0.7295        [35m0.2334[0m       [31m0.9117[0m        [94m0.2764[0m     +  9.0960
      9   [36m0.3560[0m   0.7180        [35m0.2307[0m       0.9117        0.2785        10.0135
     10   [36m0.3587[0m   0.7171        [35m0.2306[0m       0.9117        0.2783        9.6738
     11   [36m0.3695[0m   [32m0.7378[0m        [35m0.2265[0m       0.9117        [94m0.2741[0m     +  9.2221
     12   [36m0.3697[0m   0.7309        0.2300       [31m0.9129[0m        [94m0.2737[0m     +  8.9600
     13   [36m0.3705[0m   0.7191        0.2272       [31m0.9141[0m        0.2750        9.0598
     14   [36m0.3741[0m   0.7325        [35m0.2251[0m       0.9141        [94m0.2734[0m     +  8.8118
     15   [36m0.3837[0m   0.7302        [35m0.2244[0m       0.9141        [94m0.2711[0m     +  8.9812
     16   0.3795   0.7371        [35m0.2228[0m       0.9105        0.2711        8.9979
     17   0.3771   0.7261        [35m0.2209[0m       0.9141        0.2746        8.7192
     18   [36m0.3978[0m   [32m0.7445[0m        [35m0.2207[0m       [31m0.9154[0m        [94m0.2659[0m     +  8.8069
     19   [36m0.4007[0m   [32m0.7472[0m        0.2212       0.9154        0.2682        8.8502
     20   0.3975   0.7466        [35m0.2187[0m       0.9117        0.2659        8.9749
     21   0.3981   0.7430        [35m0.2169[0m       0.9141        0.2704        8.9208
     22   0.3930   0.7334        0.2176       0.9117        0.2718        8.7697
     23   0.3951   0.7385        [35m0.2148[0m       0.9117        0.2709        8.8108
     24   [36m0.4261[0m   [32m0.7789[0m        0.2155       0.9141        [94m0.2593[0m     +  8.9261
     25   0.4003   0.7404        0.2163       0.9154        0.2694        8.9396
     26   0.4200   0.7646        [35m0.2139[0m       0.9154        0.2630        8.8208
     27   0.4231   0.7644        0.2157       0.9117        0.2610        8.7929
     28   0.4247   0.7498        [35m0.2133[0m       [31m0.9166[0m        0.2679        8.8773
     29   0.4127   0.7470        [35m0.2127[0m       0.9166        0.2667        8.9125
     30   [36m0.4275[0m   0.7594        0.2140       [31m0.9190[0m        0.2628        8.6409
     31   0.4137   0.7520        0.2169       0.9166        0.2659        8.8453
     32   0.4192   0.7672        [35m0.2112[0m       0.9166        0.2627        8.9035
     33   0.4160   0.7554        [35m0.2110[0m       0.9154        0.2653        9.2186
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 14:00:48,123][0m Trial 139 finished with value: 0.25926623681724575 and parameters: {'lr': 0.000776160476222672, 'dropout': 0.41566220459881564, 'd_model_multiplier': 1, 'num_layers': 1, 'n_heads': 4, 'dim_feedforward': 479, 'batch_size': 28, 'pos_encoding': 'learnable', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 138}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 150
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1954[0m   [32m0.5362[0m        [35m0.3914[0m       [31m0.9214[0m        [94m0.3282[0m     +  21.1954
      2   [36m0.2432[0m   [32m0.7502[0m        [35m0.3532[0m       0.8815        0.3911        21.2988
      3   0.2380   0.7403        [35m0.2891[0m       0.9057        [94m0.2912[0m     +  21.4528
      4   0.2402   0.7243        [35m0.2887[0m       0.9202        [94m0.2785[0m     +  21.4530
      5   0.2309   0.7182        [35m0.2656[0m       0.9202        [94m0.2573[0m     +  21.3918
      6   0.2211   0.7396        [35m0.2610[0m       0.9202        [94m0.2570[0m     +  21.1914
      7   0.2301   0.7333        [35m0.2593[0m       0.9202        [94m0.2555[0m     +  21.2813
      8   0.2254   0.7369        [35m0.2581[0m       0.9190        0.2596        21.2337
      9   0.2271   0.7363        [35m0.2573[0m       0.9202        0.2588        21.2890
     10   0.2192   0.7465        [35m0.2572[0m       0.9129        0.2594        21.1977
     11   0.2346   0.7483        [35m0.2554[0m       0.9081        0.2658        21.4492
     12   0.2198   0.7362        0.2581       0.8984        0.2680        21.3057
     13   0.2228   [32m0.7532[0m        0.2560       0.9033        0.2630        21.3903
     14   0.2357   0.7513        0.2558       0.8912        0.2805        21.4378
     15   [36m0.2463[0m   0.7478        0.2565       0.8791        0.2951        21.4030
     16   0.2355   [32m0.7565[0m        0.2557       0.8791        0.2948        21.3410
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 14:06:53,476][0m Trial 140 finished with value: 0.2554762654429912 and parameters: {'lr': 0.0022320041475178696, 'dropout': 0.2897773138723362, 'd_model_multiplier': 32, 'num_layers': 2, 'n_heads': 32, 'dim_feedforward': 142, 'batch_size': 34, 'pos_encoding': 'learnable', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0, 'top_n_features': 150}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 190
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.2101[0m   [32m0.6390[0m        [35m0.3730[0m       [31m0.9166[0m        [94m0.3499[0m     +  8.7787
      2   [36m0.2212[0m   [32m0.6872[0m        [35m0.2667[0m       0.9141        [94m0.3092[0m     +  9.6984
      3   [36m0.2532[0m   [32m0.7263[0m        [35m0.2561[0m       0.9154        [94m0.2997[0m     +  9.9776
      4   [36m0.2840[0m   [32m0.7289[0m        [35m0.2522[0m       0.9166        [94m0.2749[0m     +  9.6818
      5   [36m0.2871[0m   [32m0.7319[0m        [35m0.2429[0m       0.9154        [94m0.2686[0m     +  10.1025
      6   [36m0.2914[0m   [32m0.7503[0m        0.2455       0.9129        [94m0.2605[0m     +  10.2352
      7   [36m0.2953[0m   [32m0.7657[0m        [35m0.2378[0m       0.9141        [94m0.2570[0m     +  9.9611
      8   [36m0.3001[0m   [32m0.7730[0m        [35m0.2347[0m       0.9117        0.2595        9.8223
      9   [36m0.3045[0m   [32m0.7796[0m        0.2359       0.9105        [94m0.2557[0m     +  9.9807
     10   0.3012   [32m0.7963[0m        [35m0.2326[0m       0.9129        [94m0.2492[0m     +  10.1148
     11   [36m0.3057[0m   [32m0.7984[0m        [35m0.2280[0m       0.9081        [94m0.2468[0m     +  10.0294
     12   0.3000   0.7960        [35m0.2261[0m       0.9105        0.2483        9.8640
     13   0.3030   [32m0.8038[0m        [35m0.2256[0m       0.9117        [94m0.2434[0m     +  10.0873
     14   [36m0.3162[0m   [32m0.8071[0m        0.2285       0.9105        [94m0.2419[0m     +  10.3488
     15   0.3147   0.8036        [35m0.2249[0m       0.9117        0.2421        9.9740
     16   0.3103   0.8051        [35m0.2247[0m       0.9105        0.2428        10.0693
     17   0.3039   0.8044        [35m0.2193[0m       0.9129        0.2443        10.1825
     18   0.3051   0.8035        0.2193       0.9105        0.2444        10.2215
     19   0.3027   0.8005        [35m0.2174[0m       0.9093        0.2435        9.7713
     20   0.3062   0.8070        0.2189       0.9129        0.2428        10.0153
     21   [36m0.3186[0m   0.8066        0.2191       0.9141        [94m0.2406[0m     +  10.1134
     22   0.3121   [32m0.8074[0m        0.2188       0.9141        0.2449        10.2050
     23   0.3072   0.8037        0.2181       0.9117        0.2425        9.9698
     24   0.2970   0.8044        0.2194       0.9093        0.2433        9.8266
     25   0.2955   0.8015        [35m0.2143[0m       0.9105        0.2454        9.8501
     26   0.3071   0.8064        0.2154       0.9117        0.2417        9.9003
     27   0.2815   0.7904        [35m0.2119[0m       0.9093        0.2486        9.8176
     28   0.2863   0.7904        0.2158       0.9141        0.2497        9.9245
     29   0.2589   0.7719        0.2123       0.9081        0.2564        10.0144
     30   0.3067   0.8069        0.2164       0.9117        0.2439        9.8988
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 14:12:02,547][0m Trial 141 finished with value: 0.24057038972534897 and parameters: {'lr': 0.0011486480319960924, 'dropout': 0.5429987978371931, 'd_model_multiplier': 1, 'num_layers': 2, 'n_heads': 16, 'dim_feedforward': 458, 'batch_size': 55, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 190}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 172
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.3007[0m   [32m0.8064[0m        [35m0.3609[0m       [31m0.9311[0m        [94m0.2373[0m     +  8.6900
      2   [36m0.3291[0m   0.8024        [35m0.2539[0m       [31m0.9323[0m        [94m0.2258[0m     +  8.6169
      3   0.3256   0.7941        [35m0.2490[0m       [31m0.9335[0m        [94m0.2154[0m     +  9.4159
      4   0.3023   0.7997        [35m0.2410[0m       0.9299        0.2195        9.4619
      5   0.2783   0.7877        [35m0.2369[0m       0.9299        0.2215        9.2883
      6   0.2683   0.7947        [35m0.2315[0m       0.9311        0.2211        9.2598
      7   0.2494   0.7805        [35m0.2270[0m       0.9262        0.2253        9.3533
      8   0.2554   0.7809        [35m0.2255[0m       0.9274        0.2255        9.1066
      9   0.2356   0.7764        [35m0.2209[0m       0.9226        0.2380        9.3418
     10   0.2356   0.7929        0.2212       0.9226        0.2284        9.4526
     11   0.2092   0.7819        [35m0.2161[0m       0.9190        0.2424        9.7903
     12   0.2095   0.7733        [35m0.2159[0m       0.9154        0.2440        9.6207
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 14:14:03,822][0m Trial 142 finished with value: 0.21544783475032886 and parameters: {'lr': 0.00036075392707762556, 'dropout': 0.3584929436441342, 'd_model_multiplier': 16, 'num_layers': 3, 'n_heads': 4, 'dim_feedforward': 445, 'batch_size': 61, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 172}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 173
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.1874[0m   [32m0.6581[0m        [35m0.3647[0m       [31m0.9262[0m        [94m0.2922[0m     +  8.7153
      2   [36m0.2116[0m   [32m0.7050[0m        [35m0.2550[0m       [31m0.9274[0m        [94m0.2779[0m     +  9.0579
      3   0.2073   [32m0.7096[0m        [35m0.2452[0m       0.9238        [94m0.2646[0m     +  9.1072
      4   0.2090   [32m0.7163[0m        [35m0.2417[0m       0.9202        [94m0.2587[0m     +  9.3445
      5   [36m0.2174[0m   [32m0.7194[0m        [35m0.2390[0m       0.9238        0.2612        9.4210
      6   [36m0.2197[0m   [32m0.7316[0m        [35m0.2331[0m       0.9262        [94m0.2538[0m     +  9.5525
      7   [36m0.2432[0m   0.7226        [35m0.2320[0m       0.9262        [94m0.2522[0m     +  9.7575
      8   0.2323   0.7187        [35m0.2302[0m       0.9274        0.2533        9.8478
      9   [36m0.2454[0m   0.7248        [35m0.2270[0m       0.9274        [94m0.2511[0m     +  9.4768
     10   0.2420   0.7286        [35m0.2250[0m       [31m0.9299[0m        [94m0.2438[0m     +  9.5335
     11   [36m0.2585[0m   [32m0.7409[0m        0.2252       0.9274        [94m0.2407[0m     +  9.4802
     12   0.2442   0.7290        [35m0.2236[0m       0.9202        0.2487        9.4891
     13   [36m0.2636[0m   0.7316        [35m0.2209[0m       0.9202        0.2572        9.4325
     14   0.2543   0.7326        [35m0.2187[0m       0.9226        0.2453        9.7556
     15   0.2393   0.7265        [35m0.2169[0m       0.9214        0.2553        9.3564
     16   0.2396   0.7350        [35m0.2115[0m       0.9238        0.2458        9.7095
     17   0.2302   0.7269        [35m0.2100[0m       0.9238        0.2609        9.5308
     18   0.2464   0.7296        [35m0.2041[0m       0.9226        0.2494        9.5742
     19   0.2259   0.7219        0.2046       0.9226        0.2539        9.7284
     20   0.2297   0.7296        [35m0.1989[0m       0.9190        0.2600        9.5246
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 14:17:23,533][0m Trial 143 finished with value: 0.2406596307526765 and parameters: {'lr': 0.0002920353300688259, 'dropout': 0.3586645599182816, 'd_model_multiplier': 16, 'num_layers': 3, 'n_heads': 4, 'dim_feedforward': 436, 'batch_size': 64, 'pos_encoding': 'learnable', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 173}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 163
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.0983[0m   [32m0.5089[0m        [35m0.4792[0m       [31m0.9033[0m        [94m0.4588[0m     +  9.0081
      2   [36m0.2641[0m   [32m0.7615[0m        [35m0.2706[0m       0.8996        [94m0.3621[0m     +  8.8012
      3   [36m0.2850[0m   [32m0.7805[0m        [35m0.2484[0m       0.9021        [94m0.3381[0m     +  9.2812
      4   [36m0.3023[0m   0.7778        [35m0.2441[0m       0.9021        [94m0.3322[0m     +  9.6623
      5   [36m0.3156[0m   [32m0.7883[0m        [35m0.2390[0m       0.9008        0.3332        9.4921
      6   0.3118   [32m0.7924[0m        [35m0.2379[0m       0.8996        [94m0.3081[0m     +  9.6859
      7   [36m0.3237[0m   [32m0.7950[0m        [35m0.2310[0m       0.9021        0.3093        9.2350
      8   0.3101   0.7849        0.2313       0.9008        [94m0.3029[0m     +  9.5017
      9   [36m0.3269[0m   0.7939        [35m0.2270[0m       0.9008        0.3145        9.1765
     10   [36m0.3355[0m   0.7942        [35m0.2256[0m       [31m0.9057[0m        [94m0.2981[0m     +  9.4970
     11   0.3273   0.7950        [35m0.2219[0m       0.9021        0.3090        9.5540
     12   0.3235   0.7866        [35m0.2179[0m       0.8984        [94m0.2892[0m     +  9.6886
     13   0.3208   0.7888        [35m0.2159[0m       0.8984        0.2941        9.3805
     14   0.3164   0.7939        [35m0.2126[0m       0.9008        0.2939        9.4550
     15   0.3064   0.7788        [35m0.2095[0m       0.8984        0.3035        9.4502
     16   0.3138   0.7882        0.2105       0.8996        0.2954        9.6228
     17   0.3191   0.7935        [35m0.2056[0m       0.8996        0.2929        9.5884
     18   0.2981   0.7839        [35m0.2028[0m       0.8984        0.2963        9.6166
     19   0.3055   0.7928        0.2035       0.8996        0.2907        9.7889
     20   0.3062   0.7920        [35m0.1988[0m       0.8996        [94m0.2872[0m     +  9.4140
     21   0.2974   0.7879        [35m0.1948[0m       0.8972        0.3013        9.6689
     22   0.2942   0.7838        0.1960       0.8960        [94m0.2827[0m     +  9.5110
     23   0.2777   0.7780        [35m0.1937[0m       0.8936        0.2935        9.6620
     24   0.2870   0.7880        [35m0.1876[0m       0.8900        0.2995        9.3357
     25   0.3066   0.7929        [35m0.1870[0m       0.8948        0.2908        9.8054
     26   0.2903   0.7817        [35m0.1833[0m       0.8936        0.2935        9.7660
     27   0.3105   [32m0.7971[0m        [35m0.1819[0m       0.8984        0.2969        9.7853
     28   0.2881   0.7826        [35m0.1768[0m       0.8948        0.2981        9.4669
     29   0.2725   0.7672        [35m0.1753[0m       0.8912        0.2978        9.5334
     30   0.2600   0.7503        [35m0.1722[0m       0.8948        0.3051        9.3437
     31   0.2893   0.7660        [35m0.1661[0m       0.8960        0.2931        9.2006
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 14:22:28,175][0m Trial 144 finished with value: 0.2826743222673974 and parameters: {'lr': 0.000144391638135102, 'dropout': 0.31897686251931023, 'd_model_multiplier': 16, 'num_layers': 3, 'n_heads': 4, 'dim_feedforward': 441, 'batch_size': 69, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 163}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 168
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1971[0m   [32m0.6112[0m        [35m0.4443[0m       [31m0.9214[0m        [94m0.3344[0m     +  10.2744
      2   [36m0.2348[0m   [32m0.7175[0m        [35m0.2611[0m       0.9190        [94m0.2845[0m     +  10.4568
      3   [36m0.2609[0m   [32m0.7319[0m        [35m0.2484[0m       0.9178        [94m0.2699[0m     +  10.4338
      4   0.2540   [32m0.7446[0m        [35m0.2452[0m       0.9190        [94m0.2633[0m     +  10.4915
      5   [36m0.2709[0m   [32m0.7531[0m        [35m0.2423[0m       0.9202        [94m0.2572[0m     +  10.3139
      6   [36m0.2746[0m   [32m0.7578[0m        [35m0.2382[0m       0.9202        [94m0.2543[0m     +  10.4365
      7   [36m0.2818[0m   [32m0.7675[0m        [35m0.2369[0m       0.9190        0.2602        10.7074
      8   [36m0.2917[0m   [32m0.7678[0m        [35m0.2342[0m       0.9202        0.2591        10.5146
      9   [36m0.3122[0m   [32m0.7768[0m        [35m0.2286[0m       0.9202        [94m0.2525[0m     +  11.0746
     10   0.2975   0.7653        [35m0.2280[0m       0.9214        0.2548        10.2449
     11   0.3059   0.7686        [35m0.2243[0m       0.9214        0.2566        10.7068
     12   0.3086   0.7715        [35m0.2206[0m       [31m0.9250[0m        0.2595        10.7454
     13   0.2902   0.7627        0.2220       0.9214        0.2676        10.9512
     14   0.2961   0.7414        [35m0.2191[0m       0.9238        0.2752        10.7210
     15   0.2985   0.7540        [35m0.2152[0m       [31m0.9262[0m        0.2651        10.8161
     16   0.2703   0.7339        [35m0.2135[0m       0.9190        0.2913        10.9796
     17   0.2776   0.7310        [35m0.2109[0m       0.9178        0.2953        10.5996
     18   0.2878   0.7289        [35m0.2082[0m       0.9238        0.3173        10.9181
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 14:25:50,995][0m Trial 145 finished with value: 0.2524598456351616 and parameters: {'lr': 0.000572531462326904, 'dropout': 0.3786100333041993, 'd_model_multiplier': 16, 'num_layers': 4, 'n_heads': 4, 'dim_feedforward': 400, 'batch_size': 132, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 168}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 129
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.1812[0m   [32m0.7229[0m        [35m0.2690[0m       [31m0.9226[0m        [94m0.2548[0m     +  8.0952
      2   [36m0.2030[0m   [32m0.7387[0m        [35m0.2427[0m       0.9166        [94m0.2527[0m     +  9.3519
      3   0.1992   0.7269        [35m0.2381[0m       0.9226        0.2653        9.1974
      4   [36m0.2167[0m   0.7375        [35m0.2367[0m       0.9141        0.2661        9.1652
      5   [36m0.2304[0m   [32m0.7455[0m        [35m0.2300[0m       0.9129        0.2590        9.3100
      6   [36m0.2455[0m   [32m0.7709[0m        [35m0.2283[0m       0.9178        [94m0.2469[0m     +  9.4948
      7   [36m0.2463[0m   [32m0.7726[0m        [35m0.2251[0m       0.9166        0.2576        9.3208
      8   0.2379   0.7637        [35m0.2211[0m       0.9190        0.2713        9.4811
      9   0.2398   [32m0.7993[0m        0.2241       0.9178        0.2578        9.6021
     10   0.2387   0.7859        [35m0.2203[0m       0.9117        0.2674        9.6927
     11   0.2408   0.7859        [35m0.2168[0m       0.9154        0.2660        9.5000
     12   0.2326   0.7889        0.2168       0.9117        0.2713        9.4734
     13   0.2215   0.7858        [35m0.2154[0m       0.9202        0.2716        9.7300
     14   0.2417   0.7911        0.2170       0.9178        0.2546        9.7177
     15   0.2462   0.7896        [35m0.2135[0m       0.9166        0.2608        9.8616
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 14:28:21,945][0m Trial 146 finished with value: 0.24686573579132629 and parameters: {'lr': 0.0008895813602058226, 'dropout': 0.33768677855924534, 'd_model_multiplier': 16, 'num_layers': 3, 'n_heads': 4, 'dim_feedforward': 421, 'batch_size': 60, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 129}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 178
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.1420[0m   [32m0.6331[0m        [35m0.4157[0m       [31m0.9347[0m        [94m0.2333[0m     +  8.3669
      2   [36m0.1782[0m   [32m0.6969[0m        [35m0.2668[0m       [31m0.9383[0m        [94m0.2139[0m     +  9.4787
      3   [36m0.1871[0m   [32m0.7174[0m        [35m0.2529[0m       [31m0.9395[0m        [94m0.2111[0m     +  9.6821
      4   [36m0.2037[0m   [32m0.7384[0m        [35m0.2495[0m       0.9371        [94m0.2111[0m     +  9.4261
      5   0.1985   0.7328        [35m0.2452[0m       0.9359        0.2156        9.3954
      6   0.2001   [32m0.7385[0m        [35m0.2417[0m       0.9347        0.2177        9.8530
      7   0.1841   0.7347        [35m0.2410[0m       0.9311        0.2204        9.3121
      8   0.1776   0.7381        [35m0.2406[0m       0.9299        0.2207        9.6528
      9   0.1787   0.7367        [35m0.2405[0m       0.9287        0.2226        9.3747
     10   0.1809   [32m0.7425[0m        [35m0.2364[0m       0.9250        0.2225        9.4137
     11   0.1853   0.7415        [35m0.2359[0m       0.9274        0.2224        9.5673
     12   0.1935   [32m0.7526[0m        [35m0.2341[0m       0.9287        0.2234        9.4673
     13   0.1857   0.7476        0.2354       0.9311        0.2242        9.5493
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 14:30:34,359][0m Trial 147 finished with value: 0.21108453835224442 and parameters: {'lr': 0.00042378365852081763, 'dropout': 0.4709984562549456, 'd_model_multiplier': 8, 'num_layers': 1, 'n_heads': 8, 'dim_feedforward': 128, 'batch_size': 78, 'pos_encoding': 'learnable', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 178}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 178
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.2116[0m   [32m0.7002[0m        [35m0.2809[0m       [31m0.9154[0m        [94m0.2602[0m     +  8.5315
      2   [36m0.2303[0m   [32m0.7184[0m        [35m0.2459[0m       [31m0.9178[0m        [94m0.2569[0m     +  9.1173
      3   [36m0.2366[0m   [32m0.7495[0m        [35m0.2428[0m       0.9105        [94m0.2489[0m     +  8.9491
      4   [36m0.2425[0m   0.7433        [35m0.2402[0m       0.9129        0.2517        9.3729
      5   [36m0.2807[0m   [32m0.7569[0m        [35m0.2389[0m       [31m0.9190[0m        [94m0.2437[0m     +  9.5934
      6   0.2690   0.7508        [35m0.2349[0m       0.9190        0.2479        9.4008
      7   [36m0.2839[0m   [32m0.7633[0m        [35m0.2336[0m       [31m0.9226[0m        0.2446        9.3590
      8   [36m0.2995[0m   [32m0.7643[0m        0.2370       0.9166        0.2442        9.3591
      9   0.2922   0.7636        [35m0.2325[0m       0.9166        0.2473        9.7092
     10   0.2909   [32m0.7674[0m        [35m0.2319[0m       [31m0.9238[0m        0.2490        9.4211
     11   0.2911   0.7596        0.2321       0.9214        0.2503        9.3201
     12   [36m0.3138[0m   0.7653        0.2320       0.9214        0.2452        9.4268
     13   0.2975   0.7584        [35m0.2317[0m       0.9214        0.2462        9.7506
     14   0.3022   0.7559        [35m0.2294[0m       0.9154        0.2472        9.8814
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 14:32:55,777][0m Trial 148 finished with value: 0.24370623065525848 and parameters: {'lr': 0.00041391999346651175, 'dropout': 0.46548562508695807, 'd_model_multiplier': 8, 'num_layers': 1, 'n_heads': 8, 'dim_feedforward': 153, 'batch_size': 76, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 178}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 237
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.1763[0m   [32m0.6196[0m        [35m0.4372[0m       [31m0.9214[0m        [94m0.2667[0m     +  8.7364
      2   [36m0.2694[0m   [32m0.7465[0m        [35m0.2634[0m       [31m0.9226[0m        [94m0.2321[0m     +  9.0746
      3   [36m0.2902[0m   [32m0.7727[0m        [35m0.2475[0m       [31m0.9238[0m        [94m0.2269[0m     +  9.2820
      4   [36m0.3121[0m   [32m0.7841[0m        [35m0.2430[0m       0.9214        [94m0.2236[0m     +  9.3663
      5   [36m0.3241[0m   [32m0.7867[0m        [35m0.2403[0m       0.9226        0.2237        9.5571
      6   0.3226   [32m0.7894[0m        [35m0.2380[0m       0.9202        0.2241        9.6574
      7   [36m0.3306[0m   [32m0.7947[0m        [35m0.2337[0m       0.9202        [94m0.2232[0m     +  9.7067
      8   [36m0.3384[0m   0.7839        [35m0.2322[0m       [31m0.9250[0m        0.2276        9.4041
      9   0.3275   0.7901        [35m0.2314[0m       0.9202        0.2276        9.3609
     10   0.3282   0.7875        [35m0.2282[0m       0.9214        0.2281        9.3995
     11   [36m0.3487[0m   [32m0.8004[0m        [35m0.2270[0m       0.9226        0.2232        9.2448
     12   [36m0.3532[0m   0.7996        [35m0.2231[0m       0.9250        [94m0.2229[0m     +  9.3329
     13   0.3427   [32m0.8014[0m        [35m0.2219[0m       0.9250        0.2268        9.5391
     14   [36m0.3535[0m   0.7846        [35m0.2212[0m       [31m0.9274[0m        0.2322        9.7085
     15   0.3357   0.7921        [35m0.2183[0m       0.9238        0.2270        9.4127
     16   0.3351   0.7729        [35m0.2146[0m       0.9274        0.2345        9.7504
     17   [36m0.3847[0m   0.7933        [35m0.2130[0m       [31m0.9323[0m        [94m0.2208[0m     +  10.1938
     18   0.3694   [32m0.8023[0m        [35m0.2108[0m       0.9287        0.2256        9.9036
     19   0.3094   0.7445        [35m0.2075[0m       0.9262        0.2460        9.7083
     20   0.3390   0.7780        [35m0.2068[0m       0.9274        0.2310        9.6772
     21   0.3291   0.7818        [35m0.2058[0m       0.9299        0.2333        9.5880
     22   0.3230   0.7712        [35m0.2004[0m       0.9262        0.2384        9.9455
     23   0.3094   0.7797        [35m0.1983[0m       0.9226        0.2394        9.9947
     24   0.3255   0.7960        0.1985       0.9274        0.2316        9.7957
     25   0.3227   0.7841        [35m0.1968[0m       0.9202        0.2369        9.8897
     26   0.3028   0.7751        [35m0.1906[0m       0.9250        0.2431        9.7561
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 14:37:14,902][0m Trial 149 finished with value: 0.22075664393486683 and parameters: {'lr': 0.00038903399955954337, 'dropout': 0.3270753622887121, 'd_model_multiplier': 8, 'num_layers': 1, 'n_heads': 8, 'dim_feedforward': 128, 'batch_size': 84, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 237}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 201
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.2254[0m   [32m0.7549[0m        [35m0.2795[0m       [31m0.9202[0m        [94m0.2362[0m     +  9.1473
      2   [36m0.2263[0m   [32m0.7770[0m        [35m0.2462[0m       0.9069        0.2366        8.9285
      3   0.2250   [32m0.7824[0m        [35m0.2451[0m       0.9057        0.2417        8.9913
      4   [36m0.2395[0m   [32m0.7928[0m        [35m0.2414[0m       0.9021        0.2511        9.5896
      5   [36m0.2426[0m   0.7926        [35m0.2401[0m       0.9081        0.2422        9.1995
      6   0.2236   0.7785        0.2402       0.9069        0.2503        9.0844
      7   0.2235   0.7756        [35m0.2388[0m       0.9105        0.2472        9.8039
      8   0.2316   0.7795        [35m0.2363[0m       0.9093        0.2505        9.6792
      9   0.2275   0.7710        0.2372       0.9081        0.2596        9.2659
     10   0.2288   0.7684        [35m0.2359[0m       0.9093        0.2530        9.2116
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 14:38:57,471][0m Trial 150 finished with value: 0.2362463576257445 and parameters: {'lr': 0.00021417701037855313, 'dropout': 0.4918299003200124, 'd_model_multiplier': 8, 'num_layers': 1, 'n_heads': 8, 'dim_feedforward': 138, 'batch_size': 57, 'pos_encoding': 'learnable', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 201}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 161
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.2447[0m   [32m0.6764[0m        [35m0.3169[0m       [31m0.9021[0m        [94m0.3080[0m     +  8.5667
      2   [36m0.2629[0m   [32m0.7089[0m        [35m0.2464[0m       0.9008        0.3094        9.3715
      3   [36m0.2671[0m   [32m0.7253[0m        [35m0.2412[0m       0.8960        [94m0.3052[0m     +  9.8891
      4   [36m0.2745[0m   [32m0.7341[0m        [35m0.2401[0m       0.8972        [94m0.3017[0m     +  9.5226
      5   0.2743   [32m0.7441[0m        [35m0.2354[0m       0.8996        0.3095        9.2874
      6   [36m0.2801[0m   [32m0.7470[0m        [35m0.2344[0m       0.9008        0.3187        9.5215
      7   [36m0.2847[0m   [32m0.7522[0m        [35m0.2328[0m       0.8996        0.3114        9.3979
      8   [36m0.2948[0m   [32m0.7595[0m        [35m0.2313[0m       0.9021        0.3102        9.4725
      9   0.2899   0.7561        0.2326       [31m0.9033[0m        0.3069        9.5003
     10   [36m0.2968[0m   [32m0.7596[0m        [35m0.2294[0m       0.9021        0.3018        9.7779
     11   [36m0.3085[0m   0.7581        [35m0.2289[0m       0.9033        0.3037        9.4566
     12   0.2903   0.7594        [35m0.2262[0m       0.9008        0.3044        9.7864
     13   [36m0.3091[0m   [32m0.7607[0m        [35m0.2242[0m       0.9008        0.3252        9.7905
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 14:41:10,881][0m Trial 151 finished with value: 0.3016959781946554 and parameters: {'lr': 0.0007133914233064297, 'dropout': 0.5164412087064566, 'd_model_multiplier': 8, 'num_layers': 2, 'n_heads': 8, 'dim_feedforward': 447, 'batch_size': 46, 'pos_encoding': 'learnable', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 161}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 172
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.1728[0m   [32m0.6450[0m        [35m0.3291[0m       [31m0.9287[0m        [94m0.2541[0m     +  8.7268
      2   [36m0.1918[0m   [32m0.7146[0m        [35m0.2504[0m       0.9274        [94m0.2445[0m     +  8.2854
      3   [36m0.2247[0m   [32m0.7476[0m        [35m0.2461[0m       0.9238        [94m0.2327[0m     +  8.8041
      4   0.2244   [32m0.7485[0m        0.2467       0.9226        0.2352        9.3521
      5   [36m0.2320[0m   [32m0.7490[0m        [35m0.2443[0m       0.9226        0.2350        9.2160
      6   0.2289   0.7462        [35m0.2408[0m       0.9190        0.2354        9.6567
      7   [36m0.2406[0m   [32m0.7496[0m        0.2410       0.9238        0.2353        9.1172
      8   [36m0.2451[0m   0.7384        [35m0.2407[0m       0.9274        0.2344        9.6153
      9   [36m0.2550[0m   [32m0.7572[0m        [35m0.2406[0m       0.9250        [94m0.2283[0m     +  9.0274
     10   0.2497   0.7451        [35m0.2376[0m       0.9238        0.2355        9.0336
     11   0.2421   0.7449        [35m0.2368[0m       0.9214        0.2349        9.1140
     12   [36m0.2646[0m   0.7448        0.2386       0.9190        0.2415        8.8959
     13   [36m0.2707[0m   0.7407        [35m0.2361[0m       0.9262        0.2373        9.3521
     14   [36m0.2714[0m   0.7398        [35m0.2351[0m       0.9262        0.2340        9.1323
     15   [36m0.2741[0m   0.7493        0.2373       0.9238        0.2292        9.1528
     16   [36m0.2933[0m   0.7467        0.2356       0.9274        0.2353        9.3503
     17   0.2921   0.7491        [35m0.2323[0m       [31m0.9299[0m        0.2334        9.3705
     18   0.2788   0.7496        [35m0.2309[0m       0.9238        0.2352        9.0661
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 14:44:04,831][0m Trial 152 finished with value: 0.228304927454678 and parameters: {'lr': 0.0015240960452724925, 'dropout': 0.4730154447419574, 'd_model_multiplier': 8, 'num_layers': 1, 'n_heads': 4, 'dim_feedforward': 426, 'batch_size': 51, 'pos_encoding': 'learnable', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 172}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 154
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.2111[0m   [32m0.6904[0m        [35m0.3279[0m       [31m0.9166[0m        [94m0.2722[0m     +  8.1560
      2   [36m0.2148[0m   [32m0.6957[0m        [35m0.2528[0m       0.9154        [94m0.2668[0m     +  9.3444
      3   [36m0.2287[0m   [32m0.6979[0m        [35m0.2482[0m       0.9154        0.2708        9.1447
      4   [36m0.2357[0m   [32m0.7053[0m        [35m0.2440[0m       0.9166        0.2700        9.1123
      5   [36m0.2541[0m   [32m0.7173[0m        [35m0.2391[0m       [31m0.9202[0m        0.2675        9.2949
      6   0.2449   [32m0.7181[0m        [35m0.2362[0m       0.9178        0.2693        8.9356
      7   0.2491   [32m0.7186[0m        [35m0.2327[0m       0.9202        0.2733        9.3830
      8   [36m0.2752[0m   [32m0.7370[0m        [35m0.2324[0m       0.9178        [94m0.2639[0m     +  9.0742
      9   0.2685   [32m0.7430[0m        [35m0.2305[0m       0.9178        0.2707        9.3325
     10   0.2552   0.7260        [35m0.2284[0m       0.9166        0.2749        9.2095
     11   [36m0.2787[0m   0.7385        [35m0.2263[0m       0.9166        0.2655        9.2491
     12   [36m0.2870[0m   [32m0.7483[0m        [35m0.2247[0m       0.9166        0.2644        9.3090
     13   0.2859   [32m0.7610[0m        [35m0.2193[0m       0.9178        0.2645        9.1878
     14   [36m0.2932[0m   0.7538        [35m0.2193[0m       0.9190        0.2676        9.1116
     15   0.2731   0.7498        [35m0.2184[0m       0.9178        0.2735        9.2093
     16   0.2710   0.7419        [35m0.2159[0m       0.9190        0.2702        9.2252
     17   0.2777   0.7525        [35m0.2139[0m       0.9178        0.2666        9.5515
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 14:46:50,770][0m Trial 153 finished with value: 0.26394232134576867 and parameters: {'lr': 0.0005552695030581862, 'dropout': 0.3686368281187119, 'd_model_multiplier': 8, 'num_layers': 2, 'n_heads': 4, 'dim_feedforward': 137, 'batch_size': 42, 'pos_encoding': 'learnable', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0, 'top_n_features': 154}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 223
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.2008[0m   [32m0.7132[0m        [35m0.2743[0m       [31m0.9226[0m        [94m0.2511[0m     +  9.7017
      2   0.1884   [32m0.7225[0m        [35m0.2626[0m       0.9226        0.2570        9.4106
      3   0.1814   0.7209        [35m0.2610[0m       0.9226        0.2686        9.7646
      4   0.1983   [32m0.7298[0m        [35m0.2588[0m       0.9226        0.2657        10.0500
      5   [36m0.2056[0m   0.7196        [35m0.2576[0m       0.9226        0.2654        9.8229
      6   0.1885   0.7190        [35m0.2557[0m       0.9226        0.2666        9.8218
      7   0.1955   0.7273        0.2566       0.9226        0.2614        9.7782
      8   0.1836   0.7211        0.2559       0.9226        0.2596        9.9825
      9   0.1919   0.7273        [35m0.2546[0m       0.9226        0.2520        10.0267
     10   0.1878   0.7238        [35m0.2545[0m       0.9226        0.2556        10.1124
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 14:48:39,334][0m Trial 154 finished with value: 0.25109714257926147 and parameters: {'lr': 0.004186840641730334, 'dropout': 0.45165312560381, 'd_model_multiplier': 4, 'num_layers': 3, 'n_heads': 8, 'dim_feedforward': 487, 'batch_size': 21, 'pos_encoding': 'learnable', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 223}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 142
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1486[0m   [32m0.6704[0m        [35m0.3259[0m       [31m0.9105[0m        [94m0.2767[0m     +  26.9778
      2   [36m0.1526[0m   [32m0.6717[0m        [35m0.2484[0m       0.9105        [94m0.2706[0m     +  27.1009
      3   0.1479   [32m0.6727[0m        [35m0.2457[0m       [31m0.9190[0m        [94m0.2668[0m     +  26.9574
      4   [36m0.1576[0m   [32m0.6727[0m        [35m0.2368[0m       0.9190        0.2708        27.1373
      5   [36m0.1686[0m   [32m0.6775[0m        [35m0.2341[0m       0.9154        0.2696        27.2277
      6   0.1647   0.6760        [35m0.2308[0m       0.9141        0.2769        27.3650
      7   0.1630   [32m0.6943[0m        [35m0.2275[0m       0.9141        0.2672        27.2752
      8   0.1630   [32m0.6952[0m        [35m0.2250[0m       0.9154        0.2684        27.3919
      9   0.1562   0.6786        [35m0.2234[0m       0.9178        0.2698        27.2465
     10   [36m0.1708[0m   [32m0.6977[0m        [35m0.2214[0m       0.9166        [94m0.2657[0m     +  27.4505
     11   [36m0.1919[0m   [32m0.7079[0m        [35m0.2183[0m       0.9129        0.2665        27.4813
     12   0.1694   0.7037        [35m0.2173[0m       0.9190        [94m0.2586[0m     +  27.3024
     13   [36m0.1982[0m   [32m0.7136[0m        [35m0.2139[0m       0.9117        0.2764        27.3324
     14   0.1759   0.7089        [35m0.2089[0m       0.9141        0.2702        27.2854
     15   0.1783   0.6840        0.2109       0.9166        0.2722        27.2935
     16   0.1800   0.6937        [35m0.2037[0m       0.9154        0.2726        27.1004
     17   [36m0.1994[0m   0.6982        0.2044       0.9117        0.2726        27.2233
     18   0.1716   0.6822        [35m0.2028[0m       0.9117        0.2755        27.0925
     19   0.1888   0.7012        [35m0.2004[0m       0.9129        0.2685        27.1336
     20   0.1645   0.6846        [35m0.1988[0m       0.9141        0.2767        27.3818
     21   0.1816   0.7112        0.1989       0.9093        0.2825        27.0556
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 14:58:39,004][0m Trial 155 finished with value: 0.2585639011614844 and parameters: {'lr': 0.00030989628943015496, 'dropout': 0.3525002694335515, 'd_model_multiplier': 1, 'num_layers': 4, 'n_heads': 64, 'dim_feedforward': 407, 'batch_size': 37, 'pos_encoding': 'learnable', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0, 'top_n_features': 142}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 165
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.1621[0m   [32m0.6429[0m        [35m0.2788[0m       [31m0.9226[0m        [94m0.2671[0m     +  8.9397
      2   [36m0.1877[0m   [32m0.6570[0m        [35m0.2592[0m       0.9214        [94m0.2603[0m     +  9.3506
      3   [36m0.2231[0m   [32m0.7059[0m        [35m0.2485[0m       0.9214        [94m0.2498[0m     +  9.2862
      4   [36m0.2765[0m   [32m0.7228[0m        [35m0.2375[0m       0.9226        [94m0.2421[0m     +  9.2845
      5   [36m0.2916[0m   [32m0.7232[0m        [35m0.2350[0m       [31m0.9250[0m        [94m0.2419[0m     +  9.5223
      6   0.2802   [32m0.7374[0m        [35m0.2324[0m       0.9238        [94m0.2397[0m     +  9.7010
      7   0.2724   0.7246        [35m0.2314[0m       0.9238        0.2425        9.5225
      8   0.2611   [32m0.7413[0m        [35m0.2305[0m       0.9226        0.2455        9.5821
      9   0.2557   0.7276        0.2311       0.9226        0.2447        9.5353
     10   0.2636   [32m0.7415[0m        0.2316       0.9238        0.2399        9.3305
     11   0.2655   [32m0.7445[0m        [35m0.2280[0m       0.9250        0.2401        9.5636
     12   0.2632   0.7323        0.2298       0.9238        0.2439        9.5064
     13   0.2769   0.7421        [35m0.2269[0m       0.9238        0.2422        9.6008
     14   0.2541   0.7350        [35m0.2242[0m       0.9226        0.2486        9.7228
     15   0.2915   0.7405        0.2290       [31m0.9274[0m        0.2399        9.2980
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 15:01:11,128][0m Trial 156 finished with value: 0.23968510997302586 and parameters: {'lr': 0.0010220627563868759, 'dropout': 0.30870364625507335, 'd_model_multiplier': 16, 'num_layers': 2, 'n_heads': 4, 'dim_feedforward': 150, 'batch_size': 14, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 165}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 158
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1042[0m   [32m0.4027[0m        [35m0.2962[0m       [31m0.1487[0m        [94m1.3613[0m     +  10.6901
      2   [36m0.2474[0m   [32m0.6684[0m        [35m0.2718[0m       [31m0.5961[0m        [94m0.7068[0m     +  10.8067
      3   0.0915   0.5057        [35m0.2678[0m       0.1245        1.3279        10.6969
      4   [36m0.2571[0m   [32m0.7221[0m        0.2699       [31m0.6759[0m        [94m0.6179[0m     +  10.7350
      5   0.1060   0.5287        [35m0.2660[0m       [31m0.9093[0m        [94m0.3623[0m     +  10.6810
      6   0.1789   0.6380        [35m0.2592[0m       0.9093        [94m0.3199[0m     +  11.2536
      7   0.2237   0.6898        [35m0.2452[0m       0.8888        0.3258        10.7932
      8   0.2419   0.6993        0.2454       0.9081        [94m0.2862[0m     +  11.2835
      9   0.1814   0.6763        [35m0.2445[0m       0.7678        0.4702        10.9041
     10   0.1940   0.6786        [35m0.2368[0m       0.8464        0.3624        11.1160
     11   0.1655   0.6354        0.2372       0.8827        0.3415        11.0718
     12   0.1518   0.6293        [35m0.2346[0m       0.9008        0.3418        11.2003
     13   0.2303   0.6815        0.2425       0.9081        0.2943        10.9178
     14   0.2570   0.7121        [35m0.2311[0m       [31m0.9141[0m        0.2872        10.9600
     15   0.2481   0.6892        0.2333       0.9081        0.2923        11.4067
     16   0.2069   0.6681        0.2332       0.9069        0.3186        11.0630
     17   0.2147   0.6617        0.2318       0.9069        0.3452        11.0549
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 15:04:29,170][0m Trial 157 finished with value: 0.2862013964908155 and parameters: {'lr': 0.0023724681170972433, 'dropout': 0.3407434990638322, 'd_model_multiplier': 8, 'num_layers': 1, 'n_heads': 32, 'dim_feedforward': 466, 'batch_size': 97, 'pos_encoding': 'learnable', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.01, 'top_n_features': 158}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 182
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.2242[0m   [32m0.7169[0m        [35m0.2985[0m       [31m0.9166[0m        [94m0.3033[0m     +  8.2281
      2   [36m0.2415[0m   [32m0.7415[0m        [35m0.2562[0m       0.9166        [94m0.2557[0m     +  8.5706
      3   [36m0.2538[0m   0.7398        [35m0.2457[0m       0.9154        0.2574        9.5139
      4   [36m0.2832[0m   [32m0.7568[0m        [35m0.2432[0m       [31m0.9178[0m        [94m0.2534[0m     +  9.4147
      5   0.2513   0.7307        [35m0.2373[0m       0.9166        0.2571        9.2632
      6   0.2818   0.7511        [35m0.2346[0m       0.9166        [94m0.2497[0m     +  9.6691
      7   0.2828   [32m0.7593[0m        [35m0.2318[0m       [31m0.9190[0m        0.2528        9.5415
      8   [36m0.2998[0m   [32m0.7685[0m        [35m0.2304[0m       0.9178        [94m0.2449[0m     +  9.5215
      9   [36m0.3063[0m   0.7674        [35m0.2283[0m       [31m0.9214[0m        0.2500        9.6431
     10   [36m0.3123[0m   [32m0.7728[0m        0.2304       0.9178        0.2482        9.6283
     11   0.3056   0.7630        [35m0.2271[0m       0.9166        0.2471        9.6245
     12   [36m0.3219[0m   0.7531        [35m0.2244[0m       0.9166        0.2530        9.8034
     13   [36m0.3506[0m   0.7657        [35m0.2237[0m       0.9190        0.2477        9.4839
     14   0.3040   0.7513        0.2252       0.9178        0.2518        9.5690
     15   0.3109   0.7501        0.2245       0.9178        0.2626        9.1934
     16   0.3373   0.7524        [35m0.2219[0m       0.9178        0.2479        9.2080
     17   0.3206   0.7452        0.2225       0.9178        0.2531        9.5588
     18   [36m0.3515[0m   0.7638        0.2265       0.9178        [94m0.2449[0m     +  9.6208
     19   0.3413   0.7670        0.2229       [31m0.9226[0m        0.2507        9.3892
     20   0.3466   0.7613        [35m0.2199[0m       0.9202        [94m0.2434[0m     +  9.7020
     21   0.3085   0.7463        0.2214       0.9178        0.2491        9.6249
     22   0.2963   0.7346        0.2217       0.9166        0.2550        9.8920
     23   0.2816   0.7350        0.2214       0.9166        0.2575        9.4717
     24   0.3273   0.7514        [35m0.2147[0m       0.9202        0.2484        9.9006
     25   0.3190   0.7544        0.2180       0.9178        0.2486        9.7304
     26   0.3188   0.7536        [35m0.2144[0m       0.9178        0.2547        9.5858
     27   0.3374   0.7665        0.2182       0.9202        0.2437        9.4052
     28   0.3257   0.7521        0.2183       0.9190        0.2516        9.6840
     29   0.3175   0.7546        0.2170       0.9178        0.2516        9.6159
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 15:09:14,008][0m Trial 158 finished with value: 0.2433878176157535 and parameters: {'lr': 0.006173282726970838, 'dropout': 0.4035141068260572, 'd_model_multiplier': 1, 'num_layers': 2, 'n_heads': 4, 'dim_feedforward': 207, 'batch_size': 72, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 182}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 231
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1438[0m   [32m0.6162[0m        [35m2.2690[0m       [31m0.8573[0m        [94m0.4047[0m     +  16.3521
      2   [36m0.1819[0m   [32m0.6451[0m        [35m0.3088[0m       0.7449        0.4796        16.8696
      3   0.1726   [32m0.6465[0m        [35m0.2743[0m       [31m0.9081[0m        [94m0.3119[0m     +  17.0551
      4   [36m0.1820[0m   0.6456        [35m0.2659[0m       0.9045        0.3159        16.8754
      5   0.1473   0.6359        [35m0.2656[0m       [31m0.9250[0m        [94m0.2954[0m     +  16.9789
      6   0.1734   [32m0.6537[0m        [35m0.2644[0m       [31m0.9299[0m        0.3151        17.2184
      7   0.1642   0.6475        [35m0.2621[0m       0.9299        [94m0.2933[0m     +  17.4256
      8   0.1820   0.6384        [35m0.2597[0m       0.9299        [94m0.2865[0m     +  17.0858
      9   0.1581   0.6410        0.2609       0.9299        0.3779        16.8948
     10   0.1793   0.6312        0.2600       0.9299        0.3980        17.1875
     11   0.1119   0.5976        0.2602       0.9299        0.2962        16.8678
     12   0.1023   0.5989        [35m0.2565[0m       0.9299        0.3215        16.9708
     13   0.1363   0.6313        0.2614       0.9299        0.3118        17.0122
     14   0.1688   [32m0.6598[0m        0.2579       0.9299        [94m0.2771[0m     +  16.8168
     15   0.1628   0.6581        [35m0.2565[0m       0.9299        0.3369        16.7937
     16   0.1523   [32m0.6618[0m        0.2569       0.9299        0.3158        17.0583
     17   0.1467   0.6418        [35m0.2552[0m       0.9299        0.3267        17.1947
     18   0.0824   0.4808        0.2558       0.9299        0.3340        17.3248
     19   0.1648   0.6511        [35m0.2550[0m       0.9299        0.2893        16.7306
     20   0.1424   0.6425        [35m0.2532[0m       0.9299        0.3115        17.2555
     21   0.1601   0.6468        0.2539       0.9299        0.2855        17.0732
     22   0.1495   0.6364        0.2542       0.9299        0.2934        16.8771
     23   0.1478   0.6374        [35m0.2529[0m       0.9299        [94m0.2643[0m     +  16.9674
     24   0.0741   0.3970        0.2546       0.9299        0.2860        17.1166
     25   0.1532   0.6472        [35m0.2522[0m       0.9299        0.2698        17.0113
     26   0.1716   0.6489        [35m0.2518[0m       0.9299        0.2727        16.8307
     27   0.0701   0.5000        0.2526       0.9299        0.2703        17.2332
     28   0.1096   0.5934        [35m0.2495[0m       0.9299        0.2661        17.0451
     29   0.0701   0.5000        0.2527       0.9299        0.2646        17.0173
     30   0.1341   0.6400        0.2531       0.9299        [94m0.2617[0m     +  17.0714
     31   0.1085   0.6049        0.2526       0.9299        0.2981        17.1269
     32   0.1368   0.6418        0.2519       0.9299        [94m0.2583[0m     +  17.2130
     33   0.1081   0.4727        0.2533       0.9299        0.2585        17.1726
     34   0.1412   0.6152        0.2503       0.9299        [94m0.2569[0m     +  16.8780
     35   0.0920   0.4438        0.2543       0.9299        [94m0.2566[0m     +  17.1651
     36   0.0701   0.5000        0.2518       0.9299        [94m0.2557[0m     +  17.1110
     37   0.0701   0.5000        0.2498       0.9299        [94m0.2553[0m     +  17.0839
     38   0.1300   0.6100        0.2523       0.9299        [94m0.2533[0m     +  17.1540
     39   0.0780   0.4783        0.2552       0.9299        0.2586        16.9799
     40   0.0701   0.5000        0.2535       0.9299        0.2543        17.1387
     41   0.1727   0.6488        0.2536       0.9299        [94m0.2487[0m     +  17.0609
     42   0.0701   0.5000        0.2523       0.9299        0.2540        16.8475
     43   0.0701   0.5000        0.2524       0.9299        0.2541        16.9162
     44   0.1527   0.6407        0.2527       0.9299        [94m0.2481[0m     +  16.9765
     45   0.1335   0.5854        0.2526       0.9299        0.2541        16.9951
     46   0.0701   0.5000        0.2521       0.9299        0.2542        16.9498
     47   0.0701   0.5000        0.2516       0.9299        0.2543        16.8111
     48   0.0701   0.5000        [35m0.2494[0m       0.9299        0.2545        17.1238
     49   0.1370   0.6118        0.2508       0.9299        0.2497        16.9054
     50   0.1482   [32m0.6951[0m        0.2511       0.9299        0.2543        17.1066
[32m[I 2023-05-02 15:23:28,328][0m Trial 159 finished with value: 0.2481499861705116 and parameters: {'lr': 0.01044719799127782, 'dropout': 0.4794807994307057, 'd_model_multiplier': 32, 'num_layers': 3, 'n_heads': 16, 'dim_feedforward': 227, 'batch_size': 114, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0, 'top_n_features': 231}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 26
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.3585[0m   [32m0.8247[0m        [35m0.3333[0m       [31m0.9178[0m        [94m0.5731[0m     +  13.0333
      2   0.3535   [32m0.8283[0m        [35m0.2542[0m       0.9178        [94m0.5157[0m     +  13.0708
      3   0.3335   0.8099        0.2543       [31m0.9299[0m        [94m0.4649[0m     +  13.3550
      4   0.2631   0.7595        [35m0.2514[0m       0.9287        [94m0.3799[0m     +  13.3014
      5   0.2609   0.7658        [35m0.2480[0m       0.9287        [94m0.3325[0m     +  13.1926
      6   0.2158   0.7326        [35m0.2471[0m       0.9274        [94m0.3025[0m     +  13.1388
      7   0.2108   0.7366        [35m0.2443[0m       0.9274        [94m0.2874[0m     +  13.3474
      8   0.2084   0.7326        [35m0.2434[0m       0.9287        [94m0.2714[0m     +  13.3528
      9   0.2123   0.7207        [35m0.2413[0m       0.9287        [94m0.2656[0m     +  13.2565
     10   0.1971   0.7328        [35m0.2413[0m       0.9274        [94m0.2647[0m     +  13.3136
     11   0.1797   0.7062        0.2419       0.9287        [94m0.2615[0m     +  13.8959
     12   0.2526   0.7739        [35m0.2386[0m       0.9262        [94m0.2512[0m     +  13.2400
     13   0.2600   0.7676        [35m0.2378[0m       0.9262        0.2528        13.2668
     14   0.2524   0.7692        [35m0.2372[0m       0.9262        0.2556        13.4358
     15   0.2581   0.7646        0.2381       0.9262        0.2516        13.6103
     16   0.2932   0.7901        0.2395       0.9274        0.2531        13.4764
     17   0.2962   0.7952        0.2389       0.9238        [94m0.2442[0m     +  13.4658
     18   0.3019   0.8195        0.2402       0.9250        0.2544        13.2773
     19   0.3034   0.8207        0.2394       0.9250        0.2513        13.2657
     20   0.3166   [32m0.8316[0m        [35m0.2359[0m       0.9262        0.2516        13.2829
     21   0.3059   [32m0.8319[0m        0.2384       0.9250        0.2462        13.3573
     22   0.3040   [32m0.8356[0m        0.2367       0.9250        0.2464        13.2994
     23   0.3025   0.8317        0.2419       0.9238        0.2512        13.4029
     24   0.3037   [32m0.8394[0m        0.2401       0.9262        0.2706        13.2467
     25   0.3041   0.8346        [35m0.2337[0m       0.9262        0.2599        13.1679
     26   0.3070   0.8263        0.2399       0.9274        [94m0.2427[0m     +  13.2050
     27   0.3122   0.8320        0.2346       0.9287        [94m0.2420[0m     +  13.5014
     28   0.3124   0.8348        [35m0.2329[0m       0.9274        [94m0.2408[0m     +  13.4256
     29   0.3130   0.8384        0.2370       0.9287        [94m0.2397[0m     +  13.2267
     30   0.3093   0.8347        [35m0.2289[0m       0.9262        [94m0.2264[0m     +  13.2263
     31   0.3208   0.8372        0.2341       0.9274        [94m0.2204[0m     +  13.0924
     32   0.3210   0.8331        0.2307       0.9274        [94m0.2199[0m     +  13.2110
     33   0.3222   0.8284        0.2315       0.9262        [94m0.2176[0m     +  13.4772
     34   0.3193   0.8346        0.2313       0.9250        0.2221        13.3078
     35   0.3134   0.8312        0.2329       0.9274        [94m0.2169[0m     +  13.1426
     36   0.3220   0.8241        0.2300       0.9287        [94m0.2125[0m     +  13.3051
     37   0.3182   0.8187        0.2295       0.9287        0.2149        13.2436
     38   0.3198   0.8149        0.2313       0.9262        0.2155        13.2569
     39   0.3218   0.8127        [35m0.2286[0m       0.9274        0.2227        13.3118
     40   0.3284   0.8152        [35m0.2284[0m       0.9299        0.2157        13.2737
     41   0.3374   0.8193        [35m0.2283[0m       [31m0.9311[0m        0.2127        13.1696
     42   0.3126   0.8121        0.2293       0.9262        0.2207        13.2161
     43   0.3151   0.8191        [35m0.2253[0m       0.9287        0.2160        13.3524
     44   0.2988   0.8147        0.2302       0.9238        0.2207        13.3934
     45   0.3071   0.8157        [35m0.2224[0m       0.9262        0.2179        13.3017
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 15:33:43,206][0m Trial 160 finished with value: 0.2125424469450653 and parameters: {'lr': 0.00014713090182931978, 'dropout': 0.522605838812969, 'd_model_multiplier': 8, 'num_layers': 2, 'n_heads': 32, 'dim_feedforward': 132, 'batch_size': 30, 'pos_encoding': 'learnable', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.01, 'top_n_features': 26}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 60
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.3480[0m   [32m0.7770[0m        [35m0.3428[0m       [31m0.9117[0m        [94m0.3492[0m     +  13.1906
      2   [36m0.3620[0m   [32m0.7847[0m        [35m0.2556[0m       [31m0.9154[0m        [94m0.3105[0m     +  13.1017
      3   0.3572   0.7842        [35m0.2482[0m       [31m0.9178[0m        [94m0.2918[0m     +  13.4011
      4   0.3373   0.7815        [35m0.2461[0m       0.9141        [94m0.2778[0m     +  13.3740
      5   0.3412   [32m0.7911[0m        [35m0.2423[0m       0.9117        [94m0.2720[0m     +  13.3395
      6   0.3303   0.7895        [35m0.2405[0m       0.9105        [94m0.2685[0m     +  13.2916
      7   0.3292   [32m0.7957[0m        [35m0.2367[0m       0.9081        [94m0.2651[0m     +  13.3150
      8   0.3431   [32m0.8107[0m        [35m0.2366[0m       0.9057        [94m0.2622[0m     +  13.3570
      9   0.3414   0.8054        [35m0.2353[0m       0.9045        0.2663        13.3880
     10   0.3420   [32m0.8130[0m        0.2369       0.9045        0.2629        13.2430
     11   0.3445   [32m0.8214[0m        [35m0.2329[0m       0.9069        [94m0.2600[0m     +  13.4043
     12   [36m0.3666[0m   [32m0.8311[0m        [35m0.2310[0m       0.9081        [94m0.2537[0m     +  13.1766
     13   [36m0.3676[0m   [32m0.8391[0m        0.2339       0.9069        0.2544        13.2992
     14   0.3607   0.8382        [35m0.2295[0m       0.9081        0.2540        13.2573
     15   0.3571   0.8380        0.2305       0.9081        0.2577        13.2398
     16   [36m0.3682[0m   0.8388        [35m0.2269[0m       0.9117        0.2564        13.3129
     17   0.3640   0.8375        0.2283       0.9069        0.2553        13.4240
     18   0.3578   0.8381        [35m0.2261[0m       0.9081        0.2602        13.2715
     19   0.3634   0.8376        0.2273       0.9081        0.2614        13.0757
     20   0.3621   [32m0.8432[0m        0.2267       0.9057        0.2590        13.2673
     21   0.3582   [32m0.8451[0m        [35m0.2244[0m       0.9081        0.2633        13.3734
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 15:38:36,772][0m Trial 161 finished with value: 0.2537322187077754 and parameters: {'lr': 0.00010194117341991292, 'dropout': 0.5207287738319645, 'd_model_multiplier': 8, 'num_layers': 2, 'n_heads': 32, 'dim_feedforward': 133, 'batch_size': 30, 'pos_encoding': 'learnable', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.01, 'top_n_features': 60}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 92
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.2933[0m   [32m0.7196[0m        [35m0.3013[0m       [31m0.9141[0m        [94m0.3109[0m     +  9.2143
      2   [36m0.3056[0m   [32m0.7230[0m        [35m0.2502[0m       [31m0.9202[0m        [94m0.2966[0m     +  9.6913
      3   0.2902   0.7200        [35m0.2459[0m       0.9190        [94m0.2906[0m     +  9.3539
      4   0.2961   [32m0.7281[0m        [35m0.2437[0m       0.9166        [94m0.2826[0m     +  9.5324
      5   0.2810   0.7140        [35m0.2391[0m       0.9178        [94m0.2734[0m     +  9.6078
      6   0.2769   0.7213        [35m0.2376[0m       0.9154        0.2752        9.6091
      7   0.2622   0.7034        0.2402       0.9141        0.2754        9.5534
      8   0.2664   0.6934        [35m0.2363[0m       0.9129        0.2788        9.9053
      9   0.2583   0.6823        0.2381       0.9141        0.2835        9.6918
     10   0.2571   0.6789        [35m0.2322[0m       0.9141        0.2878        9.4629
     11   0.2565   0.6679        0.2348       0.9166        0.2876        9.5297
     12   0.2627   0.6639        [35m0.2320[0m       0.9166        0.2939        9.6785
     13   0.2549   0.6386        0.2323       0.9154        0.2997        9.3174
     14   0.2485   0.6506        0.2348       0.9178        0.3012        9.5797
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 15:41:00,655][0m Trial 162 finished with value: 0.27344818180821107 and parameters: {'lr': 0.00016884062257437908, 'dropout': 0.505063002968575, 'd_model_multiplier': 8, 'num_layers': 1, 'n_heads': 32, 'dim_feedforward': 128, 'batch_size': 24, 'pos_encoding': 'learnable', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.01, 'top_n_features': 92}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 36
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2739[0m   [32m0.7212[0m        [35m0.3745[0m       [31m0.9323[0m        [94m0.3372[0m     +  14.2335
      2   [36m0.3017[0m   [32m0.7492[0m        [35m0.2578[0m       [31m0.9335[0m        [94m0.3044[0m     +  14.6436
      3   [36m0.3237[0m   [32m0.7558[0m        [35m0.2485[0m       [31m0.9347[0m        [94m0.2855[0m     +  14.9033
      4   [36m0.3251[0m   0.7542        [35m0.2432[0m       0.9347        [94m0.2699[0m     +  15.2138
      5   [36m0.3303[0m   0.7545        [35m0.2388[0m       0.9347        [94m0.2614[0m     +  15.2263
      6   0.3272   [32m0.7626[0m        [35m0.2380[0m       0.9347        [94m0.2584[0m     +  15.2431
      7   0.3285   0.7592        [35m0.2341[0m       0.9335        [94m0.2509[0m     +  15.0535
      8   [36m0.3339[0m   0.7603        [35m0.2303[0m       0.9347        [94m0.2491[0m     +  14.8754
      9   [36m0.3359[0m   [32m0.7680[0m        0.2304       0.9335        0.2502        14.8711
     10   [36m0.3474[0m   [32m0.7701[0m        [35m0.2263[0m       [31m0.9359[0m        [94m0.2400[0m     +  15.0924
     11   0.3167   0.7690        0.2269       0.9311        0.2486        14.8176
     12   0.3287   [32m0.7769[0m        [35m0.2249[0m       0.9335        0.2405        14.8707
     13   0.3188   0.7729        0.2253       0.9311        [94m0.2394[0m     +  14.8961
     14   0.3191   0.7724        [35m0.2188[0m       0.9323        [94m0.2347[0m     +  15.1082
     15   0.3135   0.7619        0.2214       0.9299        0.2416        15.1041
     16   0.3249   0.7699        0.2238       0.9323        0.2420        15.1164
     17   0.3077   0.7745        [35m0.2179[0m       0.9274        0.2430        14.8596
     18   0.3045   0.7746        0.2180       0.9311        0.2368        15.0347
     19   0.2893   0.7733        0.2197       0.9287        0.2395        14.9259
     20   0.2741   [32m0.7822[0m        0.2188       0.9250        0.2446        15.2098
     21   0.2989   0.7816        [35m0.2164[0m       0.9250        0.2352        15.2367
     22   0.2928   0.7754        0.2185       0.9323        [94m0.2290[0m     +  15.1084
     23   0.2939   0.7757        [35m0.2163[0m       0.9311        0.2361        15.1180
     24   0.2851   0.7803        0.2168       0.9299        0.2370        15.2531
     25   0.2812   [32m0.7873[0m        0.2193       0.9287        0.2403        15.3077
     26   0.2697   [32m0.7891[0m        0.2163       0.9262        0.2341        15.2511
     27   0.2728   0.7857        [35m0.2133[0m       0.9262        0.2428        15.1252
     28   0.2703   [32m0.7945[0m        [35m0.2106[0m       0.9274        0.2370        15.0823
     29   0.2952   [32m0.7951[0m        0.2130       0.9287        0.2308        15.1424
     30   0.2952   [32m0.8000[0m        [35m0.2063[0m       0.9287        0.2341        15.0291
     31   0.2930   [32m0.8010[0m        0.2074       0.9262        0.2456        15.0467
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 15:49:02,922][0m Trial 163 finished with value: 0.22896853438888237 and parameters: {'lr': 0.0003499147752657581, 'dropout': 0.4327951084662699, 'd_model_multiplier': 8, 'num_layers': 2, 'n_heads': 32, 'dim_feedforward': 144, 'batch_size': 119, 'pos_encoding': 'learnable', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.01, 'top_n_features': 36}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 22
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.1241[0m   [32m0.5900[0m        [35m0.4440[0m       [31m0.9335[0m        [94m0.4116[0m     +  9.2928
      2   [36m0.1761[0m   [32m0.6809[0m        [35m0.2656[0m       0.9274        [94m0.3500[0m     +  9.3039
      3   [36m0.1913[0m   [32m0.6978[0m        [35m0.2551[0m       0.9262        [94m0.3323[0m     +  9.2932
      4   0.1910   0.6961        [35m0.2451[0m       0.9250        [94m0.3257[0m     +  9.5736
      5   [36m0.1933[0m   0.6966        0.2480       0.9238        [94m0.3195[0m     +  9.4065
      6   [36m0.1970[0m   [32m0.6983[0m        [35m0.2445[0m       0.9250        [94m0.3099[0m     +  9.8907
      7   [36m0.2009[0m   [32m0.7014[0m        [35m0.2440[0m       0.9262        [94m0.3038[0m     +  9.6473
      8   [36m0.2022[0m   0.6992        [35m0.2439[0m       0.9262        [94m0.3020[0m     +  9.6525
      9   [36m0.2026[0m   [32m0.7021[0m        [35m0.2414[0m       0.9274        [94m0.2904[0m     +  9.9180
     10   [36m0.2103[0m   [32m0.7060[0m        [35m0.2410[0m       0.9274        0.2911        9.8324
     11   0.2078   0.7023        [35m0.2405[0m       0.9311        [94m0.2833[0m     +  9.6036
     12   0.2056   0.7030        [35m0.2379[0m       0.9299        [94m0.2820[0m     +  9.8488
     13   0.2082   0.6998        0.2409       0.9274        [94m0.2803[0m     +  9.7274
     14   0.2086   0.6955        0.2397       0.9323        [94m0.2759[0m     +  9.6194
     15   [36m0.2112[0m   0.7011        [35m0.2372[0m       0.9299        [94m0.2725[0m     +  9.7004
     16   [36m0.2130[0m   0.7040        0.2372       0.9323        [94m0.2699[0m     +  9.7710
     17   [36m0.2140[0m   0.7056        [35m0.2369[0m       0.9335        0.2727        9.8581
     18   0.2134   0.7033        0.2373       0.9323        [94m0.2632[0m     +  9.5922
     19   [36m0.2143[0m   0.6994        0.2395       0.9335        0.2633        9.7872
     20   [36m0.2181[0m   [32m0.7107[0m        0.2385       0.9335        [94m0.2632[0m     +  9.6280
     21   0.2163   [32m0.7129[0m        0.2370       0.9299        0.2648        9.7522
     22   [36m0.2183[0m   0.7110        [35m0.2344[0m       0.9335        [94m0.2613[0m     +  9.8902
     23   [36m0.2187[0m   0.7100        [35m0.2325[0m       0.9311        0.2635        9.6908
     24   0.2135   0.7074        [35m0.2322[0m       0.9323        [94m0.2603[0m     +  9.8414
     25   [36m0.2193[0m   [32m0.7133[0m        0.2340       0.9323        [94m0.2589[0m     +  9.8263
     26   0.2181   [32m0.7147[0m        0.2342       0.9323        [94m0.2577[0m     +  9.4969
     27   [36m0.2216[0m   [32m0.7170[0m        0.2335       0.9299        0.2594        10.0105
     28   0.2185   0.7157        0.2343       0.9323        [94m0.2561[0m     +  9.8904
     29   0.2182   0.7112        0.2327       0.9323        [94m0.2544[0m     +  9.5096
     30   [36m0.2218[0m   [32m0.7198[0m        [35m0.2310[0m       0.9311        0.2587        10.0715
     31   0.2179   0.7184        [35m0.2282[0m       0.9335        [94m0.2497[0m     +  9.8721
     32   0.2213   [32m0.7226[0m        0.2316       0.9323        0.2507        9.6950
     33   0.2180   0.7200        0.2304       0.9311        [94m0.2496[0m     +  9.9551
     34   0.2126   0.7166        [35m0.2272[0m       0.9335        [94m0.2480[0m     +  9.9279
     35   0.2124   0.7172        0.2342       0.9335        [94m0.2477[0m     +  9.6204
     36   0.2204   [32m0.7313[0m        0.2274       0.9323        0.2477        10.0251
     37   0.2171   0.7281        0.2302       0.9323        [94m0.2468[0m     +  10.0233
     38   0.2150   0.7254        0.2295       0.9323        [94m0.2456[0m     +  9.6857
     39   0.2171   0.7268        0.2274       0.9311        [94m0.2456[0m     +  9.4355
     40   0.2139   0.7258        0.2286       0.9323        0.2482        9.7021
     41   0.2130   0.7258        [35m0.2270[0m       0.9323        0.2465        9.8297
     42   0.2121   0.7253        0.2292       0.9311        0.2469        9.8298
     43   0.2132   0.7300        0.2287       0.9299        0.2479        9.9691
     44   0.2129   0.7298        [35m0.2268[0m       0.9323        0.2461        9.9187
     45   0.2137   [32m0.7343[0m        [35m0.2242[0m       0.9311        [94m0.2451[0m     +  9.7732
     46   0.2155   [32m0.7353[0m        0.2263       0.9299        0.2491        9.5867
     47   0.2129   0.7329        0.2279       0.9311        0.2467        9.4854
     48   0.2118   0.7326        0.2273       0.9299        0.2469        9.6685
     49   0.2097   0.7281        0.2282       0.9311        0.2462        9.9067
     50   0.2107   0.7310        [35m0.2240[0m       0.9311        0.2455        9.7976
[32m[I 2023-05-02 15:57:12,227][0m Trial 164 finished with value: 0.24507393447418743 and parameters: {'lr': 6.090191694780922e-05, 'dropout': 0.5310945783237468, 'd_model_multiplier': 8, 'num_layers': 1, 'n_heads': 32, 'dim_feedforward': 164, 'batch_size': 47, 'pos_encoding': 'learnable', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.01, 'top_n_features': 22}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 69
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.2644[0m   [32m0.7103[0m        [35m0.3986[0m       [31m0.9226[0m        [94m0.5097[0m     +  8.6694
      2   [36m0.3131[0m   [32m0.7347[0m        [35m0.2687[0m       0.9214        [94m0.4469[0m     +  9.0948
      3   0.3002   [32m0.7351[0m        [35m0.2590[0m       0.9226        [94m0.4008[0m     +  9.2163
      4   0.2563   0.7067        [35m0.2553[0m       0.9202        [94m0.3911[0m     +  8.8459
      5   0.2558   0.7076        [35m0.2520[0m       0.9202        [94m0.3704[0m     +  9.1158
      6   0.2442   0.7023        [35m0.2467[0m       0.9214        [94m0.3537[0m     +  9.1360
      7   0.2397   0.7046        [35m0.2441[0m       0.9202        0.3667        9.0256
      8   0.2306   0.6865        [35m0.2402[0m       0.9214        [94m0.3486[0m     +  9.2281
      9   0.2134   0.6800        [35m0.2388[0m       0.9202        [94m0.3314[0m     +  9.0658
     10   0.2228   0.6938        [35m0.2347[0m       0.9214        [94m0.3046[0m     +  9.1011
     11   0.2160   0.6923        0.2357       0.9166        [94m0.3032[0m     +  9.3782
     12   0.1990   0.6529        [35m0.2333[0m       0.9190        [94m0.2974[0m     +  9.1778
     13   0.1969   0.6656        0.2338       0.9178        [94m0.2905[0m     +  8.9846
     14   0.1973   0.6773        [35m0.2321[0m       0.9190        [94m0.2815[0m     +  9.0655
     15   0.2050   0.6881        [35m0.2293[0m       0.9202        0.2829        9.2013
     16   0.2176   0.6940        [35m0.2272[0m       0.9202        [94m0.2716[0m     +  9.1115
     17   0.2178   0.6984        0.2301       0.9190        0.2768        9.0118
     18   0.2263   0.6902        0.2299       0.9190        0.2737        9.3513
     19   0.2243   0.7053        [35m0.2267[0m       0.9214        0.2741        9.1437
     20   0.2152   0.6892        0.2278       0.9178        [94m0.2689[0m     +  9.0982
     21   0.2203   0.7022        [35m0.2224[0m       0.9190        [94m0.2666[0m     +  9.2919
     22   0.2191   0.7002        0.2242       0.9214        0.2670        9.0048
     23   0.2365   0.7150        [35m0.2217[0m       0.9166        0.2685        9.1495
     24   0.2251   0.7107        0.2219       0.9178        [94m0.2630[0m     +  9.1481
     25   0.2223   0.7038        0.2247       0.9166        [94m0.2629[0m     +  9.2932
     26   0.2322   0.7137        0.2223       0.9178        [94m0.2623[0m     +  9.2872
     27   0.2264   0.7043        0.2236       0.9166        0.2623        9.1955
     28   0.2233   0.6979        [35m0.2213[0m       0.9202        [94m0.2591[0m     +  8.9662
     29   0.2148   0.6998        [35m0.2161[0m       0.9190        0.2600        9.0717
     30   0.2149   0.6988        0.2187       0.9190        0.2600        9.3630
     31   0.2218   0.7010        0.2223       0.9190        0.2623        9.2930
     32   0.2209   0.6933        0.2194       0.9202        0.2615        9.3677
     33   0.2156   0.7026        0.2204       0.9141        0.2624        9.0757
     34   0.2182   0.6941        [35m0.2156[0m       0.9178        0.2651        9.2224
     35   0.2270   0.6912        0.2208       0.9178        0.2681        8.8572
     36   0.2122   0.6777        0.2184       0.9166        0.2647        9.2267
     37   0.2321   0.7096        [35m0.2137[0m       0.9178        0.2631        9.3646
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 16:03:01,806][0m Trial 165 finished with value: 0.259074751274185 and parameters: {'lr': 0.0001238369631831607, 'dropout': 0.5554027838963232, 'd_model_multiplier': 64, 'num_layers': 2, 'n_heads': 4, 'dim_feedforward': 144, 'batch_size': 56, 'pos_encoding': 'learnable', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.01, 'top_n_features': 69}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 145
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2899[0m   [32m0.7723[0m        [35m0.2855[0m       [31m0.8912[0m        [94m0.4038[0m     +  19.9776
      2   0.2876   [32m0.7760[0m        [35m0.2441[0m       0.8875        [94m0.4023[0m     +  19.9131
      3   0.2882   [32m0.7828[0m        [35m0.2396[0m       [31m0.8972[0m        [94m0.3831[0m     +  20.5924
      4   [36m0.3061[0m   0.7721        [35m0.2356[0m       0.8742        0.4240        20.6751
      5   0.3019   0.7792        [35m0.2312[0m       [31m0.9057[0m        [94m0.3675[0m     +  20.5590
      6   0.3000   0.7768        0.2326       0.9057        [94m0.3614[0m     +  20.4423
      7   [36m0.3132[0m   0.7733        [35m0.2294[0m       0.8730        0.4078        20.3880
      8   0.3076   [32m0.7833[0m        [35m0.2267[0m       0.9045        [94m0.3357[0m     +  20.6446
      9   0.3119   [32m0.7885[0m        [35m0.2264[0m       [31m0.9081[0m        [94m0.3188[0m     +  20.0557
     10   0.3110   0.7749        0.2273       [31m0.9141[0m        0.3333        20.9599
     11   [36m0.3162[0m   0.7791        [35m0.2212[0m       0.9093        0.3482        20.6698
     12   0.2967   0.7780        0.2233       0.9033        0.3322        20.7541
     13   0.2878   0.7602        [35m0.2201[0m       0.9117        0.3420        20.5464
     14   0.2901   0.7451        [35m0.2179[0m       0.9141        0.3491        20.7639
     15   0.3007   0.7357        [35m0.2152[0m       0.8996        0.3752        20.8966
     16   0.2967   0.7544        0.2174       0.9069        0.3429        20.3108
     17   0.3020   0.7472        0.2186       [31m0.9154[0m        0.3219        20.3419
     18   0.3010   0.7513        0.2166       0.9129        0.3357        20.4488
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 16:09:31,904][0m Trial 166 finished with value: 0.3187749431239939 and parameters: {'lr': 0.00023362610611762862, 'dropout': 0.4884821854555869, 'd_model_multiplier': 8, 'num_layers': 3, 'n_heads': 32, 'dim_feedforward': 138, 'batch_size': 162, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 145}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 13
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.1674[0m   [32m0.6116[0m        [35m0.4326[0m       [31m0.9262[0m        [94m0.2673[0m     +  7.7151
      2   [36m0.2394[0m   [32m0.7583[0m        [35m0.2730[0m       0.9262        [94m0.2323[0m     +  8.5791
      3   [36m0.2555[0m   [32m0.7694[0m        [35m0.2598[0m       [31m0.9274[0m        [94m0.2275[0m     +  8.9218
      4   [36m0.2596[0m   0.7687        [35m0.2548[0m       0.9238        0.2320        8.5647
      5   0.2548   [32m0.7753[0m        0.2569       0.9045        0.2351        8.4917
      6   [36m0.2615[0m   [32m0.7833[0m        [35m0.2542[0m       0.8996        0.2399        8.5661
      7   [36m0.2861[0m   0.7820        [35m0.2526[0m       0.8924        0.2458        8.3195
      8   [36m0.2941[0m   0.7605        [35m0.2505[0m       0.8960        0.2429        8.5202
      9   [36m0.3310[0m   0.7611        [35m0.2491[0m       0.9190        0.2296        8.7825
     10   0.2881   0.7575        [35m0.2423[0m       0.8851        0.2740        8.4951
     11   0.2917   0.7670        0.2425       0.8972        0.2594        8.8775
     12   0.2879   0.7701        0.2460       0.9081        0.2530        8.4555
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 16:11:23,015][0m Trial 167 finished with value: 0.22753312978375492 and parameters: {'lr': 0.0006553400295116223, 'dropout': 0.5387255598943966, 'd_model_multiplier': 1, 'num_layers': 1, 'n_heads': 16, 'dim_feedforward': 155, 'batch_size': 40, 'pos_encoding': 'learnable', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0, 'top_n_features': 13}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 168
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2503[0m   [32m0.7850[0m        [35m0.3015[0m       [31m0.9238[0m        [94m0.2319[0m     +  11.4134
      2   [36m0.2604[0m   0.7841        [35m0.2712[0m       0.8658        0.3527        11.0730
      3   0.2548   [32m0.7899[0m        [35m0.2649[0m       0.8996        0.2799        11.2258
      4   0.2546   0.7737        [35m0.2636[0m       0.9117        0.2486        11.6189
      5   0.2494   0.7831        [35m0.2620[0m       0.8888        0.2723        11.3884
      6   0.2194   0.7756        [35m0.2578[0m       0.9033        0.2681        11.5218
      7   0.2460   0.7835        [35m0.2517[0m       0.9093        0.2426        11.4136
      8   0.2515   0.7864        0.2520       0.9166        0.2360        11.2007
      9   0.2159   0.7682        0.2539       0.9166        0.2565        11.3265
     10   0.2376   0.7780        0.2525       0.9166        0.2487        11.2296
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 16:13:28,260][0m Trial 168 finished with value: 0.23193709949141836 and parameters: {'lr': 0.0013526418920952478, 'dropout': 0.4568097546609654, 'd_model_multiplier': 8, 'num_layers': 4, 'n_heads': 4, 'dim_feedforward': 429, 'batch_size': 17, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.01, 'top_n_features': 168}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 151
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0879[0m   [32m0.5027[0m        [35m0.4010[0m       [31m0.8380[0m        [94m0.4158[0m     +  52.7105
      2   [36m0.1882[0m   [32m0.6698[0m        [35m0.3973[0m       [31m0.8706[0m        [94m0.3845[0m     +  52.8438
      3   0.1359   0.6661        0.4876       [31m0.9226[0m        [94m0.2967[0m     +  52.7548
      4   0.1243   0.6243        0.4849       0.9226        0.3101        52.7027
      5   0.1655   0.6258        0.5795       0.9226        [94m0.2723[0m     +  52.9057
      6   0.1438   0.6259        0.5578       0.9226        0.2990        52.9948
      7   0.1106   0.5455        0.6101       0.9226        0.2831        52.9064
      8   0.1534   0.6662        0.5487       0.8839        0.3139        53.0020
      9   0.1508   0.6646        0.5701       0.9226        0.2763        52.9447
     10   0.1548   0.6652        0.6454       0.9226        0.2876        52.6766
     11   0.1478   0.6310        0.7382       0.9226        0.3951        52.9473
     12   [36m0.1979[0m   [32m0.6788[0m        0.6794       0.7799        0.7171        52.8380
     13   0.1601   0.6296        0.8287       0.9190        0.2779        52.7564
     14   0.1646   0.6157        0.7176       0.9226        0.3731        52.9219
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 16:26:43,459][0m Trial 169 finished with value: 0.2722642102527647 and parameters: {'lr': 0.00048168941012985774, 'dropout': 0.5012165669347332, 'd_model_multiplier': 16, 'num_layers': 8, 'n_heads': 32, 'dim_feedforward': 439, 'batch_size': 9, 'pos_encoding': 'learnable', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0, 'top_n_features': 151}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 177
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0597[0m   [32m0.3555[0m        [35m0.6428[0m       [31m0.9202[0m        [94m0.6307[0m     +  11.8112
      2   0.0554   0.3371        [35m0.6315[0m       [31m0.9274[0m        [94m0.6142[0m     +  11.9564
      3   0.0536   0.3226        [35m0.6171[0m       [31m0.9347[0m        [94m0.5943[0m     +  11.9446
      4   0.0531   0.3102        [35m0.6003[0m       [31m0.9359[0m        [94m0.5727[0m     +  11.6902
      5   0.0524   0.3010        [35m0.5804[0m       0.9359        [94m0.5504[0m     +  10.6703
      6   0.0530   0.2957        [35m0.5612[0m       0.9359        [94m0.5284[0m     +  11.3164
      7   0.0541   0.2926        [35m0.5429[0m       0.9359        [94m0.5071[0m     +  11.4440
      8   0.0544   0.2914        [35m0.5255[0m       0.9359        [94m0.4864[0m     +  11.7479
      9   0.0544   0.2906        [35m0.5062[0m       0.9359        [94m0.4660[0m     +  11.6433
     10   0.0561   0.2918        [35m0.4895[0m       0.9359        [94m0.4455[0m     +  11.8096
     11   0.0559   0.2949        [35m0.4725[0m       0.9359        [94m0.4251[0m     +  11.6708
     12   0.0575   0.2990        [35m0.4548[0m       0.9359        [94m0.4048[0m     +  11.8963
     13   0.0595   0.3050        [35m0.4398[0m       0.9359        [94m0.3846[0m     +  11.7130
     14   0.0549   0.3160        [35m0.4217[0m       0.9359        [94m0.3653[0m     +  11.7706
     15   0.0560   0.3313        [35m0.4045[0m       0.9359        [94m0.3469[0m     +  11.8117
     16   0.0563   0.3509        [35m0.3916[0m       0.9359        [94m0.3309[0m     +  11.6805
     17   0.0559   [32m0.3691[0m        [35m0.3788[0m       0.9359        [94m0.3171[0m     +  11.6266
     18   0.0548   [32m0.3848[0m        [35m0.3650[0m       0.9359        [94m0.3053[0m     +  11.5084
     19   0.0557   [32m0.4041[0m        [35m0.3540[0m       0.9359        [94m0.2952[0m     +  11.7068
     20   0.0572   [32m0.4258[0m        [35m0.3430[0m       0.9359        [94m0.2866[0m     +  11.6179
     21   0.0588   [32m0.4467[0m        [35m0.3347[0m       0.9359        [94m0.2793[0m     +  11.5638
     22   [36m0.0617[0m   [32m0.4676[0m        [35m0.3271[0m       0.9359        [94m0.2731[0m     +  12.0900
     23   [36m0.0674[0m   [32m0.4885[0m        [35m0.3180[0m       0.9359        [94m0.2678[0m     +  11.4975
     24   [36m0.0739[0m   [32m0.5077[0m        [35m0.3133[0m       0.9359        [94m0.2633[0m     +  11.9588
     25   [36m0.0822[0m   [32m0.5232[0m        [35m0.3081[0m       0.9359        [94m0.2594[0m     +  11.7159
     26   [36m0.0881[0m   [32m0.5364[0m        [35m0.3044[0m       0.9359        [94m0.2560[0m     +  11.8205
     27   [36m0.0920[0m   [32m0.5454[0m        [35m0.2992[0m       0.9359        [94m0.2531[0m     +  11.5344
     28   [36m0.0959[0m   [32m0.5544[0m        [35m0.2955[0m       0.9359        [94m0.2505[0m     +  11.9704
     29   [36m0.0989[0m   [32m0.5612[0m        [35m0.2921[0m       0.9359        [94m0.2482[0m     +  11.9114
     30   [36m0.1024[0m   [32m0.5660[0m        [35m0.2885[0m       0.9359        [94m0.2461[0m     +  11.5352
     31   [36m0.1064[0m   [32m0.5724[0m        [35m0.2860[0m       0.9359        [94m0.2442[0m     +  11.7699
     32   [36m0.1102[0m   [32m0.5799[0m        [35m0.2860[0m       0.9359        [94m0.2426[0m     +  11.4579
     33   [36m0.1129[0m   [32m0.5863[0m        [35m0.2810[0m       0.9359        [94m0.2411[0m     +  11.7640
     34   [36m0.1166[0m   [32m0.5920[0m        [35m0.2791[0m       0.9359        [94m0.2397[0m     +  11.8590
     35   [36m0.1179[0m   [32m0.5976[0m        [35m0.2781[0m       0.9359        [94m0.2386[0m     +  11.8498
     36   [36m0.1192[0m   [32m0.6012[0m        [35m0.2763[0m       0.9359        [94m0.2375[0m     +  11.8446
     37   [36m0.1204[0m   [32m0.6059[0m        [35m0.2722[0m       0.9359        [94m0.2365[0m     +  11.8631
     38   [36m0.1223[0m   [32m0.6109[0m        0.2731       0.9359        [94m0.2356[0m     +  11.7952
     39   [36m0.1237[0m   [32m0.6170[0m        [35m0.2705[0m       0.9359        [94m0.2348[0m     +  11.8202
     40   [36m0.1244[0m   [32m0.6197[0m        0.2708       0.9359        [94m0.2341[0m     +  11.6175
     41   [36m0.1260[0m   [32m0.6235[0m        [35m0.2674[0m       0.9359        [94m0.2334[0m     +  12.2527
     42   0.1260   [32m0.6256[0m        [35m0.2671[0m       0.9359        [94m0.2328[0m     +  12.0597
     43   [36m0.1265[0m   [32m0.6281[0m        0.2676       0.9359        [94m0.2322[0m     +  11.8416
     44   0.1264   [32m0.6307[0m        [35m0.2651[0m       0.9359        [94m0.2317[0m     +  11.9344
     45   [36m0.1268[0m   [32m0.6328[0m        [35m0.2633[0m       0.9359        [94m0.2312[0m     +  11.9126
     46   [36m0.1279[0m   [32m0.6354[0m        0.2653       0.9359        [94m0.2308[0m     +  11.9156
     47   [36m0.1285[0m   [32m0.6370[0m        0.2634       0.9359        [94m0.2304[0m     +  11.8166
     48   0.1277   [32m0.6388[0m        [35m0.2622[0m       0.9359        [94m0.2300[0m     +  12.1803
     49   0.1278   [32m0.6414[0m        [35m0.2616[0m       0.9359        [94m0.2297[0m     +  11.7274
     50   [36m0.1290[0m   [32m0.6444[0m        0.2628       0.9359        [94m0.2294[0m     +  11.7958
[32m[I 2023-05-02 16:36:33,091][0m Trial 170 finished with value: 0.2293756976993946 and parameters: {'lr': 2.175870936045356e-05, 'dropout': 0.44262386674815585, 'd_model_multiplier': 1, 'num_layers': 2, 'n_heads': 16, 'dim_feedforward': 128, 'batch_size': 178, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 177}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 186
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.2145[0m   [32m0.7005[0m        [35m0.3091[0m       [31m0.9226[0m        [94m0.4609[0m     +  9.1594
      2   [36m0.2500[0m   0.6910        [35m0.2612[0m       0.9214        [94m0.3957[0m     +  9.4362
      3   0.2468   0.6628        [35m0.2573[0m       0.9226        [94m0.3560[0m     +  9.6787
      4   0.2092   0.6467        0.2577       0.9226        [94m0.3135[0m     +  9.5772
      5   0.2385   0.6757        [35m0.2544[0m       0.9190        0.3255        9.7395
      6   [36m0.2522[0m   [32m0.7144[0m        [35m0.2533[0m       0.9045        0.3363        9.4608
      7   0.2448   [32m0.7160[0m        [35m0.2528[0m       0.9141        [94m0.2821[0m     +  9.6819
      8   0.2435   0.7062        0.2555       0.9117        [94m0.2616[0m     +  9.4732
      9   0.2417   [32m0.7276[0m        [35m0.2502[0m       0.9166        [94m0.2546[0m     +  9.8755
     10   [36m0.2732[0m   [32m0.7511[0m        [35m0.2471[0m       0.9141        0.2647        9.4640
     11   0.2721   0.7480        [35m0.2431[0m       0.9154        [94m0.2513[0m     +  9.6354
     12   0.2692   0.7403        0.2467       0.9202        [94m0.2494[0m     +  9.7822
     13   0.2652   [32m0.7519[0m        0.2433       0.9154        0.2565        9.9415
     14   0.2631   0.7377        [35m0.2407[0m       0.8779        0.2974        9.7252
     15   0.2690   [32m0.7538[0m        0.2437       0.9093        0.2609        9.6328
     16   0.2681   0.7499        [35m0.2366[0m       0.9093        0.2617        9.7649
     17   [36m0.2912[0m   [32m0.7724[0m        0.2387       0.9202        [94m0.2431[0m     +  9.4752
     18   [36m0.2970[0m   0.7546        [35m0.2337[0m       0.9081        0.2584        9.3616
     19   [36m0.3073[0m   0.7702        0.2348       [31m0.9262[0m        [94m0.2363[0m     +  9.6296
     20   [36m0.3132[0m   [32m0.7769[0m        0.2351       0.9226        0.2369        9.8000
     21   0.3126   0.7611        [35m0.2312[0m       0.9178        0.2454        9.7110
     22   [36m0.3233[0m   0.7664        [35m0.2308[0m       0.9202        0.2433        9.6754
     23   0.3170   0.7709        [35m0.2275[0m       0.9154        0.2423        9.6668
     24   0.3123   0.7718        0.2300       0.9214        0.2402        9.6424
     25   0.3038   0.7667        0.2284       0.9190        0.2404        9.6428
     26   0.3060   0.7643        0.2297       0.9166        0.2377        9.6138
     27   0.3091   0.7686        [35m0.2254[0m       0.9081        0.2481        9.6128
     28   0.2982   0.7690        0.2269       0.9166        0.2403        9.5287
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 16:41:13,190][0m Trial 171 finished with value: 0.23633875667220014 and parameters: {'lr': 0.0009592699617144404, 'dropout': 0.5600384026453157, 'd_model_multiplier': 2, 'num_layers': 2, 'n_heads': 16, 'dim_feedforward': 445, 'batch_size': 30, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.01, 'top_n_features': 186}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 38
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.1973[0m   [32m0.7276[0m        [35m0.4261[0m       [31m0.9226[0m        [94m0.4102[0m     +  9.0872
      2   [36m0.2589[0m   [32m0.7887[0m        [35m0.2852[0m       0.9214        [94m0.3173[0m     +  9.4509
      3   [36m0.2816[0m   [32m0.7953[0m        [35m0.2745[0m       0.9202        [94m0.2676[0m     +  9.7964
      4   [36m0.2950[0m   [32m0.7973[0m        [35m0.2653[0m       0.9190        [94m0.2435[0m     +  9.3412
      5   [36m0.3046[0m   [32m0.8010[0m        [35m0.2586[0m       0.9190        [94m0.2348[0m     +  9.2700
      6   [36m0.3255[0m   [32m0.8102[0m        [35m0.2583[0m       0.9226        [94m0.2298[0m     +  9.6852
      7   [36m0.3339[0m   [32m0.8175[0m        [35m0.2543[0m       0.9214        [94m0.2255[0m     +  9.8084
      8   0.3336   0.8109        [35m0.2509[0m       [31m0.9250[0m        [94m0.2241[0m     +  9.2469
      9   [36m0.3533[0m   0.8155        [35m0.2472[0m       0.9250        [94m0.2188[0m     +  9.5271
     10   [36m0.3566[0m   0.8166        [35m0.2435[0m       0.9238        [94m0.2172[0m     +  9.2210
     11   0.3560   0.8130        0.2442       0.9238        [94m0.2169[0m     +  9.1725
     12   [36m0.3577[0m   0.8164        [35m0.2360[0m       0.9250        [94m0.2157[0m     +  9.2552
     13   [36m0.3651[0m   [32m0.8183[0m        0.2410       0.9250        [94m0.2149[0m     +  9.3067
     14   0.3602   0.8157        0.2362       [31m0.9262[0m        [94m0.2148[0m     +  9.5066
     15   [36m0.3713[0m   0.8103        [35m0.2340[0m       [31m0.9287[0m        0.2154        9.5096
     16   0.3554   0.8134        0.2350       0.9250        0.2187        9.5953
     17   0.3652   0.8117        [35m0.2337[0m       0.9287        0.2191        9.4554
     18   0.3619   0.8124        [35m0.2316[0m       0.9141        0.2282        9.5193
     19   [36m0.3769[0m   0.8153        [35m0.2295[0m       0.9226        0.2220        9.4869
     20   0.3652   0.8134        0.2324       0.9154        0.2264        9.9173
     21   0.3615   0.8106        0.2338       0.9178        0.2254        9.7455
     22   0.3598   0.8142        0.2296       0.9202        0.2297        9.5372
     23   0.3452   0.8124        [35m0.2290[0m       0.9154        0.2255        9.6959
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 16:45:01,670][0m Trial 172 finished with value: 0.21479462647524536 and parameters: {'lr': 0.0006559134807030924, 'dropout': 0.5758441739106079, 'd_model_multiplier': 2, 'num_layers': 2, 'n_heads': 16, 'dim_feedforward': 450, 'batch_size': 62, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.01, 'top_n_features': 38}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 33
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.3232[0m   [32m0.7754[0m        [35m0.3749[0m       [31m0.9214[0m        [94m0.4717[0m     +  9.5636
      2   [36m0.3520[0m   0.7702        [35m0.2657[0m       0.9190        [94m0.4056[0m     +  9.4768
      3   [36m0.3609[0m   0.7716        [35m0.2599[0m       [31m0.9287[0m        [94m0.3593[0m     +  10.0007
      4   [36m0.3694[0m   0.7656        [35m0.2530[0m       [31m0.9299[0m        [94m0.3283[0m     +  9.8537
      5   [36m0.3833[0m   [32m0.7769[0m        0.2548       0.9299        [94m0.2955[0m     +  9.7937
      6   0.3519   [32m0.7839[0m        [35m0.2511[0m       0.9274        [94m0.2756[0m     +  9.8931
      7   0.3527   [32m0.7934[0m        [35m0.2482[0m       0.9262        [94m0.2493[0m     +  9.7302
      8   0.3572   0.7910        0.2487       0.9274        [94m0.2426[0m     +  9.8926
      9   0.3584   [32m0.7958[0m        [35m0.2476[0m       0.9287        0.2444        9.7729
     10   [36m0.3957[0m   [32m0.8068[0m        [35m0.2425[0m       0.9299        [94m0.2396[0m     +  10.4000
     11   [36m0.3996[0m   0.8055        [35m0.2411[0m       0.9287        [94m0.2345[0m     +  10.1565
     12   [36m0.4017[0m   [32m0.8094[0m        [35m0.2407[0m       0.9274        0.2360        9.9005
     13   [36m0.4041[0m   0.8068        [35m0.2375[0m       0.9274        [94m0.2297[0m     +  9.7084
     14   [36m0.4124[0m   [32m0.8167[0m        0.2398       [31m0.9311[0m        0.2336        9.8325
     15   [36m0.4242[0m   [32m0.8215[0m        [35m0.2374[0m       0.9299        [94m0.2287[0m     +  9.9788
     16   [36m0.4245[0m   0.8211        0.2385       [31m0.9323[0m        0.2292        10.0946
     17   0.4234   [32m0.8257[0m        0.2393       0.9311        0.2310        9.9290
     18   [36m0.4245[0m   [32m0.8313[0m        0.2395       0.9311        [94m0.2227[0m     +  10.0405
     19   [36m0.4301[0m   [32m0.8351[0m        0.2386       0.9311        0.2268        9.7693
     20   [36m0.4339[0m   0.8337        [35m0.2355[0m       0.9287        [94m0.2181[0m     +  9.9181
     21   0.4321   [32m0.8369[0m        [35m0.2340[0m       0.9299        0.2231        9.9702
     22   0.4163   0.8361        0.2352       0.9299        [94m0.2175[0m     +  9.8800
     23   0.4304   [32m0.8393[0m        0.2351       0.9299        [94m0.2133[0m     +  10.2212
     24   [36m0.4375[0m   [32m0.8403[0m        [35m0.2327[0m       0.9323        0.2147        10.0702
     25   0.4325   [32m0.8427[0m        0.2401       0.9299        0.2148        9.9396
     26   0.4173   0.8347        0.2390       0.9311        0.2133        9.9885
     27   0.4221   0.8422        0.2357       0.9299        [94m0.2109[0m     +  9.9822
     28   0.3917   0.8415        0.2330       0.9299        0.2128        9.8298
     29   0.3893   0.8411        [35m0.2311[0m       0.9299        0.2143        10.1257
     30   0.3736   0.8410        [35m0.2307[0m       0.9287        0.2172        10.1089
     31   0.3900   [32m0.8459[0m        [35m0.2293[0m       0.9299        0.2112        9.9072
     32   0.4061   [32m0.8484[0m        0.2351       0.9202        0.2127        9.8648
     33   0.4032   0.8475        0.2331       0.9299        [94m0.2096[0m     +  10.0724
     34   0.4045   0.8428        [35m0.2283[0m       0.9274        0.2102        10.0501
     35   0.3813   0.8408        0.2288       0.9226        0.2125        9.9115
     36   0.3760   0.8442        [35m0.2237[0m       0.9250        0.2118        9.7504
     37   0.3876   0.8443        0.2268       0.9262        0.2110        12.2179
     38   0.3933   0.8468        [35m0.2233[0m       0.9287        0.2110        10.2692
     39   0.3879   0.8456        0.2270       0.9274        0.2116        10.2126
     40   0.4016   0.8434        0.2257       0.9274        0.2123        10.1634
     41   0.4036   0.8435        [35m0.2232[0m       0.9262        0.2098        10.2678
     42   0.3954   0.8425        [35m0.2231[0m       0.9274        0.2113        10.4041
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 16:52:14,497][0m Trial 173 finished with value: 0.2096020353928731 and parameters: {'lr': 0.00037170562109550863, 'dropout': 0.5765206372983221, 'd_model_multiplier': 8, 'num_layers': 2, 'n_heads': 16, 'dim_feedforward': 457, 'batch_size': 62, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.01, 'top_n_features': 33}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 34
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.4441[0m   [32m0.8177[0m        [35m0.4005[0m       [31m0.9166[0m        [94m0.4403[0m     +  11.7572
      2   [36m0.4491[0m   [32m0.8270[0m        [35m0.2721[0m       0.9141        [94m0.3683[0m     +  11.8413
      3   [36m0.4496[0m   [32m0.8310[0m        [35m0.2643[0m       [31m0.9214[0m        [94m0.3242[0m     +  11.9883
      4   0.4419   [32m0.8334[0m        [35m0.2601[0m       [31m0.9226[0m        [94m0.2927[0m     +  12.3328
      5   0.4476   [32m0.8376[0m        [35m0.2577[0m       [31m0.9250[0m        [94m0.2737[0m     +  12.3750
      6   0.4318   [32m0.8400[0m        [35m0.2546[0m       0.9214        [94m0.2649[0m     +  11.8945
      7   0.4290   0.8290        [35m0.2508[0m       0.9214        [94m0.2526[0m     +  12.0789
      8   [36m0.4512[0m   [32m0.8438[0m        0.2517       0.9250        [94m0.2438[0m     +  12.2013
      9   0.4355   0.8299        [35m0.2465[0m       0.9250        [94m0.2412[0m     +  11.9793
     10   0.4372   0.8268        0.2474       0.9214        0.2416        12.1455
     11   [36m0.4535[0m   0.8387        0.2467       0.9214        0.2436        12.0638
     12   0.4503   0.8355        [35m0.2425[0m       0.9226        0.2435        12.1379
     13   0.4394   0.8376        [35m0.2409[0m       0.9190        [94m0.2403[0m     +  12.0298
     14   [36m0.4568[0m   0.8393        0.2413       0.9214        [94m0.2312[0m     +  12.2039
     15   0.4510   0.8389        [35m0.2386[0m       0.9238        [94m0.2259[0m     +  12.0393
     16   0.4534   0.8376        0.2399       0.9166        0.2292        12.0929
     17   0.4505   0.8369        0.2394       0.9178        0.2298        11.9371
     18   0.4427   0.8355        [35m0.2324[0m       0.9190        0.2293        11.9234
     19   0.4392   0.8332        [35m0.2317[0m       0.9166        0.2327        12.0387
     20   0.4355   0.8337        [35m0.2311[0m       0.9178        0.2325        12.0877
     21   0.4184   0.8176        [35m0.2257[0m       0.9178        0.2384        12.2785
     22   0.4251   0.8207        0.2292       0.9141        0.2350        12.1670
     23   0.4403   0.8364        0.2337       0.9154        0.2280        11.9585
     24   0.4329   0.8337        0.2288       0.9190        0.2331        12.0949
     25   [36m0.4615[0m   [32m0.8444[0m        0.2282       0.9154        [94m0.2242[0m     +  12.0300
     26   0.4461   0.8357        [35m0.2254[0m       0.9190        0.2286        12.0428
     27   0.4416   0.8313        0.2260       0.9178        0.2324        12.3454
     28   0.4509   0.8330        [35m0.2222[0m       0.9105        0.2379        11.8702
     29   0.4326   0.8343        0.2236       0.9129        0.2405        11.9318
     30   0.4377   0.8385        0.2239       0.9141        0.2353        12.1196
     31   0.4445   0.8376        [35m0.2220[0m       0.9129        0.2297        12.0237
     32   0.4358   0.8376        [35m0.2186[0m       0.9154        0.2286        12.6581
     33   0.4352   0.8330        0.2211       0.9166        0.2358        12.2157
     34   0.4368   0.8337        [35m0.2183[0m       0.9105        0.2404        12.0641
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 16:59:19,031][0m Trial 174 finished with value: 0.22416453739175485 and parameters: {'lr': 0.0003104187079351194, 'dropout': 0.5703859972693992, 'd_model_multiplier': 8, 'num_layers': 3, 'n_heads': 16, 'dim_feedforward': 455, 'batch_size': 61, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.01, 'top_n_features': 34}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 37
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.2500[0m   [32m0.7421[0m        [35m0.3593[0m       [31m0.9093[0m        [94m0.3896[0m     +  9.4352
      2   [36m0.2791[0m   [32m0.7553[0m        [35m0.2587[0m       0.9081        [94m0.3554[0m     +  9.9320
      3   [36m0.2949[0m   [32m0.7619[0m        [35m0.2536[0m       [31m0.9129[0m        [94m0.3334[0m     +  10.1546
      4   [36m0.3107[0m   0.7491        [35m0.2483[0m       [31m0.9250[0m        [94m0.3017[0m     +  10.3037
      5   [36m0.3277[0m   [32m0.7664[0m        [35m0.2450[0m       0.9226        [94m0.2970[0m     +  9.9763
      6   [36m0.3377[0m   0.7615        [35m0.2437[0m       0.9238        [94m0.2879[0m     +  9.9791
      7   [36m0.3495[0m   0.7527        [35m0.2395[0m       0.9250        [94m0.2801[0m     +  10.1767
      8   [36m0.3602[0m   [32m0.7902[0m        0.2407       0.9238        [94m0.2794[0m     +  10.0161
      9   [36m0.3689[0m   [32m0.7998[0m        [35m0.2389[0m       [31m0.9262[0m        [94m0.2766[0m     +  10.2779
     10   [36m0.3722[0m   [32m0.8096[0m        0.2407       0.9214        0.2829        10.5435
     11   0.3712   0.8001        0.2396       0.9262        [94m0.2649[0m     +  10.1616
     12   0.3664   0.8025        [35m0.2346[0m       0.9226        0.2727        10.3013
     13   0.3668   0.8095        0.2368       0.9214        0.2724        10.1582
     14   [36m0.3767[0m   0.8032        0.2378       0.9214        0.2929        10.0880
     15   0.3690   0.7941        0.2400       0.9226        0.2867        10.2246
     16   [36m0.3785[0m   0.8001        0.2388       0.9226        0.2855        10.0062
     17   [36m0.3800[0m   0.7902        0.2384       0.9202        0.2971        9.7860
     18   [36m0.3905[0m   0.7960        [35m0.2310[0m       0.9214        0.2859        10.0599
     19   0.3879   0.7912        0.2366       [31m0.9274[0m        0.2684        9.8954
     20   [36m0.3967[0m   0.7848        0.2370       0.9238        0.2814        10.2576
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 17:02:51,744][0m Trial 175 finished with value: 0.2648738203919352 and parameters: {'lr': 0.00046645362002451494, 'dropout': 0.5857874036586703, 'd_model_multiplier': 8, 'num_layers': 2, 'n_heads': 16, 'dim_feedforward': 285, 'batch_size': 68, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.01, 'top_n_features': 37}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 44
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.2307[0m   [32m0.7318[0m        [35m0.3116[0m       [31m0.9190[0m        [94m0.3198[0m     +  8.6840
      2   [36m0.2357[0m   [32m0.7326[0m        [35m0.2539[0m       0.9105        [94m0.3138[0m     +  8.7610
      3   0.2267   0.7279        [35m0.2508[0m       0.9166        [94m0.3026[0m     +  8.5374
      4   0.2214   0.7259        [35m0.2456[0m       0.9154        [94m0.3012[0m     +  9.1101
      5   0.2124   0.7190        0.2509       0.9141        0.3056        8.9385
      6   0.2152   0.7246        [35m0.2424[0m       0.9178        [94m0.2924[0m     +  9.1903
      7   0.2105   0.7196        0.2470       [31m0.9202[0m        [94m0.2797[0m     +  9.5405
      8   0.1944   0.7006        [35m0.2412[0m       0.9141        0.2853        9.7262
      9   0.1950   0.7167        0.2427       0.9105        0.2882        9.1316
     10   0.1900   0.6926        0.2460       0.9166        0.2912        9.5813
     11   0.2004   0.7185        [35m0.2406[0m       0.9081        0.2979        9.4246
     12   0.2015   0.7264        0.2409       0.9057        0.2999        9.3660
     13   0.2068   0.7295        [35m0.2384[0m       0.9141        0.2816        9.1222
     14   0.1893   0.6870        0.2429       0.9166        0.2940        9.2330
     15   0.2092   [32m0.7521[0m        0.2390       0.9105        0.2877        9.1060
     16   0.2097   [32m0.7553[0m        [35m0.2366[0m       0.9141        [94m0.2714[0m     +  9.2500
     17   0.2152   [32m0.7709[0m        0.2388       0.9129        [94m0.2654[0m     +  9.1690
     18   0.2137   [32m0.7772[0m        [35m0.2357[0m       0.9154        [94m0.2576[0m     +  9.1948
     19   0.1998   0.7462        0.2370       0.9141        0.2830        9.3568
     20   0.2163   [32m0.7779[0m        0.2358       0.9141        0.2845        9.2894
     21   0.2206   [32m0.7864[0m        [35m0.2345[0m       0.9154        0.2856        9.0692
     22   0.2213   0.7846        [35m0.2314[0m       0.9154        0.2787        9.1083
     23   0.2153   0.7859        [35m0.2302[0m       0.9069        0.2954        9.1636
     24   0.2287   [32m0.7871[0m        0.2318       0.9129        0.2847        9.2775
     25   0.2202   0.7846        [35m0.2302[0m       0.9069        0.2750        9.2172
     26   0.2197   0.7799        [35m0.2300[0m       0.9117        0.2667        9.3211
     27   0.2200   0.7728        0.2356       0.9129        0.2721        9.4123
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 17:07:09,627][0m Trial 176 finished with value: 0.25757241377014634 and parameters: {'lr': 0.0006640507659161188, 'dropout': 0.5453787382229386, 'd_model_multiplier': 8, 'num_layers': 1, 'n_heads': 16, 'dim_feedforward': 418, 'batch_size': 54, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.01, 'top_n_features': 44}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 27
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2967[0m   [32m0.7235[0m        [35m0.2967[0m       [31m0.8742[0m        [94m0.4403[0m     +  11.6191
      2   [36m0.2976[0m   0.7209        [35m0.2587[0m       [31m0.9081[0m        [94m0.3520[0m     +  11.7408
      3   0.2968   [32m0.7265[0m        [35m0.2565[0m       [31m0.9166[0m        [94m0.2928[0m     +  12.2808
      4   [36m0.3007[0m   [32m0.7283[0m        [35m0.2511[0m       [31m0.9178[0m        [94m0.2780[0m     +  11.9643
      5   [36m0.3037[0m   [32m0.7333[0m        [35m0.2446[0m       0.9178        [94m0.2705[0m     +  11.9898
      6   [36m0.3115[0m   [32m0.7501[0m        [35m0.2435[0m       [31m0.9190[0m        0.2748        11.9805
      7   0.3042   0.7500        [35m0.2427[0m       0.9190        0.2775        11.9366
      8   0.3073   [32m0.7565[0m        [35m0.2402[0m       0.9190        [94m0.2675[0m     +  11.9105
      9   0.3034   0.7558        0.2407       [31m0.9202[0m        [94m0.2626[0m     +  12.1319
     10   0.3026   0.7409        0.2417       0.9190        0.2713        11.9958
     11   [36m0.3169[0m   [32m0.7573[0m        0.2434       0.9190        [94m0.2583[0m     +  12.0902
     12   [36m0.3223[0m   [32m0.7643[0m        [35m0.2335[0m       0.9129        0.2641        12.0621
     13   0.3175   [32m0.7660[0m        0.2376       0.9166        [94m0.2567[0m     +  12.1394
     14   0.3184   [32m0.7710[0m        [35m0.2321[0m       [31m0.9214[0m        [94m0.2549[0m     +  12.0852
     15   [36m0.3225[0m   [32m0.7763[0m        0.2337       0.9190        [94m0.2530[0m     +  12.0291
     16   [36m0.3351[0m   [32m0.7910[0m        0.2363       0.9178        [94m0.2495[0m     +  12.2005
     17   0.3290   0.7848        [35m0.2288[0m       0.9178        [94m0.2489[0m     +  11.9692
     18   [36m0.3377[0m   [32m0.7920[0m        0.2347       0.9178        0.2501        11.9262
     19   [36m0.3475[0m   [32m0.7991[0m        0.2303       0.9214        [94m0.2469[0m     +  11.9432
     20   [36m0.3529[0m   [32m0.8131[0m        0.2313       0.9190        [94m0.2457[0m     +  12.0564
     21   0.3334   0.8078        0.2290       0.9166        [94m0.2456[0m     +  12.1809
     22   0.3494   0.8059        0.2313       0.9190        0.2457        12.1944
     23   0.3483   0.8112        [35m0.2281[0m       0.9154        [94m0.2422[0m     +  12.0785
     24   0.3501   [32m0.8216[0m        0.2305       0.9154        [94m0.2409[0m     +  12.1073
     25   0.3358   [32m0.8223[0m        [35m0.2280[0m       0.9154        0.2415        12.1865
     26   [36m0.3572[0m   [32m0.8296[0m        [35m0.2259[0m       0.9166        [94m0.2371[0m     +  12.0898
     27   0.3359   0.8266        0.2284       0.9166        0.2402        12.0607
     28   0.3444   0.8256        0.2264       0.9154        0.2401        11.9184
     29   0.3495   0.8231        [35m0.2228[0m       0.9178        0.2394        12.0901
     30   0.3406   0.8233        0.2251       0.9141        0.2396        12.2380
     31   0.3453   0.8216        0.2264       0.9154        0.2391        12.1955
     32   0.3482   0.8253        0.2249       0.9166        0.2381        12.1965
     33   [36m0.3585[0m   0.8265        0.2266       0.9178        [94m0.2364[0m     +  12.1616
     34   [36m0.3616[0m   0.8250        0.2242       0.9202        [94m0.2362[0m     +  12.0271
     35   0.3574   0.8256        0.2251       0.9178        0.2368        12.1778
     36   0.3572   0.8294        [35m0.2208[0m       0.9166        0.2372        12.0933
     37   0.3525   [32m0.8315[0m        0.2214       0.9154        [94m0.2357[0m     +  12.1394
     38   0.3524   0.8307        [35m0.2207[0m       0.9166        0.2361        12.0459
     39   0.3496   0.8285        0.2232       0.9129        0.2373        11.7997
     40   0.3519   0.8277        0.2228       0.9178        0.2359        12.0821
     41   0.3446   0.8283        [35m0.2199[0m       0.9178        0.2368        12.0580
     42   0.3289   0.8241        [35m0.2177[0m       0.9166        0.2389        12.0703
     43   0.3257   0.8245        0.2205       0.9129        0.2399        12.1458
     44   0.3208   0.8252        0.2199       0.9166        0.2385        11.8962
     45   0.3252   0.8280        0.2180       0.9154        0.2378        11.9977
     46   0.3407   0.8285        0.2186       0.9154        0.2366        11.9000
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 17:16:37,879][0m Trial 177 finished with value: 0.23565268932764014 and parameters: {'lr': 0.0002447890506191105, 'dropout': 0.5781991412849506, 'd_model_multiplier': 8, 'num_layers': 3, 'n_heads': 16, 'dim_feedforward': 435, 'batch_size': 62, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.01, 'top_n_features': 27}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 17
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.0594[0m   [32m0.3503[0m        [35m0.5116[0m       [31m0.9214[0m        [94m0.3495[0m     +  9.4561
      2   [36m0.0998[0m   [32m0.5631[0m        [35m0.3225[0m       0.9214        [94m0.2804[0m     +  8.8930
      3   [36m0.1456[0m   [32m0.6402[0m        [35m0.2761[0m       0.9214        [94m0.2697[0m     +  9.7334
      4   [36m0.1614[0m   [32m0.6517[0m        [35m0.2606[0m       0.9214        [94m0.2681[0m     +  9.2261
      5   [36m0.1680[0m   [32m0.6650[0m        [35m0.2565[0m       0.9214        0.2693        9.6374
      6   0.1625   [32m0.6699[0m        [35m0.2541[0m       0.9214        0.2683        9.4883
      7   0.1655   [32m0.6762[0m        [35m0.2490[0m       0.9214        0.2685        9.6536
      8   [36m0.1709[0m   [32m0.6831[0m        0.2512       0.9202        [94m0.2667[0m     +  9.6679
      9   [36m0.1806[0m   [32m0.6902[0m        0.2510       0.9202        [94m0.2648[0m     +  9.7716
     10   [36m0.1956[0m   [32m0.7038[0m        [35m0.2464[0m       0.9190        [94m0.2630[0m     +  9.8444
     11   [36m0.2214[0m   [32m0.7246[0m        [35m0.2437[0m       0.9154        [94m0.2580[0m     +  9.4209
     12   [36m0.2330[0m   [32m0.7374[0m        [35m0.2426[0m       0.9093        0.2594        9.4717
     13   [36m0.2366[0m   [32m0.7409[0m        [35m0.2404[0m       0.9057        0.2623        9.6009
     14   0.2361   [32m0.7437[0m        [35m0.2376[0m       0.9081        0.2660        9.4792
     15   [36m0.2396[0m   [32m0.7444[0m        [35m0.2352[0m       0.9069        0.2676        9.7033
     16   0.2359   [32m0.7454[0m        0.2371       0.9057        0.2673        9.8430
     17   0.2330   [32m0.7457[0m        0.2376       0.8984        0.2749        9.8541
     18   0.2339   [32m0.7464[0m        0.2357       0.9033        0.2698        9.5677
     19   0.2333   [32m0.7476[0m        [35m0.2350[0m       0.9033        0.2704        9.5711
     20   0.2364   [32m0.7489[0m        0.2375       0.9045        0.2710        9.6099
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 17:19:59,523][0m Trial 178 finished with value: 0.25795992115172395 and parameters: {'lr': 0.00016029283319120408, 'dropout': 0.606037313795123, 'd_model_multiplier': 2, 'num_layers': 2, 'n_heads': 16, 'dim_feedforward': 451, 'batch_size': 50, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.01, 'top_n_features': 17}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 26
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.3475[0m   [32m0.7914[0m        [35m0.2727[0m       [31m0.8815[0m        [94m0.4000[0m     +  8.4704
      2   [36m0.3664[0m   [32m0.7949[0m        [35m0.2536[0m       [31m0.8936[0m        [94m0.3881[0m     +  8.4539
      3   [36m0.3709[0m   0.7926        [35m0.2493[0m       0.8888        0.3944        8.9556
      4   [36m0.3818[0m   0.7855        [35m0.2443[0m       [31m0.9057[0m        [94m0.3434[0m     +  9.3827
      5   0.3814   0.7637        0.2450       [31m0.9117[0m        0.3602        9.6500
      6   [36m0.3837[0m   0.7913        [35m0.2410[0m       0.9033        0.3566        9.5654
      7   0.3816   0.7837        0.2420       0.9117        [94m0.3153[0m     +  9.5363
      8   0.3787   0.7887        [35m0.2404[0m       0.9081        [94m0.3138[0m     +  9.6359
      9   0.3739   0.7937        [35m0.2360[0m       [31m0.9166[0m        [94m0.2984[0m     +  9.6932
     10   0.3790   0.7682        [35m0.2356[0m       [31m0.9202[0m        [94m0.2891[0m     +  9.2875
     11   0.3699   0.7792        [35m0.2318[0m       0.9178        [94m0.2869[0m     +  9.2746
     12   [36m0.3926[0m   [32m0.8011[0m        0.2373       0.9154        0.2872        9.3367
     13   [36m0.3931[0m   0.8009        0.2363       0.9141        0.2947        9.3869
     14   0.3829   [32m0.8021[0m        [35m0.2306[0m       0.9141        0.2975        9.5021
     15   [36m0.3946[0m   0.7972        0.2349       [31m0.9214[0m        [94m0.2782[0m     +  9.5435
     16   [36m0.4094[0m   0.8013        0.2327       0.9202        [94m0.2728[0m     +  9.6554
     17   [36m0.4122[0m   [32m0.8034[0m        0.2315       0.9214        [94m0.2494[0m     +  9.4768
     18   0.3907   0.7955        0.2335       0.9190        0.2582        9.2935
     19   [36m0.4128[0m   [32m0.8074[0m        [35m0.2285[0m       0.9214        [94m0.2408[0m     +  9.6452
     20   0.3957   0.8058        0.2304       0.9202        [94m0.2343[0m     +  9.5057
     21   0.4088   [32m0.8075[0m        [35m0.2278[0m       0.9202        [94m0.2327[0m     +  9.2614
     22   0.3969   0.8070        0.2287       0.9214        [94m0.2316[0m     +  9.4401
     23   [36m0.4129[0m   [32m0.8133[0m        [35m0.2252[0m       [31m0.9262[0m        [94m0.2271[0m     +  9.5506
     24   [36m0.4193[0m   0.8104        0.2262       0.9262        [94m0.2259[0m     +  9.4261
     25   0.4096   0.8101        0.2261       0.9238        0.2262        9.7853
     26   [36m0.4198[0m   [32m0.8155[0m        0.2259       [31m0.9274[0m        [94m0.2244[0m     +  9.4748
     27   [36m0.4262[0m   [32m0.8204[0m        [35m0.2227[0m       0.9238        [94m0.2237[0m     +  9.2501
     28   0.4236   [32m0.8220[0m        [35m0.2209[0m       0.9238        0.2240        9.2466
     29   0.4245   [32m0.8247[0m        0.2233       0.9262        [94m0.2229[0m     +  9.6018
     30   0.4178   [32m0.8249[0m        0.2233       0.9274        0.2235        9.5975
     31   0.4228   [32m0.8255[0m        0.2227       0.9274        [94m0.2225[0m     +  9.3518
     32   0.4118   0.8227        0.2258       0.9274        0.2244        9.3260
     33   0.4203   0.8213        0.2245       0.9262        0.2253        9.1514
     34   0.4041   0.8221        0.2226       0.9274        0.2248        9.6553
     35   0.4148   0.8220        0.2221       0.9262        0.2262        9.5190
     36   0.4179   0.8228        0.2238       0.9238        0.2251        9.5838
     37   0.4085   0.8209        [35m0.2204[0m       0.9226        0.2280        9.5295
     38   0.4193   0.8231        [35m0.2179[0m       0.9238        0.2255        9.5025
     39   0.4205   0.8240        [35m0.2179[0m       0.9238        0.2259        9.4308
     40   0.4199   0.8250        0.2186       0.9238        0.2241        9.7348
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 17:26:26,966][0m Trial 179 finished with value: 0.2225004667562614 and parameters: {'lr': 0.00045319465802970014, 'dropout': 0.5203817719103137, 'd_model_multiplier': 8, 'num_layers': 1, 'n_heads': 16, 'dim_feedforward': 390, 'batch_size': 74, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.01, 'top_n_features': 26}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 42
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2666[0m   [32m0.7472[0m        [35m0.4897[0m       [31m0.8900[0m        [94m0.4209[0m     +  35.6971
      2   [36m0.3034[0m   [32m0.7912[0m        [35m0.3623[0m       [31m0.8960[0m        [94m0.3547[0m     +  36.2287
      3   [36m0.3063[0m   [32m0.7960[0m        [35m0.3387[0m       0.8936        [94m0.3164[0m     +  36.3870
      4   0.2946   [32m0.7987[0m        [35m0.3264[0m       0.8948        0.3459        36.7406
      5   0.2694   0.7889        [35m0.3169[0m       [31m0.9117[0m        [94m0.2942[0m     +  36.3965
      6   0.2387   0.7484        [35m0.3051[0m       [31m0.9178[0m        [94m0.2753[0m     +  36.1389
      7   0.2696   0.7726        [35m0.3040[0m       0.8960        0.4294        36.5071
      8   0.2659   0.7857        0.3054       0.9154        0.3383        36.4008
      9   0.2733   0.7870        [35m0.2950[0m       0.9166        0.2766        36.3587
     10   0.2584   0.7593        0.2966       0.9154        0.3108        36.3783
     11   0.2523   0.7677        0.2963       0.9141        0.3766        36.2120
     12   0.2268   0.7543        [35m0.2947[0m       0.9166        0.3558        37.3061
     13   0.2176   0.7612        0.2990       0.9178        0.3640        36.4621
     14   0.2202   0.7596        0.2969       0.9178        0.3208        36.2516
     15   0.1992   0.7426        0.2997       0.9178        0.2960        36.3147
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 17:36:10,496][0m Trial 180 finished with value: 0.2753278235246077 and parameters: {'lr': 0.0007458076491164024, 'dropout': 0.5648418784959867, 'd_model_multiplier': 8, 'num_layers': 14, 'n_heads': 16, 'dim_feedforward': 464, 'batch_size': 67, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.01, 'top_n_features': 42}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 25
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.1394[0m   [32m0.5793[0m        [35m0.4526[0m       [31m0.9141[0m        [94m0.3106[0m     +  8.1751
      2   [36m0.2026[0m   [32m0.6820[0m        [35m0.2687[0m       0.9141        [94m0.2777[0m     +  8.7121
      3   [36m0.2287[0m   [32m0.7035[0m        [35m0.2542[0m       0.9141        [94m0.2741[0m     +  8.4306
      4   [36m0.2683[0m   [32m0.7153[0m        [35m0.2486[0m       0.9141        0.2911        9.0317
      5   [36m0.2893[0m   [32m0.7221[0m        [35m0.2429[0m       0.9141        0.2873        9.0640
      6   [36m0.2994[0m   [32m0.7277[0m        0.2438       0.9141        [94m0.2608[0m     +  9.4137
      7   [36m0.3089[0m   [32m0.7290[0m        [35m0.2385[0m       [31m0.9154[0m        0.2696        9.3502
      8   [36m0.3198[0m   [32m0.7303[0m        [35m0.2376[0m       0.9154        0.2895        9.5163
      9   [36m0.3258[0m   [32m0.7428[0m        [35m0.2351[0m       0.9141        0.2974        8.9091
     10   [36m0.3351[0m   [32m0.7439[0m        [35m0.2347[0m       0.9154        [94m0.2566[0m     +  9.0995
     11   [36m0.3415[0m   [32m0.7520[0m        [35m0.2311[0m       [31m0.9190[0m        0.3082        9.7630
     12   [36m0.3429[0m   0.7512        [35m0.2305[0m       0.9178        0.3986        9.2908
     13   [36m0.3432[0m   [32m0.7581[0m        0.2306       0.9190        0.2895        9.2963
     14   [36m0.3440[0m   [32m0.7616[0m        [35m0.2298[0m       0.9178        0.3657        9.4989
     15   0.3415   0.7603        [35m0.2296[0m       [31m0.9202[0m        0.2756        9.2927
     16   0.3436   [32m0.7673[0m        [35m0.2267[0m       0.9178        0.2868        9.4293
     17   0.3404   [32m0.7687[0m        [35m0.2265[0m       [31m0.9262[0m        0.3156        9.2713
     18   0.3397   [32m0.7759[0m        0.2275       0.9190        0.3060        9.3710
     19   [36m0.3448[0m   [32m0.7799[0m        [35m0.2244[0m       0.9214        0.3220        9.3800
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 17:39:14,327][0m Trial 181 finished with value: 0.2565970069119547 and parameters: {'lr': 0.0017366470822023732, 'dropout': 0.3261314151501804, 'd_model_multiplier': 1, 'num_layers': 2, 'n_heads': 4, 'dim_feedforward': 506, 'batch_size': 80, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.01, 'top_n_features': 25}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 161
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.1373[0m   [32m0.6899[0m        [35m0.3134[0m       [31m0.9274[0m        [94m0.3063[0m     +  8.2227
      2   [36m0.1743[0m   [32m0.7087[0m        [35m0.2665[0m       0.9190        [94m0.2882[0m     +  9.1285
      3   [36m0.1871[0m   [32m0.7102[0m        [35m0.2648[0m       [31m0.9287[0m        [94m0.2701[0m     +  8.5701
      4   [36m0.2022[0m   [32m0.7186[0m        [35m0.2550[0m       0.9274        [94m0.2473[0m     +  9.3604
      5   [36m0.2068[0m   [32m0.7186[0m        [35m0.2542[0m       [31m0.9299[0m        [94m0.2418[0m     +  9.4172
      6   [36m0.2082[0m   [32m0.7235[0m        [35m0.2502[0m       0.9129        0.2519        9.2655
      7   0.1979   [32m0.7355[0m        0.2507       0.9057        0.2564        9.5693
      8   0.2049   [32m0.7377[0m        0.2506       0.9141        0.2463        9.4904
      9   0.2058   [32m0.7454[0m        [35m0.2460[0m       0.9105        0.2465        9.3547
     10   0.2001   0.7385        [35m0.2456[0m       0.9105        0.2509        9.3422
     11   0.1889   0.7293        0.2462       0.9214        0.2455        9.4091
     12   [36m0.2158[0m   0.7334        [35m0.2396[0m       0.9069        0.2500        9.4872
     13   [36m0.2176[0m   0.7365        0.2414       0.9262        0.2446        9.2824
     14   [36m0.2303[0m   0.7324        [35m0.2396[0m       0.9262        [94m0.2397[0m     +  9.1488
     15   0.2289   0.7320        [35m0.2381[0m       0.9190        0.2433        9.2391
     16   0.2302   0.7352        [35m0.2371[0m       0.9166        0.2453        9.5186
     17   0.1964   0.7277        [35m0.2360[0m       0.9250        [94m0.2381[0m     +  9.3406
     18   0.2236   0.7368        [35m0.2343[0m       0.9190        0.2419        9.4510
     19   0.1971   0.7306        [35m0.2324[0m       0.9226        0.2439        9.4649
     20   0.1892   0.7302        0.2332       0.9190        0.2421        9.4105
     21   0.2098   0.7426        0.2324       0.9262        0.2421        9.2061
     22   0.2284   0.7417        [35m0.2302[0m       0.9226        0.2386        9.1734
     23   0.1892   0.7373        0.2312       0.9250        0.2468        9.3654
     24   0.2013   0.7274        [35m0.2301[0m       0.9021        0.2691        9.3100
     25   0.1926   0.7399        0.2328       0.9250        0.2493        9.1894
     26   0.1972   0.7372        [35m0.2280[0m       0.9190        0.2545        9.3523
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 17:43:25,427][0m Trial 182 finished with value: 0.23809618492945217 and parameters: {'lr': 0.0036323630336575264, 'dropout': 0.5503068253184226, 'd_model_multiplier': 2, 'num_layers': 2, 'n_heads': 8, 'dim_feedforward': 426, 'batch_size': 44, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0, 'top_n_features': 161}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 213
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2247[0m   [32m0.6805[0m        [35m0.2827[0m       [31m0.9021[0m        [94m0.3146[0m     +  21.9758
      2   0.2204   0.6761        [35m0.2593[0m       0.9021        0.3279        21.6163
      3   0.1735   0.5782        [35m0.2524[0m       0.8912        0.4722        21.7148
      4   0.2032   0.6526        0.2585       0.8452        0.4994        21.4820
      5   0.2172   [32m0.6893[0m        0.2583       0.7062        0.5988        21.8535
      6   [36m0.2805[0m   [32m0.6944[0m        0.2556       0.7219        0.5878        21.7372
      7   0.2412   0.6901        0.2602       0.8452        0.4665        21.7523
      8   [36m0.2818[0m   [32m0.7013[0m        0.2577       0.8912        0.3308        21.9324
      9   [36m0.3023[0m   [32m0.7311[0m        0.2634       0.8960        [94m0.2917[0m     +  21.7342
     10   0.2808   0.7146        [35m0.2483[0m       [31m0.9166[0m        [94m0.2850[0m     +  21.7931
     11   0.2702   [32m0.7557[0m        0.2680       0.8742        0.3322        21.8588
     12   0.2924   0.7357        0.3318       0.9141        [94m0.2664[0m     +  21.8572
     13   0.2689   0.7330        0.3747       0.6288        0.9526        21.9002
     14   0.2508   0.7423        0.3426       0.8622        0.3701        21.7402
     15   0.2111   0.7496        0.3355       0.8597        0.4027        21.9006
     16   0.0732   0.4370        0.3565       0.9141        0.3991        21.9351
     17   0.2379   [32m0.7563[0m        0.3233       0.8694        0.3092        21.7121
     18   0.2531   0.7259        0.3177       0.9129        0.3164        21.6659
     19   0.2323   0.7479        0.3193       0.8815        0.2958        21.7613
     20   0.2443   0.7393        0.3084       0.8005        0.5576        21.9163
     21   0.2254   0.7141        0.3125       0.9129        0.2913        21.9525
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 17:51:26,470][0m Trial 183 finished with value: 0.2663839164459143 and parameters: {'lr': 0.0003601102148264907, 'dropout': 0.3675298339183161, 'd_model_multiplier': 16, 'num_layers': 3, 'n_heads': 32, 'dim_feedforward': 445, 'batch_size': 26, 'pos_encoding': 'learnable', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.01, 'top_n_features': 213}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 138
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.2180[0m   [32m0.7302[0m        [35m0.2906[0m       [31m0.8960[0m        [94m0.3683[0m     +  9.8234
      2   [36m0.2236[0m   0.7156        [35m0.2507[0m       [31m0.9057[0m        [94m0.3420[0m     +  9.6497
      3   0.2150   0.7246        [35m0.2476[0m       0.9021        0.3950        10.0563
      4   0.2061   0.6916        0.2504       0.9033        0.4684        9.8481
      5   [36m0.2342[0m   [32m0.7327[0m        0.2549       0.8948        0.4861        10.3332
      6   0.2288   0.7236        0.2611       0.9008        0.3775        9.7791
      7   0.2023   0.6785        0.2598       [31m0.9069[0m        [94m0.3121[0m     +  9.9823
      8   [36m0.2444[0m   [32m0.7412[0m        0.2554       0.8996        [94m0.2948[0m     +  10.2286
      9   0.2409   0.7365        [35m0.2474[0m       0.9033        [94m0.2754[0m     +  10.0027
     10   [36m0.2600[0m   [32m0.7801[0m        [35m0.2411[0m       0.8863        0.2982        10.1063
     11   0.2503   0.7683        [35m0.2389[0m       0.8948        0.2775        9.8132
     12   0.2517   0.7718        0.2393       0.8984        [94m0.2700[0m     +  10.0432
     13   0.2563   0.7676        0.2389       [31m0.9081[0m        [94m0.2596[0m     +  10.0937
     14   [36m0.2702[0m   0.7643        [35m0.2381[0m       0.9021        0.2691        10.1556
     15   0.2598   0.7702        [35m0.2362[0m       0.8755        0.3403        9.8128
     16   [36m0.2730[0m   0.7683        0.2374       0.8501        0.3460        9.8394
     17   0.2638   0.7719        [35m0.2353[0m       0.9045        0.2683        10.0677
     18   [36m0.2806[0m   0.7653        [35m0.2320[0m       0.9069        0.2630        10.2518
     19   [36m0.2959[0m   0.7650        0.2337       0.8863        0.3042        10.0660
     20   [36m0.2987[0m   0.7742        0.2333       0.9021        0.2798        10.1637
     21   [36m0.3022[0m   0.7699        [35m0.2301[0m       0.9057        0.2774        10.0861
     22   [36m0.3025[0m   0.7723        [35m0.2295[0m       0.8924        0.2949        10.0372
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 17:55:17,351][0m Trial 184 finished with value: 0.25960992904607444 and parameters: {'lr': 0.0012361427648773524, 'dropout': 0.5332641725439985, 'd_model_multiplier': 8, 'num_layers': 2, 'n_heads': 16, 'dim_feedforward': 473, 'batch_size': 35, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 138}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 29
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.3042[0m   [32m0.7228[0m        [35m0.3576[0m       [31m0.9226[0m        [94m0.2707[0m     +  7.9043
      2   [36m0.3547[0m   [32m0.7404[0m        [35m0.2635[0m       [31m0.9250[0m        [94m0.2525[0m     +  8.7109
      3   [36m0.3584[0m   [32m0.7413[0m        [35m0.2550[0m       [31m0.9262[0m        [94m0.2440[0m     +  8.3058
      4   [36m0.3697[0m   [32m0.7476[0m        [35m0.2548[0m       0.9262        0.2639        8.6331
      5   0.3591   0.7384        [35m0.2499[0m       [31m0.9274[0m        0.2465        9.2729
      6   0.3459   0.7364        [35m0.2452[0m       [31m0.9299[0m        [94m0.2432[0m     +  9.1983
      7   0.3603   0.7411        [35m0.2429[0m       0.9262        0.2496        9.0094
      8   0.3563   0.7377        [35m0.2419[0m       0.9287        [94m0.2372[0m     +  9.0151
      9   [36m0.3706[0m   [32m0.7512[0m        [35m0.2383[0m       0.9299        [94m0.2362[0m     +  8.8838
     10   0.3689   [32m0.7572[0m        [35m0.2336[0m       [31m0.9311[0m        [94m0.2314[0m     +  9.0423
     11   [36m0.3751[0m   [32m0.7690[0m        [35m0.2330[0m       0.9274        0.2357        8.8743
     12   [36m0.3752[0m   0.7646        [35m0.2330[0m       0.9250        [94m0.2293[0m     +  9.0974
     13   [36m0.3778[0m   [32m0.7695[0m        [35m0.2319[0m       0.9299        [94m0.2274[0m     +  8.9674
     14   0.3672   [32m0.7713[0m        [35m0.2268[0m       0.9274        [94m0.2270[0m     +  9.2150
     15   0.3646   0.7689        [35m0.2265[0m       0.9311        0.2289        9.0512
     16   [36m0.3788[0m   [32m0.7743[0m        0.2273       0.9238        [94m0.2265[0m     +  9.4052
     17   0.3666   [32m0.7745[0m        [35m0.2257[0m       0.9274        [94m0.2255[0m     +  9.1962
     18   0.3699   [32m0.7815[0m        [35m0.2235[0m       0.9262        [94m0.2253[0m     +  8.9944
     19   0.3366   0.7805        0.2235       0.9250        0.2254        8.8443
     20   0.3583   [32m0.7821[0m        [35m0.2233[0m       0.9250        [94m0.2253[0m     +  8.8058
     21   0.3666   [32m0.7848[0m        [35m0.2209[0m       0.9274        [94m0.2234[0m     +  9.0983
     22   0.3555   0.7758        0.2212       0.9262        0.2247        9.0082
     23   0.3294   0.7813        0.2221       0.9262        0.2266        9.0079
     24   0.3649   0.7815        [35m0.2198[0m       0.9262        0.2245        9.3258
     25   0.3550   0.7840        0.2203       0.9262        0.2237        9.3788
     26   0.3559   0.7813        0.2199       0.9250        0.2241        9.2818
     27   0.3609   [32m0.7856[0m        0.2211       0.9262        0.2239        9.1484
     28   0.3659   0.7854        [35m0.2174[0m       0.9262        0.2239        9.0027
     29   0.3596   [32m0.7858[0m        0.2180       0.9262        [94m0.2231[0m     +  8.9952
     30   0.3716   0.7814        [35m0.2170[0m       0.9311        [94m0.2231[0m     +  8.9584
     31   0.3625   0.7831        0.2210       0.9299        [94m0.2230[0m     +  8.8424
     32   0.3635   0.7834        0.2190       0.9274        0.2245        9.2930
     33   0.3387   0.7831        0.2182       0.9238        0.2258        9.2317
     34   0.3440   0.7795        [35m0.2170[0m       0.9262        0.2253        9.3752
     35   0.3521   0.7829        0.2172       0.9250        0.2244        9.3665
     36   0.3525   [32m0.7874[0m        [35m0.2147[0m       0.9250        [94m0.2229[0m     +  9.2440
     37   0.3484   0.7847        0.2170       0.9238        0.2241        8.7506
     38   0.3514   0.7872        [35m0.2142[0m       0.9262        0.2234        8.9812
     39   0.3502   0.7847        0.2157       0.9262        0.2254        8.9703
     40   0.3456   0.7865        0.2159       0.9238        0.2245        8.8246
     41   0.3436   0.7803        [35m0.2140[0m       0.9238        0.2256        9.0170
     42   0.3412   0.7784        0.2147       0.9299        0.2277        9.1530
     43   0.3746   0.7863        0.2155       0.9250        [94m0.2229[0m     +  9.2689
     44   0.3620   0.7834        0.2180       0.9250        0.2230        9.3583
     45   0.3488   0.7845        0.2149       0.9274        0.2251        9.0770
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 18:02:13,975][0m Trial 185 finished with value: 0.22292784103243066 and parameters: {'lr': 0.0029021281851671654, 'dropout': 0.3847640504987417, 'd_model_multiplier': 1, 'num_layers': 1, 'n_heads': 4, 'dim_feedforward': 138, 'batch_size': 56, 'pos_encoding': 'learnable', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.01, 'top_n_features': 29}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 10
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.2029[0m   [32m0.6373[0m        [35m0.2994[0m       [31m0.9202[0m        [94m0.2811[0m     +  9.4400
      2   [36m0.2040[0m   [32m0.6771[0m        [35m0.2670[0m       0.9202        [94m0.2772[0m     +  9.5376
      3   [36m0.2142[0m   [32m0.7023[0m        [35m0.2611[0m       0.9202        0.2804        9.4323
      4   [36m0.2187[0m   0.6996        [35m0.2575[0m       0.9202        0.2820        9.5961
      5   [36m0.2191[0m   [32m0.7202[0m        0.2587       0.9202        0.2841        9.8284
      6   [36m0.2192[0m   0.7148        0.2580       0.9202        0.2853        10.3199
      7   [36m0.2466[0m   [32m0.7494[0m        [35m0.2567[0m       0.9202        0.2846        9.2953
      8   [36m0.2561[0m   [32m0.7547[0m        [35m0.2548[0m       0.9202        0.2774        9.7089
      9   0.2559   [32m0.7584[0m        [35m0.2543[0m       0.9202        [94m0.2730[0m     +  9.4162
     10   0.2119   0.6840        0.2557       0.9202        0.2945        9.6356
     11   [36m0.2709[0m   [32m0.7611[0m        0.2552       0.9202        [94m0.2637[0m     +  9.6001
     12   [36m0.2767[0m   [32m0.7643[0m        0.2554       0.9202        [94m0.2608[0m     +  9.5808
     13   0.2602   0.7626        0.2546       0.9202        0.2629        9.5749
     14   0.2505   0.7581        0.2547       0.9202        0.2612        9.8126
     15   0.2674   0.7627        [35m0.2535[0m       0.9202        [94m0.2537[0m     +  9.4822
     16   [36m0.2817[0m   0.7630        0.2558       0.9202        [94m0.2487[0m     +  9.7239
     17   0.2612   0.7540        [35m0.2507[0m       0.9202        0.2500        9.6181
     18   0.2468   0.7516        [35m0.2502[0m       0.9202        0.2580        9.8451
     19   0.2551   0.7623        0.2520       0.9202        [94m0.2481[0m     +  9.7220
     20   0.2531   0.7539        0.2521       0.9202        0.2494        9.5727
     21   0.2454   0.7562        0.2532       0.9202        [94m0.2451[0m     +  9.6596
     22   0.2555   0.7554        0.2538       0.9202        [94m0.2433[0m     +  9.4304
     23   0.2366   0.7411        0.2519       0.9202        0.2484        9.5073
     24   0.2609   0.7575        0.2525       0.9202        0.2437        9.5474
     25   0.2737   0.7579        0.2525       0.9202        [94m0.2414[0m     +  9.9500
     26   0.2207   0.7180        0.2514       0.9202        0.2527        9.7003
     27   0.2572   0.7531        0.2507       0.9202        0.2436        9.8126
     28   0.2787   0.7566        0.2525       0.9202        [94m0.2393[0m     +  10.0052
     29   0.2700   0.7564        0.2509       0.9202        0.2403        9.5766
     30   0.2390   0.7494        0.2524       0.9202        0.2455        9.6995
     31   0.2576   0.7581        0.2513       0.9202        0.2433        9.4213
     32   0.2654   0.7581        0.2521       0.9202        0.2433        9.5613
     33   0.2772   0.7560        0.2523       0.9202        0.2447        9.8239
     34   0.2614   0.7562        [35m0.2496[0m       0.9202        0.2467        9.7025
     35   0.2708   0.7574        0.2509       0.9202        0.2485        9.4292
     36   [36m0.2872[0m   0.7630        [35m0.2496[0m       0.9202        0.2440        9.6399
     37   0.2653   0.7529        0.2497       0.9202        0.2465        9.7062
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 18:08:22,460][0m Trial 186 finished with value: 0.23928272250269458 and parameters: {'lr': 0.0008645977745934663, 'dropout': 0.3541616972018524, 'd_model_multiplier': 8, 'num_layers': 2, 'n_heads': 16, 'dim_feedforward': 459, 'batch_size': 20, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0, 'top_n_features': 10}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 47
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.3465[0m   [32m0.8004[0m        [35m0.3467[0m       [31m0.9359[0m        [94m0.3258[0m     +  16.4619
      2   0.3304   [32m0.8015[0m        [35m0.2727[0m       0.9335        [94m0.2748[0m     +  16.9278
      3   0.3454   [32m0.8090[0m        [35m0.2628[0m       0.9335        [94m0.2580[0m     +  16.9417
      4   [36m0.3966[0m   [32m0.8352[0m        [35m0.2551[0m       0.9287        0.2626        16.7243
      5   0.3840   0.8234        [35m0.2467[0m       0.9359        [94m0.2324[0m     +  17.2233
      6   [36m0.4184[0m   [32m0.8376[0m        [35m0.2447[0m       0.9335        [94m0.2317[0m     +  16.9610
      7   [36m0.4326[0m   [32m0.8380[0m        [35m0.2381[0m       [31m0.9371[0m        [94m0.2169[0m     +  17.0678
      8   0.4185   0.8340        [35m0.2341[0m       0.9347        0.2228        16.9971
      9   0.4232   0.8336        0.2348       [31m0.9395[0m        [94m0.2138[0m     +  16.9980
     10   [36m0.4408[0m   0.8345        [35m0.2333[0m       0.9395        0.2259        16.9106
     11   0.4072   0.8294        [35m0.2272[0m       0.9383        [94m0.2118[0m     +  16.8047
     12   0.4211   0.8280        0.2283       0.9395        0.2126        17.0056
     13   0.4225   0.8298        [35m0.2261[0m       0.9371        0.2198        16.9008
     14   0.4082   0.8301        [35m0.2218[0m       0.9335        0.2178        16.8038
     15   0.3963   0.8252        0.2248       0.9299        0.2273        16.8983
     16   0.4052   0.8220        0.2270       0.9323        0.2207        16.8748
     17   0.4193   0.8245        0.2227       0.9335        0.2207        17.1336
     18   [36m0.4421[0m   0.8344        [35m0.2197[0m       0.9395        [94m0.2087[0m     +  17.0265
     19   0.4181   0.8315        [35m0.2183[0m       0.9323        0.2170        17.0329
     20   0.4108   0.8302        0.2204       0.9347        0.2227        16.9141
     21   [36m0.4429[0m   0.8252        0.2202       0.9383        0.2168        16.9995
     22   0.4278   0.8140        [35m0.2179[0m       0.9395        0.2123        16.8703
     23   [36m0.4696[0m   0.8294        [35m0.2106[0m       0.9395        0.2171        16.8620
     24   0.4491   0.8274        0.2158       0.9323        0.2214        16.8877
     25   0.4388   0.8271        0.2134       0.9323        0.2142        17.1067
     26   0.4398   0.8258        0.2177       0.9311        0.2151        17.0584
     27   0.4236   0.8281        0.2126       0.9359        0.2102        16.9236
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 18:16:17,891][0m Trial 187 finished with value: 0.20869648737659477 and parameters: {'lr': 0.0005649340249787225, 'dropout': 0.4678630705316008, 'd_model_multiplier': 4, 'num_layers': 3, 'n_heads': 32, 'dim_feedforward': 433, 'batch_size': 63, 'pos_encoding': 'learnable', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 47}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 49
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2992[0m   [32m0.7832[0m        [35m0.3492[0m       [31m0.9202[0m        [94m0.3477[0m     +  34.1321
      2   [36m0.3196[0m   [32m0.8112[0m        [35m0.2688[0m       [31m0.9250[0m        [94m0.2774[0m     +  33.9986
      3   [36m0.3240[0m   [32m0.8189[0m        [35m0.2611[0m       [31m0.9335[0m        [94m0.2495[0m     +  34.3362
      4   0.3183   0.8165        [35m0.2567[0m       0.9287        0.2554        34.0056
      5   0.3127   0.8123        [35m0.2551[0m       0.9299        [94m0.2267[0m     +  34.2383
      6   0.3076   [32m0.8241[0m        [35m0.2496[0m       0.9335        [94m0.2139[0m     +  33.9735
      7   0.3002   0.8071        [35m0.2469[0m       0.9311        [94m0.2108[0m     +  34.0918
      8   [36m0.3345[0m   [32m0.8281[0m        [35m0.2426[0m       0.9299        0.2161        34.2327
      9   [36m0.3400[0m   [32m0.8286[0m        [35m0.2403[0m       0.9262        0.2220        34.1850
     10   0.3390   0.8063        [35m0.2390[0m       0.9299        0.2184        34.1924
     11   0.3114   0.7976        [35m0.2358[0m       0.9335        0.2165        34.1242
     12   0.3014   0.7985        [35m0.2337[0m       [31m0.9371[0m        0.2112        34.1743
     13   0.3041   0.7982        0.2345       0.9371        [94m0.2096[0m     +  34.3143
     14   0.3097   0.7942        0.2361       0.9323        0.2161        34.3372
     15   0.3156   0.7963        [35m0.2277[0m       0.9299        0.2219        34.1388
     16   0.2953   0.7900        0.2302       0.9359        0.2189        34.0682
     17   0.2961   0.7891        0.2310       0.9371        0.2119        34.1399
     18   0.3204   0.8139        0.2292       0.9214        0.2197        34.2844
     19   0.3116   0.8015        [35m0.2242[0m       0.9347        0.2103        34.3268
     20   0.2811   0.7781        [35m0.2227[0m       0.9335        0.2156        34.2756
     21   0.2995   0.7997        [35m0.2197[0m       0.9214        0.2350        34.2544
     22   0.3063   0.8030        0.2229       0.9238        0.2322        34.2280
     23   0.3133   0.8041        0.2215       [31m0.9383[0m        [94m0.2062[0m     +  34.2018
     24   0.2903   0.7904        0.2216       0.9323        0.2234        34.7667
     25   0.3118   0.8042        [35m0.2175[0m       0.9311        0.2155        34.0465
     26   0.2948   0.8118        0.2182       0.9262        0.2208        34.1525
     27   0.3230   0.8227        0.2209       0.9311        0.2111        34.2055
     28   0.2969   0.8071        [35m0.2148[0m       0.9274        0.2170        34.3269
     29   0.2898   0.8027        0.2158       0.9129        0.2369        34.2850
     30   0.3248   0.8169        [35m0.2134[0m       0.9311        0.2172        34.2372
     31   0.3083   0.8153        [35m0.2127[0m       0.9154        0.2250        34.1782
     32   0.3158   0.8154        0.2138       0.9214        0.2219        34.1368
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 18:35:07,678][0m Trial 188 finished with value: 0.20621031728159847 and parameters: {'lr': 0.0006255674029442698, 'dropout': 0.4742317768395718, 'd_model_multiplier': 2, 'num_layers': 4, 'n_heads': 64, 'dim_feedforward': 414, 'batch_size': 63, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 49}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 50
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2397[0m   [32m0.7455[0m        [35m0.2856[0m       [31m0.9371[0m        [94m0.2400[0m     +  34.1671
      2   0.2386   [32m0.7623[0m        [35m0.2602[0m       0.9335        [94m0.2343[0m     +  34.3608
      3   0.2338   0.7512        [35m0.2560[0m       0.9371        [94m0.2245[0m     +  34.5617
      4   [36m0.2429[0m   0.7530        [35m0.2430[0m       0.9347        0.2272        34.3109
      5   0.2126   0.7540        [35m0.2424[0m       0.9287        0.2482        34.4327
      6   [36m0.2561[0m   [32m0.7714[0m        [35m0.2422[0m       0.9371        0.2248        34.4935
      7   [36m0.2664[0m   0.7560        [35m0.2367[0m       0.9371        0.2358        34.6260
      8   0.2586   0.7417        0.2382       0.9371        0.2401        34.9546
      9   0.2515   0.7621        [35m0.2334[0m       0.9347        0.2399        34.7682
     10   0.2651   0.7655        [35m0.2313[0m       0.9347        0.2444        34.6414
     11   [36m0.2954[0m   [32m0.7800[0m        0.2317       [31m0.9383[0m        0.2349        34.4946
     12   0.2925   0.7697        [35m0.2287[0m       [31m0.9407[0m        0.2436        34.9224
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 18:42:37,415][0m Trial 189 finished with value: 0.22445629099260075 and parameters: {'lr': 0.0005809389825392102, 'dropout': 0.46736973741133203, 'd_model_multiplier': 4, 'num_layers': 4, 'n_heads': 64, 'dim_feedforward': 409, 'batch_size': 65, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 50}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 50
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.3248[0m   [32m0.7719[0m        [35m0.3397[0m       [31m0.9129[0m        [94m0.3622[0m     +  35.4858
      2   [36m0.3383[0m   [32m0.7935[0m        [35m0.2595[0m       [31m0.9154[0m        [94m0.3166[0m     +  35.6950
      3   [36m0.3558[0m   [32m0.8037[0m        [35m0.2542[0m       [31m0.9178[0m        [94m0.2979[0m     +  35.8855
      4   0.3470   0.8017        [35m0.2503[0m       0.9129        [94m0.2795[0m     +  35.5632
      5   0.3385   [32m0.8094[0m        [35m0.2446[0m       0.9166        [94m0.2768[0m     +  35.9828
      6   0.3414   0.8054        [35m0.2408[0m       0.9141        [94m0.2669[0m     +  35.6478
      7   0.3315   0.7986        [35m0.2407[0m       [31m0.9190[0m        [94m0.2511[0m     +  36.0550
      8   0.3235   0.8082        [35m0.2372[0m       0.9117        0.2533        35.7389
      9   0.3135   0.7934        [35m0.2306[0m       0.9093        0.2551        36.1767
     10   0.3318   0.7920        0.2311       0.9154        [94m0.2494[0m     +  35.7480
     11   0.3168   0.7901        [35m0.2306[0m       0.9141        0.2500        35.9570
     12   0.3396   0.8000        [35m0.2290[0m       0.9166        [94m0.2430[0m     +  35.7933
     13   0.3274   0.7957        [35m0.2273[0m       0.9190        [94m0.2426[0m     +  35.7689
     14   0.3343   0.8022        [35m0.2245[0m       0.9166        [94m0.2422[0m     +  35.6172
     15   0.3266   0.7955        [35m0.2232[0m       0.9105        0.2534        35.6884
     16   0.3193   0.7884        [35m0.2194[0m       0.9093        0.2517        35.8835
     17   0.3107   0.7810        0.2207       0.9129        0.2492        35.7874
     18   0.2982   0.7979        0.2194       0.9117        0.2557        35.8242
     19   0.3102   0.7619        [35m0.2164[0m       0.9129        0.2544        36.0421
     20   0.3107   0.7717        [35m0.2126[0m       0.9141        0.2606        35.9450
     21   0.2832   0.7575        0.2139       0.9081        0.2659        35.8132
     22   0.3028   0.7852        0.2168       0.9105        0.2526        35.7544
     23   0.3244   0.7877        [35m0.2115[0m       0.9081        0.2512        35.4991
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 18:56:58,095][0m Trial 190 finished with value: 0.24215590474539253 and parameters: {'lr': 0.0002703106330515357, 'dropout': 0.4780679205945861, 'd_model_multiplier': 4, 'num_layers': 4, 'n_heads': 64, 'dim_feedforward': 414, 'batch_size': 59, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 50}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 44
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2627[0m   [32m0.7423[0m        [35m0.3257[0m       [31m0.9021[0m        [94m0.3524[0m     +  27.9674
      2   0.2530   [32m0.7487[0m        [35m0.2547[0m       [31m0.9129[0m        [94m0.3112[0m     +  28.0602
      3   0.2517   [32m0.7664[0m        [35m0.2481[0m       0.9105        [94m0.2990[0m     +  28.3393
      4   0.2571   [32m0.7705[0m        [35m0.2420[0m       [31m0.9141[0m        [94m0.2778[0m     +  28.3143
      5   0.2526   [32m0.7867[0m        [35m0.2369[0m       0.9081        [94m0.2768[0m     +  28.1630
      6   0.2379   0.7847        [35m0.2351[0m       0.9093        [94m0.2672[0m     +  28.1933
      7   0.2365   [32m0.7871[0m        [35m0.2296[0m       0.9105        0.2752        28.4870
      8   0.2402   0.7821        0.2311       0.9141        [94m0.2633[0m     +  28.2756
      9   0.2191   0.7565        0.2299       0.9117        0.2678        28.2588
     10   0.2598   0.7665        0.2308       0.9141        [94m0.2621[0m     +  28.1477
     11   0.2336   0.7839        [35m0.2292[0m       0.9081        0.2651        28.2175
     12   0.2353   0.7735        [35m0.2201[0m       [31m0.9154[0m        0.2669        28.2082
     13   0.2341   0.7836        0.2212       0.9093        0.2741        28.3887
     14   0.2412   [32m0.7940[0m        [35m0.2201[0m       0.9117        0.2703        28.4477
     15   0.2570   0.7939        0.2251       [31m0.9166[0m        0.2656        28.4324
     16   0.2511   0.7931        [35m0.2182[0m       0.9105        [94m0.2549[0m     +  28.5188
     17   0.2593   0.7908        0.2214       0.9154        0.2654        28.1292
     18   0.2499   0.7874        [35m0.2149[0m       0.9154        0.2684        28.2630
     19   0.2569   0.7853        0.2166       [31m0.9178[0m        0.2729        28.1976
     20   0.2248   0.7810        [35m0.2147[0m       0.9093        0.2856        28.1955
     21   0.2467   0.7862        0.2148       0.9069        0.2786        28.2487
     22   [36m0.2844[0m   0.7910        [35m0.2108[0m       [31m0.9190[0m        0.2648        28.6720
     23   0.2550   0.7899        0.2122       0.9141        0.2777        28.3303
     24   0.2542   0.7875        [35m0.2082[0m       0.9117        0.2703        28.3228
     25   0.2575   0.7918        0.2093       0.9129        0.2640        28.2021
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 19:09:14,849][0m Trial 191 finished with value: 0.2548786903739408 and parameters: {'lr': 0.0003768387756067129, 'dropout': 0.4621680175841575, 'd_model_multiplier': 4, 'num_layers': 3, 'n_heads': 64, 'dim_feedforward': 430, 'batch_size': 61, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 44}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 38
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2748[0m   [32m0.7146[0m        [35m0.3638[0m       [31m0.9105[0m        [94m0.3856[0m     +  26.9714
      2   0.2728   [32m0.7466[0m        [35m0.2599[0m       [31m0.9141[0m        [94m0.3280[0m     +  26.9436
      3   [36m0.2877[0m   [32m0.7693[0m        [35m0.2523[0m       0.9129        [94m0.3065[0m     +  27.3301
      4   [36m0.2926[0m   [32m0.7903[0m        [35m0.2507[0m       0.9117        [94m0.2851[0m     +  27.2196
      5   0.2900   0.7832        [35m0.2465[0m       [31m0.9154[0m        [94m0.2731[0m     +  27.1577
      6   0.2914   0.7867        [35m0.2423[0m       [31m0.9166[0m        [94m0.2704[0m     +  27.1618
      7   [36m0.2989[0m   0.7849        [35m0.2361[0m       0.9166        [94m0.2614[0m     +  27.3054
      8   [36m0.3069[0m   0.7901        [35m0.2353[0m       0.9081        0.2683        27.4036
      9   0.3046   0.7894        [35m0.2311[0m       [31m0.9190[0m        [94m0.2608[0m     +  27.6958
     10   0.2945   [32m0.7989[0m        0.2314       0.9190        [94m0.2581[0m     +  27.4152
     11   0.2799   0.7970        [35m0.2270[0m       0.9141        0.2626        27.1557
     12   0.2764   0.7950        [35m0.2254[0m       0.9141        0.2631        27.0160
     13   0.3051   [32m0.8023[0m        0.2275       [31m0.9202[0m        [94m0.2537[0m     +  27.2992
     14   0.2899   0.7986        [35m0.2213[0m       0.9178        0.2545        27.1849
     15   0.3040   [32m0.8082[0m        0.2233       0.9129        0.2554        27.1422
     16   0.2948   [32m0.8100[0m        0.2235       0.9117        [94m0.2501[0m     +  27.1615
     17   [36m0.3122[0m   0.7996        [35m0.2179[0m       0.9154        [94m0.2471[0m     +  27.6107
     18   0.3116   0.8026        0.2190       0.9190        [94m0.2462[0m     +  27.2677
     19   0.3106   0.8025        [35m0.2162[0m       0.9129        0.2481        27.1031
     20   [36m0.3204[0m   [32m0.8157[0m        0.2175       0.9129        [94m0.2450[0m     +  27.0878
     21   [36m0.3266[0m   0.8152        [35m0.2146[0m       0.9154        [94m0.2434[0m     +  27.2639
     22   0.3147   0.8099        [35m0.2124[0m       0.9141        0.2470        27.1693
     23   0.3228   0.8123        0.2145       0.9117        0.2515        27.2688
     24   0.2995   [32m0.8190[0m        0.2138       0.9105        0.2458        27.2740
     25   [36m0.3387[0m   0.8113        [35m0.2102[0m       0.9141        0.2482        27.3000
     26   [36m0.3506[0m   0.8158        0.2139       0.9178        0.2463        27.3979
     27   0.3381   0.8096        [35m0.2076[0m       0.9154        0.2462        27.1196
     28   0.3382   [32m0.8234[0m        0.2111       0.9154        0.2445        27.2835
     29   0.3095   0.8167        0.2116       0.9129        0.2488        27.0170
     30   [36m0.3587[0m   0.8206        [35m0.2065[0m       0.9117        [94m0.2424[0m     +  27.1098
     31   0.3468   0.8143        0.2096       0.9141        0.2430        27.4789
     32   0.3430   0.8132        0.2091       0.9129        0.2452        27.2913
     33   0.3249   0.8110        0.2086       0.9129        0.2524        27.1580
     34   0.3498   0.8156        [35m0.2042[0m       0.9129        0.2433        27.1816
     35   0.3274   0.8118        0.2065       0.9129        0.2452        27.0587
     36   0.3249   0.8089        [35m0.2031[0m       0.9154        0.2509        27.2322
     37   0.3047   0.8016        0.2064       0.9093        0.2577        27.1590
     38   0.3015   0.7969        0.2032       0.9129        0.2601        27.3238
     39   0.2911   0.7961        [35m0.2010[0m       0.9117        0.2590        27.2177
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 19:27:26,153][0m Trial 192 finished with value: 0.24243191605323458 and parameters: {'lr': 0.0005807392501049608, 'dropout': 0.49273801870109835, 'd_model_multiplier': 2, 'num_layers': 3, 'n_heads': 64, 'dim_feedforward': 421, 'batch_size': 71, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 38}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 33
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.2006[0m   [32m0.6628[0m        [35m0.4239[0m       [31m0.9214[0m        [94m0.3016[0m     +  9.0608
      2   [36m0.3203[0m   [32m0.7439[0m        [35m0.2707[0m       0.9214        [94m0.2502[0m     +  9.8499
      3   [36m0.3387[0m   [32m0.7599[0m        [35m0.2600[0m       [31m0.9238[0m        [94m0.2377[0m     +  10.0668
      4   [36m0.3421[0m   0.7471        [35m0.2523[0m       0.9226        0.2424        9.5693
      5   [36m0.3812[0m   [32m0.7784[0m        0.2529       [31m0.9250[0m        0.2425        9.5900
      6   [36m0.3891[0m   0.7722        [35m0.2488[0m       0.9226        [94m0.2352[0m     +  9.3680
      7   [36m0.3986[0m   [32m0.7799[0m        [35m0.2417[0m       [31m0.9299[0m        [94m0.2251[0m     +  9.5704
      8   [36m0.4055[0m   [32m0.7897[0m        0.2423       0.9250        [94m0.2248[0m     +  9.6072
      9   0.4016   [32m0.7947[0m        [35m0.2414[0m       0.9238        [94m0.2229[0m     +  9.7829
     10   [36m0.4137[0m   0.7946        [35m0.2405[0m       [31m0.9311[0m        [94m0.2221[0m     +  9.7652
     11   [36m0.4321[0m   [32m0.8059[0m        [35m0.2363[0m       0.9311        [94m0.2175[0m     +  9.7877
     12   [36m0.4326[0m   [32m0.8093[0m        0.2386       0.9311        [94m0.2156[0m     +  9.6547
     13   [36m0.4408[0m   [32m0.8111[0m        0.2389       0.9287        0.2168        9.6626
     14   0.4391   [32m0.8222[0m        [35m0.2326[0m       0.9299        [94m0.2136[0m     +  9.4325
     15   0.4387   0.8168        0.2343       [31m0.9335[0m        0.2142        9.5225
     16   0.4267   0.8187        [35m0.2325[0m       0.9299        0.2152        9.5119
     17   0.4289   0.8206        [35m0.2309[0m       0.9299        0.2146        10.0268
     18   0.4235   0.8199        0.2314       0.9311        0.2150        9.2830
     19   [36m0.4430[0m   [32m0.8289[0m        [35m0.2298[0m       0.9311        [94m0.2118[0m     +  9.3640
     20   0.4308   0.8163        0.2304       0.9287        0.2194        9.2467
     21   0.4204   0.8230        0.2309       0.9274        0.2170        9.6042
     22   0.4345   [32m0.8441[0m        [35m0.2273[0m       0.9287        [94m0.2088[0m     +  9.3739
     23   [36m0.4446[0m   0.8365        [35m0.2245[0m       0.9311        0.2116        9.5403
     24   0.4241   0.8237        0.2257       0.9262        0.2132        9.3174
     25   0.4377   [32m0.8470[0m        0.2272       0.9250        [94m0.2064[0m     +  8.8240
     26   [36m0.4457[0m   [32m0.8513[0m        0.2265       0.9323        0.2066        9.2698
     27   [36m0.4480[0m   [32m0.8549[0m        0.2249       0.9287        [94m0.2036[0m     +  9.2908
     28   0.4480   0.8506        0.2248       0.9274        0.2048        9.5962
     29   [36m0.4500[0m   0.8441        0.2259       0.9299        0.2055        9.4713
     30   [36m0.4599[0m   0.8487        [35m0.2232[0m       0.9299        0.2054        9.3284
     31   0.4548   0.8459        0.2244       0.9287        0.2080        9.6343
     32   0.4470   [32m0.8611[0m        0.2246       0.9287        0.2123        9.6927
     33   0.4463   0.8512        [35m0.2223[0m       0.9299        0.2056        9.3573
     34   [36m0.4642[0m   0.8575        0.2230       0.9299        [94m0.2011[0m     +  9.2428
     35   0.4582   0.8553        [35m0.2200[0m       0.9311        0.2024        9.4851
     36   [36m0.4643[0m   0.8570        0.2206       [31m0.9347[0m        0.2083        9.1915
     37   0.4594   0.8572        [35m0.2200[0m       0.9311        0.2076        9.3002
     38   [36m0.4669[0m   0.8589        0.2224       0.9262        0.2039        9.3624
     39   [36m0.4776[0m   [32m0.8612[0m        0.2211       0.9335        0.2042        9.5138
     40   0.4660   0.8608        [35m0.2189[0m       0.9287        [94m0.2010[0m     +  9.3952
     41   0.4556   [32m0.8654[0m        0.2201       0.9323        0.2012        9.2606
     42   0.4620   0.8623        0.2192       0.9323        0.2031        9.4626
     43   0.4326   0.8589        0.2202       0.9262        0.2085        9.5420
     44   0.4584   0.8633        [35m0.2184[0m       0.9311        0.2035        9.1400
     45   0.4683   [32m0.8667[0m        0.2196       0.9299        [94m0.1994[0m     +  9.5819
     46   0.4678   [32m0.8682[0m        0.2188       0.9274        0.1998        9.8974
     47   [36m0.4796[0m   0.8669        0.2186       0.9311        [94m0.1975[0m     +  9.6158
     48   [36m0.4841[0m   [32m0.8692[0m        [35m0.2178[0m       0.9299        [94m0.1965[0m     +  9.4929
     49   [36m0.4963[0m   0.8686        0.2201       0.9311        0.2051        9.4369
     50   0.4645   0.8680        0.2184       0.9311        0.2028        9.2607
[32m[I 2023-05-02 19:35:22,103][0m Trial 193 finished with value: 0.19645712939975052 and parameters: {'lr': 0.0009585082678445079, 'dropout': 0.45314243457968195, 'd_model_multiplier': 2, 'num_layers': 5, 'n_heads': 4, 'dim_feedforward': 329, 'batch_size': 52, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 33}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 54
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.1292[0m   [32m0.6337[0m        [35m0.4938[0m       [31m0.9202[0m        [94m0.3254[0m     +  8.9236
      2   [36m0.2015[0m   [32m0.7452[0m        [35m0.2830[0m       0.9202        [94m0.2546[0m     +  10.0656
      3   0.1934   [32m0.7525[0m        [35m0.2729[0m       0.9190        [94m0.2545[0m     +  9.6192
      4   [36m0.2454[0m   [32m0.7798[0m        [35m0.2578[0m       0.9166        [94m0.2471[0m     +  9.2996
      5   0.2408   [32m0.7828[0m        [35m0.2551[0m       0.9166        [94m0.2433[0m     +  9.4639
      6   0.2338   0.7801        [35m0.2511[0m       0.9178        [94m0.2427[0m     +  9.5198
      7   [36m0.2564[0m   [32m0.7889[0m        [35m0.2496[0m       0.9166        [94m0.2401[0m     +  9.4656
      8   [36m0.2653[0m   0.7856        [35m0.2419[0m       0.9178        [94m0.2392[0m     +  9.6281
      9   [36m0.3005[0m   [32m0.7983[0m        [35m0.2418[0m       0.9178        [94m0.2321[0m     +  9.5780
     10   0.2807   0.7944        [35m0.2384[0m       0.9178        0.2391        9.6820
     11   [36m0.3013[0m   [32m0.7993[0m        [35m0.2374[0m       0.9190        0.2335        9.7528
     12   [36m0.3027[0m   0.7947        [35m0.2345[0m       [31m0.9226[0m        0.2344        9.5441
     13   0.2966   0.7966        0.2354       0.9202        0.2348        9.5031
     14   0.2999   [32m0.8024[0m        [35m0.2317[0m       0.9202        [94m0.2307[0m     +  9.7419
     15   0.3006   [32m0.8037[0m        0.2318       0.9178        0.2312        9.4934
     16   [36m0.3036[0m   [32m0.8086[0m        [35m0.2306[0m       0.9190        0.2309        9.6094
     17   0.2990   0.8048        [35m0.2301[0m       0.9202        [94m0.2307[0m     +  9.8707
     18   0.2927   0.8084        [35m0.2281[0m       0.9178        0.2316        9.8880
     19   [36m0.3267[0m   [32m0.8101[0m        [35m0.2260[0m       0.9202        0.2310        9.8766
     20   [36m0.3339[0m   [32m0.8195[0m        0.2296       [31m0.9238[0m        [94m0.2256[0m     +  9.6718
     21   0.3038   0.8154        0.2276       0.9202        0.2318        9.6782
     22   0.3086   [32m0.8212[0m        0.2273       0.9190        0.2276        9.5225
     23   0.3232   0.8195        0.2269       0.9238        0.2257        9.5857
     24   [36m0.3403[0m   [32m0.8318[0m        [35m0.2241[0m       0.9226        [94m0.2244[0m     +  10.0987
     25   0.3195   0.8225        [35m0.2227[0m       0.9178        0.2266        9.8590
     26   0.3149   0.8272        [35m0.2221[0m       0.9214        0.2256        9.5728
     27   0.3309   [32m0.8361[0m        0.2255       0.9202        [94m0.2225[0m     +  9.7959
     28   0.3344   0.8356        [35m0.2185[0m       0.9190        [94m0.2209[0m     +  9.6024
     29   0.3268   0.8234        0.2204       0.9166        0.2266        9.4405
     30   0.3382   0.8342        0.2200       0.9190        [94m0.2207[0m     +  9.7351
     31   0.3385   0.8321        0.2206       0.9190        0.2213        9.6769
     32   0.3223   0.8309        0.2190       0.9190        0.2222        9.5126
     33   0.3266   0.8277        0.2201       0.9141        0.2243        9.4584
     34   [36m0.3492[0m   [32m0.8363[0m        [35m0.2185[0m       0.9178        0.2212        9.7619
     35   0.3308   [32m0.8388[0m        0.2186       0.9226        [94m0.2202[0m     +  9.6816
     36   [36m0.3609[0m   [32m0.8424[0m        [35m0.2180[0m       0.9190        0.2234        9.8308
     37   0.3554   0.8389        [35m0.2169[0m       0.9178        [94m0.2171[0m     +  9.6675
     38   0.3268   0.8336        0.2178       0.9190        0.2246        9.8724
     39   0.3034   0.8251        0.2188       0.9178        0.2295        9.7365
     40   [36m0.3710[0m   0.8416        0.2186       0.9190        0.2185        9.5999
     41   0.3526   0.8364        0.2173       0.9214        0.2410        9.6268
     42   0.3493   0.8402        0.2181       0.9202        0.2293        10.1304
     43   0.3584   0.8376        [35m0.2159[0m       0.9214        0.2272        9.5114
     44   0.3240   0.8298        [35m0.2154[0m       0.9129        0.2495        9.7623
     45   [36m0.3922[0m   [32m0.8431[0m        0.2166       0.9178        0.2376        9.7521
     46   0.3767   0.8390        [35m0.2144[0m       0.9202        0.2210        9.9260
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 19:42:58,140][0m Trial 194 finished with value: 0.2170962120216011 and parameters: {'lr': 0.000902881619289035, 'dropout': 0.4446932502999547, 'd_model_multiplier': 2, 'num_layers': 6, 'n_heads': 4, 'dim_feedforward': 314, 'batch_size': 49, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 54}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 30
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.2333[0m   [32m0.7463[0m        [35m0.3901[0m       [31m0.9105[0m        [94m0.2683[0m     +  8.5914
      2   [36m0.2798[0m   [32m0.7922[0m        [35m0.2628[0m       0.9105        [94m0.2574[0m     +  8.9040
      3   [36m0.3189[0m   [32m0.8049[0m        [35m0.2581[0m       0.9105        0.2685        9.1513
      4   [36m0.3686[0m   [32m0.8148[0m        [35m0.2515[0m       0.9105        0.2667        8.9800
      5   0.3551   0.8101        [35m0.2417[0m       0.9105        0.2911        9.2342
      6   0.3658   [32m0.8230[0m        [35m0.2384[0m       0.9105        0.2688        8.9054
      7   [36m0.3893[0m   [32m0.8249[0m        [35m0.2352[0m       0.9105        0.2679        9.3334
      8   0.3869   0.8241        0.2372       0.9105        0.2663        9.1711
      9   [36m0.3953[0m   [32m0.8292[0m        0.2360       0.9105        [94m0.2535[0m     +  9.1388
     10   0.3853   0.8267        [35m0.2333[0m       0.9105        0.2572        9.0012
     11   [36m0.3975[0m   0.8264        [35m0.2318[0m       0.9105        0.2685        8.9608
     12   [36m0.4032[0m   [32m0.8339[0m        [35m0.2313[0m       0.9105        0.2561        9.1491
     13   [36m0.4160[0m   0.8333        [35m0.2311[0m       0.9105        [94m0.2506[0m     +  8.9547
     14   [36m0.4198[0m   [32m0.8392[0m        [35m0.2300[0m       0.9105        [94m0.2505[0m     +  9.0355
     15   [36m0.4249[0m   0.8390        [35m0.2279[0m       0.9105        [94m0.2450[0m     +  9.3640
     16   0.4017   [32m0.8396[0m        [35m0.2274[0m       0.9105        0.2471        9.4704
     17   0.4005   0.8385        0.2276       [31m0.9117[0m        0.2492        9.3247
     18   0.4108   0.8369        [35m0.2265[0m       0.9105        0.2462        9.2891
     19   0.4194   [32m0.8399[0m        [35m0.2256[0m       0.9105        0.2511        9.1552
     20   [36m0.4347[0m   [32m0.8453[0m        0.2270       [31m0.9129[0m        [94m0.2349[0m     +  8.8491
     21   0.4208   0.8436        [35m0.2253[0m       0.9129        0.2375        9.2887
     22   [36m0.4402[0m   [32m0.8501[0m        0.2255       0.9129        0.2359        9.4247
     23   0.4338   0.8481        [35m0.2225[0m       [31m0.9141[0m        [94m0.2307[0m     +  9.4039
     24   [36m0.4528[0m   [32m0.8515[0m        0.2246       0.9141        [94m0.2288[0m     +  8.9812
     25   0.4373   [32m0.8524[0m        [35m0.2216[0m       0.9141        0.2320        9.3297
     26   0.4148   0.8458        0.2223       0.9117        0.2394        9.3142
     27   0.4288   0.8485        0.2237       0.9141        0.2328        9.2904
     28   0.4190   0.8486        [35m0.2209[0m       0.9141        0.2321        9.1029
     29   0.4427   0.8512        0.2219       0.9141        [94m0.2283[0m     +  9.3269
     30   0.4505   [32m0.8531[0m        0.2212       [31m0.9166[0m        [94m0.2255[0m     +  8.8482
     31   0.4389   [32m0.8546[0m        [35m0.2201[0m       0.9129        0.2293        9.3479
     32   0.4413   0.8530        0.2208       0.9129        [94m0.2255[0m     +  9.0788
     33   0.4340   [32m0.8551[0m        0.2213       0.9129        0.2316        9.3518
     34   0.4236   0.8518        [35m0.2177[0m       0.9129        0.2303        9.3738
     35   0.4238   0.8524        0.2179       0.9129        0.2263        9.3046
     36   0.4132   [32m0.8552[0m        [35m0.2175[0m       0.9141        0.2322        9.6366
     37   0.4384   0.8527        0.2197       0.9141        0.2294        9.3125
     38   0.4420   [32m0.8564[0m        0.2177       0.9141        0.2261        9.1294
     39   0.4432   [32m0.8577[0m        [35m0.2174[0m       0.9141        0.2272        9.2734
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 19:49:06,536][0m Trial 195 finished with value: 0.22550833133356746 and parameters: {'lr': 0.0011838044060671809, 'dropout': 0.4519361144843358, 'd_model_multiplier': 2, 'num_layers': 4, 'n_heads': 4, 'dim_feedforward': 376, 'batch_size': 53, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 30}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 33
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.0679[0m   [32m0.3309[0m        [35m0.6253[0m       [31m0.9190[0m        [94m0.6313[0m     +  8.7756
      2   [36m0.0799[0m   [32m0.4749[0m        [35m0.3770[0m       [31m0.9226[0m        [94m0.3850[0m     +  9.0438
      3   [36m0.1529[0m   [32m0.6931[0m        [35m0.2999[0m       0.9214        [94m0.3033[0m     +  9.2729
      4   [36m0.2037[0m   [32m0.7543[0m        [35m0.2760[0m       0.9226        [94m0.2665[0m     +  9.2146
      5   [36m0.2158[0m   [32m0.7735[0m        0.2760       0.9226        [94m0.2491[0m     +  9.5160
      6   [36m0.2374[0m   [32m0.7950[0m        [35m0.2640[0m       0.9226        [94m0.2427[0m     +  9.6466
      7   [36m0.2525[0m   [32m0.7989[0m        [35m0.2593[0m       [31m0.9238[0m        [94m0.2370[0m     +  9.5844
      8   [36m0.2631[0m   [32m0.8062[0m        [35m0.2566[0m       0.9238        [94m0.2361[0m     +  9.6422
      9   [36m0.2683[0m   [32m0.8091[0m        [35m0.2543[0m       0.9238        [94m0.2334[0m     +  9.2426
     10   [36m0.2714[0m   [32m0.8131[0m        [35m0.2497[0m       0.9238        [94m0.2314[0m     +  9.0315
     11   [36m0.2755[0m   [32m0.8171[0m        [35m0.2476[0m       0.9214        [94m0.2313[0m     +  8.8865
     12   0.2744   0.8124        [35m0.2457[0m       0.9226        [94m0.2303[0m     +  9.1836
     13   [36m0.2764[0m   [32m0.8193[0m        [35m0.2449[0m       0.9226        [94m0.2294[0m     +  9.4380
     14   0.2764   [32m0.8221[0m        [35m0.2429[0m       0.9226        [94m0.2292[0m     +  9.2819
     15   0.2687   [32m0.8251[0m        [35m0.2413[0m       0.9214        0.2293        9.4793
     16   [36m0.2875[0m   [32m0.8277[0m        [35m0.2390[0m       0.9238        [94m0.2264[0m     +  9.4565
     17   0.2797   0.8254        0.2395       0.9202        0.2276        9.5190
     18   0.2799   0.8264        0.2390       0.9202        [94m0.2262[0m     +  9.4488
     19   0.2791   [32m0.8315[0m        [35m0.2349[0m       0.9202        [94m0.2251[0m     +  9.5818
     20   0.2803   [32m0.8362[0m        [35m0.2333[0m       0.9154        0.2252        9.8344
     21   0.2861   0.8324        0.2349       0.9190        [94m0.2249[0m     +  9.8090
     22   [36m0.2927[0m   0.8326        [35m0.2326[0m       0.9214        [94m0.2244[0m     +  9.2139
     23   [36m0.2949[0m   0.8296        [35m0.2310[0m       0.9190        0.2245        9.1630
     24   [36m0.2965[0m   0.8313        0.2331       0.9214        [94m0.2233[0m     +  9.2443
     25   0.2907   0.8321        0.2320       0.9202        0.2234        9.2593
     26   0.2926   0.8343        [35m0.2300[0m       0.9226        0.2243        9.5015
     27   0.2928   0.8323        [35m0.2272[0m       0.9178        0.2240        9.5171
     28   [36m0.2984[0m   0.8347        0.2317       0.9190        0.2239        9.2043
     29   [36m0.2986[0m   0.8356        0.2292       0.9166        0.2235        9.5446
     30   [36m0.3035[0m   [32m0.8371[0m        0.2299       0.9214        [94m0.2230[0m     +  10.0209
     31   [36m0.3074[0m   [32m0.8375[0m        0.2300       0.9190        [94m0.2225[0m     +  9.5831
     32   [36m0.3099[0m   [32m0.8388[0m        [35m0.2253[0m       0.9214        [94m0.2220[0m     +  9.4431
     33   0.3050   [32m0.8402[0m        0.2269       0.9178        0.2231        9.0902
     34   0.3028   [32m0.8407[0m        0.2256       0.9202        0.2236        9.3267
     35   0.2947   0.8394        0.2269       0.9190        0.2262        9.7273
     36   0.3003   [32m0.8419[0m        [35m0.2251[0m       0.9202        0.2262        9.5760
     37   0.3022   0.8410        0.2263       0.9166        0.2250        9.5916
     38   0.3070   0.8417        0.2264       0.9178        0.2237        9.4792
     39   0.3037   [32m0.8424[0m        0.2256       0.9202        0.2234        9.5710
     40   0.3093   [32m0.8466[0m        0.2266       0.9178        0.2231        9.4361
     41   0.3058   0.8440        [35m0.2235[0m       0.9178        0.2231        9.4303
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 19:55:43,861][0m Trial 196 finished with value: 0.2219839455354891 and parameters: {'lr': 0.00047603452054154933, 'dropout': 0.4773640085696296, 'd_model_multiplier': 2, 'num_layers': 5, 'n_heads': 4, 'dim_feedforward': 133, 'batch_size': 64, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 33}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 42
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.2368[0m   [32m0.7543[0m        [35m0.3091[0m       [31m0.9299[0m        [94m0.2253[0m     +  9.3633
      2   [36m0.2389[0m   [32m0.7621[0m        [35m0.2600[0m       0.9287        [94m0.2221[0m     +  8.9344
      3   [36m0.2954[0m   [32m0.7794[0m        [35m0.2525[0m       0.9274        [94m0.2177[0m     +  9.1730
      4   0.2710   [32m0.7844[0m        [35m0.2400[0m       0.9226        0.2204        9.2581
      5   0.2880   [32m0.7858[0m        [35m0.2399[0m       0.9250        0.2197        9.3191
      6   0.2827   [32m0.7885[0m        [35m0.2369[0m       0.9214        0.2278        9.3965
      7   0.2775   [32m0.7975[0m        [35m0.2349[0m       0.9202        0.2260        9.6511
      8   0.2825   0.7959        0.2362       0.9214        0.2275        9.2167
      9   [36m0.2975[0m   0.7903        [35m0.2338[0m       0.9250        0.2203        9.5739
     10   [36m0.3010[0m   [32m0.8046[0m        0.2352       0.9250        [94m0.2164[0m     +  9.3655
     11   [36m0.3047[0m   0.7927        0.2349       0.9238        0.2182        9.4774
     12   [36m0.3319[0m   0.7967        0.2360       0.9250        0.2164        9.6266
     13   0.2935   0.8040        0.2346       [31m0.9335[0m        [94m0.2143[0m     +  9.3623
     14   0.3012   0.7949        [35m0.2331[0m       0.9274        0.2221        9.6403
     15   0.3149   0.7982        [35m0.2293[0m       0.9226        0.2216        9.6352
     16   0.3096   0.7879        0.2322       0.9335        0.2191        9.8502
     17   [36m0.3372[0m   0.7925        0.2315       0.9299        0.2159        9.5860
     18   0.3203   0.7940        0.2322       0.9262        0.2175        9.3028
     19   0.2998   0.7847        0.2302       0.9250        0.2225        9.6020
     20   0.3055   0.7958        [35m0.2292[0m       0.9274        0.2193        9.6622
     21   0.3229   0.7900        [35m0.2287[0m       0.9214        0.2269        9.9251
     22   0.3011   0.7946        0.2291       0.9226        0.2227        9.7683
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 19:59:22,929][0m Trial 197 finished with value: 0.21428996263276853 and parameters: {'lr': 0.0006967179773445828, 'dropout': 0.43392033997106644, 'd_model_multiplier': 2, 'num_layers': 5, 'n_heads': 4, 'dim_feedforward': 329, 'batch_size': 56, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 42}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 39
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.2837[0m   [32m0.7783[0m        [35m0.3400[0m       [31m0.9178[0m        [94m0.2490[0m     +  9.0558
      2   [36m0.3155[0m   [32m0.7945[0m        [35m0.2745[0m       [31m0.9190[0m        [94m0.2380[0m     +  9.2870
      3   0.3146   0.7932        [35m0.2621[0m       0.9178        0.2402        9.9070
      4   [36m0.3357[0m   [32m0.8037[0m        [35m0.2567[0m       0.9166        [94m0.2363[0m     +  9.4895
      5   0.3269   [32m0.8059[0m        [35m0.2495[0m       0.9166        [94m0.2351[0m     +  9.9561
      6   0.3329   [32m0.8075[0m        [35m0.2447[0m       0.9190        [94m0.2340[0m     +  9.8898
      7   [36m0.3371[0m   [32m0.8126[0m        [35m0.2370[0m       0.9117        0.2376        9.7668
      8   0.3343   [32m0.8157[0m        0.2392       0.9129        0.2376        9.8922
      9   [36m0.3449[0m   0.8154        [35m0.2354[0m       0.9154        [94m0.2340[0m     +  9.8989
     10   0.3368   0.8148        [35m0.2346[0m       0.9141        [94m0.2329[0m     +  9.7739
     11   0.3335   [32m0.8158[0m        0.2356       0.9117        0.2360        9.7008
     12   0.3280   0.8126        [35m0.2333[0m       0.8948        0.2570        9.6788
     13   [36m0.3459[0m   0.8122        0.2357       0.9105        0.2422        9.8022
     14   0.3404   [32m0.8202[0m        [35m0.2303[0m       0.9117        0.2340        9.9501
     15   0.3447   0.8181        0.2330       0.9117        [94m0.2327[0m     +  9.6085
     16   0.3373   0.8153        0.2325       0.9069        0.2370        9.7177
     17   0.3456   0.8171        0.2305       0.9057        0.2378        9.7931
     18   0.3373   0.8192        [35m0.2292[0m       0.9093        0.2363        9.5534
     19   0.3412   0.8177        0.2317       0.9178        [94m0.2302[0m     +  9.9027
     20   [36m0.3475[0m   [32m0.8221[0m        0.2302       0.9093        0.2379        9.7546
     21   [36m0.3492[0m   0.8202        0.2303       0.9057        0.2413        9.9531
     22   0.3416   0.8215        0.2304       0.9081        0.2421        9.7844
     23   0.3316   0.8156        [35m0.2291[0m       0.9129        0.2396        10.2084
     24   0.3291   0.8153        [35m0.2264[0m       0.9008        0.2450        9.7471
     25   0.3391   0.8204        0.2276       0.8972        0.2483        9.7108
     26   0.3362   0.8178        0.2284       0.8996        0.2444        9.7031
     27   0.3375   [32m0.8222[0m        0.2269       0.9190        0.2315        9.8534
     28   0.3389   0.8173        0.2266       0.8996        0.2414        9.7532
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 20:04:06,444][0m Trial 198 finished with value: 0.2301663290236276 and parameters: {'lr': 0.0007372425220586961, 'dropout': 0.5141592886256751, 'd_model_multiplier': 2, 'num_layers': 6, 'n_heads': 4, 'dim_feedforward': 337, 'batch_size': 58, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 39}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 47
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
Warning, assumed runtime error: CUDA out of memory. Tried to allocate 1.59 GiB (GPU 0; 23.70 GiB total capacity; 18.48 GiB already allocated; 405.25 MiB free; 22.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[32m[I 2023-05-02 20:04:10,108][0m Trial 199 finished with value: 100.0 and parameters: {'lr': 0.0010930152419118034, 'dropout': 0.4190552988771547, 'd_model_multiplier': 2, 'num_layers': 5, 'n_heads': 64, 'dim_feedforward': 308, 'batch_size': 67, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 47}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 64
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.0651[0m   [32m0.3098[0m        [35m0.6575[0m       [31m0.9262[0m        [94m0.6065[0m     +  8.8583
      2   [36m0.1614[0m   [32m0.6333[0m        [35m0.3858[0m       0.9250        [94m0.3486[0m     +  9.7272
      3   [36m0.2183[0m   [32m0.6977[0m        [35m0.2780[0m       0.9250        [94m0.2685[0m     +  10.3778
      4   [36m0.2340[0m   [32m0.7251[0m        [35m0.2644[0m       0.9250        [94m0.2556[0m     +  9.2068
      5   [36m0.2379[0m   0.7235        [35m0.2592[0m       0.9250        [94m0.2457[0m     +  9.7009
      6   0.2315   [32m0.7468[0m        [35m0.2540[0m       0.9262        [94m0.2444[0m     +  10.0996
      7   0.2372   [32m0.7487[0m        [35m0.2516[0m       0.9250        [94m0.2365[0m     +  9.8744
      8   [36m0.2486[0m   [32m0.7526[0m        [35m0.2488[0m       [31m0.9274[0m        [94m0.2334[0m     +  9.7466
      9   [36m0.2516[0m   [32m0.7528[0m        [35m0.2408[0m       0.9250        [94m0.2327[0m     +  10.0731
     10   0.2442   [32m0.7610[0m        [35m0.2397[0m       0.9250        [94m0.2319[0m     +  9.8695
     11   0.2429   [32m0.7678[0m        [35m0.2386[0m       0.9238        [94m0.2302[0m     +  9.9785
     12   [36m0.2861[0m   0.7646        [35m0.2379[0m       [31m0.9287[0m        [94m0.2293[0m     +  9.9852
     13   0.2670   0.7648        [35m0.2359[0m       0.9262        0.2314        9.8740
     14   [36m0.2941[0m   0.7660        [35m0.2342[0m       0.9287        [94m0.2288[0m     +  9.8569
     15   0.2921   0.7670        [35m0.2328[0m       [31m0.9299[0m        [94m0.2281[0m     +  10.1263
     16   [36m0.3022[0m   0.7665        [35m0.2304[0m       0.9274        [94m0.2273[0m     +  9.8471
     17   0.2960   0.7675        0.2313       0.9262        [94m0.2266[0m     +  9.4539
     18   0.2679   0.7656        [35m0.2303[0m       0.9287        0.2272        9.7117
     19   0.2654   0.7650        [35m0.2286[0m       0.9287        0.2276        10.0715
     20   0.2746   [32m0.7713[0m        0.2316       0.9262        0.2270        9.9635
     21   0.2648   0.7685        0.2311       0.9250        0.2308        9.8232
     22   0.2819   0.7682        [35m0.2282[0m       0.9274        0.2291        9.9286
     23   0.2744   0.7657        0.2287       0.9274        0.2296        9.7910
     24   0.2768   [32m0.7728[0m        [35m0.2275[0m       0.9299        0.2277        10.1070
     25   0.2766   0.7652        [35m0.2272[0m       0.9262        0.2280        9.8306
     26   0.2776   0.7706        [35m0.2271[0m       0.9287        0.2272        10.0163
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 20:08:36,789][0m Trial 200 finished with value: 0.22663982652190806 and parameters: {'lr': 0.0006371484293758012, 'dropout': 0.431961572086766, 'd_model_multiplier': 2, 'num_layers': 5, 'n_heads': 4, 'dim_feedforward': 337, 'batch_size': 77, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 64}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 42
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.2144[0m   [32m0.6922[0m        [35m0.3572[0m       [31m0.9226[0m        [94m0.2549[0m     +  8.5654
      2   [36m0.3592[0m   [32m0.7674[0m        [35m0.2601[0m       [31m0.9262[0m        [94m0.2300[0m     +  8.5883
      3   [36m0.3712[0m   [32m0.7748[0m        [35m0.2461[0m       [31m0.9299[0m        [94m0.2243[0m     +  9.7000
      4   [36m0.3791[0m   [32m0.7754[0m        [35m0.2446[0m       0.9299        0.2248        8.8477
      5   0.3635   [32m0.7806[0m        [35m0.2402[0m       0.9299        0.2247        9.4148
      6   0.3711   [32m0.7869[0m        [35m0.2391[0m       0.9262        0.2251        9.4742
      7   0.3705   [32m0.7874[0m        [35m0.2372[0m       0.9274        0.2251        9.6466
      8   0.3764   [32m0.7931[0m        0.2377       0.9250        0.2260        9.2752
      9   0.3661   0.7858        [35m0.2342[0m       0.9299        0.2278        9.6471
     10   [36m0.3794[0m   0.7911        [35m0.2317[0m       0.9274        0.2269        9.6191
     11   0.3514   0.7928        0.2341       0.9262        0.2292        9.2133
     12   0.3691   [32m0.7968[0m        [35m0.2313[0m       0.9262        [94m0.2240[0m     +  9.4738
     13   0.3561   0.7916        0.2334       0.9238        0.2272        9.1922
     14   0.3511   0.7947        [35m0.2289[0m       0.9238        0.2271        9.4998
     15   0.3500   0.7948        0.2318       0.9238        0.2270        9.5389
     16   0.3395   0.7917        0.2317       0.9250        0.2276        9.5636
     17   0.3401   0.7829        0.2310       0.9202        0.2335        9.2338
     18   0.3521   [32m0.8004[0m        [35m0.2287[0m       0.9226        0.2248        9.4972
     19   0.3522   0.7966        0.2290       0.9214        0.2270        9.4773
     20   0.3601   0.7993        0.2287       0.9287        0.2250        9.9137
     21   0.3578   [32m0.8020[0m        [35m0.2276[0m       0.9226        0.2247        9.3582
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 20:12:03,455][0m Trial 201 finished with value: 0.224015316262643 and parameters: {'lr': 0.00038454315114372784, 'dropout': 0.46485321808352753, 'd_model_multiplier': 2, 'num_layers': 5, 'n_heads': 4, 'dim_feedforward': 324, 'batch_size': 56, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 42}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 21
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1337[0m   [32m0.5730[0m        [35m0.3799[0m       [31m0.9299[0m        [94m0.2689[0m     +  11.2488
      2   [36m0.2056[0m   [32m0.6462[0m        [35m0.2776[0m       0.9299        [94m0.2432[0m     +  11.6765
      3   0.1856   [32m0.6679[0m        [35m0.2649[0m       0.9299        [94m0.2380[0m     +  11.3906
      4   0.1898   [32m0.6763[0m        [35m0.2640[0m       0.9299        [94m0.2348[0m     +  11.7190
      5   0.1922   [32m0.6780[0m        [35m0.2593[0m       0.9299        [94m0.2334[0m     +  11.5632
      6   0.1907   [32m0.6883[0m        [35m0.2549[0m       0.9299        [94m0.2329[0m     +  11.6775
      7   0.1908   [32m0.6969[0m        0.2579       0.9299        [94m0.2323[0m     +  11.6098
      8   0.1924   [32m0.7009[0m        0.2569       0.9299        [94m0.2320[0m     +  11.6790
      9   0.1956   [32m0.7026[0m        0.2568       0.9299        [94m0.2312[0m     +  11.7260
     10   0.1892   [32m0.7029[0m        0.2585       0.9287        0.2315        11.7075
     11   0.1920   [32m0.7068[0m        [35m0.2533[0m       0.9287        0.2312        11.6037
     12   0.1945   [32m0.7077[0m        0.2537       0.9299        [94m0.2301[0m     +  11.8772
     13   0.1930   [32m0.7087[0m        0.2549       0.9299        0.2309        11.5593
     14   0.1892   0.7087        0.2554       0.9299        0.2316        11.7014
     15   0.1881   0.7085        [35m0.2526[0m       0.9299        0.2309        11.8763
     16   0.1897   0.7068        0.2559       0.9299        0.2320        11.5044
     17   0.1868   0.7067        0.2545       0.9299        0.2322        11.6221
     18   0.1900   0.7074        0.2538       0.9299        0.2321        11.7829
     19   0.1883   0.7085        0.2537       0.9287        0.2337        11.5728
     20   0.1891   0.7078        0.2542       0.9287        0.2319        11.7594
     21   0.1882   0.7069        0.2564       0.9287        0.2326        11.5966
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 20:16:20,407][0m Trial 202 finished with value: 0.23012849236269817 and parameters: {'lr': 0.00022815407381422228, 'dropout': 0.45273244248176003, 'd_model_multiplier': 2, 'num_layers': 10, 'n_heads': 4, 'dim_feedforward': 403, 'batch_size': 52, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 21}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 46
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.3569[0m   [32m0.7832[0m        [35m0.2862[0m       [31m0.9226[0m        [94m0.2293[0m     +  9.0241
      2   [36m0.3624[0m   [32m0.7986[0m        [35m0.2444[0m       0.9226        [94m0.2186[0m     +  9.0366
      3   [36m0.3851[0m   [32m0.8233[0m        [35m0.2406[0m       [31m0.9311[0m        [94m0.2076[0m     +  8.9389
      4   [36m0.3984[0m   [32m0.8253[0m        0.2407       [31m0.9323[0m        0.2077        9.2647
      5   [36m0.4133[0m   [32m0.8304[0m        [35m0.2374[0m       [31m0.9359[0m        [94m0.2060[0m     +  9.2783
      6   0.3931   [32m0.8351[0m        [35m0.2350[0m       0.9335        0.2100        9.3422
      7   0.3981   [32m0.8431[0m        [35m0.2333[0m       0.9347        [94m0.2058[0m     +  9.2139
      8   0.3851   0.8427        [35m0.2325[0m       0.9347        0.2068        9.2288
      9   0.4017   [32m0.8468[0m        [35m0.2290[0m       0.9299        [94m0.2051[0m     +  9.6184
     10   0.3824   [32m0.8540[0m        [35m0.2268[0m       0.9311        0.2056        9.6085
     11   0.3838   0.8530        0.2288       0.9262        0.2062        9.4764
     12   0.3815   0.8539        [35m0.2248[0m       0.9262        0.2142        9.8162
     13   0.3870   [32m0.8545[0m        [35m0.2244[0m       0.9287        [94m0.2047[0m     +  9.6651
     14   0.3898   [32m0.8586[0m        [35m0.2223[0m       0.9287        [94m0.2037[0m     +  9.3087
     15   0.3912   0.8562        0.2232       0.9202        0.2139        9.6086
     16   0.3764   0.8521        [35m0.2220[0m       0.9250        0.2126        9.3351
     17   0.3928   [32m0.8640[0m        [35m0.2213[0m       0.9226        0.2052        9.3711
     18   0.3776   0.8599        [35m0.2199[0m       0.9226        0.2086        9.5241
     19   0.4090   [32m0.8642[0m        [35m0.2181[0m       0.9274        0.2041        9.5063
     20   0.3828   0.8551        [35m0.2160[0m       0.9299        0.2095        9.5973
     21   0.3819   0.8554        0.2173       0.9250        0.2092        9.7506
     22   0.3681   0.8540        [35m0.2152[0m       0.9250        0.2148        9.3260
     23   0.3446   0.8419        0.2161       0.9238        0.2180        9.5759
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 20:20:06,977][0m Trial 203 finished with value: 0.20372162883630543 and parameters: {'lr': 0.0008385144646361651, 'dropout': 0.44064492296846197, 'd_model_multiplier': 4, 'num_layers': 4, 'n_heads': 4, 'dim_feedforward': 215, 'batch_size': 61, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 46}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 49
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.2220[0m   [32m0.6913[0m        [35m0.2697[0m       [31m0.9129[0m        [94m0.2706[0m     +  8.4835
      2   [36m0.2275[0m   [32m0.6979[0m        [35m0.2540[0m       0.8996        0.2718        9.0925
      3   [36m0.2903[0m   [32m0.7568[0m        [35m0.2493[0m       [31m0.9154[0m        [94m0.2572[0m     +  8.9172
      4   [36m0.3132[0m   [32m0.7675[0m        [35m0.2407[0m       0.9129        0.2603        9.1271
      5   [36m0.3155[0m   0.7627        [35m0.2341[0m       0.9069        0.2661        8.9430
      6   [36m0.3318[0m   [32m0.7719[0m        0.2344       0.9129        0.2585        9.1216
      7   0.3163   [32m0.7838[0m        [35m0.2309[0m       0.9057        0.2593        9.2691
      8   [36m0.3320[0m   0.7799        [35m0.2295[0m       0.9021        0.2643        9.0237
      9   [36m0.3426[0m   0.7789        0.2299       [31m0.9166[0m        [94m0.2559[0m     +  9.1312
     10   0.3160   [32m0.7838[0m        [35m0.2279[0m       0.9141        0.2562        9.4405
     11   0.3173   0.7819        [35m0.2274[0m       0.9141        0.2573        9.4286
     12   0.3199   [32m0.7857[0m        [35m0.2259[0m       0.9141        0.2578        9.0283
     13   0.2987   [32m0.7882[0m        0.2319       0.9105        0.2578        9.4398
     14   0.3199   0.7824        0.2273       0.9141        [94m0.2546[0m     +  9.3365
     15   0.3153   0.7834        0.2268       0.9117        0.2572        9.2593
     16   0.3238   0.7787        [35m0.2251[0m       0.9166        0.2581        9.2755
     17   0.3088   0.7731        0.2274       0.9141        0.2635        9.3531
     18   0.3030   0.7747        0.2278       0.9141        0.2588        9.6620
     19   0.3173   0.7830        0.2256       0.9141        0.2578        9.5246
     20   0.3263   0.7821        0.2263       [31m0.9178[0m        0.2558        9.3118
     21   0.3169   [32m0.7946[0m        0.2268       0.9069        0.2578        9.3459
     22   0.2911   0.7746        0.2253       0.9093        0.2568        9.3029
     23   0.3261   0.7813        0.2255       [31m0.9214[0m        [94m0.2490[0m     +  9.6964
     24   0.3121   0.7532        0.2263       0.9190        0.2601        9.1498
     25   0.3253   0.7723        [35m0.2245[0m       0.9178        0.2541        9.3806
     26   0.3290   0.7788        [35m0.2224[0m       0.9178        0.2572        9.4052
     27   0.3342   0.7834        0.2253       0.9117        0.2562        9.3624
     28   0.3215   0.7837        [35m0.2224[0m       0.9178        0.2536        9.5530
     29   0.3411   0.7874        0.2235       0.9190        0.2505        9.5601
     30   0.3173   0.7905        0.2246       0.9178        0.2493        9.4278
     31   0.3048   0.7888        0.2230       0.9178        0.2506        9.0160
     32   0.2802   0.7811        0.2264       0.9093        0.2528        9.3469
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 20:25:13,822][0m Trial 204 finished with value: 0.24901873812962327 and parameters: {'lr': 0.00171608273968334, 'dropout': 0.41772149452391244, 'd_model_multiplier': 4, 'num_layers': 4, 'n_heads': 4, 'dim_feedforward': 248, 'batch_size': 61, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 49}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 33
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2805[0m   [32m0.8049[0m        [35m0.2768[0m       [31m0.9057[0m        [94m0.2316[0m     +  19.5915
      2   [36m0.3918[0m   [32m0.8227[0m        [35m0.2489[0m       [31m0.9287[0m        [94m0.2207[0m     +  19.6626
      3   0.3917   [32m0.8371[0m        [35m0.2446[0m       0.9287        [94m0.2166[0m     +  19.9889
      4   [36m0.4120[0m   [32m0.8407[0m        [35m0.2418[0m       0.9274        [94m0.2146[0m     +  19.8684
      5   [36m0.4133[0m   [32m0.8414[0m        [35m0.2388[0m       0.9262        [94m0.2105[0m     +  19.8360
      6   0.4050   0.8351        [35m0.2316[0m       [31m0.9299[0m        0.2170        20.2680
      7   0.3891   0.8406        0.2352       0.9226        0.2161        19.8619
      8   [36m0.4266[0m   [32m0.8489[0m        [35m0.2313[0m       0.9299        [94m0.2065[0m     +  20.0143
      9   0.4226   0.8482        [35m0.2309[0m       0.9299        0.2080        20.0187
     10   0.4137   0.8463        [35m0.2282[0m       0.9250        0.2127        19.8936
     11   0.4106   0.8458        [35m0.2276[0m       0.9238        0.2112        19.7628
     12   0.4226   0.8465        [35m0.2261[0m       0.9274        0.2082        19.8890
     13   0.3933   0.8473        [35m0.2250[0m       0.9238        0.2103        20.1259
     14   0.4152   0.8479        [35m0.2241[0m       0.9226        0.2118        19.8394
     15   0.4077   [32m0.8503[0m        0.2246       0.9287        0.2096        19.8790
     16   0.4164   [32m0.8581[0m        0.2242       0.9274        0.2108        20.4789
     17   0.4034   0.8543        [35m0.2213[0m       0.9262        0.2176        20.1546
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 20:31:13,568][0m Trial 205 finished with value: 0.2065182991208736 and parameters: {'lr': 0.0009022044845196198, 'dropout': 0.4361250064121629, 'd_model_multiplier': 4, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 216, 'batch_size': 70, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 33}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 31
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2453[0m   [32m0.7278[0m        [35m0.2699[0m       [31m0.8501[0m        [94m0.3744[0m     +  19.7076
      2   [36m0.2755[0m   [32m0.7800[0m        [35m0.2519[0m       [31m0.9141[0m        [94m0.2513[0m     +  19.5187
      3   [36m0.2896[0m   [32m0.7938[0m        [35m0.2487[0m       0.9117        [94m0.2461[0m     +  19.8572
      4   [36m0.2924[0m   [32m0.8103[0m        [35m0.2440[0m       [31m0.9154[0m        [94m0.2417[0m     +  19.9752
      5   [36m0.3135[0m   [32m0.8253[0m        [35m0.2394[0m       [31m0.9166[0m        [94m0.2359[0m     +  20.2076
      6   0.2967   0.8145        [35m0.2319[0m       0.9166        0.2524        20.3966
      7   [36m0.3223[0m   [32m0.8319[0m        [35m0.2315[0m       0.9129        0.2473        19.9393
      8   0.3146   0.8317        [35m0.2279[0m       0.9141        0.2488        19.9001
      9   0.3215   0.8314        0.2319       0.9166        0.2447        19.9073
     10   0.2943   0.8175        0.2317       0.9154        0.2720        19.6525
     11   0.3186   0.8303        0.2296       0.9154        0.2458        19.7948
     12   0.3173   [32m0.8334[0m        [35m0.2277[0m       0.9141        0.2439        21.3416
     13   0.3098   0.8293        [35m0.2270[0m       0.9154        0.2563        19.8492
     14   [36m0.3246[0m   0.8259        0.2288       [31m0.9178[0m        0.2543        20.0936
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 20:36:14,158][0m Trial 206 finished with value: 0.23593741759856063 and parameters: {'lr': 0.0010351081455878663, 'dropout': 0.43496520761547325, 'd_model_multiplier': 4, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 215, 'batch_size': 69, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 31}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 38
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1896[0m   [32m0.7198[0m        [35m0.2799[0m       [31m0.9202[0m        [94m0.2586[0m     +  23.8429
      2   [36m0.2410[0m   [32m0.7743[0m        [35m0.2515[0m       0.9057        [94m0.2565[0m     +  23.5534
      3   [36m0.2761[0m   [32m0.8071[0m        [35m0.2439[0m       0.9105        [94m0.2481[0m     +  23.8953
      4   0.2679   0.7960        [35m0.2369[0m       0.9069        0.2490        23.9438
      5   [36m0.2885[0m   [32m0.8107[0m        0.2374       0.9117        [94m0.2428[0m     +  24.0327
      6   0.2741   0.8042        [35m0.2337[0m       0.9093        [94m0.2422[0m     +  23.8862
      7   [36m0.3014[0m   [32m0.8159[0m        [35m0.2325[0m       0.9166        [94m0.2390[0m     +  23.7222
      8   0.2988   0.8131        [35m0.2297[0m       0.9178        0.2425        23.9597
      9   [36m0.3358[0m   [32m0.8244[0m        [35m0.2267[0m       0.9202        [94m0.2327[0m     +  23.9384
     10   0.3096   0.8150        [35m0.2242[0m       0.9105        0.2485        23.7680
     11   0.3249   [32m0.8301[0m        0.2252       0.9154        0.2400        24.0496
     12   0.3054   0.8199        0.2254       0.9178        0.2409        23.7968
     13   0.2984   0.8270        [35m0.2233[0m       0.9166        0.2348        24.0968
     14   0.3073   0.8257        [35m0.2228[0m       0.9069        0.2530        24.0576
     15   0.3099   0.8233        [35m0.2201[0m       0.9178        0.2465        23.8796
     16   [36m0.3577[0m   [32m0.8307[0m        [35m0.2166[0m       [31m0.9238[0m        0.2428        23.7563
     17   0.3009   0.8219        0.2179       0.9057        0.2636        23.7636
     18   0.3300   0.8248        0.2221       0.9178        0.2469        24.0851
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 20:43:49,238][0m Trial 207 finished with value: 0.23265445506270016 and parameters: {'lr': 0.0008035506599557241, 'dropout': 0.44290879244617704, 'd_model_multiplier': 4, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 229, 'batch_size': 86, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 38}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 54
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1595[0m   [32m0.7302[0m        [35m0.3011[0m       [31m0.9432[0m        [94m0.1994[0m     +  20.0089
      2   [36m0.1779[0m   [32m0.7326[0m        [35m0.2665[0m       0.9432        [94m0.1957[0m     +  19.8167
      3   0.1624   0.7185        [35m0.2643[0m       0.9432        0.1974        19.9846
      4   0.1622   0.7269        [35m0.2630[0m       0.9432        0.2022        20.2051
      5   0.1736   0.7279        [35m0.2619[0m       0.9432        0.2010        20.1226
      6   0.1770   [32m0.7368[0m        [35m0.2606[0m       0.9432        0.1990        20.2130
      7   0.1712   0.7287        0.2622       0.9432        0.1977        20.0741
      8   0.1727   0.7309        [35m0.2593[0m       0.9432        0.1978        20.0441
      9   0.1684   0.7311        [35m0.2581[0m       0.9420        0.1992        20.2104
     10   0.1661   0.7307        0.2593       0.9420        0.1992        20.1681
     11   0.1767   0.7311        [35m0.2578[0m       0.9420        0.1992        19.9734
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 20:47:50,699][0m Trial 208 finished with value: 0.1957275327272542 and parameters: {'lr': 0.0015262082354163939, 'dropout': 0.44762570164082943, 'd_model_multiplier': 4, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 242, 'batch_size': 72, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 54}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 58
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1458[0m   [32m0.6690[0m        [35m0.2818[0m       [31m0.9250[0m        [94m0.2699[0m     +  20.1569
      2   0.1361   [32m0.6776[0m        [35m0.2661[0m       0.9250        [94m0.2655[0m     +  20.0169
      3   [36m0.1581[0m   [32m0.6868[0m        [35m0.2590[0m       0.9250        0.2660        20.3940
      4   0.1482   0.6857        [35m0.2551[0m       0.9250        [94m0.2640[0m     +  20.3696
      5   [36m0.1774[0m   0.6839        [35m0.2547[0m       0.9238        [94m0.2597[0m     +  20.3455
      6   [36m0.1831[0m   [32m0.6910[0m        [35m0.2541[0m       0.9214        [94m0.2587[0m     +  20.3110
      7   0.1724   0.6889        [35m0.2507[0m       0.9238        0.2620        20.3314
      8   [36m0.1855[0m   0.6881        0.2526       0.9202        0.2616        20.1440
      9   [36m0.1864[0m   0.6884        [35m0.2506[0m       0.9214        0.2623        20.2817
     10   0.1836   0.6801        [35m0.2497[0m       0.9226        0.2644        20.4347
     11   [36m0.1868[0m   0.6835        0.2499       0.9202        0.2626        20.3412
     12   0.1807   0.6781        0.2498       0.9226        0.2667        20.2662
     13   [36m0.1876[0m   0.6868        [35m0.2490[0m       0.9226        0.2608        20.2108
     14   0.1839   0.6858        [35m0.2482[0m       0.9226        0.2664        20.2046
     15   0.1869   0.6813        0.2498       0.9190        0.2652        20.2122
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 20:53:15,655][0m Trial 209 finished with value: 0.25874730186061673 and parameters: {'lr': 0.0015331031732113292, 'dropout': 0.42330803322235927, 'd_model_multiplier': 4, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 253, 'batch_size': 79, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 58}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 79
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1391[0m   [32m0.5874[0m        [35m0.3063[0m       [31m0.9311[0m        [94m0.2771[0m     +  23.4699
      2   0.1192   [32m0.6213[0m        [35m0.2672[0m       0.9311        [94m0.2592[0m     +  23.5378
      3   0.1318   [32m0.6717[0m        [35m0.2601[0m       0.9311        0.2630        23.6427
      4   0.1301   0.6663        [35m0.2581[0m       0.9311        0.2638        23.5371
      5   0.1374   [32m0.6766[0m        [35m0.2576[0m       0.9311        0.2684        23.4437
      6   0.1383   0.6759        0.2582       0.9311        0.2648        23.4820
      7   0.1367   0.6724        [35m0.2551[0m       0.9299        0.2608        23.4550
      8   [36m0.1408[0m   [32m0.6829[0m        [35m0.2534[0m       0.9299        [94m0.2589[0m     +  23.6038
      9   [36m0.1479[0m   0.6829        0.2539       0.9311        0.2618        23.8130
     10   0.1427   0.6795        0.2541       0.9311        0.2619        23.6932
     11   0.1443   0.6785        0.2558       0.9311        [94m0.2572[0m     +  23.7806
     12   [36m0.1546[0m   0.6814        0.2551       0.9299        [94m0.2539[0m     +  23.8040
     13   [36m0.1607[0m   [32m0.6843[0m        0.2534       0.9299        [94m0.2526[0m     +  23.6480
     14   0.1533   0.6792        0.2536       0.9311        0.2534        23.7218
     15   0.1502   0.6807        0.2536       0.9311        [94m0.2495[0m     +  23.8504
     16   0.1477   0.6792        [35m0.2531[0m       0.9299        [94m0.2430[0m     +  23.5797
     17   0.1481   0.6834        0.2532       0.9299        [94m0.2429[0m     +  23.4139
     18   0.1501   0.6802        [35m0.2522[0m       0.9299        [94m0.2382[0m     +  23.4796
     19   0.1541   0.6819        0.2543       0.9299        0.2412        24.6753
     20   0.1504   0.6829        0.2524       0.9299        [94m0.2348[0m     +  23.3998
     21   0.1507   0.6805        [35m0.2515[0m       0.9311        0.2376        23.4583
     22   0.1536   0.6784        0.2531       0.9311        0.2373        23.3623
     23   [36m0.1713[0m   0.6818        0.2527       0.9299        0.2372        23.7939
     24   0.1468   0.6768        0.2527       0.9299        0.2363        23.6719
     25   0.1510   0.6832        0.2534       0.9299        0.2349        23.7928
     26   0.1590   0.6800        0.2529       0.9299        0.2357        23.4948
     27   0.1604   0.6821        [35m0.2515[0m       0.9299        0.2359        23.5638
     28   0.1519   0.6774        0.2519       0.9299        [94m0.2348[0m     +  23.5291
     29   0.1513   0.6775        [35m0.2509[0m       0.9287        0.2366        23.5725
     30   0.1550   0.6771        0.2521       0.9287        0.2360        23.5880
     31   0.1513   0.6801        0.2522       0.9299        0.2379        23.4959
     32   0.1653   0.6794        0.2525       0.9299        0.2432        23.6657
     33   0.1528   0.6760        [35m0.2506[0m       0.9299        0.2405        23.5660
     34   0.1526   0.6798        0.2529       0.9311        0.2460        23.6550
     35   0.1647   0.6765        0.2509       0.9299        0.2421        23.5559
     36   0.1516   0.6779        0.2517       0.9311        0.2406        23.8335
     37   0.1568   0.6796        0.2515       0.9311        0.2451        23.7261
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 21:08:15,064][0m Trial 210 finished with value: 0.23480377212941575 and parameters: {'lr': 0.0020307384358731985, 'dropout': 0.4363671445976443, 'd_model_multiplier': 4, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 194, 'batch_size': 66, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 79}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 46
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2379[0m   [32m0.8243[0m        [35m0.2636[0m       [31m0.9081[0m        [94m0.2579[0m     +  19.5083
      2   [36m0.2823[0m   [32m0.8328[0m        [35m0.2458[0m       [31m0.9154[0m        [94m0.2233[0m     +  20.1865
      3   [36m0.2886[0m   0.8299        [35m0.2434[0m       [31m0.9250[0m        [94m0.2099[0m     +  19.8809
      4   0.2754   0.8298        [35m0.2386[0m       0.9214        0.2155        19.9001
      5   [36m0.2990[0m   0.8292        [35m0.2355[0m       0.9250        0.2122        20.0568
      6   [36m0.3345[0m   0.8317        0.2358       [31m0.9311[0m        [94m0.2085[0m     +  20.0828
      7   0.3323   0.8289        [35m0.2308[0m       0.9299        0.2137        19.8929
      8   [36m0.3388[0m   0.8234        [35m0.2298[0m       0.9311        0.2135        20.1596
      9   0.3058   0.8220        [35m0.2295[0m       0.9299        0.2178        20.0822
     10   0.3050   0.8221        [35m0.2261[0m       0.9287        0.2152        20.1719
     11   0.2942   0.8188        [35m0.2225[0m       0.9311        0.2179        20.1460
     12   0.2752   0.8276        0.2246       0.9299        0.2270        20.0166
     13   0.2904   [32m0.8330[0m        0.2227       [31m0.9323[0m        0.2161        20.1356
     14   0.2429   0.8226        [35m0.2176[0m       0.9262        0.2202        20.2449
     15   0.2881   0.8282        0.2189       0.9287        0.2171        20.2582
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 21:13:36,343][0m Trial 211 finished with value: 0.2084964440984622 and parameters: {'lr': 0.0005653479476396005, 'dropout': 0.45347788124148747, 'd_model_multiplier': 4, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 215, 'batch_size': 71, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 46}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 48
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2669[0m   [32m0.7620[0m        [35m0.2799[0m       [31m0.8984[0m        [94m0.2608[0m     +  19.9089
      2   0.2136   0.7424        [35m0.2615[0m       [31m0.9287[0m        [94m0.2324[0m     +  20.1711
      3   0.2094   0.7358        [35m0.2588[0m       0.9287        0.2421        19.8987
      4   0.2124   0.7344        [35m0.2554[0m       0.9274        0.2342        20.2285
      5   0.2023   0.7429        0.2559       0.9287        0.2546        19.9291
      6   0.1812   0.7396        [35m0.2544[0m       0.9287        0.2483        19.9718
      7   0.2006   0.7398        0.2565       0.9287        0.2509        20.0387
      8   0.1664   0.7354        [35m0.2540[0m       0.9287        0.2544        20.0680
      9   0.1960   0.7302        [35m0.2529[0m       0.9287        0.2398        20.0826
     10   0.1829   0.7340        0.2544       0.9287        0.2398        20.3989
     11   0.1857   0.7331        0.2550       0.9287        0.2388        20.2085
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 21:17:37,892][0m Trial 212 finished with value: 0.23237991776258798 and parameters: {'lr': 0.0012542688082907544, 'dropout': 0.447432462458258, 'd_model_multiplier': 4, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 243, 'batch_size': 72, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 48}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 46
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2141[0m   [32m0.7654[0m        [35m0.2711[0m       [31m0.9214[0m        [94m0.2391[0m     +  19.9516
      2   [36m0.2519[0m   [32m0.7781[0m        [35m0.2454[0m       [31m0.9262[0m        [94m0.2239[0m     +  20.2201
      3   0.2488   0.7772        [35m0.2382[0m       0.9262        0.2341        20.3851
      4   [36m0.2522[0m   [32m0.7800[0m        [35m0.2355[0m       0.9238        0.2332        20.7602
      5   [36m0.2720[0m   [32m0.7892[0m        [35m0.2326[0m       0.9238        0.2362        20.3087
      6   [36m0.2806[0m   [32m0.7905[0m        [35m0.2302[0m       0.9262        0.2265        20.5079
      7   [36m0.2820[0m   0.7870        0.2311       0.9226        0.2327        20.3891
      8   [36m0.2851[0m   [32m0.7948[0m        [35m0.2274[0m       0.9250        0.2259        20.4479
      9   0.2441   0.7866        [35m0.2273[0m       0.9250        0.2268        20.4213
     10   [36m0.2945[0m   [32m0.7975[0m        [35m0.2259[0m       0.9238        0.2342        20.3923
     11   0.2748   0.7886        0.2281       0.9250        0.2327        20.3208
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 21:21:42,891][0m Trial 213 finished with value: 0.22391220927238464 and parameters: {'lr': 0.000822865536250981, 'dropout': 0.4526375206830447, 'd_model_multiplier': 4, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 204, 'batch_size': 90, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 46}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 53
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2595[0m   [32m0.7090[0m        [35m0.2656[0m       [31m0.9141[0m        [94m0.3169[0m     +  19.6367
      2   0.2381   [32m0.7207[0m        [35m0.2355[0m       0.9117        0.3388        19.9180
      3   0.2486   [32m0.7224[0m        [35m0.2335[0m       0.9129        0.3274        19.8473
      4   0.2429   [32m0.7268[0m        [35m0.2273[0m       0.9129        0.3320        20.0629
      5   0.2515   [32m0.7445[0m        0.2273       0.9129        0.3189        20.0054
      6   [36m0.2599[0m   [32m0.7452[0m        [35m0.2263[0m       0.9105        0.3286        19.9783
      7   0.2525   [32m0.7479[0m        [35m0.2256[0m       0.9117        0.3192        20.1066
      8   0.2479   0.7391        [35m0.2237[0m       0.9081        0.3271        20.0531
      9   0.2445   [32m0.7515[0m        [35m0.2225[0m       0.9117        [94m0.3168[0m     +  20.2250
     10   [36m0.2653[0m   0.7486        0.2227       0.9141        0.3356        20.2790
     11   [36m0.2769[0m   [32m0.7599[0m        [35m0.2175[0m       0.9141        [94m0.2990[0m     +  19.9643
     12   0.2709   0.7566        [35m0.2169[0m       [31m0.9154[0m        0.3117        19.9657
     13   0.2746   [32m0.7677[0m        0.2184       0.9154        0.3004        20.1804
     14   [36m0.2839[0m   0.7597        [35m0.2159[0m       0.9141        0.3106        20.2844
     15   0.2826   [32m0.7729[0m        [35m0.2151[0m       0.9105        [94m0.2926[0m     +  19.9895
     16   [36m0.2886[0m   0.7592        [35m0.2144[0m       0.9141        0.3177        19.9977
     17   0.2780   0.7682        [35m0.2124[0m       0.9105        0.3151        20.4167
     18   [36m0.3053[0m   [32m0.7814[0m        0.2146       [31m0.9178[0m        [94m0.2817[0m     +  19.9014
     19   0.2993   [32m0.7849[0m        0.2137       0.9105        0.2955        20.1408
     20   [36m0.3109[0m   [32m0.7875[0m        0.2126       0.9093        0.3054        20.0371
     21   0.3042   0.7831        0.2135       0.9093        0.2957        20.1189
     22   0.3074   [32m0.7894[0m        0.2127       0.9093        0.3008        20.2716
     23   0.3077   [32m0.7899[0m        0.2128       0.9141        0.2895        19.9111
     24   [36m0.3209[0m   0.7847        0.2159       0.9129        0.3020        19.9435
     25   0.2995   0.7795        [35m0.2090[0m       0.9105        0.3234        19.9805
     26   0.3055   0.7670        0.2102       0.9178        0.3240        20.0758
     27   0.3044   0.7801        [35m0.2081[0m       0.9117        0.3255        20.1425
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 21:31:05,035][0m Trial 214 finished with value: 0.2816870144806001 and parameters: {'lr': 0.0004892629990425124, 'dropout': 0.46803238903995387, 'd_model_multiplier': 4, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 219, 'batch_size': 74, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 53}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 43
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1884[0m   [32m0.7828[0m        [35m0.2719[0m       [31m0.8960[0m        [94m0.2448[0m     +  21.3163
      2   [36m0.2168[0m   [32m0.7887[0m        [35m0.2414[0m       [31m0.9154[0m        [94m0.2367[0m     +  21.7758
      3   0.2078   [32m0.7919[0m        [35m0.2385[0m       [31m0.9190[0m        [94m0.2292[0m     +  21.5473
      4   0.2114   [32m0.7971[0m        [35m0.2352[0m       0.9190        [94m0.2289[0m     +  21.8890
      5   0.1998   0.7891        [35m0.2328[0m       0.9154        0.2325        21.6305
      6   0.1903   0.7814        [35m0.2326[0m       0.9154        0.2353        21.7438
      7   0.2107   [32m0.8045[0m        0.2335       0.9154        0.2332        21.9177
      8   [36m0.2171[0m   0.7978        [35m0.2284[0m       0.9057        0.2359        22.2988
      9   [36m0.2262[0m   0.7965        [35m0.2270[0m       0.9154        [94m0.2271[0m     +  21.9574
     10   0.2171   [32m0.8101[0m        [35m0.2226[0m       0.9129        0.2274        21.7182
     11   0.2165   0.7989        0.2241       0.9129        0.2327        21.9990
     12   [36m0.2288[0m   [32m0.8119[0m        0.2233       0.9117        0.2278        22.1631
     13   0.2271   0.8030        0.2236       0.9117        0.2338        22.2448
     14   0.2181   0.8083        0.2249       0.9057        0.2344        21.8592
     15   0.2269   [32m0.8260[0m        0.2275       0.9141        0.2321        22.1690
     16   [36m0.2324[0m   0.8177        [35m0.2205[0m       0.9141        0.2296        22.0983
     17   [36m0.2468[0m   0.8222        0.2215       0.9129        0.2328        21.9288
     18   0.2444   0.8241        0.2212       0.9129        [94m0.2229[0m     +  22.1006
     19   0.2354   [32m0.8339[0m        [35m0.2188[0m       0.9166        [94m0.2219[0m     +  22.3586
     20   0.2319   0.8258        [35m0.2178[0m       0.9129        0.2223        21.9177
     21   [36m0.2544[0m   0.8282        0.2183       0.9105        0.2328        22.1734
     22   0.2439   0.8338        [35m0.2162[0m       0.9105        0.2296        22.0446
     23   0.2416   0.8307        [35m0.2136[0m       0.9166        0.2261        22.0167
     24   0.2394   0.8332        0.2144       0.9141        0.2295        21.9335
     25   0.2475   0.8297        0.2172       0.9129        0.2266        21.9077
     26   0.2355   0.8337        0.2150       0.9141        0.2262        21.8745
     27   0.2338   0.8143        0.2170       0.9105        0.2411        22.0680
     28   0.2309   0.8211        0.2153       0.9105        0.2313        21.8947
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 21:41:42,190][0m Trial 215 finished with value: 0.22191024723912092 and parameters: {'lr': 0.000904139466386563, 'dropout': 0.43068552570121166, 'd_model_multiplier': 4, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 233, 'batch_size': 146, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 43}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 34
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2526[0m   [32m0.7313[0m        [35m0.2660[0m       [31m0.9202[0m        [94m0.2611[0m     +  23.1974
      2   [36m0.3175[0m   [32m0.7552[0m        [35m0.2468[0m       0.9178        [94m0.2421[0m     +  23.8328
      3   [36m0.3304[0m   [32m0.7679[0m        [35m0.2418[0m       0.9190        [94m0.2414[0m     +  23.8404
      4   0.3236   [32m0.7790[0m        [35m0.2381[0m       0.9202        [94m0.2359[0m     +  23.7283
      5   0.3100   0.7750        [35m0.2368[0m       0.9190        0.2472        23.8362
      6   0.3240   0.7751        [35m0.2318[0m       0.9190        [94m0.2346[0m     +  23.5115
      7   0.3146   0.7764        [35m0.2313[0m       0.9202        0.2370        23.8096
      8   [36m0.3381[0m   [32m0.7865[0m        [35m0.2298[0m       0.9202        0.2365        23.4910
      9   [36m0.3436[0m   [32m0.7879[0m        [35m0.2285[0m       [31m0.9226[0m        0.2347        23.6069
     10   0.3399   0.7813        [35m0.2284[0m       0.9202        0.2383        23.5898
     11   0.3079   0.7808        0.2285       0.9190        0.2382        23.7113
     12   [36m0.3453[0m   0.7818        [35m0.2259[0m       [31m0.9238[0m        0.2361        23.4014
     13   0.3450   0.7702        0.2269       0.9226        0.2356        23.5830
     14   0.2864   0.7746        0.2283       0.9202        0.2403        23.3414
     15   [36m0.3499[0m   0.7860        [35m0.2232[0m       0.9202        0.2348        23.5027
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 21:48:00,546][0m Trial 216 finished with value: 0.23457465110912 and parameters: {'lr': 0.0005829061826180158, 'dropout': 0.46100556136276055, 'd_model_multiplier': 4, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 219, 'batch_size': 70, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 34}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 55
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2490[0m   [32m0.7395[0m        [35m0.2799[0m       [31m0.9069[0m        [94m0.2701[0m     +  19.5739
      2   [36m0.2839[0m   [32m0.8029[0m        [35m0.2554[0m       [31m0.9105[0m        [94m0.2674[0m     +  19.9685
      3   0.2713   0.7937        [35m0.2485[0m       0.9093        [94m0.2546[0m     +  19.7595
      4   0.2713   0.7804        [35m0.2466[0m       0.9105        0.2659        19.7174
      5   0.2363   0.7297        0.2515       0.9093        0.2768        20.1837
      6   0.2292   0.7352        0.2542       0.9093        0.2700        20.2727
      7   0.2384   0.7311        0.2522       0.9069        0.2782        19.7891
      8   0.2335   0.7329        0.2520       0.9093        0.2843        19.8084
      9   0.2366   0.7308        0.2512       0.9069        0.2770        19.9953
     10   0.2296   0.7309        0.2528       0.9105        0.2861        19.8382
     11   0.2258   0.7319        0.2494       0.9093        0.2823        19.8572
     12   0.2538   0.7798        0.2501       0.9069        0.2754        19.9308
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 21:52:19,483][0m Trial 217 finished with value: 0.2545783437220909 and parameters: {'lr': 0.0014358561251839148, 'dropout': 0.44231160865813673, 'd_model_multiplier': 4, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 238, 'batch_size': 56, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 55}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 30
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1570[0m   [32m0.7058[0m        [35m0.2825[0m       [31m0.9311[0m        [94m0.2403[0m     +  23.6556
      2   [36m0.1726[0m   [32m0.7208[0m        [35m0.2645[0m       0.9311        0.2463        23.3427
      3   [36m0.2255[0m   [32m0.7211[0m        [35m0.2588[0m       0.9311        [94m0.2362[0m     +  23.6324
      4   0.1892   0.7194        [35m0.2545[0m       0.9311        0.2392        23.7147
      5   0.2009   [32m0.7264[0m        0.2552       0.9311        0.2365        23.5462
      6   [36m0.2422[0m   [32m0.7274[0m        [35m0.2544[0m       [31m0.9323[0m        0.2379        23.3802
      7   0.2212   0.7256        [35m0.2529[0m       0.9311        0.2377        23.4305
      8   0.2114   0.7249        [35m0.2523[0m       0.9323        0.2389        23.5702
      9   0.2172   0.7246        0.2531       0.9311        0.2378        23.3109
     10   0.1975   0.7239        0.2533       0.9311        0.2381        23.3379
     11   0.1967   0.7258        0.2523       0.9311        0.2374        23.5228
     12   0.2074   [32m0.7297[0m        [35m0.2523[0m       0.9311        0.2378        23.4349
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 21:57:25,273][0m Trial 218 finished with value: 0.23617248963271836 and parameters: {'lr': 0.0010897223924425702, 'dropout': 0.40599224355632485, 'd_model_multiplier': 4, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 216, 'batch_size': 66, 'pos_encoding': 'learnable', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 30}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 41
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2008[0m   [32m0.7310[0m        [35m0.3118[0m       [31m0.9238[0m        [94m0.2465[0m     +  19.5412
      2   [36m0.2023[0m   0.7202        [35m0.2722[0m       [31m0.9335[0m        0.2700        19.7803
      3   0.1984   0.7174        [35m0.2617[0m       0.9335        0.2797        19.7044
      4   0.1991   0.7157        [35m0.2604[0m       0.9335        0.2779        19.8826
      5   0.2001   0.7240        [35m0.2597[0m       [31m0.9347[0m        0.2655        19.6695
      6   0.1994   0.7277        [35m0.2584[0m       0.9335        0.2699        19.9391
      7   [36m0.2045[0m   0.7227        0.2586       0.9335        0.2674        19.7634
      8   0.2022   0.7198        [35m0.2578[0m       0.9335        0.2685        20.0159
      9   0.2031   [32m0.7362[0m        0.2588       0.9335        0.2529        19.8813
     10   [36m0.2049[0m   0.7342        0.2582       0.9347        0.2476        19.8504
     11   0.2036   [32m0.7377[0m        [35m0.2569[0m       0.9347        [94m0.2346[0m     +  19.9590
     12   0.2019   0.7350        0.2580       0.9335        0.2352        19.8421
     13   [36m0.2103[0m   [32m0.7561[0m        [35m0.2563[0m       0.9335        0.2365        19.9877
     14   0.2023   0.7327        0.2568       0.9335        [94m0.2309[0m     +  19.7091
     15   0.2016   0.7388        [35m0.2563[0m       0.9335        [94m0.2254[0m     +  19.6899
     16   0.1938   0.7348        0.2565       0.9335        [94m0.2239[0m     +  19.7413
     17   0.1931   0.7336        [35m0.2553[0m       0.9335        [94m0.2226[0m     +  19.9134
     18   0.1982   0.7393        0.2553       0.9335        [94m0.2201[0m     +  19.6145
     19   0.2032   0.7385        0.2558       [31m0.9359[0m        0.2208        19.8645
     20   0.1991   0.7340        [35m0.2545[0m       0.9335        0.2208        19.7602
     21   0.2026   0.7382        0.2561       0.9359        0.2219        19.7632
     22   0.2017   0.7349        0.2556       0.9359        0.2209        19.7272
     23   0.2030   0.7385        0.2557       0.9347        0.2263        19.9229
     24   0.2015   0.7306        0.2552       0.9335        0.2220        19.6439
     25   0.2017   0.7387        0.2553       0.9347        0.2278        19.7892
     26   0.2034   0.7382        [35m0.2539[0m       0.9335        0.2232        19.9266
     27   0.2045   0.7448        0.2541       0.9335        0.2221        19.7058
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 22:06:40,634][0m Trial 219 finished with value: 0.22012465294932512 and parameters: {'lr': 0.002205244274796743, 'dropout': 0.48591025464914145, 'd_model_multiplier': 4, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 236, 'batch_size': 52, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 41}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 219
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1639[0m   [32m0.7345[0m        [35m0.2648[0m       [31m0.9166[0m        [94m0.2356[0m     +  10.9909
      2   [36m0.1862[0m   [32m0.7449[0m        [35m0.2527[0m       [31m0.9190[0m        [94m0.2340[0m     +  10.9298
      3   [36m0.1937[0m   [32m0.7481[0m        [35m0.2480[0m       0.9117        0.2345        11.0301
      4   [36m0.1972[0m   [32m0.7546[0m        [35m0.2460[0m       0.9045        0.2492        11.0791
      5   [36m0.2066[0m   [32m0.7554[0m        [35m0.2423[0m       0.9154        0.2370        11.2160
      6   [36m0.2273[0m   [32m0.7668[0m        [35m0.2404[0m       [31m0.9287[0m        [94m0.2268[0m     +  11.1387
      7   [36m0.2511[0m   [32m0.7735[0m        [35m0.2346[0m       0.9202        0.2280        11.2791
      8   [36m0.2587[0m   [32m0.7782[0m        [35m0.2346[0m       0.9178        [94m0.2249[0m     +  11.2064
      9   0.2573   [32m0.7857[0m        [35m0.2329[0m       0.9250        [94m0.2137[0m     +  11.3312
     10   0.2566   [32m0.7869[0m        [35m0.2320[0m       0.9081        0.2340        10.9947
     11   0.2493   0.7791        [35m0.2314[0m       0.9141        0.2292        10.9800
     12   0.2535   [32m0.7895[0m        [35m0.2303[0m       0.9238        0.2221        11.3125
     13   [36m0.2597[0m   [32m0.7932[0m        0.2306       0.9190        0.2228        11.2643
     14   [36m0.2605[0m   0.7882        [35m0.2302[0m       0.9166        0.2286        11.2462
     15   [36m0.2717[0m   0.7923        [35m0.2280[0m       0.9202        0.2257        11.3851
     16   0.2429   0.7824        [35m0.2278[0m       0.9021        0.2642        11.0856
     17   0.2578   0.7910        [35m0.2272[0m       0.9057        0.2458        11.5304
     18   0.1870   0.7482        [35m0.2258[0m       0.8924        0.2743        11.3991
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 22:10:14,072][0m Trial 220 finished with value: 0.21372676134433827 and parameters: {'lr': 0.0005253114968894962, 'dropout': 0.47185208818076413, 'd_model_multiplier': 4, 'num_layers': 3, 'n_heads': 8, 'dim_feedforward': 328, 'batch_size': 14, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 219}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 219
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2335[0m   [32m0.7515[0m        [35m0.2658[0m       [31m0.9262[0m        [94m0.2462[0m     +  11.4862
      2   0.2057   0.6875        [35m0.2513[0m       0.9250        0.2961        11.6413
      3   [36m0.2776[0m   [32m0.7810[0m        [35m0.2490[0m       0.9250        0.2532        11.4819
      4   0.2678   0.7790        [35m0.2448[0m       0.9214        0.2487        11.6492
      5   [36m0.2815[0m   [32m0.7952[0m        [35m0.2398[0m       0.9238        0.2500        11.6105
      6   0.2625   0.7925        [35m0.2389[0m       0.9238        0.2488        11.6467
      7   0.2771   [32m0.7973[0m        [35m0.2359[0m       0.9238        0.2561        11.5720
      8   0.2765   [32m0.8039[0m        [35m0.2338[0m       0.9226        0.2590        11.4236
      9   0.2712   [32m0.8107[0m        [35m0.2320[0m       0.9238        0.2485        11.4679
     10   0.2670   0.7893        0.2321       0.9202        0.2860        11.4932
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 22:12:22,012][0m Trial 221 finished with value: 0.24616259446383387 and parameters: {'lr': 0.0005204679548555498, 'dropout': 0.45535377657672754, 'd_model_multiplier': 4, 'num_layers': 3, 'n_heads': 8, 'dim_feedforward': 259, 'batch_size': 13, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 219}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 46
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.2729[0m   [32m0.7307[0m        [35m0.2565[0m       [31m0.9214[0m        [94m0.2649[0m     +  9.9782
      2   [36m0.3062[0m   [32m0.7692[0m        [35m0.2437[0m       0.9202        [94m0.2455[0m     +  10.1356
      3   [36m0.3117[0m   0.7679        [35m0.2383[0m       0.9166        [94m0.2437[0m     +  10.3090
      4   [36m0.3245[0m   [32m0.7752[0m        [35m0.2353[0m       [31m0.9226[0m        [94m0.2436[0m     +  10.0321
      5   0.3215   0.7741        [35m0.2337[0m       0.9202        [94m0.2408[0m     +  10.2507
      6   0.3184   0.7583        [35m0.2332[0m       0.9214        0.2459        10.2611
      7   [36m0.3541[0m   0.7749        [35m0.2297[0m       0.9226        [94m0.2396[0m     +  10.2939
      8   0.3462   [32m0.7805[0m        [35m0.2276[0m       0.9190        [94m0.2391[0m     +  10.2308
      9   [36m0.3555[0m   0.7772        [35m0.2250[0m       [31m0.9238[0m        0.2446        10.2792
     10   0.3489   0.7754        0.2258       0.9238        0.2503        10.1704
     11   0.3407   0.7773        [35m0.2234[0m       0.9214        0.2541        10.2655
     12   [36m0.3591[0m   [32m0.7845[0m        [35m0.2224[0m       0.9214        0.2504        10.2204
     13   [36m0.3631[0m   [32m0.7851[0m        [35m0.2188[0m       0.9214        0.2543        10.3334
     14   0.3601   0.7839        [35m0.2185[0m       0.9214        0.2467        10.0526
     15   0.3617   [32m0.7885[0m        [35m0.2177[0m       [31m0.9250[0m        0.2455        10.2026
     16   [36m0.3651[0m   0.7835        [35m0.2172[0m       0.9226        0.2423        10.3776
     17   [36m0.3673[0m   [32m0.7902[0m        [35m0.2162[0m       0.9238        0.2420        9.8468
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 22:15:25,992][0m Trial 222 finished with value: 0.23906676570334198 and parameters: {'lr': 0.0007537008178325868, 'dropout': 0.46765714262928537, 'd_model_multiplier': 4, 'num_layers': 3, 'n_heads': 8, 'dim_feedforward': 185, 'batch_size': 16, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 46}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 227
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2615[0m   [32m0.7227[0m        [35m0.2898[0m       [31m0.9178[0m        [94m0.2569[0m     +  10.2589
      2   [36m0.2852[0m   [32m0.7499[0m        [35m0.2474[0m       0.9178        0.2658        10.1307
      3   [36m0.2940[0m   [32m0.7573[0m        [35m0.2430[0m       [31m0.9202[0m        [94m0.2534[0m     +  10.5951
      4   0.2832   0.7531        [35m0.2390[0m       [31m0.9214[0m        0.2603        10.6613
      5   [36m0.3162[0m   [32m0.7629[0m        [35m0.2381[0m       0.9214        [94m0.2485[0m     +  10.8005
      6   [36m0.3163[0m   [32m0.7693[0m        [35m0.2365[0m       0.9214        0.2520        10.4357
      7   0.2897   0.7624        [35m0.2335[0m       0.9214        0.2502        10.6948
      8   0.2969   0.7656        [35m0.2328[0m       [31m0.9226[0m        0.2485        10.6904
      9   0.2963   0.7598        [35m0.2314[0m       0.9226        0.2561        10.6965
     10   0.2909   0.7611        [35m0.2304[0m       0.9190        0.2546        10.4257
     11   0.3079   0.7623        [35m0.2304[0m       0.9202        0.2538        10.5755
     12   [36m0.3188[0m   0.7675        [35m0.2281[0m       0.9226        0.2538        10.8164
     13   0.3108   [32m0.7718[0m        [35m0.2255[0m       0.9214        0.2555        10.5307
     14   0.3091   [32m0.7739[0m        [35m0.2247[0m       0.9226        0.2548        10.6171
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 22:18:04,806][0m Trial 223 finished with value: 0.24848271986557466 and parameters: {'lr': 0.0004106338922247913, 'dropout': 0.47956316412649, 'd_model_multiplier': 4, 'num_layers': 4, 'n_heads': 8, 'dim_feedforward': 323, 'batch_size': 75, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 227}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 221
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.2063[0m   [32m0.7182[0m        [35m0.2741[0m       [31m0.9202[0m        [94m0.2584[0m     +  9.3426
      2   [36m0.2381[0m   [32m0.7630[0m        [35m0.2472[0m       0.9141        [94m0.2486[0m     +  9.9346
      3   [36m0.2482[0m   [32m0.7694[0m        [35m0.2423[0m       0.9141        [94m0.2475[0m     +  9.7302
      4   [36m0.2634[0m   [32m0.7757[0m        [35m0.2393[0m       0.9154        0.2479        9.5555
      5   [36m0.2954[0m   [32m0.7791[0m        [35m0.2336[0m       0.9178        [94m0.2445[0m     +  9.7593
      6   [36m0.2981[0m   0.7771        [35m0.2312[0m       0.9202        [94m0.2422[0m     +  10.0203
      7   [36m0.3382[0m   [32m0.7877[0m        [35m0.2307[0m       [31m0.9226[0m        0.2433        9.9120
      8   0.3341   0.7845        [35m0.2268[0m       0.9214        0.2434        9.9381
      9   [36m0.3838[0m   0.7859        0.2276       [31m0.9250[0m        [94m0.2413[0m     +  10.1503
     10   0.3501   0.7792        [35m0.2216[0m       0.9238        0.2446        9.8995
     11   0.3628   0.7865        0.2239       [31m0.9274[0m        [94m0.2384[0m     +  9.7577
     12   0.3408   0.7841        0.2217       0.9202        0.2405        9.9591
     13   0.3444   0.7833        0.2221       0.9250        0.2488        9.6964
     14   0.3512   [32m0.7902[0m        [35m0.2189[0m       0.9274        0.2399        9.8536
     15   0.3472   0.7851        [35m0.2184[0m       0.9250        0.2538        9.8837
     16   0.3127   0.7849        0.2199       0.9141        0.2507        9.9358
     17   0.3552   0.7869        [35m0.2182[0m       0.9238        0.2535        9.7297
     18   0.3314   0.7810        0.2201       0.9214        0.2595        9.9285
     19   0.3252   0.7724        0.2188       0.9250        0.2624        10.0518
     20   0.3460   0.7789        [35m0.2149[0m       0.9262        0.2513        9.9602
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 22:21:32,284][0m Trial 224 finished with value: 0.23839068591162257 and parameters: {'lr': 0.0005539911692656839, 'dropout': 0.4263436585524305, 'd_model_multiplier': 4, 'num_layers': 3, 'n_heads': 8, 'dim_feedforward': 355, 'batch_size': 63, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 221}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 22
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2335[0m   [32m0.6861[0m        [35m0.3157[0m       [31m0.9081[0m        [94m0.3811[0m     +  21.5310
      2   0.2332   [32m0.6956[0m        [35m0.2665[0m       0.9081        [94m0.3796[0m     +  21.9687
      3   [36m0.2375[0m   [32m0.6962[0m        [35m0.2587[0m       0.9081        0.3800        21.8110
      4   0.2237   0.6881        [35m0.2576[0m       0.9081        [94m0.3782[0m     +  21.7970
      5   0.2287   0.6942        [35m0.2534[0m       0.9081        [94m0.3670[0m     +  21.8687
      6   0.2208   0.6895        0.2539       0.9081        [94m0.3636[0m     +  21.8175
      7   0.2282   [32m0.6976[0m        [35m0.2518[0m       0.9081        [94m0.3476[0m     +  21.7245
      8   0.2290   0.6959        [35m0.2512[0m       0.9081        [94m0.3419[0m     +  21.6455
      9   0.2261   0.6927        0.2516       0.9081        [94m0.3288[0m     +  21.7605
     10   0.2173   0.6948        [35m0.2500[0m       0.9081        [94m0.3194[0m     +  21.8421
     11   0.2358   0.6928        0.2515       0.9081        [94m0.3148[0m     +  21.6996
     12   0.2256   [32m0.6986[0m        [35m0.2498[0m       0.9081        [94m0.3064[0m     +  21.8433
     13   [36m0.2478[0m   0.6972        0.2508       0.9081        [94m0.2953[0m     +  21.6611
     14   0.2247   0.6966        0.2501       0.9081        0.2999        21.8314
     15   0.2376   0.6981        [35m0.2494[0m       0.9081        [94m0.2852[0m     +  21.8066
     16   0.2187   0.6981        [35m0.2486[0m       0.9081        0.2867        21.7383
     17   0.2238   0.6970        [35m0.2483[0m       0.9081        0.2859        21.7171
     18   [36m0.2510[0m   [32m0.7048[0m        [35m0.2464[0m       0.9081        0.2854        21.7633
     19   0.2487   0.7026        0.2482       0.9081        [94m0.2836[0m     +  21.7865
     20   0.2455   0.6990        0.2483       0.9081        0.2844        21.7405
     21   0.2445   0.7043        0.2471       0.9081        [94m0.2830[0m     +  21.7821
     22   [36m0.2525[0m   [32m0.7052[0m        0.2481       0.9081        0.2861        21.7577
     23   0.2493   0.7007        0.2480       0.9081        0.2831        21.7455
     24   0.2458   0.6958        0.2473       0.9081        0.2841        22.5166
     25   0.2467   0.6962        0.2472       0.9081        [94m0.2830[0m     +  21.7031
     26   0.2521   0.6936        [35m0.2455[0m       0.9081        [94m0.2829[0m     +  21.7756
     27   0.2425   0.6977        0.2465       0.9081        [94m0.2828[0m     +  21.6091
     28   0.2477   0.6961        0.2474       0.9081        [94m0.2826[0m     +  21.7018
     29   0.2436   0.6988        0.2470       0.9081        0.2839        21.7746
     30   0.2418   0.6989        0.2462       0.9081        [94m0.2825[0m     +  21.8072
     31   0.2319   0.6946        [35m0.2449[0m       0.9081        0.2829        21.7311
     32   0.2391   0.6958        0.2475       0.9081        [94m0.2825[0m     +  21.8022
     33   0.2349   0.6921        0.2463       0.9081        0.2827        21.8065
     34   0.2436   0.6926        0.2460       0.9081        0.2825        21.7102
     35   0.2439   0.6941        0.2453       0.9081        0.2840        21.7670
     36   0.2461   0.6973        0.2466       0.9081        0.2832        21.6124
     37   0.2393   0.7002        0.2458       0.9081        0.2831        21.8671
     38   0.2407   0.6955        0.2459       0.9081        0.2837        21.7798
     39   0.2394   0.6969        0.2459       0.9081        0.2839        21.6707
     40   0.2253   0.6968        0.2465       0.9081        0.2862        21.8604
     41   0.2297   0.6949        0.2471       0.9081        0.2864        21.6630
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 22:36:51,277][0m Trial 225 finished with value: 0.2824831089002622 and parameters: {'lr': 0.000950881908069491, 'dropout': 0.4404815427529027, 'd_model_multiplier': 8, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 344, 'batch_size': 15, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 22}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 63
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.0636[0m   [32m0.4245[0m        [35m0.6156[0m       [31m0.9274[0m        [94m0.5142[0m     +  9.3520
      2   0.0630   0.4216        [35m0.5062[0m       0.9274        [94m0.4232[0m     +  9.6456
      3   [36m0.0661[0m   [32m0.4392[0m        [35m0.4365[0m       0.9274        [94m0.3695[0m     +  9.3101
      4   [36m0.0685[0m   [32m0.4617[0m        [35m0.3906[0m       0.9274        [94m0.3362[0m     +  9.5761
      5   [36m0.0720[0m   [32m0.4877[0m        [35m0.3623[0m       0.9274        [94m0.3131[0m     +  9.5671
      6   [36m0.0798[0m   [32m0.5159[0m        [35m0.3376[0m       0.9274        [94m0.2968[0m     +  9.9188
      7   [36m0.0912[0m   [32m0.5406[0m        [35m0.3222[0m       0.9274        [94m0.2850[0m     +  9.9146
      8   [36m0.1047[0m   [32m0.5615[0m        [35m0.3085[0m       0.9274        [94m0.2759[0m     +  9.9562
      9   [36m0.1156[0m   [32m0.5835[0m        [35m0.2987[0m       0.9274        [94m0.2689[0m     +  9.6483
     10   [36m0.1283[0m   [32m0.6015[0m        [35m0.2900[0m       0.9274        [94m0.2636[0m     +  10.0600
     11   [36m0.1366[0m   [32m0.6172[0m        [35m0.2826[0m       0.9274        [94m0.2592[0m     +  9.9000
     12   [36m0.1423[0m   [32m0.6317[0m        [35m0.2745[0m       0.9262        [94m0.2557[0m     +  9.8827
     13   [36m0.1489[0m   [32m0.6435[0m        [35m0.2710[0m       0.9238        [94m0.2529[0m     +  10.1633
     14   [36m0.1546[0m   [32m0.6550[0m        [35m0.2674[0m       0.9238        [94m0.2505[0m     +  9.6729
     15   [36m0.1632[0m   [32m0.6659[0m        [35m0.2630[0m       0.9226        [94m0.2483[0m     +  9.3960
     16   [36m0.1713[0m   [32m0.6757[0m        [35m0.2620[0m       0.9238        [94m0.2464[0m     +  9.4624
     17   [36m0.1772[0m   [32m0.6861[0m        [35m0.2572[0m       0.9226        [94m0.2448[0m     +  9.5607
     18   [36m0.1845[0m   [32m0.6947[0m        [35m0.2549[0m       0.9226        [94m0.2433[0m     +  9.7738
     19   [36m0.1911[0m   [32m0.7022[0m        [35m0.2531[0m       0.9226        [94m0.2420[0m     +  9.8023
     20   [36m0.1958[0m   [32m0.7081[0m        [35m0.2526[0m       0.9226        [94m0.2408[0m     +  9.7626
     21   [36m0.2011[0m   [32m0.7136[0m        [35m0.2509[0m       0.9226        [94m0.2397[0m     +  9.3959
     22   [36m0.2057[0m   [32m0.7177[0m        [35m0.2493[0m       0.9250        [94m0.2388[0m     +  9.9478
     23   [36m0.2088[0m   [32m0.7215[0m        [35m0.2453[0m       0.9262        [94m0.2381[0m     +  9.8820
     24   [36m0.2121[0m   [32m0.7253[0m        0.2456       0.9250        [94m0.2376[0m     +  9.9443
     25   [36m0.2156[0m   [32m0.7295[0m        [35m0.2448[0m       0.9250        [94m0.2370[0m     +  9.8904
     26   [36m0.2201[0m   [32m0.7326[0m        0.2452       0.9238        [94m0.2366[0m     +  9.6673
     27   [36m0.2227[0m   [32m0.7354[0m        [35m0.2433[0m       0.9226        [94m0.2361[0m     +  9.5714
     28   [36m0.2259[0m   [32m0.7382[0m        [35m0.2425[0m       0.9226        [94m0.2357[0m     +  9.6883
     29   [36m0.2280[0m   [32m0.7404[0m        [35m0.2417[0m       0.9226        [94m0.2354[0m     +  9.4747
     30   [36m0.2324[0m   [32m0.7415[0m        [35m0.2415[0m       0.9226        [94m0.2353[0m     +  9.6195
     31   0.2315   [32m0.7428[0m        0.2416       0.9214        [94m0.2353[0m     +  9.6661
     32   [36m0.2334[0m   [32m0.7443[0m        [35m0.2411[0m       0.9214        [94m0.2350[0m     +  9.7152
     33   [36m0.2375[0m   [32m0.7452[0m        [35m0.2409[0m       0.9202        0.2350        9.7988
     34   [36m0.2385[0m   [32m0.7457[0m        [35m0.2393[0m       0.9214        [94m0.2348[0m     +  9.8099
     35   [36m0.2390[0m   [32m0.7465[0m        [35m0.2389[0m       0.9202        0.2348        9.8314
     36   [36m0.2402[0m   [32m0.7475[0m        0.2394       0.9190        [94m0.2345[0m     +  9.9276
     37   [36m0.2425[0m   [32m0.7479[0m        [35m0.2383[0m       0.9190        [94m0.2344[0m     +  9.7044
     38   [36m0.2441[0m   [32m0.7483[0m        0.2384       0.9190        0.2345        9.8782
     39   [36m0.2445[0m   [32m0.7488[0m        [35m0.2377[0m       0.9190        [94m0.2344[0m     +  9.7691
     40   0.2444   [32m0.7492[0m        0.2384       0.9190        [94m0.2343[0m     +  9.6156
     41   0.2439   [32m0.7492[0m        [35m0.2376[0m       0.9190        [94m0.2343[0m     +  9.6072
     42   0.2444   [32m0.7497[0m        [35m0.2350[0m       0.9202        [94m0.2342[0m     +  9.7938
     43   0.2442   0.7497        0.2384       0.9202        0.2343        9.5525
     44   [36m0.2446[0m   [32m0.7497[0m        0.2360       0.9202        0.2344        9.6810
     45   [36m0.2456[0m   [32m0.7505[0m        0.2361       0.9202        0.2343        9.7658
     46   [36m0.2472[0m   [32m0.7510[0m        0.2369       0.9202        [94m0.2341[0m     +  9.5342
     47   0.2460   0.7507        0.2373       0.9202        0.2342        9.6385
     48   0.2467   [32m0.7512[0m        0.2352       0.9178        [94m0.2341[0m     +  9.8632
     49   0.2461   0.7508        0.2355       0.9190        0.2343        9.6271
     50   0.2464   0.7509        0.2357       0.9178        [94m0.2341[0m     +  9.7048
[32m[I 2023-05-02 22:44:59,802][0m Trial 226 finished with value: 0.23406230279006865 and parameters: {'lr': 7.230278331841934e-06, 'dropout': 0.5032461335834945, 'd_model_multiplier': 4, 'num_layers': 3, 'n_heads': 8, 'dim_feedforward': 274, 'batch_size': 58, 'pos_encoding': 'learnable', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 63}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 233
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0699[0m   [32m0.3962[0m        [35m0.4103[0m       [31m0.9190[0m        [94m0.7312[0m     +  66.7632
      2   [36m0.0923[0m   [32m0.5289[0m        [35m0.3641[0m       0.9190        [94m0.6142[0m     +  67.1201
      3   [36m0.1556[0m   [32m0.6063[0m        [35m0.3089[0m       0.9190        [94m0.4326[0m     +  67.2689
      4   [36m0.1791[0m   [32m0.6352[0m        [35m0.2775[0m       0.9190        [94m0.3770[0m     +  67.1482
      5   0.1690   [32m0.6523[0m        [35m0.2657[0m       0.9190        [94m0.3650[0m     +  67.2472
      6   [36m0.1831[0m   0.6423        [35m0.2616[0m       0.9190        [94m0.3599[0m     +  67.0659
      7   0.1676   0.6338        [35m0.2590[0m       0.9190        [94m0.3482[0m     +  67.4968
      8   0.1745   0.6493        [35m0.2570[0m       0.9190        [94m0.3272[0m     +  67.0632
      9   0.1781   0.6496        0.2571       0.9190        [94m0.3136[0m     +  67.4965
     10   0.1690   0.6380        [35m0.2565[0m       0.9190        [94m0.3054[0m     +  67.5366
     11   [36m0.1848[0m   0.6489        0.2584       0.9190        [94m0.3035[0m     +  67.5070
     12   0.1737   0.6366        [35m0.2553[0m       0.9190        0.3090        67.3322
     13   0.1776   [32m0.6620[0m        0.2555       0.9190        [94m0.2922[0m     +  67.3790
     14   0.1620   0.6396        [35m0.2551[0m       0.9190        0.2936        67.4938
     15   0.1670   0.6510        0.2555       0.9190        [94m0.2910[0m     +  67.4763
     16   0.1602   0.6334        [35m0.2534[0m       0.9190        0.2917        67.2852
     17   0.1729   [32m0.6649[0m        0.2562       0.9190        [94m0.2786[0m     +  67.0809
     18   0.1724   0.6315        0.2541       0.9190        0.2896        67.2932
     19   0.1847   0.6594        0.2537       0.9190        0.2803        67.3030
     20   [36m0.1857[0m   [32m0.6681[0m        0.2538       0.9190        [94m0.2762[0m     +  67.1422
     21   0.1754   0.6468        [35m0.2530[0m       0.9190        [94m0.2736[0m     +  67.2896
     22   [36m0.1867[0m   [32m0.6864[0m        [35m0.2522[0m       0.9190        [94m0.2653[0m     +  66.9564
     23   0.1734   0.6637        0.2532       0.9190        0.2716        67.0587
     24   0.1704   [32m0.6874[0m        0.2524       0.9190        [94m0.2635[0m     +  67.2721
     25   0.1665   0.6829        0.2529       0.9190        0.2681        67.3111
     26   0.1822   [32m0.6889[0m        [35m0.2508[0m       0.9190        0.2653        67.5764
     27   0.1757   0.6816        0.2520       0.9190        0.2665        67.3453
     28   0.1800   [32m0.6924[0m        [35m0.2487[0m       0.9190        [94m0.2615[0m     +  66.9190
     29   0.1807   0.6915        0.2497       0.9190        0.2627        67.2059
     30   0.1787   0.6824        0.2501       0.9190        0.2653        67.0806
     31   0.1806   [32m0.6928[0m        0.2505       0.9190        0.2627        66.9188
     32   0.1841   [32m0.6939[0m        0.2508       0.9190        [94m0.2607[0m     +  66.8615
     33   0.1865   0.6905        0.2487       0.9190        0.2671        67.2694
     34   0.1860   [32m0.6957[0m        0.2516       0.9190        [94m0.2601[0m     +  67.0649
     35   [36m0.1934[0m   [32m0.6989[0m        0.2496       0.9190        [94m0.2589[0m     +  67.3548
     36   0.1867   0.6970        0.2502       0.9190        [94m0.2584[0m     +  67.2164
     37   0.1815   0.6923        0.2504       0.9190        0.2604        67.2114
     38   0.1837   0.6907        0.2495       0.9190        0.2602        67.2079
     39   0.1807   0.6924        0.2503       0.9190        0.2608        67.3639
     40   0.1812   0.6962        0.2493       0.9190        0.2585        67.2140
     41   0.1865   0.6930        0.2496       0.9190        0.2596        67.1549
     42   0.1877   0.6963        0.2494       0.9190        0.2587        66.9514
     43   0.1874   0.6978        [35m0.2483[0m       0.9190        0.2587        66.9730
     44   0.1844   0.6982        0.2493       0.9190        0.2586        67.1889
     45   0.1856   0.6983        0.2484       0.9190        [94m0.2581[0m     +  67.2046
     46   0.1830   0.6978        0.2485       0.9190        [94m0.2580[0m     +  67.2270
     47   0.1802   0.6941        0.2486       0.9190        0.2588        67.2424
     48   0.1848   0.6976        0.2484       0.9190        0.2590        67.0088
     49   0.1848   0.6973        0.2490       0.9190        0.2590        67.1891
     50   0.1847   0.6958        0.2489       0.9190        0.2593        67.1475
[32m[I 2023-05-02 23:41:28,274][0m Trial 227 finished with value: 0.25799551013825595 and parameters: {'lr': 0.0007187496198949496, 'dropout': 0.4708772043719489, 'd_model_multiplier': 32, 'num_layers': 7, 'n_heads': 32, 'dim_feedforward': 199, 'batch_size': 9, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 233}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 225
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2275[0m   [32m0.7470[0m        [35m0.2853[0m       [31m0.9166[0m        [94m0.2511[0m     +  21.4070
      2   0.2130   0.7351        [35m0.2615[0m       [31m0.9202[0m        0.2565        21.4720
      3   [36m0.2815[0m   [32m0.7544[0m        [35m0.2525[0m       0.9166        [94m0.2385[0m     +  21.2421
      4   [36m0.2830[0m   [32m0.7584[0m        [35m0.2478[0m       [31m0.9262[0m        0.2436        21.4348
      5   [36m0.2941[0m   [32m0.7600[0m        [35m0.2438[0m       0.9250        0.2556        21.6129
      6   [36m0.3180[0m   [32m0.7608[0m        [35m0.2427[0m       0.9262        0.2391        21.7427
      7   [36m0.3252[0m   [32m0.7644[0m        0.2435       [31m0.9274[0m        0.2438        21.7474
      8   0.3166   [32m0.7665[0m        [35m0.2362[0m       0.9250        0.2426        21.4724
      9   0.3211   0.7605        [35m0.2342[0m       0.9262        0.2532        21.3124
     10   0.3046   0.7651        0.2366       0.9238        0.2430        21.6961
     11   0.3183   0.7610        [35m0.2324[0m       [31m0.9287[0m        0.2519        21.4468
     12   [36m0.3364[0m   0.7654        0.2345       0.9262        0.2387        21.3423
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 23:46:08,091][0m Trial 228 finished with value: 0.2384942468377382 and parameters: {'lr': 0.0003268853026616177, 'dropout': 0.4890547637883249, 'd_model_multiplier': 8, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 267, 'batch_size': 47, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 225}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 40
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
Warning, assumed runtime error: CUDA out of memory. Tried to allocate 982.00 MiB (GPU 0; 23.70 GiB total capacity; 20.80 GiB already allocated; 259.25 MiB free; 22.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[32m[I 2023-05-02 23:46:13,146][0m Trial 229 finished with value: 100.0 and parameters: {'lr': 0.001409447674554667, 'dropout': 0.4560429895093182, 'd_model_multiplier': 64, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 331, 'batch_size': 81, 'pos_encoding': 'learnable', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 40}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 33
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.3519[0m   [32m0.7975[0m        [35m0.2580[0m       [31m0.9154[0m        [94m0.2526[0m     +  8.6302
      2   [36m0.3622[0m   [32m0.8089[0m        [35m0.2305[0m       [31m0.9166[0m        0.2566        9.6300
      3   [36m0.3698[0m   [32m0.8185[0m        [35m0.2271[0m       [31m0.9178[0m        [94m0.2501[0m     +  9.4660
      4   [36m0.3780[0m   [32m0.8241[0m        [35m0.2209[0m       [31m0.9190[0m        [94m0.2491[0m     +  9.5630
      5   0.3728   [32m0.8343[0m        [35m0.2206[0m       0.9190        [94m0.2350[0m     +  9.5264
      6   [36m0.3993[0m   [32m0.8409[0m        [35m0.2160[0m       [31m0.9226[0m        0.2394        10.0701
      7   [36m0.4145[0m   0.8404        0.2162       0.9214        [94m0.2322[0m     +  9.5995
      8   0.3744   0.8398        [35m0.2143[0m       0.9226        0.2374        9.7508
      9   0.3792   0.8232        [35m0.2119[0m       0.9226        0.2437        9.5245
     10   0.3806   0.8307        [35m0.2091[0m       0.9202        0.2417        9.6616
     11   0.3481   0.8128        0.2104       0.9202        0.2550        9.5613
     12   0.3748   0.8245        [35m0.2047[0m       0.9202        0.2520        9.7717
     13   0.3746   0.8274        0.2050       0.9226        0.2486        9.4413
     14   0.3498   0.8269        [35m0.2020[0m       0.9190        0.2494        9.9629
     15   0.3574   0.7963        [35m0.2005[0m       0.9202        0.2667        9.7009
     16   0.3453   0.7989        0.2020       0.9178        0.2648        9.8870
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 23:48:57,416][0m Trial 230 finished with value: 0.2321921671226062 and parameters: {'lr': 0.0005983675000352911, 'dropout': 0.2882311397835498, 'd_model_multiplier': 8, 'num_layers': 3, 'n_heads': 8, 'dim_feedforward': 223, 'batch_size': 71, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 33}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 38
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.3799[0m   [32m0.7977[0m        [35m0.2990[0m       [31m0.9154[0m        [94m0.3488[0m     +  8.9350
      2   [36m0.3959[0m   [32m0.8072[0m        [35m0.2627[0m       [31m0.9190[0m        [94m0.2961[0m     +  9.6256
      3   [36m0.4017[0m   [32m0.8163[0m        [35m0.2540[0m       0.9166        [94m0.2746[0m     +  9.9267
      4   [36m0.4079[0m   [32m0.8185[0m        [35m0.2521[0m       [31m0.9202[0m        [94m0.2498[0m     +  9.5718
      5   [36m0.4133[0m   0.8178        [35m0.2494[0m       [31m0.9226[0m        [94m0.2342[0m     +  9.7690
      6   [36m0.4220[0m   0.8173        [35m0.2425[0m       0.9226        [94m0.2282[0m     +  9.9786
      7   [36m0.4233[0m   [32m0.8233[0m        [35m0.2397[0m       0.9226        [94m0.2237[0m     +  9.7707
      8   [36m0.4241[0m   0.8219        [35m0.2389[0m       0.9226        [94m0.2202[0m     +  9.7382
      9   0.4240   [32m0.8264[0m        [35m0.2362[0m       [31m0.9250[0m        [94m0.2190[0m     +  9.6706
     10   [36m0.4308[0m   [32m0.8304[0m        0.2367       0.9202        [94m0.2172[0m     +  9.6340
     11   0.4209   0.8267        [35m0.2334[0m       0.9202        0.2194        9.8627
     12   0.4193   0.8267        [35m0.2324[0m       0.9214        0.2193        10.1422
     13   0.4112   0.8274        [35m0.2314[0m       0.9202        0.2216        9.8347
     14   0.4105   [32m0.8310[0m        [35m0.2312[0m       0.9202        0.2197        9.6272
     15   0.4182   [32m0.8346[0m        [35m0.2276[0m       0.9226        0.2187        9.9703
     16   0.4118   0.8338        0.2285       0.9214        0.2184        9.5499
     17   0.4227   [32m0.8356[0m        [35m0.2266[0m       0.9226        0.2178        9.7935
     18   0.4172   [32m0.8369[0m        [35m0.2252[0m       0.9238        0.2175        9.8565
     19   0.4011   0.8330        [35m0.2241[0m       0.9226        0.2187        9.8997
     20   0.4247   0.8361        0.2260       0.9250        [94m0.2162[0m     +  10.1594
     21   0.4075   0.8315        [35m0.2217[0m       0.9226        0.2182        9.9300
     22   0.3911   0.8335        0.2218       0.9250        0.2179        9.9471
     23   0.3893   0.8316        0.2233       0.9226        0.2193        9.7577
     24   0.3836   0.8266        [35m0.2190[0m       0.9214        0.2206        9.5579
     25   0.3834   0.8298        0.2200       0.9202        0.2200        9.7701
     26   0.3864   0.8270        [35m0.2182[0m       0.9214        0.2199        10.2090
     27   0.3961   0.8289        0.2198       0.9226        0.2191        9.8893
     28   0.3918   0.8270        [35m0.2154[0m       0.9250        0.2201        10.0653
     29   0.4045   0.8295        0.2215       0.9238        0.2188        10.0725
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 23:53:52,416][0m Trial 231 finished with value: 0.2162284619178276 and parameters: {'lr': 0.0007970113606457658, 'dropout': 0.5752214616296705, 'd_model_multiplier': 2, 'num_layers': 2, 'n_heads': 16, 'dim_feedforward': 209, 'batch_size': 63, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 38}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 49
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.0607[0m   [32m0.3595[0m        [35m0.5665[0m       [31m0.9226[0m        [94m0.3978[0m     +  8.5319
      2   [36m0.0735[0m   [32m0.4813[0m        [35m0.3648[0m       0.9226        [94m0.2961[0m     +  8.5462
      3   [36m0.1202[0m   [32m0.5924[0m        [35m0.2914[0m       0.9226        [94m0.2728[0m     +  8.7795
      4   [36m0.1407[0m   [32m0.6412[0m        [35m0.2716[0m       0.9226        [94m0.2650[0m     +  9.0839
      5   [36m0.2012[0m   [32m0.6996[0m        [35m0.2600[0m       0.9202        [94m0.2525[0m     +  8.9341
      6   [36m0.2229[0m   [32m0.7094[0m        [35m0.2454[0m       0.9226        [94m0.2517[0m     +  9.0098
      7   [36m0.2238[0m   [32m0.7115[0m        [35m0.2437[0m       0.9214        0.2542        8.9081
      8   0.2183   [32m0.7156[0m        [35m0.2416[0m       0.9214        0.2537        9.1681
      9   0.2188   [32m0.7186[0m        0.2430       0.9202        0.2545        9.0958
     10   0.2212   [32m0.7195[0m        [35m0.2398[0m       0.9178        0.2582        8.8270
     11   0.2224   [32m0.7216[0m        0.2403       0.9214        0.2542        9.1550
     12   [36m0.2355[0m   [32m0.7240[0m        [35m0.2367[0m       0.9202        0.2558        9.0473
     13   0.2304   [32m0.7246[0m        [35m0.2360[0m       0.9226        0.2554        9.3710
     14   0.2325   [32m0.7263[0m        [35m0.2356[0m       0.9178        0.2572        8.9934
     15   [36m0.2371[0m   [32m0.7265[0m        0.2363       0.9202        0.2553        9.0448
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-02 23:56:16,390][0m Trial 232 finished with value: 0.2516780508076265 and parameters: {'lr': 0.0004609944579492345, 'dropout': 0.5445693215484475, 'd_model_multiplier': 2, 'num_layers': 2, 'n_heads': 4, 'dim_feedforward': 230, 'batch_size': 59, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.01, 'top_n_features': 49}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 35
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1993[0m   [32m0.6454[0m        [35m0.3044[0m       [31m0.8017[0m        [94m0.5427[0m     +  19.3316
      2   [36m0.2017[0m   [32m0.6678[0m        [35m0.2928[0m       [31m0.9287[0m        [94m0.3101[0m     +  19.2352
      3   [36m0.2173[0m   [32m0.7282[0m        [35m0.2786[0m       0.9238        [94m0.2611[0m     +  19.6172
      4   0.1870   0.7158        [35m0.2734[0m       0.9287        [94m0.2242[0m     +  19.5387
      5   0.2148   [32m0.7289[0m        [35m0.2701[0m       0.9093        0.2545        19.6766
      6   0.2027   [32m0.7341[0m        [35m0.2647[0m       0.9214        0.2395        19.4981
      7   0.2151   0.7341        [35m0.2639[0m       0.9154        0.2544        19.6239
      8   0.1976   [32m0.7421[0m        [35m0.2593[0m       0.9226        0.2331        19.4474
      9   0.1936   [32m0.7496[0m        [35m0.2527[0m       0.9238        0.2297        19.5504
     10   0.1906   [32m0.7502[0m        [35m0.2508[0m       0.9250        0.2352        19.3653
     11   0.2120   0.7487        [35m0.2495[0m       [31m0.9323[0m        0.2293        19.6433
     12   0.2041   [32m0.7545[0m        [35m0.2470[0m       0.9299        [94m0.2237[0m     +  19.4784
     13   0.1937   0.7520        [35m0.2463[0m       0.9226        0.2288        19.6142
     14   0.2040   [32m0.7639[0m        0.2523       0.9323        [94m0.2231[0m     +  19.6511
     15   0.2095   [32m0.7640[0m        0.2504       0.9238        0.2400        19.6385
     16   0.2105   [32m0.7642[0m        0.2479       0.9323        [94m0.2230[0m     +  19.4556
     17   0.2080   0.7566        [35m0.2409[0m       0.9287        0.2283        19.5646
     18   0.2164   0.7588        [35m0.2401[0m       0.9274        [94m0.2230[0m     +  19.5374
     19   [36m0.2220[0m   0.7600        [35m0.2385[0m       0.9323        [94m0.2196[0m     +  19.4983
     20   0.2023   0.7458        0.2387       0.9299        [94m0.2190[0m     +  19.5723
     21   0.2163   0.7621        [35m0.2359[0m       0.9311        0.2210        19.4590
     22   0.2161   [32m0.7672[0m        0.2392       0.9311        0.2260        19.5940
     23   0.1997   0.7450        0.2374       0.9262        0.2280        19.5140
     24   0.2112   0.7590        0.2368       0.9226        0.2340        19.4385
     25   0.2027   0.7574        0.2362       0.9311        0.2223        19.3853
     26   0.2141   0.7492        [35m0.2351[0m       [31m0.9347[0m        0.2192        19.5596
     27   0.2159   0.7604        0.2400       0.9323        0.2250        20.9308
     28   0.2175   0.7625        0.2391       [31m0.9359[0m        0.2272        19.5950
     29   0.2189   0.7556        [35m0.2343[0m       0.9335        0.2231        19.5394
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 00:06:04,711][0m Trial 233 finished with value: 0.21902460533733667 and parameters: {'lr': 0.0010837949510680012, 'dropout': 0.5600587721085268, 'd_model_multiplier': 2, 'num_layers': 2, 'n_heads': 64, 'dim_feedforward': 303, 'batch_size': 21, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.001, 'top_n_features': 35}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 54
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2314[0m   [32m0.7527[0m        [35m0.3288[0m       [31m0.9238[0m        [94m0.4074[0m     +  21.8984
      2   [36m0.2605[0m   [32m0.7582[0m        [35m0.2671[0m       [31m0.9323[0m        [94m0.3153[0m     +  21.8716
      3   [36m0.3027[0m   [32m0.7804[0m        [35m0.2598[0m       0.9299        [94m0.2684[0m     +  22.5462
      4   0.2759   0.7787        [35m0.2562[0m       0.9299        [94m0.2334[0m     +  22.1830
      5   [36m0.3033[0m   [32m0.7968[0m        [35m0.2467[0m       [31m0.9347[0m        [94m0.2323[0m     +  22.3459
      6   0.2731   [32m0.8086[0m        [35m0.2458[0m       0.9250        [94m0.2226[0m     +  22.3034
      7   0.2738   0.8030        [35m0.2391[0m       0.9274        [94m0.2141[0m     +  22.4536
      8   [36m0.3161[0m   0.8002        0.2402       0.9299        0.2175        22.2504
      9   [36m0.3451[0m   [32m0.8160[0m        0.2394       0.9287        [94m0.2107[0m     +  22.2548
     10   0.3391   [32m0.8221[0m        [35m0.2377[0m       0.9274        [94m0.2071[0m     +  22.3236
     11   0.3283   [32m0.8306[0m        0.2432       0.9299        0.2077        22.2782
     12   0.3403   0.8238        0.2401       0.9335        [94m0.2052[0m     +  22.4126
     13   [36m0.3502[0m   0.8057        0.2400       0.9335        0.2080        22.4534
     14   0.3158   [32m0.8352[0m        [35m0.2364[0m       0.9323        0.2089        22.2905
     15   0.3181   0.8286        [35m0.2317[0m       0.9335        0.2110        22.5956
     16   0.3360   0.8216        0.2350       0.9274        0.2091        22.4411
     17   0.3213   0.8279        0.2322       0.9214        0.2237        22.2462
     18   0.2956   0.7853        0.2328       0.9274        0.2152        22.4516
     19   0.2877   0.7880        0.2319       0.9274        0.2161        22.4031
     20   [36m0.3560[0m   0.8164        0.2319       0.9323        0.2080        22.1204
     21   0.3190   0.8140        [35m0.2291[0m       0.9335        0.2126        22.4854
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 00:14:17,121][0m Trial 234 finished with value: 0.2052096268105824 and parameters: {'lr': 0.0005836620267634272, 'dropout': 0.4458859352365449, 'd_model_multiplier': 8, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 317, 'batch_size': 66, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 54}. Best is trial 117 with value: 0.191397856754288.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 53
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1839[0m   [32m0.7854[0m        [35m0.2678[0m       [31m0.9383[0m        [94m0.1950[0m     +  21.0803
      2   [36m0.2060[0m   [32m0.7931[0m        [35m0.2482[0m       0.9323        0.1998        21.6641
      3   [36m0.2066[0m   [32m0.7991[0m        [35m0.2430[0m       0.9250        0.2073        21.3239
      4   [36m0.2271[0m   [32m0.8057[0m        [35m0.2424[0m       0.9287        0.2023        21.5036
      5   [36m0.2437[0m   [32m0.8113[0m        [35m0.2408[0m       0.9323        [94m0.1877[0m     +  21.8421
      6   [36m0.2755[0m   0.8042        [35m0.2348[0m       0.9371        0.1930        21.6439
      7   0.2572   [32m0.8137[0m        [35m0.2341[0m       0.9347        [94m0.1845[0m     +  21.6280
      8   0.2284   [32m0.8173[0m        [35m0.2306[0m       0.9335        0.1887        21.5936
      9   0.2546   [32m0.8174[0m        0.2316       0.9323        0.1874        21.4224
     10   0.2580   0.8161        [35m0.2303[0m       0.9311        0.1900        21.6348
     11   [36m0.2917[0m   [32m0.8202[0m        [35m0.2285[0m       0.9335        0.1865        21.5864
     12   0.2849   0.8160        [35m0.2277[0m       0.9323        [94m0.1837[0m     +  21.3903
     13   0.2738   0.8176        [35m0.2246[0m       0.9335        0.1865        21.6036
     14   [36m0.2984[0m   0.8155        0.2254       0.9335        [94m0.1784[0m     +  21.6605
     15   [36m0.3009[0m   [32m0.8211[0m        0.2247       0.9371        0.1785        21.7114
     16   0.2795   [32m0.8224[0m        [35m0.2218[0m       0.9359        0.1808        21.8449
     17   [36m0.3026[0m   0.8205        0.2245       0.9383        [94m0.1761[0m     +  21.8292
     18   0.2823   [32m0.8255[0m        [35m0.2209[0m       0.9323        0.1860        21.7720
     19   0.2957   0.8240        0.2237       0.9383        0.1771        21.6192
     20   0.2612   0.8161        0.2233       0.9371        0.1858        21.6896
     21   0.2922   0.8234        0.2232       [31m0.9444[0m        0.1761        21.8625
     22   0.2909   0.8230        [35m0.2196[0m       0.9335        0.1842        21.6162
     23   0.2586   0.8137        0.2200       0.9383        0.1821        21.6620
     24   0.2616   0.8093        [35m0.2185[0m       0.9359        0.1942        21.6947
     25   0.2527   0.8118        0.2187       0.9274        0.1973        21.6830
     26   0.2645   0.8204        [35m0.2163[0m       0.9359        0.1855        21.5012
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 00:24:02,004][0m Trial 235 finished with value: 0.17609698172014526 and parameters: {'lr': 0.0002801273584286464, 'dropout': 0.4371420372105658, 'd_model_multiplier': 8, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 329, 'batch_size': 68, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 53}. Best is trial 235 with value: 0.17609698172014526.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 52
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1934[0m   [32m0.7193[0m        [35m0.3585[0m       [31m0.9178[0m        [94m0.3869[0m     +  21.9008
      2   0.1860   0.7059        [35m0.2726[0m       [31m0.9262[0m        [94m0.3102[0m     +  22.4751
      3   0.1926   0.7179        [35m0.2662[0m       [31m0.9287[0m        [94m0.2952[0m     +  22.0250
      4   0.1785   [32m0.7352[0m        [35m0.2615[0m       0.9262        [94m0.2620[0m     +  22.5890
      5   0.1917   [32m0.7403[0m        [35m0.2556[0m       [31m0.9311[0m        [94m0.2560[0m     +  22.4987
      6   0.1855   0.7331        [35m0.2514[0m       0.9311        [94m0.2308[0m     +  22.5120
      7   0.1866   [32m0.7530[0m        [35m0.2452[0m       0.9299        [94m0.2301[0m     +  22.4476
      8   [36m0.2066[0m   [32m0.7558[0m        0.2466       [31m0.9335[0m        [94m0.2276[0m     +  22.4669
      9   0.1974   0.7356        [35m0.2399[0m       0.9311        0.2298        22.5622
     10   0.1941   0.7337        [35m0.2389[0m       0.9323        0.2289        22.3118
     11   0.2039   0.7427        [35m0.2381[0m       0.9274        [94m0.2264[0m     +  22.2771
     12   0.2048   0.7369        [35m0.2292[0m       0.9323        0.2270        22.3864
     13   0.2065   0.7383        0.2348       0.9287        0.2289        22.2602
     14   [36m0.2244[0m   [32m0.7612[0m        0.2301       0.9311        0.2280        22.5542
     15   0.2144   0.7527        [35m0.2271[0m       0.9323        [94m0.2218[0m     +  22.5088
     16   0.2035   0.7510        [35m0.2238[0m       0.9311        0.2385        22.5570
     17   [36m0.2325[0m   [32m0.7802[0m        0.2255       0.9262        0.2240        22.4419
     18   0.2217   [32m0.7818[0m        [35m0.2213[0m       0.9262        0.2288        22.4684
     19   0.2103   0.7548        0.2219       0.9287        0.2338        22.3086
     20   0.2254   [32m0.7834[0m        0.2252       0.9274        0.2284        22.6477
     21   [36m0.2432[0m   [32m0.7901[0m        [35m0.2206[0m       0.9226        0.2222        22.1884
     22   0.2172   0.7730        [35m0.2191[0m       0.9238        0.2361        22.4112
     23   0.2340   0.7841        [35m0.2169[0m       0.9311        0.2232        22.7093
     24   [36m0.2536[0m   [32m0.7915[0m        0.2190       0.9238        0.2246        22.4622
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 00:33:24,044][0m Trial 236 finished with value: 0.22184130340418187 and parameters: {'lr': 0.0002916013754725196, 'dropout': 0.45654454877545175, 'd_model_multiplier': 8, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 320, 'batch_size': 70, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 52}. Best is trial 235 with value: 0.17609698172014526.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 61
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.3072[0m   [32m0.7555[0m        [35m0.2677[0m       [31m0.9202[0m        [94m0.2459[0m     +  21.3631
      2   0.3021   [32m0.7634[0m        [35m0.2438[0m       0.9202        [94m0.2436[0m     +  21.1737
      3   0.2992   [32m0.7689[0m        [35m0.2423[0m       0.9190        0.2512        21.4476
      4   [36m0.3121[0m   [32m0.7699[0m        [35m0.2395[0m       0.9166        0.2631        21.4300
      5   0.3023   0.7623        [35m0.2324[0m       0.9178        0.2709        21.5775
      6   0.3010   [32m0.7729[0m        0.2325       0.9008        0.2755        21.6135
      7   [36m0.3197[0m   0.7706        [35m0.2258[0m       0.9141        0.2674        21.4233
      8   [36m0.3216[0m   [32m0.7758[0m        [35m0.2256[0m       0.9069        0.2732        21.4718
      9   [36m0.3221[0m   0.7698        [35m0.2244[0m       0.9057        0.2692        21.6431
     10   0.3068   0.7711        0.2249       0.9081        0.2715        21.6974
     11   0.3092   0.7663        0.2245       0.9069        0.2845        21.6262
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 00:37:42,440][0m Trial 237 finished with value: 0.2435934559465752 and parameters: {'lr': 0.00041636475605908433, 'dropout': 0.4451068824882733, 'd_model_multiplier': 8, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 248, 'batch_size': 66, 'pos_encoding': 'learnable', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 61}. Best is trial 235 with value: 0.17609698172014526.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 57
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
Warning, assumed runtime error: CUDA out of memory. Tried to allocate 2.02 GiB (GPU 0; 23.70 GiB total capacity; 15.88 GiB already allocated; 255.25 MiB free; 22.36 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[32m[I 2023-05-03 00:37:48,493][0m Trial 238 finished with value: 100.0 and parameters: {'lr': 0.0002466742679751637, 'dropout': 0.4736159251911276, 'd_model_multiplier': 8, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 414, 'batch_size': 171, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 57}. Best is trial 235 with value: 0.17609698172014526.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 213
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
Warning, assumed runtime error: CUDA out of memory. Tried to allocate 2.26 GiB (GPU 0; 23.70 GiB total capacity; 17.07 GiB already allocated; 1.98 GiB free; 20.63 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[32m[I 2023-05-03 00:37:55,419][0m Trial 239 finished with value: 100.0 and parameters: {'lr': 0.0003451867247131358, 'dropout': 0.42003002216710633, 'd_model_multiplier': 8, 'num_layers': 3, 'n_heads': 32, 'dim_feedforward': 367, 'batch_size': 191, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 213}. Best is trial 235 with value: 0.17609698172014526.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 52
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.3262[0m   [32m0.7592[0m        [35m0.2733[0m       [31m0.9178[0m        [94m0.2635[0m     +  21.2333
      2   0.3024   0.7571        [35m0.2556[0m       0.9154        0.2678        21.3840
      3   0.3126   [32m0.7785[0m        [35m0.2493[0m       0.9154        [94m0.2611[0m     +  21.2804
      4   0.2960   [32m0.7841[0m        [35m0.2463[0m       0.9154        [94m0.2560[0m     +  21.9359
      5   0.3059   [32m0.7876[0m        [35m0.2426[0m       0.9129        [94m0.2499[0m     +  21.3021
      6   0.2913   0.7824        [35m0.2374[0m       0.9117        0.2678        21.7930
      7   0.2789   0.7667        0.2385       0.9141        0.2573        21.7693
      8   0.3218   [32m0.8043[0m        [35m0.2311[0m       0.9178        [94m0.2433[0m     +  21.6443
      9   0.2974   0.7878        0.2322       0.9178        0.2531        21.6090
     10   [36m0.3299[0m   0.8033        0.2331       0.9129        0.2443        21.6749
     11   0.2697   0.7456        0.2377       0.9141        0.2690        21.5493
     12   0.2005   0.6896        0.2346       0.9129        0.3158        21.6673
     13   0.2092   0.6921        0.2516       0.9129        0.2984        21.5763
     14   0.2494   0.7066        0.2430       0.9141        0.3249        22.0621
     15   0.1995   0.6938        0.2399       0.9141        0.3342        21.8785
     16   0.2064   0.6956        0.2487       0.9141        0.2955        21.7113
     17   0.2063   0.6865        0.2477       0.9141        0.2974        21.7437
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 00:44:25,862][0m Trial 240 finished with value: 0.2432960588057067 and parameters: {'lr': 0.00173084215236878, 'dropout': 0.4472910317445574, 'd_model_multiplier': 8, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 295, 'batch_size': 77, 'pos_encoding': 'learnable', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 52}. Best is trial 235 with value: 0.17609698172014526.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 66
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2401[0m   [32m0.6701[0m        [35m0.2796[0m       [31m0.9154[0m        [94m0.2990[0m     +  25.3827
      2   [36m0.3216[0m   [32m0.7486[0m        [35m0.2639[0m       [31m0.9178[0m        [94m0.2496[0m     +  25.2158
      3   0.2480   0.7191        0.2711       0.9154        0.2583        25.5518
      4   0.2412   0.7157        [35m0.2565[0m       0.9154        0.2640        25.6922
      5   0.2348   0.7153        [35m0.2545[0m       0.9154        0.2706        25.5240
      6   0.2659   0.7194        [35m0.2544[0m       0.9154        0.2747        25.5800
      7   0.2595   0.7199        0.2550       0.9154        0.2758        25.4475
      8   0.2531   0.7152        [35m0.2511[0m       0.9154        0.2796        25.5359
      9   0.2698   0.7203        0.2531       0.9154        0.2746        25.5917
     10   0.2626   0.7175        0.2522       0.9154        0.2774        25.2000
     11   0.2681   0.7231        [35m0.2508[0m       0.9154        0.2764        25.5117
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 00:49:32,494][0m Trial 241 finished with value: 0.24962741495115726 and parameters: {'lr': 0.0006140001450228134, 'dropout': 0.4324879626395417, 'd_model_multiplier': 8, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 326, 'batch_size': 66, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 66}. Best is trial 235 with value: 0.17609698172014526.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 45
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2724[0m   [32m0.7210[0m        [35m0.2732[0m       [31m0.9081[0m        [94m0.3225[0m     +  21.1486
      2   [36m0.2855[0m   [32m0.7272[0m        [35m0.2420[0m       [31m0.9093[0m        [94m0.3122[0m     +  21.2581
      3   0.2743   [32m0.7469[0m        [35m0.2367[0m       0.9093        [94m0.3087[0m     +  21.2885
      4   0.2689   0.7312        [35m0.2343[0m       0.9081        [94m0.3074[0m     +  21.2174
      5   0.2686   0.7467        [35m0.2285[0m       0.9033        0.3093        21.3225
      6   0.2592   [32m0.7723[0m        [35m0.2250[0m       0.8984        0.3156        21.6458
      7   0.2518   0.7598        0.2250       0.9021        0.3179        21.3030
      8   0.2661   0.7650        [35m0.2219[0m       0.9033        0.3271        21.2381
      9   0.2573   0.7687        0.2238       0.9033        0.3281        21.5014
     10   0.2447   0.7524        0.2237       0.8972        0.3263        21.3527
     11   0.2694   [32m0.7727[0m        [35m0.2209[0m       0.9045        0.3248        21.2691
     12   0.2644   0.7692        0.2223       0.9033        0.3134        21.1558
     13   0.2819   [32m0.7750[0m        0.2220       0.8960        0.3266        21.3702
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 00:54:31,770][0m Trial 242 finished with value: 0.3074318096852504 and parameters: {'lr': 0.0005131045958668071, 'dropout': 0.4085113306164989, 'd_model_multiplier': 8, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 343, 'batch_size': 54, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 45}. Best is trial 235 with value: 0.17609698172014526.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 54
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1560[0m   [32m0.6268[0m        [35m0.3110[0m       [31m0.9323[0m        [94m0.2746[0m     +  29.3459
      2   [36m0.1605[0m   [32m0.6626[0m        [35m0.2772[0m       0.9311        [94m0.2715[0m     +  29.5492
      3   [36m0.1686[0m   [32m0.7029[0m        [35m0.2651[0m       0.9311        [94m0.2674[0m     +  29.7953
      4   [36m0.1723[0m   [32m0.7115[0m        [35m0.2615[0m       0.9311        [94m0.2617[0m     +  29.7731
      5   [36m0.1785[0m   [32m0.7212[0m        [35m0.2585[0m       0.9323        [94m0.2551[0m     +  29.6825
      6   [36m0.1791[0m   [32m0.7239[0m        [35m0.2570[0m       0.9311        [94m0.2448[0m     +  29.7391
      7   0.1788   0.7230        [35m0.2557[0m       0.9323        0.2494        29.6514
      8   [36m0.1856[0m   0.7210        0.2561       0.9311        0.2501        29.6530
      9   0.1693   0.7038        0.2584       0.9323        0.2645        29.7114
     10   0.1813   0.7231        0.2558       0.9323        0.2461        29.7240
     11   [36m0.1968[0m   [32m0.7301[0m        0.2558       0.9311        [94m0.2418[0m     +  29.9045
     12   0.1919   0.7232        0.2558       0.9323        [94m0.2396[0m     +  29.6328
     13   0.1841   [32m0.7308[0m        [35m0.2546[0m       0.9323        [94m0.2358[0m     +  29.8843
     14   0.1911   0.7289        [35m0.2544[0m       0.9323        0.2365        29.7724
     15   0.1921   0.7259        [35m0.2535[0m       0.9323        [94m0.2319[0m     +  29.6791
     16   0.1915   0.7290        [35m0.2524[0m       0.9323        [94m0.2268[0m     +  29.8895
     17   0.1850   [32m0.7310[0m        0.2538       0.9323        0.2293        29.7587
     18   0.1933   0.7285        0.2535       0.9323        [94m0.2266[0m     +  29.7516
     19   0.1926   0.7287        0.2530       0.9323        [94m0.2238[0m     +  29.8161
     20   0.1920   0.7268        0.2532       0.9323        0.2242        29.6564
     21   0.1868   0.7277        0.2534       0.9323        0.2252        29.7299
     22   0.1842   0.7267        0.2532       0.9323        [94m0.2235[0m     +  29.7067
     23   0.1872   0.7239        [35m0.2522[0m       0.9323        0.2245        29.7585
     24   0.1919   [32m0.7321[0m        0.2541       0.9323        0.2242        29.7177
     25   0.1873   0.7268        0.2528       0.9323        0.2248        29.7322
     26   0.1862   0.7292        [35m0.2513[0m       0.9323        0.2255        29.7935
     27   0.1807   0.7267        0.2526       0.9323        0.2252        29.6888
     28   0.1864   0.7272        0.2518       0.9323        0.2244        29.6259
     29   [36m0.1970[0m   0.7299        0.2519       0.9323        0.2238        29.6762
     30   0.1931   0.7274        0.2526       0.9323        0.2250        29.5260
     31   0.1821   0.7278        0.2521       0.9323        0.2241        29.7911
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 01:10:25,458][0m Trial 243 finished with value: 0.22350563972900106 and parameters: {'lr': 0.0008698529161347708, 'dropout': 0.4397244889644902, 'd_model_multiplier': 8, 'num_layers': 6, 'n_heads': 32, 'dim_feedforward': 311, 'batch_size': 25, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 54}. Best is trial 235 with value: 0.17609698172014526.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 229
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.1957[0m   [32m0.7175[0m        [35m0.2865[0m       [31m0.9129[0m        [94m0.2627[0m     +  8.8543
      2   [36m0.2037[0m   0.6960        [35m0.2566[0m       [31m0.9178[0m        0.2716        9.5557
      3   [36m0.2284[0m   [32m0.7455[0m        [35m0.2532[0m       0.9178        0.2763        9.9510
      4   [36m0.2457[0m   0.7450        [35m0.2481[0m       [31m0.9190[0m        0.2811        9.9136
      5   [36m0.2722[0m   [32m0.7767[0m        [35m0.2443[0m       0.9178        0.2748        9.9083
      6   [36m0.2830[0m   [32m0.7816[0m        [35m0.2418[0m       0.9178        0.2807        9.4126
      7   [36m0.2938[0m   [32m0.7829[0m        [35m0.2392[0m       [31m0.9202[0m        [94m0.2519[0m     +  9.7579
      8   [36m0.2982[0m   0.7780        [35m0.2386[0m       0.9178        0.2849        9.9284
      9   [36m0.3128[0m   [32m0.7910[0m        [35m0.2339[0m       0.9178        0.2521        9.8462
     10   0.3090   0.7872        0.2364       0.9202        0.2558        9.5988
     11   [36m0.3251[0m   [32m0.7946[0m        0.2343       [31m0.9214[0m        [94m0.2479[0m     +  9.7118
     12   [36m0.3336[0m   [32m0.7966[0m        0.2352       0.9202        [94m0.2410[0m     +  9.6013
     13   [36m0.3390[0m   0.7860        [35m0.2311[0m       0.9190        0.2499        9.8247
     14   [36m0.3465[0m   [32m0.8002[0m        0.2340       0.9214        0.2435        9.7959
     15   [36m0.3576[0m   [32m0.8015[0m        [35m0.2276[0m       [31m0.9238[0m        [94m0.2383[0m     +  9.8583
     16   [36m0.3734[0m   [32m0.8105[0m        [35m0.2262[0m       0.9238        0.2436        9.7216
     17   0.3668   0.8097        0.2287       0.9202        0.2413        9.7787
     18   0.3603   0.8067        0.2297       0.9238        0.2456        9.9033
     19   0.3597   0.8004        0.2290       0.9226        0.2467        9.7314
     20   0.3636   0.8000        0.2281       0.9226        0.2413        9.9523
     21   0.3277   0.7843        [35m0.2249[0m       0.9178        0.2503        9.6165
     22   0.3456   0.7895        [35m0.2227[0m       0.9214        0.2449        9.7611
     23   0.3452   0.7875        0.2236       0.9202        0.2395        10.0203
     24   0.3638   0.8012        0.2256       0.9214        [94m0.2380[0m     +  9.9567
     25   0.3641   0.8062        0.2258       0.9238        0.2408        9.8661
     26   0.3634   0.8067        0.2247       0.9226        0.2390        9.6602
     27   0.3693   0.8047        [35m0.2218[0m       0.9238        [94m0.2359[0m     +  9.9182
     28   [36m0.3825[0m   0.7989        0.2223       0.9238        0.2374        9.7964
     29   0.3572   0.7884        0.2227       0.9226        0.2411        9.5850
     30   0.3674   0.7998        0.2230       0.9214        0.2386        10.0459
     31   0.3536   0.7916        0.2242       0.9214        0.2402        9.6515
     32   0.3411   0.7871        0.2228       0.9190        0.2484        9.6191
     33   0.3513   0.7870        [35m0.2192[0m       0.9202        0.2384        9.5597
     34   0.3148   0.7664        0.2201       0.9190        0.2472        9.6503
     35   0.3426   0.7763        0.2212       0.9214        0.2462        9.5073
     36   0.3703   0.7963        0.2200       0.9226        0.2368        9.9233
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 01:16:26,627][0m Trial 244 finished with value: 0.23593477728216938 and parameters: {'lr': 0.0011074734820749016, 'dropout': 0.5295215334630522, 'd_model_multiplier': 4, 'num_layers': 5, 'n_heads': 4, 'dim_feedforward': 128, 'batch_size': 60, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 229}. Best is trial 235 with value: 0.17609698172014526.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 142
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2047[0m   [32m0.7054[0m        [35m0.2923[0m       [31m0.8839[0m        [94m0.2923[0m     +  21.5811
      2   [36m0.2059[0m   [32m0.7121[0m        [35m0.2677[0m       [31m0.8875[0m        [94m0.2871[0m     +  21.4434
      3   [36m0.2362[0m   [32m0.7465[0m        [35m0.2635[0m       [31m0.8984[0m        [94m0.2734[0m     +  21.5528
      4   0.2354   [32m0.7555[0m        [35m0.2554[0m       [31m0.9008[0m        [94m0.2575[0m     +  21.7448
      5   0.2221   0.7380        [35m0.2504[0m       [31m0.9117[0m        0.2578        21.6860
      6   0.2276   0.7394        [35m0.2486[0m       0.8996        0.2613        21.7731
      7   0.2284   [32m0.7609[0m        [35m0.2432[0m       0.9081        0.2580        21.8902
      8   0.2151   0.7533        [35m0.2406[0m       0.9093        [94m0.2563[0m     +  21.7577
      9   0.2248   0.7396        [35m0.2389[0m       0.8948        0.2778        21.8715
     10   [36m0.2431[0m   0.7603        0.2390       0.8996        0.2670        21.6651
     11   0.2375   0.7597        [35m0.2380[0m       0.8996        0.2696        21.4834
     12   0.2322   0.7552        [35m0.2358[0m       0.8996        0.2766        21.7132
     13   0.2390   0.7570        0.2447       [31m0.9178[0m        [94m0.2463[0m     +  21.9461
     14   [36m0.2486[0m   [32m0.7716[0m        0.2380       0.9045        0.2560        21.5147
     15   0.2214   0.7534        [35m0.2338[0m       0.9069        0.2641        21.7307
     16   0.2377   0.7657        [35m0.2321[0m       0.9045        0.2625        21.7329
     17   [36m0.2562[0m   0.7679        [35m0.2301[0m       0.9008        0.2830        21.6632
     18   0.2555   [32m0.7735[0m        0.2335       0.9093        0.2677        21.8513
     19   0.2379   0.7413        0.2309       0.9008        0.2739        21.8879
     20   0.2291   0.7324        0.2339       [31m0.9190[0m        0.2617        21.9017
     21   [36m0.2628[0m   0.7684        0.2317       0.9129        0.2612        21.5328
     22   [36m0.2792[0m   [32m0.7757[0m        [35m0.2252[0m       0.9069        0.2653        21.6444
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 01:24:47,042][0m Trial 245 finished with value: 0.24634377319622502 and parameters: {'lr': 0.0004420125746935512, 'dropout': 0.4282336447695232, 'd_model_multiplier': 8, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 332, 'batch_size': 71, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 142}. Best is trial 235 with value: 0.17609698172014526.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 100
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2917[0m   [32m0.7680[0m        [35m0.2975[0m       [31m0.9432[0m        [94m0.2434[0m     +  17.8910
      2   0.2737   0.7622        [35m0.2769[0m       0.9420        0.3440        18.0855
      3   0.2259   0.6399        [35m0.2720[0m       0.9371        0.3967        17.9963
      4   0.2769   0.6738        0.2817       0.9383        0.3500        18.1410
      5   0.2217   0.6870        0.2822       0.9371        [94m0.2118[0m     +  18.1449
      6   0.1695   0.6195        0.2788       0.9432        0.2474        18.3284
      7   0.0922   0.6279        0.2774       0.9395        0.2454        18.1941
      8   0.2910   [32m0.8504[0m        0.2726       0.9359        [94m0.1875[0m     +  18.0550
      9   [36m0.3476[0m   0.8274        [35m0.2578[0m       0.9347        0.1939        18.0973
     10   0.2906   0.8325        0.2650       0.9214        0.2150        18.1773
     11   0.2941   0.8155        0.3224       0.9008        0.2903        18.1731
     12   0.2825   0.8445        0.3104       0.9359        0.1932        18.3110
     13   0.2761   0.8245        0.3273       0.9008        0.2775        18.1532
     14   0.3026   0.8107        0.3057       0.9226        0.2404        18.4319
     15   0.2970   0.7850        0.3121       0.9407        0.1914        18.2102
     16   0.2792   0.8444        0.3034       0.9335        [94m0.1860[0m     +  18.2790
     17   0.2619   0.8504        0.2956       0.9347        0.1897        18.0947
     18   0.2525   0.8409        0.2788       0.9262        0.2112        18.2497
     19   0.2996   0.8349        0.2809       0.9323        0.1966        18.2003
     20   0.2726   0.8219        0.2872       0.9323        0.1946        18.2045
     21   0.2699   0.8397        0.2836       0.9347        [94m0.1770[0m     +  18.2233
     22   0.3109   0.8501        0.2724       0.9407        0.1887        18.0807
     23   0.3154   0.8246        0.2766       0.9226        0.2244        17.9881
     24   0.2894   0.8433        0.2793       0.9057        0.2390        18.1465
     25   0.3132   0.8295        0.2698       0.9420        0.1902        18.0556
     26   0.2972   0.8367        0.2634       0.9202        0.2441        18.1263
     27   0.3234   0.8357        0.2602       0.9287        0.2144        18.2326
     28   0.3116   0.8369        0.2615       0.9359        0.1927        18.3707
     29   0.3209   0.8413        0.2578       0.9359        0.1965        18.1991
     30   0.3317   0.8345        [35m0.2546[0m       [31m0.9456[0m        [94m0.1722[0m     +  18.1014
     31   0.3347   0.8283        0.2557       0.9371        0.1891        18.1617
     32   0.3211   0.8256        0.2546       0.9456        0.1769        18.2330
     33   0.3178   0.8239        0.2571       0.9395        0.1836        18.0554
     34   0.3131   0.8423        [35m0.2542[0m       0.9420        0.1751        18.0169
     35   0.3422   0.8334        [35m0.2463[0m       0.9420        0.1747        18.0592
     36   0.3400   0.8385        0.2470       0.9420        0.1771        18.0630
     37   [36m0.3524[0m   [32m0.8551[0m        0.2491       0.9444        [94m0.1674[0m     +  18.0635
     38   [36m0.4063[0m   0.8544        [35m0.2449[0m       0.9444        [94m0.1645[0m     +  18.1664
     39   0.3161   0.8508        [35m0.2413[0m       0.9323        0.1865        18.2300
     40   0.3470   0.8534        0.2447       0.9456        0.1679        18.2771
     41   0.3696   0.8501        0.2418       [31m0.9492[0m        0.1681        18.2675
     42   0.3416   0.8440        0.2438       0.9383        0.1822        18.0991
     43   0.3645   0.8276        0.2422       0.9456        0.1693        18.1160
     44   0.3631   0.8494        0.2460       0.9456        0.1736        18.1364
     45   0.3437   0.8533        [35m0.2397[0m       0.9347        0.1819        18.2842
     46   0.3509   0.8494        [35m0.2372[0m       0.9407        0.1800        18.2383
     47   0.3581   0.8511        0.2404       0.9335        0.1876        18.3064
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 01:39:21,068][0m Trial 246 finished with value: 0.16447311791139763 and parameters: {'lr': 0.0006902481260359048, 'dropout': 0.46203984390654546, 'd_model_multiplier': 8, 'num_layers': 3, 'n_heads': 32, 'dim_feedforward': 331, 'batch_size': 18, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 100}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 107
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2386[0m   [32m0.7169[0m        [35m0.2792[0m       [31m0.9166[0m        [94m0.2765[0m     +  18.0472
      2   0.2108   [32m0.7230[0m        [35m0.2578[0m       0.9021        0.3491        18.0138
      3   0.1299   0.5958        0.2650       0.9021        0.4582        18.1907
      4   0.1825   0.6958        0.2620       0.9057        0.4565        18.0930
      5   0.1597   0.5518        0.2680       0.9033        0.3852        18.0779
      6   0.1274   0.5946        0.2595       [31m0.9190[0m        0.3140        18.0376
      7   [36m0.2458[0m   [32m0.7526[0m        0.2640       [31m0.9262[0m        [94m0.2387[0m     +  18.0109
      8   0.2220   0.7328        0.2672       0.9190        0.2441        18.1334
      9   0.2406   [32m0.7708[0m        0.2600       0.9178        0.2423        18.0674
     10   [36m0.2840[0m   0.7571        [35m0.2565[0m       0.9262        [94m0.2314[0m     +  18.0772
     11   0.2695   0.7471        [35m0.2506[0m       [31m0.9274[0m        0.2349        18.0648
     12   0.2815   0.7585        [35m0.2446[0m       0.9226        0.2330        18.0044
     13   0.2524   0.7563        0.2544       0.9057        0.2550        18.0056
     14   0.2669   [32m0.7715[0m        0.2501       0.9202        0.2354        18.0548
     15   0.2369   0.7464        0.2719       0.9226        0.2357        18.0769
     16   0.2372   0.7499        0.2789       0.9226        0.2340        18.0840
     17   0.2100   0.7243        0.2621       0.9238        0.2541        18.2065
     18   0.2092   0.7167        0.2520       0.9226        0.2429        18.0222
     19   0.2342   0.7590        0.2572       0.9226        0.2356        18.3897
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 01:45:23,593][0m Trial 247 finished with value: 0.23135652047150657 and parameters: {'lr': 0.0006368889252693615, 'dropout': 0.46049063009708474, 'd_model_multiplier': 8, 'num_layers': 3, 'n_heads': 32, 'dim_feedforward': 318, 'batch_size': 19, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 107}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 99
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2208[0m   [32m0.7166[0m        [35m0.2978[0m       [31m0.9238[0m        [94m0.2716[0m     +  19.0287
      2   0.1963   0.6922        [35m0.2759[0m       [31m0.9274[0m        [94m0.2713[0m     +  19.0104
      3   0.1969   0.6788        [35m0.2740[0m       0.9166        0.3346        19.2390
      4   0.1563   0.6259        0.2766       0.8658        0.4162        19.1362
      5   0.1378   0.6220        [35m0.2733[0m       0.8996        0.4407        19.2077
      6   0.1348   0.6118        0.2789       0.9141        0.3317        18.9995
      7   0.1629   0.6122        0.2736       0.8888        0.4759        19.0832
      8   0.1338   0.5938        [35m0.2722[0m       0.8948        0.3660        19.2100
      9   0.1556   0.6234        0.2743       0.9105        0.4288        19.0820
     10   0.1903   0.6920        0.2795       0.8924        0.3687        19.1824
     11   0.1886   0.6580        0.2727       [31m0.9299[0m        0.3063        19.0233
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 01:49:13,476][0m Trial 248 finished with value: 0.2713102595486477 and parameters: {'lr': 0.0002948605179082168, 'dropout': 0.4721155131035161, 'd_model_multiplier': 8, 'num_layers': 3, 'n_heads': 32, 'dim_feedforward': 489, 'batch_size': 11, 'pos_encoding': 'learnable', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 99}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 19
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2923[0m   [32m0.7786[0m        [35m0.2675[0m       [31m0.8561[0m        [94m0.4011[0m     +  17.4893
      2   0.1727   0.6259        [35m0.2555[0m       0.2624        0.7458        17.4579
      3   0.1886   0.6707        0.2612       0.8102        0.5309        17.4612
      4   0.2374   [32m0.7908[0m        0.2634       [31m0.9274[0m        [94m0.2682[0m     +  17.6301
      5   0.2439   0.7295        0.2616       0.9262        [94m0.2577[0m     +  17.6882
      6   0.2782   0.7397        [35m0.2482[0m       0.9250        0.2597        17.6743
      7   0.2320   0.7118        [35m0.2477[0m       0.9226        0.2598        17.5964
      8   0.2794   0.7297        [35m0.2429[0m       0.9250        [94m0.2455[0m     +  17.7004
      9   0.2791   0.7455        0.2433       0.9250        0.2466        17.6978
     10   0.2786   0.7737        [35m0.2422[0m       0.9274        [94m0.2407[0m     +  18.1271
     11   [36m0.3063[0m   [32m0.8050[0m        0.2429       0.9250        [94m0.2338[0m     +  17.7117
     12   [36m0.3278[0m   0.7979        0.2531       0.9262        [94m0.2338[0m     +  17.7628
     13   0.3123   [32m0.8292[0m        0.2583       0.9250        0.2380        17.7111
     14   0.3117   0.8059        0.2937       0.9238        [94m0.2325[0m     +  17.7327
     15   0.2890   0.8106        0.2670       0.9274        [94m0.2265[0m     +  17.6446
     16   0.3014   0.7906        0.2522       0.9250        0.2360        17.6382
     17   0.2428   0.7699        0.2607       0.9238        0.2390        17.5761
     18   0.3080   0.7954        0.2524       0.9250        0.2383        17.7508
     19   0.3194   0.8035        0.2687       0.9250        0.2351        17.5914
     20   0.2986   0.8045        0.2486       0.9250        0.2322        17.5505
     21   0.3029   0.8260        0.2512       0.9226        0.2274        17.6572
     22   0.3179   0.8010        0.2554       0.9250        0.2339        17.6659
     23   0.3134   0.8038        0.2445       0.9250        0.2296        17.6788
     24   [36m0.3315[0m   0.8137        [35m0.2393[0m       0.9262        0.2300        17.5559
     25   0.3275   0.8177        0.2523       0.9274        [94m0.2214[0m     +  17.6975
     26   0.3086   0.7909        0.2482       [31m0.9287[0m        0.2370        17.7380
     27   [36m0.3388[0m   0.8260        0.2452       0.9287        0.2265        17.6583
     28   0.3308   0.8174        [35m0.2366[0m       0.9274        0.2271        17.4755
     29   0.3220   0.8190        0.2389       0.9274        0.2292        17.5817
     30   0.3194   0.8056        0.2412       0.9274        0.2305        17.5708
     31   0.3246   0.8177        [35m0.2329[0m       0.9274        0.2284        17.5354
     32   0.3250   0.8228        0.2391       0.9262        0.2264        17.8233
     33   0.3172   0.8173        0.2358       0.9274        0.2271        17.5432
     34   0.3069   0.8198        0.2331       0.9262        0.2252        17.7006
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 01:59:32,904][0m Trial 249 finished with value: 0.22144529767145268 and parameters: {'lr': 0.0012795200173388323, 'dropout': 0.45371084080273677, 'd_model_multiplier': 8, 'num_layers': 3, 'n_heads': 32, 'dim_feedforward': 283, 'batch_size': 29, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 19}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 135
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1823[0m   [32m0.6335[0m        [35m0.7129[0m       [31m0.4196[0m        [94m0.6974[0m     +  18.0980
      2   [36m0.1916[0m   [32m0.6439[0m        [35m0.7119[0m       0.4099        0.6978        18.5731
      3   [36m0.1990[0m   [32m0.6564[0m        [35m0.7074[0m       0.4123        0.6981        18.4091
      4   0.1828   0.6260        [35m0.7021[0m       [31m0.4921[0m        [94m0.6941[0m     +  18.2487
      5   0.1930   0.6367        [35m0.6928[0m       0.4837        0.6945        18.3165
      6   0.1865   0.6314        [35m0.6897[0m       [31m0.5707[0m        [94m0.6907[0m     +  18.2768
      7   0.1675   0.5989        [35m0.6835[0m       [31m0.5768[0m        [94m0.6904[0m     +  18.3690
      8   0.1733   0.6118        [35m0.6783[0m       0.5538        0.6913        18.2801
      9   0.1697   0.5966        [35m0.6710[0m       [31m0.6554[0m        [94m0.6873[0m     +  18.3952
     10   0.1749   0.5978        [35m0.6648[0m       [31m0.6566[0m        [94m0.6873[0m     +  18.3150
     11   0.1609   0.5867        [35m0.6598[0m       [31m0.7110[0m        [94m0.6852[0m     +  18.2442
     12   0.1483   0.5625        [35m0.6517[0m       [31m0.7666[0m        [94m0.6821[0m     +  18.3656
     13   0.1614   0.5834        [35m0.6478[0m       0.7582        0.6826        18.3090
     14   0.1422   0.5583        [35m0.6410[0m       [31m0.7884[0m        [94m0.6805[0m     +  18.3306
     15   0.1386   0.5480        [35m0.6351[0m       [31m0.8065[0m        [94m0.6794[0m     +  18.3322
     16   0.1377   0.5445        [35m0.6271[0m       0.7981        0.6797        18.3510
     17   0.1315   0.5337        [35m0.6245[0m       [31m0.8501[0m        [94m0.6765[0m     +  18.2643
     18   0.1358   0.5370        [35m0.6157[0m       0.8476        [94m0.6762[0m     +  18.3152
     19   0.1328   0.5258        [35m0.6111[0m       [31m0.8694[0m        [94m0.6740[0m     +  18.2734
     20   0.1306   0.5224        [35m0.6049[0m       [31m0.8718[0m        0.6744        18.4387
     21   0.1251   0.5104        [35m0.5984[0m       [31m0.8936[0m        [94m0.6710[0m     +  18.3546
     22   0.1158   0.4912        [35m0.5949[0m       [31m0.8948[0m        [94m0.6694[0m     +  18.2793
     23   0.1077   0.4803        [35m0.5895[0m       [31m0.9033[0m        [94m0.6681[0m     +  18.3085
     24   0.1075   0.4872        [35m0.5863[0m       [31m0.9057[0m        [94m0.6658[0m     +  18.4690
     25   0.1011   0.4582        [35m0.5805[0m       0.9045        0.6667        18.3504
     26   0.1136   0.4900        [35m0.5770[0m       0.9045        0.6665        18.4025
     27   0.1010   0.4689        [35m0.5709[0m       0.9057        [94m0.6628[0m     +  18.2388
     28   0.1066   0.4678        [35m0.5659[0m       [31m0.9081[0m        0.6629        18.3736
     29   0.1032   0.4592        [35m0.5611[0m       0.9081        [94m0.6620[0m     +  18.3525
     30   0.1012   0.4554        [35m0.5565[0m       0.9069        [94m0.6603[0m     +  18.4399
     31   0.0948   0.4404        [35m0.5505[0m       0.9081        [94m0.6592[0m     +  18.3265
     32   0.1017   0.4561        [35m0.5462[0m       0.9081        0.6599        18.4711
     33   0.1010   0.4532        [35m0.5414[0m       [31m0.9093[0m        [94m0.6568[0m     +  18.4518
     34   0.0889   0.4192        [35m0.5379[0m       [31m0.9117[0m        0.6573        18.2964
     35   0.0892   0.4277        [35m0.5336[0m       [31m0.9129[0m        [94m0.6532[0m     +  18.4644
     36   0.0814   0.4099        [35m0.5328[0m       0.9129        [94m0.6510[0m     +  18.3582
     37   0.0811   0.4092        [35m0.5253[0m       0.9117        [94m0.6508[0m     +  18.3838
     38   0.0879   0.4252        [35m0.5200[0m       0.9129        [94m0.6482[0m     +  18.4800
     39   0.0826   0.4075        [35m0.5177[0m       0.9129        [94m0.6472[0m     +  18.4822
     40   0.0814   0.4133        [35m0.5141[0m       0.9129        [94m0.6452[0m     +  18.2958
     41   0.0796   0.4055        [35m0.5092[0m       0.9129        0.6463        18.4019
     42   0.0789   0.3977        [35m0.5051[0m       0.9129        [94m0.6445[0m     +  18.3634
     43   0.0783   0.3964        [35m0.5037[0m       0.9129        0.6447        18.5183
     44   0.0760   0.3897        [35m0.4986[0m       0.9129        [94m0.6435[0m     +  18.4549
     45   0.0764   0.3939        [35m0.4961[0m       0.9129        [94m0.6412[0m     +  18.3487
     46   0.0747   0.3839        [35m0.4934[0m       0.9129        [94m0.6385[0m     +  18.2900
     47   0.0745   0.3744        [35m0.4889[0m       0.9129        [94m0.6385[0m     +  18.2065
     48   0.0756   0.3818        [35m0.4839[0m       0.9129        [94m0.6370[0m     +  18.2772
     49   0.0760   0.3843        [35m0.4820[0m       0.9129        [94m0.6325[0m     +  18.5051
     50   0.0758   0.3864        [35m0.4778[0m       0.9129        0.6357        18.2823
[32m[I 2023-05-03 02:14:57,804][0m Trial 250 finished with value: 0.6324968990997074 and parameters: {'lr': 1.497203087526739e-08, 'dropout': 0.5103886506771755, 'd_model_multiplier': 8, 'num_layers': 3, 'n_heads': 32, 'dim_feedforward': 394, 'batch_size': 17, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 135}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 27
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2276[0m   [32m0.7031[0m        [35m0.2812[0m       [31m0.8730[0m        [94m0.4763[0m     +  20.3816
      2   0.2126   [32m0.7259[0m        [35m0.2687[0m       0.8380        0.4987        20.2729
      3   0.2193   [32m0.7308[0m        [35m0.2603[0m       0.7944        0.5339        20.3644
      4   0.1994   0.7304        [35m0.2589[0m       [31m0.8972[0m        [94m0.3393[0m     +  20.4830
      5   0.2009   0.7173        [35m0.2491[0m       0.8585        [94m0.3261[0m     +  20.3791
      6   0.2222   0.7233        0.2554       [31m0.9069[0m        [94m0.2724[0m     +  20.6014
      7   0.2239   0.7299        0.2649       [31m0.9117[0m        [94m0.2699[0m     +  20.3662
      8   0.2194   0.7266        0.2581       0.9045        0.2712        20.1611
      9   0.2268   0.7262        0.2598       0.9081        0.2756        20.3330
     10   [36m0.2346[0m   0.7216        0.2581       0.9117        0.2722        20.3661
     11   0.2166   0.7121        0.2572       0.9033        0.2812        20.4684
     12   [36m0.2385[0m   0.7221        0.2499       [31m0.9154[0m        0.2735        20.4237
     13   [36m0.2446[0m   0.7185        0.2495       0.9057        0.2766        20.2752
     14   0.2402   0.7213        [35m0.2426[0m       0.8682        0.3222        20.1927
     15   0.2365   0.7187        [35m0.2398[0m       0.9045        0.2784        20.2042
     16   0.2350   0.7161        [35m0.2381[0m       0.9069        0.2731        20.1657
     17   0.2436   0.7217        [35m0.2371[0m       [31m0.9166[0m        [94m0.2688[0m     +  20.2829
     18   0.2374   0.7203        [35m0.2363[0m       0.9117        0.2739        20.3704
     19   0.2314   0.7201        0.2373       0.9117        0.2707        20.1529
     20   0.2399   0.7265        [35m0.2340[0m       0.9069        0.2720        20.3600
     21   0.2403   0.7224        0.2344       0.9141        0.2705        20.3524
     22   0.2408   0.7228        [35m0.2332[0m       0.9154        [94m0.2687[0m     +  20.3068
     23   [36m0.2510[0m   0.7303        [35m0.2332[0m       0.9166        [94m0.2664[0m     +  20.3167
     24   0.2315   0.7225        [35m0.2305[0m       0.9141        0.2709        20.4016
     25   0.2405   0.7268        0.2340       0.9141        0.2719        20.3828
     26   0.2465   0.7273        0.2344       0.9105        0.2686        20.3529
     27   0.2424   0.7249        0.2321       0.9141        0.2695        20.4032
     28   0.2422   0.7186        0.2329       0.9093        0.2735        20.3538
     29   0.2419   0.7269        0.2347       0.9129        0.2690        20.4191
     30   [36m0.2695[0m   0.7285        0.2318       0.9129        0.2685        20.4253
     31   0.2376   0.7276        0.2349       0.9141        0.2700        20.4137
     32   [36m0.2739[0m   0.7299        0.2331       [31m0.9178[0m        0.2686        20.3020
     33   0.2554   0.7284        0.2322       0.9141        [94m0.2662[0m     +  20.4130
     34   0.2631   [32m0.7326[0m        0.2313       0.9154        0.2686        20.3984
     35   0.2518   [32m0.7329[0m        0.2311       0.9129        0.2708        20.2938
     36   0.2460   0.7255        [35m0.2300[0m       0.9129        0.2720        20.3191
     37   0.2441   0.7301        0.2312       0.9069        0.2797        20.2042
     38   0.2423   0.7311        [35m0.2294[0m       0.9154        0.2721        20.1844
     39   0.2604   0.7326        0.2304       0.9166        0.2698        20.2685
     40   0.2401   0.7222        0.2328       0.9154        0.2754        20.4671
     41   0.2285   0.7203        0.2308       0.9141        0.2759        20.2322
     42   0.2277   0.7047        0.2313       0.9154        0.2796        20.3745
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 02:29:33,959][0m Trial 251 finished with value: 0.26621051912872795 and parameters: {'lr': 0.0022751821495755778, 'dropout': 0.46282260328953606, 'd_model_multiplier': 4, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 136, 'batch_size': 24, 'pos_encoding': 'learnable', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 27}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 131
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1324[0m   [32m0.7170[0m        [35m0.3433[0m       [31m0.7908[0m        [94m0.4351[0m     +  19.8119
      2   [36m0.2430[0m   [32m0.7251[0m        [35m0.3185[0m       [31m0.8549[0m        0.4517        20.3399
      3   0.1933   [32m0.7581[0m        [35m0.3068[0m       [31m0.9262[0m        [94m0.3074[0m     +  20.0919
      4   0.1926   0.6884        0.3945       0.6106        1.1668        20.4909
      5   0.0866   0.3755        0.3467       0.9117        0.3388        20.1251
      6   0.0526   0.2891        0.3357       [31m0.9287[0m        0.3792        20.0781
      7   0.1969   0.7240        0.3288       0.8742        0.5598        20.3505
      8   0.1551   0.6570        0.3218       0.8875        0.4169        20.1881
      9   0.1836   0.7268        [35m0.2939[0m       0.8331        0.3931        20.1186
     10   0.2306   0.7007        [35m0.2912[0m       0.8464        0.3927        20.2232
     11   0.2175   0.7411        0.2919       0.8960        [94m0.2955[0m     +  20.5057
     12   [36m0.2453[0m   0.7533        [35m0.2704[0m       0.9262        [94m0.2801[0m     +  20.0451
     13   0.2245   [32m0.7719[0m        [35m0.2651[0m       0.9274        [94m0.2553[0m     +  20.0266
     14   0.1910   0.7301        [35m0.2598[0m       0.8658        0.3138        20.1455
     15   0.2199   0.7577        [35m0.2589[0m       0.9238        [94m0.2517[0m     +  20.2978
     16   0.2242   0.7470        [35m0.2575[0m       0.9262        0.2775        20.1665
     17   0.2379   0.7529        [35m0.2528[0m       0.9238        0.2796        20.3531
     18   [36m0.2484[0m   [32m0.7842[0m        [35m0.2484[0m       0.8984        0.2990        20.4535
     19   0.2376   0.7839        [35m0.2455[0m       0.9250        [94m0.2494[0m     +  20.1403
     20   [36m0.2697[0m   [32m0.8067[0m        0.2460       0.9250        [94m0.2141[0m     +  20.2820
     21   0.2658   [32m0.8086[0m        [35m0.2447[0m       0.9262        0.2261        20.0989
     22   [36m0.2770[0m   0.8051        [35m0.2422[0m       0.9250        0.2190        20.1375
     23   0.2673   0.7886        [35m0.2399[0m       0.9274        0.2172        20.1686
     24   [36m0.2803[0m   0.8057        [35m0.2386[0m       0.9178        0.2341        20.0370
     25   [36m0.2938[0m   [32m0.8199[0m        [35m0.2374[0m       0.9154        0.2278        20.2054
     26   0.2813   0.8089        0.2380       0.9105        0.2534        20.0004
     27   0.2795   0.8141        [35m0.2352[0m       0.9178        0.2410        20.0085
     28   0.2714   0.7991        [35m0.2350[0m       0.9250        0.2211        20.1712
     29   0.2876   0.8109        0.2362       0.9057        0.2385        20.2743
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 02:39:41,874][0m Trial 252 finished with value: 0.21408839705777427 and parameters: {'lr': 0.0008792039851638957, 'dropout': 0.30905112055560696, 'd_model_multiplier': 8, 'num_layers': 3, 'n_heads': 32, 'dim_feedforward': 422, 'batch_size': 8, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 131}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 117
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2544[0m   [32m0.7636[0m        [35m0.2931[0m       [31m0.9166[0m        [94m0.3037[0m     +  21.7821
      2   0.1983   0.7233        [35m0.2559[0m       [31m0.9238[0m        [94m0.2865[0m     +  21.7563
      3   0.1885   0.6916        [35m0.2548[0m       [31m0.9250[0m        [94m0.2817[0m     +  21.6696
      4   0.1805   0.6689        [35m0.2435[0m       0.9226        [94m0.2772[0m     +  21.8448
      5   0.1926   0.6773        0.2453       0.9214        0.2880        21.7862
      6   0.2468   0.7000        0.2487       0.9178        0.2955        21.8500
      7   0.1551   0.6128        [35m0.2433[0m       0.9069        0.3724        21.8096
      8   0.1524   0.5735        0.2462       0.9081        0.4187        21.8230
      9   0.1733   0.5926        0.2475       0.9129        0.4183        21.8658
     10   0.2138   0.6462        0.2433       [31m0.9287[0m        0.3590        21.8556
     11   0.1688   0.5747        [35m0.2413[0m       0.9202        0.3507        21.7268
     12   0.1674   0.5774        0.2432       0.9262        0.3142        21.9605
     13   0.1067   0.4435        0.2426       0.9287        0.3585        21.6334
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 02:44:47,801][0m Trial 253 finished with value: 0.2771705156766518 and parameters: {'lr': 0.0004036953111734069, 'dropout': 0.4963650490250687, 'd_model_multiplier': 8, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 211, 'batch_size': 32, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 117}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 77
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.0793[0m   [32m0.4937[0m        [35m0.4671[0m       [31m0.9190[0m        [94m0.4118[0m     +  9.3686
      2   [36m0.2885[0m   [32m0.7644[0m        [35m0.2833[0m       0.9190        [94m0.3224[0m     +  9.5890
      3   [36m0.3052[0m   [32m0.7787[0m        [35m0.2630[0m       [31m0.9202[0m        [94m0.2773[0m     +  9.4958
      4   0.2828   [32m0.7791[0m        [35m0.2555[0m       0.9190        [94m0.2601[0m     +  9.7267
      5   0.2940   [32m0.7901[0m        [35m0.2526[0m       0.9190        [94m0.2537[0m     +  9.9230
      6   0.3017   [32m0.7960[0m        [35m0.2493[0m       0.9190        [94m0.2519[0m     +  9.9292
      7   0.3041   [32m0.8001[0m        [35m0.2452[0m       0.9190        [94m0.2465[0m     +  9.8089
      8   0.3046   [32m0.8030[0m        [35m0.2435[0m       0.9202        [94m0.2380[0m     +  9.6126
      9   [36m0.3116[0m   0.8027        [35m0.2375[0m       [31m0.9214[0m        [94m0.2378[0m     +  9.6277
     10   0.3102   0.7995        0.2401       0.9214        0.2383        9.7618
     11   [36m0.3131[0m   [32m0.8051[0m        [35m0.2363[0m       0.9214        [94m0.2345[0m     +  9.5631
     12   [36m0.3164[0m   0.7951        0.2370       0.9202        0.2360        9.4937
     13   0.3142   0.8035        0.2364       0.9190        0.2350        9.7262
     14   [36m0.3323[0m   [32m0.8134[0m        [35m0.2361[0m       0.9214        [94m0.2324[0m     +  9.6386
     15   [36m0.3330[0m   [32m0.8136[0m        [35m0.2356[0m       0.9202        0.2334        9.8631
     16   [36m0.3340[0m   [32m0.8184[0m        0.2358       0.9202        [94m0.2307[0m     +  9.8601
     17   [36m0.3412[0m   0.8138        [35m0.2334[0m       0.9178        [94m0.2297[0m     +  9.5546
     18   0.3331   0.8176        [35m0.2331[0m       0.9190        0.2309        10.1128
     19   0.3376   [32m0.8222[0m        [35m0.2297[0m       0.9178        [94m0.2286[0m     +  10.0065
     20   [36m0.3625[0m   [32m0.8256[0m        [35m0.2294[0m       0.9190        [94m0.2253[0m     +  10.0715
     21   0.3524   0.8213        [35m0.2279[0m       0.9129        0.2273        9.9336
     22   0.3513   0.8241        0.2301       0.9154        0.2262        9.7789
     23   0.3571   [32m0.8278[0m        0.2290       0.9178        [94m0.2248[0m     +  9.8419
     24   0.3611   0.8272        [35m0.2253[0m       0.9154        0.2257        9.9538
     25   [36m0.3655[0m   [32m0.8303[0m        0.2257       0.9154        [94m0.2245[0m     +  9.5419
     26   0.3610   0.8283        [35m0.2238[0m       0.9178        [94m0.2243[0m     +  9.8455
     27   [36m0.3681[0m   0.8296        0.2264       0.9214        0.2258        9.6654
     28   [36m0.3811[0m   [32m0.8337[0m        [35m0.2236[0m       [31m0.9238[0m        [94m0.2226[0m     +  9.9706
     29   0.3733   0.8294        [35m0.2233[0m       0.9226        0.2244        9.7152
     30   0.3761   0.8323        [35m0.2192[0m       0.9190        [94m0.2225[0m     +  9.5496
     31   0.3795   0.8324        0.2214       0.9178        [94m0.2220[0m     +  9.9165
     32   0.3797   0.8328        [35m0.2188[0m       0.9190        [94m0.2215[0m     +  9.8641
     33   0.3657   [32m0.8343[0m        0.2221       0.9178        0.2237        9.7519
     34   0.3765   0.8307        0.2205       0.9190        0.2232        9.5954
     35   [36m0.3901[0m   [32m0.8347[0m        0.2194       0.9214        [94m0.2214[0m     +  9.7942
     36   0.3676   0.8332        [35m0.2162[0m       0.9190        0.2231        9.9244
     37   0.3728   [32m0.8370[0m        0.2200       0.9202        0.2229        9.6384
     38   0.3782   0.8368        0.2181       0.9190        0.2228        9.7710
     39   0.3785   0.8357        0.2164       0.9214        [94m0.2203[0m     +  9.8977
     40   0.3856   [32m0.8381[0m        [35m0.2148[0m       0.9202        0.2215        9.6166
     41   0.3824   0.8364        0.2161       [31m0.9262[0m        0.2204        9.8537
     42   0.3800   0.8370        [35m0.2143[0m       0.9214        0.2205        9.9485
     43   0.3861   [32m0.8408[0m        0.2148       0.9226        0.2213        9.7604
     44   0.3840   [32m0.8426[0m        [35m0.2136[0m       0.9214        0.2211        10.1962
     45   0.3871   [32m0.8438[0m        [35m0.2133[0m       0.9190        [94m0.2203[0m     +  9.8381
     46   0.3831   0.8432        [35m0.2131[0m       0.9238        0.2211        9.6572
     47   0.3846   [32m0.8448[0m        [35m0.2129[0m       0.9214        0.2225        9.7440
     48   0.3850   0.8446        [35m0.2110[0m       0.9238        0.2215        9.8177
     49   0.3859   [32m0.8453[0m        0.2122       0.9214        0.2209        9.7401
     50   0.3852   [32m0.8472[0m        0.2123       0.9238        0.2213        9.7274
[32m[I 2023-05-03 02:52:57,780][0m Trial 254 finished with value: 0.22025385014307397 and parameters: {'lr': 0.0005517159302421032, 'dropout': 0.48184390954548084, 'd_model_multiplier': 4, 'num_layers': 2, 'n_heads': 8, 'dim_feedforward': 402, 'batch_size': 101, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.01, 'top_n_features': 77}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 85
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.3490[0m   [32m0.7156[0m        [35m0.3027[0m       [31m0.9287[0m        [94m0.2477[0m     +  31.0532
      2   0.3429   0.7090        [35m0.2710[0m       0.9250        0.2557        31.2611
      3   0.3194   0.7136        [35m0.2639[0m       [31m0.9335[0m        0.2743        31.2326
      4   0.3207   0.6967        0.2651       0.9299        0.2749        31.1712
      5   0.3016   0.6686        [35m0.2624[0m       0.9287        0.3154        31.3684
      6   [36m0.3575[0m   0.6766        [35m0.2591[0m       0.9299        0.3223        31.3216
      7   0.2896   0.7000        0.2600       0.9081        0.3538        31.2338
      8   0.2874   0.6918        [35m0.2587[0m       0.9262        0.3069        31.2498
      9   0.2923   0.6955        [35m0.2568[0m       0.9287        0.2922        31.1851
     10   0.2451   0.6105        0.2592       0.9214        0.3617        31.2392
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 02:58:41,867][0m Trial 255 finished with value: 0.247683584527929 and parameters: {'lr': 0.0002010870482035013, 'dropout': 0.4492969628351782, 'd_model_multiplier': 8, 'num_layers': 3, 'n_heads': 64, 'dim_feedforward': 255, 'batch_size': 15, 'pos_encoding': 'learnable', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 85}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 57
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.3045[0m   [32m0.7623[0m        [35m0.2844[0m       [31m0.9287[0m        [94m0.2414[0m     +  9.9269
      2   [36m0.3183[0m   [32m0.7794[0m        [35m0.2556[0m       0.9262        0.2539        10.5146
      3   0.3086   0.7689        [35m0.2468[0m       0.9262        0.2840        10.3731
      4   0.2633   0.6882        [35m0.2451[0m       [31m0.9347[0m        0.3100        10.7627
      5   0.2757   0.7090        [35m0.2404[0m       0.9299        0.3821        10.6460
      6   0.2722   0.7002        0.2466       0.9178        0.3515        10.6375
      7   0.2792   0.7565        0.2461       0.9117        0.3526        10.7669
      8   0.2530   0.6959        0.2516       0.9323        0.3045        10.6460
      9   0.3054   0.7711        0.2415       0.8767        0.3926        10.6084
     10   0.3099   [32m0.7919[0m        0.2405       0.9069        0.2801        10.4709
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 03:00:37,976][0m Trial 256 finished with value: 0.241392141349947 and parameters: {'lr': 0.00171045840645196, 'dropout': 0.3966971677356514, 'd_model_multiplier': 8, 'num_layers': 1, 'n_heads': 32, 'dim_feedforward': 243, 'batch_size': 85, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 57}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 145
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2269[0m   [32m0.7708[0m        [35m0.3205[0m       [31m0.9105[0m        [94m0.3128[0m     +  16.8918
      2   [36m0.2474[0m   0.7592        [35m0.2503[0m       [31m0.9154[0m        [94m0.3040[0m     +  16.9365
      3   [36m0.2528[0m   0.7459        [35m0.2439[0m       [31m0.9178[0m        [94m0.2903[0m     +  17.1295
      4   0.2389   0.7402        [35m0.2401[0m       0.9141        0.3076        17.3068
      5   0.2450   0.7284        0.2404       [31m0.9190[0m        [94m0.2782[0m     +  17.2479
      6   0.2286   0.7223        [35m0.2336[0m       0.9190        0.2906        17.3028
      7   0.2512   0.7304        0.2346       0.9154        0.3059        17.4385
      8   0.2424   0.7339        [35m0.2328[0m       0.9154        [94m0.2744[0m     +  17.1227
      9   0.2339   0.7095        [35m0.2321[0m       0.9190        0.2912        17.3119
     10   0.2227   0.6903        [35m0.2318[0m       0.9166        0.3032        17.3449
     11   0.2395   0.6920        0.2325       0.9154        0.3117        17.2302
     12   0.2447   0.7044        0.2333       0.9129        0.3444        17.3429
     13   0.2345   0.7057        [35m0.2259[0m       0.9105        0.3247        17.2218
     14   0.2319   0.7014        0.2283       0.9178        0.3076        17.3665
     15   0.2370   0.6707        0.2274       0.9141        0.2980        17.3928
     16   0.2235   0.6549        0.2277       0.9166        0.3293        17.3519
     17   [36m0.2574[0m   0.7060        [35m0.2223[0m       0.9154        0.2841        16.9838
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 03:05:48,786][0m Trial 257 finished with value: 0.2743919920063653 and parameters: {'lr': 0.0007249270205383035, 'dropout': 0.4660797812671483, 'd_model_multiplier': 4, 'num_layers': 3, 'n_heads': 32, 'dim_feedforward': 349, 'batch_size': 65, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.01, 'top_n_features': 145}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 217
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
Warning, assumed runtime error: CUDA out of memory. Tried to allocate 2.44 GiB (GPU 0; 23.70 GiB total capacity; 21.52 GiB already allocated; 985.25 MiB free; 21.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[32m[I 2023-05-03 03:05:55,581][0m Trial 258 finished with value: 100.0 and parameters: {'lr': 0.0011313698288816561, 'dropout': 0.2118780780134212, 'd_model_multiplier': 8, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 338, 'batch_size': 206, 'pos_encoding': 'learnable', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 217}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 49
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.3137[0m   [32m0.7901[0m        [35m0.2821[0m       [31m0.9166[0m        [94m0.2392[0m     +  8.1206
      2   [36m0.3315[0m   [32m0.8040[0m        [35m0.2434[0m       0.9166        [94m0.2369[0m     +  8.2988
      3   [36m0.3500[0m   [32m0.8071[0m        [35m0.2387[0m       0.9117        0.2385        8.2548
      4   [36m0.3576[0m   [32m0.8115[0m        [35m0.2378[0m       [31m0.9214[0m        [94m0.2320[0m     +  8.3519
      5   [36m0.3628[0m   [32m0.8163[0m        [35m0.2339[0m       [31m0.9250[0m        0.2344        8.8965
      6   [36m0.3735[0m   [32m0.8203[0m        [35m0.2337[0m       [31m0.9274[0m        [94m0.2300[0m     +  8.8184
      7   0.3727   [32m0.8208[0m        0.2347       0.9274        0.2307        9.1260
      8   [36m0.3765[0m   [32m0.8223[0m        0.2367       0.9238        0.2316        9.0569
      9   0.3736   0.8193        0.2339       [31m0.9287[0m        0.2327        9.2314
     10   0.3726   0.8214        [35m0.2317[0m       0.9262        0.2335        9.4786
     11   0.3746   [32m0.8250[0m        [35m0.2292[0m       0.9226        0.2340        9.3298
     12   0.3666   [32m0.8252[0m        [35m0.2282[0m       0.9226        0.2344        9.6154
     13   0.3672   [32m0.8278[0m        0.2301       0.9238        0.2322        9.5726
     14   [36m0.3807[0m   [32m0.8292[0m        0.2290       0.9154        0.2358        9.7494
     15   [36m0.3879[0m   0.8290        [35m0.2273[0m       0.9238        [94m0.2270[0m     +  9.3643
     16   [36m0.3986[0m   [32m0.8350[0m        [35m0.2254[0m       0.9250        0.2305        9.4749
     17   [36m0.4079[0m   0.8334        0.2270       0.9238        0.2295        9.4695
     18   0.4063   [32m0.8369[0m        [35m0.2239[0m       0.9226        0.2294        9.8143
     19   0.4063   0.8330        0.2239       0.9214        0.2342        9.2889
     20   0.3969   0.8311        [35m0.2213[0m       0.9238        0.2318        9.6039
     21   0.3969   0.8326        0.2226       0.9262        0.2284        9.3047
     22   0.3980   0.8321        [35m0.2211[0m       0.9226        0.2308        9.3824
     23   0.3881   0.8342        0.2217       0.9214        0.2380        9.3357
     24   0.3987   [32m0.8443[0m        [35m0.2200[0m       0.9238        0.2287        9.7299
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 03:09:46,148][0m Trial 259 finished with value: 0.22703737207883215 and parameters: {'lr': 0.0003162004671996509, 'dropout': 0.524497476639575, 'd_model_multiplier': 8, 'num_layers': 2, 'n_heads': 8, 'dim_feedforward': 146, 'batch_size': 73, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.001, 'top_n_features': 49}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 124
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.1330[0m   [32m0.6717[0m        [35m0.3547[0m       [31m0.9299[0m        [94m0.3224[0m     +  9.1983
      2   [36m0.1615[0m   [32m0.7183[0m        [35m0.2592[0m       0.9238        [94m0.3002[0m     +  9.3767
      3   [36m0.1794[0m   [32m0.7338[0m        [35m0.2501[0m       0.9214        [94m0.2883[0m     +  9.3490
      4   [36m0.1952[0m   [32m0.7376[0m        [35m0.2474[0m       0.9262        [94m0.2769[0m     +  9.3176
      5   [36m0.2066[0m   0.7369        [35m0.2440[0m       0.9214        0.2815        9.2990
      6   [36m0.2087[0m   0.7356        [35m0.2414[0m       0.9250        [94m0.2740[0m     +  9.3464
      7   [36m0.2136[0m   0.7372        [35m0.2393[0m       0.9226        0.2757        9.4272
      8   [36m0.2142[0m   0.7292        0.2393       0.9250        [94m0.2722[0m     +  9.5799
      9   0.2106   0.7299        [35m0.2381[0m       0.9287        0.2776        9.5545
     10   [36m0.2207[0m   0.7323        [35m0.2337[0m       0.9262        0.2785        9.7681
     11   0.2191   0.7321        [35m0.2337[0m       0.9274        0.2780        9.6348
     12   [36m0.2267[0m   0.7289        [35m0.2325[0m       [31m0.9323[0m        [94m0.2716[0m     +  9.6345
     13   0.2258   0.7270        [35m0.2313[0m       0.9250        0.2751        9.6759
     14   0.2230   0.7229        [35m0.2299[0m       0.9238        0.2800        9.3815
     15   0.2196   0.7184        0.2311       0.9287        0.2777        9.5539
     16   0.2221   0.7225        [35m0.2270[0m       0.9262        0.2822        9.4537
     17   [36m0.2366[0m   0.7295        [35m0.2261[0m       0.9250        0.2834        9.3644
     18   [36m0.2396[0m   0.7244        0.2279       0.9274        0.2860        9.3573
     19   0.2291   0.7192        0.2272       0.9287        0.2757        9.2678
     20   0.2348   0.7197        [35m0.2229[0m       0.9287        0.2716        9.6109
     21   0.2314   0.7215        0.2244       0.9226        0.2848        9.3753
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 03:13:14,919][0m Trial 260 finished with value: 0.2715612587574171 and parameters: {'lr': 9.527861051784062e-05, 'dropout': 0.48188153850170334, 'd_model_multiplier': 4, 'num_layers': 1, 'n_heads': 32, 'dim_feedforward': 512, 'batch_size': 24, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 124}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 236
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1720[0m   [32m0.6078[0m        [35m0.3396[0m       [31m0.8960[0m        [94m0.2957[0m     +  16.6710
      2   [36m0.1859[0m   [32m0.7016[0m        [35m0.2857[0m       [31m0.8984[0m        [94m0.2806[0m     +  16.8721
      3   0.1793   0.6629        0.5155       0.8888        0.2960        16.8709
      4   0.1744   0.6522        0.6176       0.8948        0.2903        16.9656
      5   0.1669   0.6569        0.6164       0.8561        0.3348        16.8759
      6   0.1678   0.6543        0.5643       [31m0.9081[0m        [94m0.2790[0m     +  16.7273
      7   [36m0.1917[0m   0.6289        0.5099       [31m0.9214[0m        [94m0.2701[0m     +  16.8907
      8   0.1772   0.6544        0.5192       0.9166        0.2745        17.0005
      9   0.1793   0.6522        0.5065       0.8730        0.3299        17.0269
     10   0.1798   0.6561        0.4589       0.9105        0.2805        16.8853
     11   0.1857   0.6522        0.4506       0.9057        0.2979        16.7859
     12   0.1747   0.6523        0.4789       0.9105        0.2831        16.9456
     13   0.1482   0.5783        0.4771       0.9202        0.3072        16.8850
     14   0.1809   0.6198        0.4941       0.8996        0.3231        16.8701
     15   0.1769   0.6433        0.5075       0.9190        0.2748        16.8484
     16   0.1873   0.6446        0.4766       0.8996        0.2921        16.8572
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 03:18:02,983][0m Trial 261 finished with value: 0.2701039134311921 and parameters: {'lr': 0.0004853202585663857, 'dropout': 0.5462109767429263, 'd_model_multiplier': 32, 'num_layers': 3, 'n_heads': 16, 'dim_feedforward': 430, 'batch_size': 19, 'pos_encoding': 'learnable', 'activation': 'relu', 'norm': 'BatchNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.01, 'top_n_features': 236}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 104
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.2085[0m   [32m0.7380[0m        [35m0.2733[0m       [31m0.9105[0m        [94m0.2854[0m     +  9.8886
      2   0.1852   0.7187        [35m0.2590[0m       0.9069        0.2999        9.9842
      3   0.1865   0.7103        0.2643       0.9069        [94m0.2849[0m     +  10.1494
      4   0.1955   0.7207        [35m0.2584[0m       [31m0.9129[0m        [94m0.2799[0m     +  10.3546
      5   0.1896   0.7076        [35m0.2523[0m       0.9057        0.2969        10.1808
      6   0.1872   0.7025        0.2552       0.9081        0.2901        10.1710
      7   0.1976   0.7093        0.2542       [31m0.9154[0m        0.2898        10.5924
      8   0.1914   0.7106        0.2549       0.9129        0.2908        10.2540
      9   0.1865   0.7054        0.2526       0.9129        0.2943        10.0491
     10   0.1825   0.7036        0.2531       0.9129        0.3011        10.5149
     11   0.1850   0.7056        [35m0.2510[0m       0.9129        0.2985        10.1314
     12   0.1892   0.7074        0.2522       0.9129        0.2955        10.2569
     13   0.1824   0.7047        [35m0.2507[0m       0.9154        0.3004        9.9473
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 03:20:26,136][0m Trial 262 finished with value: 0.279854732041924 and parameters: {'lr': 0.0027016154220004637, 'dropout': 0.4416984127356728, 'd_model_multiplier': 64, 'num_layers': 4, 'n_heads': 4, 'dim_feedforward': 128, 'batch_size': 63, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'LayerNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 104}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 93
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2316[0m   [32m0.7384[0m        [35m0.2850[0m       [31m0.7074[0m        [94m0.6516[0m     +  13.8428
      2   0.1522   0.6308        [35m0.2577[0m       [31m0.8102[0m        [94m0.5541[0m     +  13.6271
      3   0.1725   0.5915        0.2590       [31m0.8501[0m        [94m0.4758[0m     +  13.7520
      4   0.1717   0.6046        [35m0.2519[0m       [31m0.8972[0m        [94m0.3910[0m     +  14.1994
      5   [36m0.2616[0m   [32m0.7604[0m        0.2551       [31m0.9117[0m        [94m0.3427[0m     +  13.9685
      6   [36m0.2791[0m   0.7452        0.2539       [31m0.9166[0m        [94m0.2576[0m     +  14.2430
      7   [36m0.2879[0m   [32m0.8149[0m        [35m0.2466[0m       0.9166        [94m0.2489[0m     +  14.2333
      8   0.2663   [32m0.8162[0m        [35m0.2424[0m       0.9093        [94m0.2425[0m     +  14.2542
      9   0.2770   [32m0.8175[0m        [35m0.2396[0m       0.8984        0.2552        14.2620
     10   [36m0.2883[0m   [32m0.8209[0m        [35m0.2394[0m       0.8972        0.2552        14.4119
     11   0.2751   0.8180        [35m0.2379[0m       0.9166        [94m0.2378[0m     +  14.3765
     12   0.2872   [32m0.8220[0m        [35m0.2360[0m       [31m0.9202[0m        [94m0.2343[0m     +  14.2626
     13   [36m0.2894[0m   0.8175        [35m0.2346[0m       0.9166        [94m0.2280[0m     +  14.5033
     14   [36m0.2994[0m   [32m0.8278[0m        [35m0.2312[0m       0.9057        0.2362        14.3037
     15   [36m0.3060[0m   0.8261        [35m0.2288[0m       0.9081        0.2355        14.1799
     16   [36m0.3206[0m   [32m0.8296[0m        [35m0.2288[0m       0.9141        0.2280        14.3634
     17   0.2915   0.8238        0.2381       0.9178        0.2294        14.2762
     18   0.2873   0.8274        0.2298       0.9117        0.2344        14.1600
     19   0.2982   0.8245        0.2310       0.9105        0.2408        14.4315
     20   0.2897   0.8222        0.2336       0.9141        0.2326        14.0225
     21   0.3038   0.8253        0.2294       0.9105        0.2351        14.3308
     22   0.3006   0.8270        0.2321       0.9141        0.2429        14.1459
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 03:25:53,712][0m Trial 263 finished with value: 0.22800183860177428 and parameters: {'lr': 0.0009146073277669634, 'dropout': 0.4995619686391888, 'd_model_multiplier': 8, 'num_layers': 2, 'n_heads': 32, 'dim_feedforward': 202, 'batch_size': 78, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 93}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 206
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2291[0m   [32m0.7676[0m        [35m0.2887[0m       [31m0.9105[0m        [94m0.2468[0m     +  20.0245
      2   0.2236   [32m0.7717[0m        [35m0.2621[0m       [31m0.9299[0m        [94m0.2233[0m     +  20.2477
      3   [36m0.2320[0m   [32m0.7823[0m        0.2627       0.9250        0.2334        20.4829
      4   [36m0.2470[0m   [32m0.7958[0m        0.2710       0.9274        [94m0.2159[0m     +  20.1314
      5   [36m0.2539[0m   [32m0.7965[0m        [35m0.2601[0m       0.9299        [94m0.2147[0m     +  20.2581
      6   [36m0.2746[0m   [32m0.8026[0m        [35m0.2509[0m       [31m0.9347[0m        [94m0.2115[0m     +  20.5188
      7   0.2677   0.8003        0.2521       0.9274        0.2139        20.6757
      8   [36m0.2754[0m   [32m0.8042[0m        [35m0.2494[0m       0.9190        0.2370        20.5544
      9   [36m0.2771[0m   [32m0.8066[0m        [35m0.2446[0m       0.9226        0.2196        20.4272
     10   [36m0.3058[0m   [32m0.8071[0m        [35m0.2422[0m       0.9287        0.2146        20.3839
     11   0.2828   0.8060        0.2427       0.9311        0.2119        20.5139
     12   0.2995   [32m0.8114[0m        [35m0.2413[0m       0.9250        0.2161        20.3429
     13   [36m0.3180[0m   0.8085        0.2415       [31m0.9420[0m        [94m0.2067[0m     +  20.6745
     14   0.2948   0.8055        [35m0.2409[0m       0.9323        0.2137        20.3424
     15   0.3031   0.8093        [35m0.2398[0m       0.9274        0.2146        20.2648
     16   0.3024   [32m0.8147[0m        [35m0.2390[0m       0.9250        0.2108        20.6996
     17   0.2921   0.8128        0.2415       0.9287        0.2143        20.4845
     18   0.3021   [32m0.8148[0m        [35m0.2364[0m       0.9359        [94m0.2053[0m     +  20.9949
     19   0.2980   [32m0.8172[0m        [35m0.2352[0m       0.9335        [94m0.2040[0m     +  20.7047
     20   0.3066   [32m0.8213[0m        [35m0.2350[0m       0.9214        0.2197        20.4713
     21   [36m0.3197[0m   [32m0.8217[0m        0.2354       0.9274        0.2056        20.5689
     22   0.3164   0.8142        [35m0.2341[0m       0.9250        0.2204        20.8527
     23   0.3177   0.8179        0.2352       0.9226        0.2229        20.6392
     24   0.3100   [32m0.8231[0m        0.2349       0.9129        0.2210        20.4838
     25   0.2923   0.8079        0.2360       0.8948        0.2718        20.7081
     26   0.3137   0.8220        0.2343       0.9202        0.2151        20.3788
     27   0.3095   0.8212        0.2352       0.9299        0.2078        20.5997
     28   [36m0.3259[0m   [32m0.8302[0m        [35m0.2333[0m       0.9214        0.2081        20.6660
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 03:35:49,528][0m Trial 264 finished with value: 0.20399591416805227 and parameters: {'lr': 0.001435362914520722, 'dropout': 0.5366260969791525, 'd_model_multiplier': 4, 'num_layers': 2, 'n_heads': 64, 'dim_feedforward': 417, 'batch_size': 68, 'pos_encoding': 'learnable', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 206}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 194
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2442[0m   [32m0.7421[0m        [35m0.2949[0m       [31m0.9045[0m        [94m0.4659[0m     +  23.0781
      2   0.0941   0.5401        [35m0.2607[0m       [31m0.9154[0m        0.4680        23.5579
      3   0.0898   0.4657        0.2649       0.6070        0.6647        23.5529
      4   0.1504   0.5491        [35m0.2594[0m       0.6723        0.6314        23.6029
      5   0.1750   0.6314        0.2672       [31m0.9274[0m        [94m0.3449[0m     +  23.7400
      6   0.2343   0.6906        0.2731       0.9274        0.3531        23.6652
      7   0.2118   0.7143        0.2705       0.9226        [94m0.2528[0m     +  23.8058
      8   [36m0.2791[0m   [32m0.7729[0m        0.2797       [31m0.9287[0m        [94m0.2462[0m     +  23.5486
      9   0.1301   0.5568        0.2978       0.9287        0.2851        23.3971
     10   0.2537   0.7719        0.2897       0.9250        [94m0.2421[0m     +  23.5564
     11   0.2663   [32m0.7998[0m        0.2703       0.9166        [94m0.2297[0m     +  23.6429
     12   0.2215   0.7697        0.2744       0.9250        0.2314        23.6770
     13   0.2703   [32m0.8059[0m        0.2734       0.8875        0.2968        23.3803
     14   [36m0.2934[0m   [32m0.8140[0m        [35m0.2558[0m       [31m0.9299[0m        [94m0.2204[0m     +  23.5076
     15   0.2774   0.7831        0.2678       0.9287        0.2275        23.6803
     16   [36m0.3038[0m   [32m0.8168[0m        [35m0.2488[0m       0.9299        [94m0.2163[0m     +  23.6002
     17   0.2643   0.8016        [35m0.2478[0m       0.9190        0.2295        23.5860
     18   0.3001   0.7999        0.2572       0.9299        0.2192        23.6376
     19   0.2337   0.7438        0.2740       0.9262        0.2337        24.0337
     20   0.2679   0.7879        0.3588       0.9287        0.2245        23.6419
     21   0.2757   0.7848        0.2945       0.9287        0.2237        23.8377
     22   0.2538   0.8003        0.2619       0.9262        0.2238        23.4401
     23   0.2997   0.8034        [35m0.2445[0m       0.9274        0.2165        23.6956
     24   [36m0.3079[0m   0.8048        0.2505       0.9274        [94m0.2144[0m     +  23.4571
     25   0.2522   0.7920        0.2878       0.9226        0.2296        23.3135
     26   0.2702   0.7920        0.2602       0.9287        0.2287        23.4697
     27   0.2497   0.7442        0.3030       0.9287        0.2289        23.4790
     28   0.2632   0.7928        0.2994       0.9287        0.2251        23.6571
     29   0.2769   0.8003        0.2962       0.9287        0.2248        23.6605
     30   0.2550   0.7962        0.2706       0.9262        0.2212        23.4616
     31   0.2853   0.7938        0.2687       0.9287        0.2237        23.7438
     32   0.2804   0.7976        0.2651       0.9287        0.2193        23.4950
     33   0.2737   0.7993        0.2688       0.9287        0.2261        23.5728
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 03:49:12,942][0m Trial 265 finished with value: 0.21440078902813947 and parameters: {'lr': 0.0015863450650290812, 'dropout': 0.5270072126102342, 'd_model_multiplier': 8, 'num_layers': 2, 'n_heads': 64, 'dim_feedforward': 414, 'batch_size': 69, 'pos_encoding': 'learnable', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.01, 'top_n_features': 194}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 24
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2030[0m   [32m0.7320[0m        [35m0.2764[0m       [31m0.8670[0m        [94m0.3932[0m     +  13.4787
      2   [36m0.2175[0m   [32m0.7322[0m        0.2875       [31m0.8912[0m        [94m0.3066[0m     +  13.6479
      3   0.2082   0.7246        [35m0.2674[0m       [31m0.9045[0m        0.3216        14.7194
      4   [36m0.2291[0m   [32m0.7374[0m        [35m0.2603[0m       0.8742        0.3391        13.6865
      5   0.2207   [32m0.7433[0m        [35m0.2455[0m       0.8706        0.3337        13.8140
      6   [36m0.2310[0m   [32m0.7496[0m        [35m0.2394[0m       0.8839        0.3185        13.6372
      7   [36m0.2356[0m   0.7406        [35m0.2359[0m       0.8912        [94m0.3018[0m     +  13.4500
      8   [36m0.2425[0m   [32m0.7529[0m        [35m0.2321[0m       0.8996        0.3036        13.6467
      9   0.2352   0.7484        0.2338       0.8996        [94m0.2970[0m     +  13.7705
     10   [36m0.2431[0m   [32m0.7550[0m        [35m0.2306[0m       0.9021        [94m0.2942[0m     +  13.7179
     11   0.2280   0.7425        0.2320       0.8900        0.2972        13.6477
     12   0.2308   0.7378        0.2323       0.9033        0.3074        13.7278
     13   0.2239   0.7480        [35m0.2306[0m       0.9045        0.3026        13.6426
     14   0.2335   [32m0.7567[0m        [35m0.2303[0m       [31m0.9093[0m        [94m0.2899[0m     +  13.8060
     15   0.2354   0.7459        [35m0.2257[0m       0.8972        0.2903        13.5066
     16   [36m0.2440[0m   0.7516        0.2310       0.8960        [94m0.2848[0m     +  13.5381
     17   0.2224   0.7296        0.2321       [31m0.9117[0m        [94m0.2813[0m     +  13.5590
     18   0.2381   0.7395        0.2296       0.8936        0.2829        13.5388
     19   0.2279   0.7338        0.2310       0.8972        [94m0.2809[0m     +  13.6747
     20   0.2353   0.7328        0.2289       0.9021        [94m0.2796[0m     +  13.5966
     21   0.2333   0.7311        0.2292       0.9008        [94m0.2793[0m     +  13.7448
     22   0.2329   0.7332        0.2292       0.8996        0.2818        13.6507
     23   0.2196   0.7219        0.2300       0.9021        0.2993        13.5931
     24   0.2336   0.7335        0.2309       0.9021        0.2810        13.8128
     25   [36m0.2513[0m   0.7352        0.2289       0.8996        0.3157        13.5578
     26   0.2347   0.7350        0.2296       0.8984        0.3239        13.6519
     27   0.2236   0.7312        0.2293       0.8984        0.2952        13.7055
     28   0.2372   0.7245        0.2280       0.8996        0.3060        13.7578
     29   0.2379   0.7285        0.2281       0.8900        0.3385        13.6651
     30   0.2370   0.7184        0.2264       0.8948        0.3259        13.6709
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 03:56:18,936][0m Trial 266 finished with value: 0.2793417136841414 and parameters: {'lr': 0.0020008403590979443, 'dropout': 0.5338232005547368, 'd_model_multiplier': 8, 'num_layers': 1, 'n_heads': 64, 'dim_feedforward': 422, 'batch_size': 51, 'pos_encoding': 'learnable', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 24}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 208
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2499[0m   [32m0.7187[0m        [35m0.2731[0m       [31m0.8634[0m        [94m0.5122[0m     +  20.9015
      2   0.2418   0.6647        [35m0.2619[0m       0.5695        0.6798        21.0205
      3   0.1857   0.5389        0.2669       [31m0.8815[0m        0.5561        21.2168
      4   0.2395   0.6745        0.2775       0.8730        [94m0.3864[0m     +  21.2274
      5   0.1929   0.6030        0.2750       [31m0.9105[0m        [94m0.3070[0m     +  21.3671
      6   0.2216   0.7074        0.2759       0.9045        [94m0.3043[0m     +  21.1325
      7   0.2319   0.6784        0.2950       0.8948        0.3573        21.3351
      8   0.1942   0.6930        0.2891       0.8948        0.3054        21.6581
      9   0.2157   0.6899        0.2659       0.8984        0.3076        21.4320
     10   0.2110   0.7036        [35m0.2610[0m       0.9021        [94m0.3036[0m     +  21.3033
     11   0.2278   0.7025        [35m0.2556[0m       0.9045        [94m0.2962[0m     +  21.1726
     12   0.2171   0.6790        [35m0.2523[0m       0.9021        0.3067        21.1763
     13   0.2197   0.7177        [35m0.2513[0m       0.9033        0.2973        21.2286
     14   0.2271   [32m0.7217[0m        [35m0.2468[0m       0.9105        [94m0.2925[0m     +  21.1966
     15   0.2350   0.7174        [35m0.2453[0m       0.9081        0.2964        21.1985
     16   0.2257   0.6994        [35m0.2386[0m       0.9033        0.2952        21.1926
     17   0.2407   0.6815        0.2577       0.8924        0.3062        21.0511
     18   0.2429   0.7170        0.2474       0.8924        0.2941        21.1577
     19   [36m0.2549[0m   0.7206        0.2421       [31m0.9117[0m        0.2959        21.2870
     20   0.2277   0.6743        0.2439       0.9081        0.3142        21.6016
     21   0.2179   0.7106        0.2484       0.9057        0.2942        21.1801
     22   0.2231   0.6979        0.2430       0.9093        0.2929        21.0979
     23   0.2377   0.7149        0.2450       0.9081        0.2938        21.4759
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 04:04:49,691][0m Trial 267 finished with value: 0.2925246706031975 and parameters: {'lr': 0.004077510962456748, 'dropout': 0.5590467639251255, 'd_model_multiplier': 4, 'num_layers': 2, 'n_heads': 64, 'dim_feedforward': 405, 'batch_size': 74, 'pos_encoding': 'learnable', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0, 'top_n_features': 208}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 202
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1790[0m   [32m0.7394[0m        [35m0.2974[0m       [31m0.9129[0m        [94m0.2686[0m     +  22.4214
      2   0.1664   0.7162        [35m0.2865[0m       [31m0.9178[0m        0.3297        22.4393
      3   0.1717   0.7347        0.2999       [31m0.9335[0m        0.3059        22.2565
      4   0.1635   0.7179        [35m0.2804[0m       0.9262        [94m0.2495[0m     +  22.5671
      5   0.1596   0.7114        [35m0.2677[0m       0.8972        0.2770        22.4397
      6   [36m0.1806[0m   [32m0.7484[0m        [35m0.2536[0m       0.9287        [94m0.2380[0m     +  22.5157
      7   0.1637   0.7193        0.2590       0.9178        0.2597        22.5058
      8   [36m0.1943[0m   [32m0.7593[0m        [35m0.2509[0m       0.9250        [94m0.2214[0m     +  22.5231
      9   [36m0.2136[0m   [32m0.7740[0m        [35m0.2455[0m       0.9262        0.2338        22.7331
     10   0.2012   0.7724        0.2468       0.9238        0.2332        22.3983
     11   [36m0.2274[0m   [32m0.7777[0m        0.2484       0.9299        0.2272        22.3502
     12   0.2207   0.7773        [35m0.2416[0m       0.9226        0.2347        22.6999
     13   [36m0.2538[0m   [32m0.7783[0m        0.2426       0.9129        0.2359        22.5834
     14   0.2370   0.7773        0.2428       0.9141        0.2415        22.4671
     15   [36m0.2565[0m   0.7781        0.2444       0.9335        [94m0.2136[0m     +  22.7555
     16   0.2424   [32m0.7821[0m        0.2417       0.9274        0.2285        22.3411
     17   0.2071   0.7674        0.2430       0.9274        0.2330        22.1434
     18   0.1961   0.7532        0.2435       0.9311        0.2737        22.3270
     19   0.2320   0.7696        0.2450       0.9250        0.2183        22.4236
     20   0.2220   0.7682        0.2448       0.9214        0.2208        22.2657
     21   0.2302   0.7689        [35m0.2392[0m       0.9214        0.2476        22.2080
     22   [36m0.2839[0m   0.7783        0.2404       0.9190        0.2601        22.5900
     23   [36m0.2898[0m   [32m0.7872[0m        0.2419       0.9274        0.2176        22.3636
     24   [36m0.2938[0m   [32m0.7888[0m        0.2406       0.9311        0.2226        22.4183
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 04:14:12,120][0m Trial 268 finished with value: 0.21362942436309093 and parameters: {'lr': 0.0012887235006957519, 'dropout': 0.5497979307361454, 'd_model_multiplier': 8, 'num_layers': 2, 'n_heads': 64, 'dim_feedforward': 413, 'batch_size': 59, 'pos_encoding': 'learnable', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 202}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 149
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.3457[0m   [32m0.7816[0m        [35m0.3503[0m       [31m0.9057[0m        [94m0.3806[0m     +  23.1486
      2   [36m0.3763[0m   [32m0.7874[0m        [35m0.2526[0m       [31m0.9141[0m        [94m0.3368[0m     +  23.4289
      3   0.3730   [32m0.7955[0m        [35m0.2471[0m       0.9129        [94m0.3305[0m     +  23.3125
      4   0.3682   0.7934        [35m0.2451[0m       0.9081        [94m0.3233[0m     +  23.4396
      5   0.3725   0.7880        [35m0.2394[0m       0.9081        [94m0.2992[0m     +  23.5557
      6   0.3645   [32m0.7972[0m        [35m0.2348[0m       0.9057        0.3121        23.5120
      7   0.3729   0.7971        [35m0.2316[0m       0.9081        [94m0.2956[0m     +  23.7036
      8   0.3686   0.7899        [35m0.2313[0m       0.9069        0.3002        23.4304
      9   0.3416   0.7832        [35m0.2290[0m       0.9069        [94m0.2901[0m     +  23.5712
     10   0.3307   0.7830        [35m0.2262[0m       0.9069        0.2907        23.6008
     11   0.3483   0.7773        0.2281       0.9105        0.3100        23.2327
     12   0.3515   0.7817        [35m0.2225[0m       0.9129        0.3100        23.6406
     13   0.3709   0.7858        0.2251       0.9129        0.3263        23.3666
     14   0.3613   0.7760        [35m0.2183[0m       0.9117        0.3230        23.4915
     15   0.3363   0.7731        0.2209       0.9081        0.3155        24.2819
     16   0.3697   0.7886        0.2198       0.9069        0.3562        23.6718
     17   0.3424   0.7865        0.2205       0.9069        0.3516        23.4663
     18   0.3313   0.7735        [35m0.2131[0m       0.9081        0.3488        23.6733
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 04:21:40,595][0m Trial 269 finished with value: 0.290140835755249 and parameters: {'lr': 0.00013397868313636585, 'dropout': 0.5150946158169112, 'd_model_multiplier': 8, 'num_layers': 2, 'n_heads': 64, 'dim_feedforward': 435, 'batch_size': 68, 'pos_encoding': 'learnable', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.01, 'top_n_features': 149}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 139
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.2071[0m   [32m0.6918[0m        [35m0.3475[0m       [31m0.9081[0m        [94m0.2905[0m     +  9.2125
      2   [36m0.2541[0m   [32m0.7380[0m        [35m0.2459[0m       0.8984        [94m0.2772[0m     +  9.2001
      3   [36m0.2688[0m   [32m0.7417[0m        [35m0.2395[0m       0.8996        [94m0.2762[0m     +  9.3276
      4   [36m0.2832[0m   [32m0.7446[0m        [35m0.2357[0m       0.8996        0.2821        9.2665
      5   0.2747   0.7386        [35m0.2319[0m       0.9021        0.2904        9.4423
      6   [36m0.2920[0m   [32m0.7487[0m        0.2336       0.8984        0.2829        9.2697
      7   [36m0.2941[0m   [32m0.7488[0m        0.2321       0.9008        0.2815        9.5635
      8   0.2880   0.7379        0.2320       0.8996        0.2870        9.8688
      9   [36m0.2957[0m   0.7415        [35m0.2313[0m       0.8996        0.2826        9.5546
     10   0.2833   0.7352        [35m0.2302[0m       0.9033        0.2917        9.4647
     11   0.2901   0.7408        0.2313       0.8996        0.2843        10.0470
     12   0.2868   0.7218        [35m0.2266[0m       0.8984        0.3027        9.5986
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 04:23:44,232][0m Trial 270 finished with value: 0.27615876839988324 and parameters: {'lr': 0.0007770307246315209, 'dropout': 0.41893891157310226, 'd_model_multiplier': 4, 'num_layers': 1, 'n_heads': 16, 'dim_feedforward': 139, 'batch_size': 62, 'pos_encoding': 'learnable', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 139}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 53
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2950[0m   [32m0.6524[0m        [35m0.3544[0m       [31m0.9021[0m        [94m0.3070[0m     +  10.1013
      2   [36m0.3023[0m   [32m0.6594[0m        [35m0.2683[0m       [31m0.9081[0m        0.3152        10.3755
      3   [36m0.3252[0m   [32m0.7408[0m        [35m0.2537[0m       0.9045        [94m0.2951[0m     +  10.3353
      4   [36m0.3586[0m   [32m0.7505[0m        [35m0.2442[0m       0.9069        [94m0.2893[0m     +  10.4318
      5   [36m0.3647[0m   0.7491        [35m0.2431[0m       0.9069        [94m0.2868[0m     +  10.3436
      6   0.3410   0.7486        [35m0.2380[0m       0.9021        0.2956        10.5756
      7   0.3307   0.7454        [35m0.2328[0m       [31m0.9093[0m        0.2944        10.3651
      8   [36m0.3773[0m   [32m0.7519[0m        [35m0.2322[0m       0.9057        0.2987        10.4488
      9   0.3355   0.7476        [35m0.2301[0m       0.9069        0.2897        10.5082
     10   0.3158   0.7487        [35m0.2291[0m       0.9057        [94m0.2860[0m     +  10.4313
     11   0.3211   0.7356        [35m0.2260[0m       0.9057        0.2974        10.4894
     12   0.2811   0.7471        0.2281       0.9069        0.2972        10.3337
     13   0.3262   0.7499        0.2293       0.9081        0.2882        10.5240
     14   0.2907   0.7283        0.2271       0.9045        0.3082        10.4863
     15   0.3051   0.7438        0.2289       0.9021        0.2919        10.3230
     16   0.3136   0.7392        [35m0.2232[0m       0.9045        0.2928        10.8859
     17   0.3397   [32m0.7600[0m        [35m0.2231[0m       0.9057        [94m0.2851[0m     +  10.5126
     18   0.3414   0.7583        [35m0.2228[0m       0.9069        0.2911        10.4782
     19   0.3182   0.7561        [35m0.2207[0m       0.9057        [94m0.2829[0m     +  10.6501
     20   0.3015   [32m0.7605[0m        [35m0.2203[0m       0.9033        0.2916        10.3215
     21   0.2899   0.7590        [35m0.2197[0m       0.9033        0.3011        10.6200
     22   0.3091   0.7598        0.2199       0.9045        0.2927        10.6362
     23   0.2754   0.7587        [35m0.2187[0m       0.8960        0.2982        10.4964
     24   0.3037   [32m0.7623[0m        [35m0.2175[0m       0.8984        0.2916        10.4480
     25   0.2980   [32m0.7722[0m        0.2186       0.8996        0.2925        10.3698
     26   0.2921   [32m0.7744[0m        0.2191       0.8996        0.2869        10.4666
     27   0.3017   0.7710        [35m0.2167[0m       0.9008        0.2933        10.5371
     28   0.3088   [32m0.7749[0m        0.2169       0.9033        0.2925        10.3329
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 04:28:48,513][0m Trial 271 finished with value: 0.2829154592903811 and parameters: {'lr': 0.0025476622149741357, 'dropout': 0.4534415421617118, 'd_model_multiplier': 8, 'num_layers': 8, 'n_heads': 4, 'dim_feedforward': 224, 'batch_size': 47, 'pos_encoding': 'learnable', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 53}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 68
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.3625[0m   [32m0.7832[0m        [35m0.2873[0m       [31m0.9105[0m        [94m0.2445[0m     +  19.6350
      2   0.3566   0.7725        [35m0.2487[0m       0.9093        0.2591        19.4844
      3   0.3258   0.7772        0.2522       0.9093        0.2632        19.8404
      4   0.3277   [32m0.7930[0m        [35m0.2457[0m       0.9057        0.2580        19.6980
      5   [36m0.3687[0m   0.7813        0.2473       0.9093        0.2554        19.9011
      6   0.3661   [32m0.8045[0m        [35m0.2444[0m       [31m0.9154[0m        [94m0.2422[0m     +  19.9594
      7   [36m0.3735[0m   0.7864        [35m0.2396[0m       [31m0.9178[0m        0.2572        19.8825
      8   [36m0.3923[0m   0.7843        [35m0.2378[0m       0.9129        0.2439        19.9779
      9   [36m0.4066[0m   [32m0.8142[0m        [35m0.2377[0m       0.9166        [94m0.2416[0m     +  19.8117
     10   0.3878   0.7905        [35m0.2345[0m       0.9129        0.2472        19.8343
     11   0.4031   [32m0.8176[0m        0.2353       0.9141        [94m0.2401[0m     +  19.9625
     12   0.3944   0.8062        [35m0.2332[0m       0.9178        0.2411        19.6622
     13   0.3701   0.7862        0.2334       0.9154        0.2564        19.8708
     14   0.3968   0.8071        0.2334       0.9178        0.2493        20.2844
     15   [36m0.4321[0m   [32m0.8291[0m        [35m0.2309[0m       0.9154        [94m0.2325[0m     +  19.7947
     16   0.3765   0.8156        0.2319       0.9105        0.2336        20.0698
     17   0.3758   0.7998        0.2330       0.9129        0.2677        20.1008
     18   0.3955   0.8288        0.2375       0.9166        0.2616        19.7100
     19   0.3802   0.8223        0.2317       0.9154        0.2550        20.0074
     20   0.3756   0.8107        0.2353       0.9154        0.2529        19.9098
     21   0.3732   0.8112        0.2358       0.9117        0.2510        19.8514
     22   0.3682   0.7995        0.2327       0.9105        0.2789        19.9256
     23   0.3771   0.8180        0.2316       0.9154        0.2542        19.9552
     24   0.3632   0.8050        0.2361       0.9105        0.2671        19.9173
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 04:37:06,760][0m Trial 272 finished with value: 0.2325157941540899 and parameters: {'lr': 0.001401955858912487, 'dropout': 0.5361583814486219, 'd_model_multiplier': 4, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 189, 'batch_size': 56, 'pos_encoding': 'learnable', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.01, 'top_n_features': 68}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 16
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2283[0m   [32m0.7787[0m        [35m0.3317[0m       [31m0.9274[0m        [94m0.5620[0m     +  10.7277
      2   [36m0.2285[0m   0.7623        [35m0.2631[0m       0.9129        [94m0.5480[0m     +  10.9388
      3   0.2062   0.7191        [35m0.2549[0m       0.9190        [94m0.4921[0m     +  10.8627
      4   0.2087   0.7106        [35m0.2530[0m       0.9250        [94m0.4087[0m     +  10.7867
      5   0.2285   0.7632        0.2567       [31m0.9311[0m        [94m0.4029[0m     +  11.1244
      6   [36m0.2325[0m   [32m0.7924[0m        [35m0.2487[0m       [31m0.9323[0m        [94m0.3338[0m     +  10.8397
      7   [36m0.2446[0m   [32m0.7946[0m        0.2511       0.9311        [94m0.2870[0m     +  10.7762
      8   [36m0.2515[0m   0.7900        [35m0.2484[0m       0.9311        [94m0.2351[0m     +  10.7719
      9   [36m0.2517[0m   0.7909        [35m0.2403[0m       0.9311        [94m0.2304[0m     +  10.9216
     10   0.2385   [32m0.8026[0m        [35m0.2383[0m       0.9311        0.2330        10.7233
     11   0.2441   0.8018        0.2384       0.9311        0.2368        10.8354
     12   0.2433   [32m0.8082[0m        [35m0.2353[0m       0.9311        0.2307        10.7656
     13   0.2514   [32m0.8104[0m        [35m0.2337[0m       0.9311        0.2329        10.8822
     14   0.2415   [32m0.8136[0m        [35m0.2330[0m       0.9311        [94m0.2300[0m     +  10.9369
     15   0.2456   [32m0.8222[0m        [35m0.2310[0m       0.9311        [94m0.2240[0m     +  11.0179
     16   [36m0.2549[0m   0.8203        [35m0.2289[0m       0.9311        0.2250        10.8288
     17   0.2432   0.8203        [35m0.2288[0m       0.9311        0.2318        10.9547
     18   [36m0.2551[0m   [32m0.8283[0m        [35m0.2255[0m       0.9299        0.2291        11.0231
     19   [36m0.2584[0m   [32m0.8317[0m        0.2257       0.9299        [94m0.2228[0m     +  11.0494
     20   [36m0.2610[0m   0.8294        [35m0.2240[0m       0.9311        0.2523        10.7960
     21   [36m0.2736[0m   [32m0.8350[0m        0.2247       0.9299        0.2938        10.8752
     22   [36m0.2776[0m   [32m0.8390[0m        0.2247       0.9299        0.2433        11.0727
     23   0.2578   0.8349        [35m0.2216[0m       0.9311        [94m0.2181[0m     +  11.0476
     24   0.2499   [32m0.8405[0m        0.2235       0.9299        0.2268        10.8527
     25   0.2611   0.8392        [35m0.2213[0m       0.9299        0.2442        11.0453
     26   0.2645   0.8401        0.2225       0.9299        [94m0.2090[0m     +  10.9610
     27   [36m0.2801[0m   [32m0.8443[0m        0.2225       0.9299        0.2668        10.9431
     28   0.2585   0.8384        [35m0.2196[0m       0.9299        0.2149        10.8737
     29   0.2566   0.8392        0.2213       0.9311        0.2261        10.8401
     30   0.2447   0.8359        [35m0.2196[0m       0.9311        0.2314        10.9076
     31   0.2547   0.8403        [35m0.2180[0m       0.9299        0.2121        10.9904
     32   0.2770   0.8359        0.2199       0.9299        0.2165        11.0019
     33   [36m0.2924[0m   0.8385        0.2204       0.9311        0.2232        10.8032
     34   0.2639   0.8370        0.2181       0.9311        0.2282        10.9562
     35   0.2727   0.8406        [35m0.2174[0m       0.9311        0.2536        10.7780
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 04:43:40,677][0m Trial 273 finished with value: 0.20895724528285584 and parameters: {'lr': 0.0009524236251089918, 'dropout': 0.5943478641154126, 'd_model_multiplier': 1, 'num_layers': 2, 'n_heads': 32, 'dim_feedforward': 423, 'batch_size': 38, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 16}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 19
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.3014[0m   [32m0.8003[0m        [35m0.3293[0m       [31m0.9093[0m        [94m0.3764[0m     +  10.8305
      2   0.2846   0.7973        [35m0.2476[0m       0.8912        [94m0.3660[0m     +  10.7917
      3   0.2749   0.7987        0.2524       0.8948        [94m0.3563[0m     +  10.9407
      4   0.2970   [32m0.8039[0m        0.2482       [31m0.9105[0m        [94m0.3047[0m     +  10.7672
      5   0.2730   0.8011        0.2498       0.9093        [94m0.2681[0m     +  10.6458
      6   0.2993   [32m0.8072[0m        [35m0.2411[0m       [31m0.9154[0m        [94m0.2479[0m     +  10.8069
      7   0.2627   0.8055        0.2416       0.9093        0.2517        10.9712
      8   0.2851   [32m0.8115[0m        [35m0.2381[0m       0.9105        [94m0.2383[0m     +  10.7536
      9   0.2671   0.8093        [35m0.2348[0m       0.9093        0.2399        10.9887
     10   0.2620   0.8106        0.2352       0.9093        0.2432        11.0000
     11   0.2647   0.8113        [35m0.2339[0m       0.9081        0.2401        10.9226
     12   0.2712   [32m0.8134[0m        [35m0.2308[0m       0.9081        0.2398        10.9456
     13   0.2735   0.8127        0.2325       0.9141        [94m0.2370[0m     +  10.8493
     14   0.2961   [32m0.8152[0m        [35m0.2281[0m       0.9093        [94m0.2351[0m     +  10.6015
     15   0.2869   0.8142        [35m0.2280[0m       0.9081        [94m0.2337[0m     +  10.7632
     16   0.3002   0.8139        [35m0.2233[0m       0.9117        0.2359        10.9646
     17   0.2866   [32m0.8169[0m        0.2261       0.9105        0.2385        10.8893
     18   [36m0.3283[0m   [32m0.8188[0m        0.2251       0.9141        [94m0.2296[0m     +  10.9837
     19   0.3170   0.8180        [35m0.2232[0m       0.9129        0.2312        10.8669
     20   [36m0.3340[0m   [32m0.8203[0m        [35m0.2228[0m       [31m0.9178[0m        0.2392        10.9944
     21   [36m0.3353[0m   [32m0.8225[0m        [35m0.2218[0m       0.9178        0.2320        10.8243
     22   0.3136   0.8195        [35m0.2202[0m       0.9166        0.2482        10.8737
     23   0.3291   0.8201        [35m0.2176[0m       0.9129        0.2465        10.7949
     24   0.3146   0.8219        0.2186       0.9154        0.2365        10.8692
     25   [36m0.3425[0m   0.8222        0.2183       0.9166        0.2356        10.9889
     26   0.3246   0.8214        0.2176       0.9154        0.3337        11.0108
     27   0.3255   [32m0.8233[0m        0.2190       0.9154        0.2497        10.7669
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 04:48:45,818][0m Trial 274 finished with value: 0.22961565760271724 and parameters: {'lr': 0.0010004518508535398, 'dropout': 0.5902202870511795, 'd_model_multiplier': 1, 'num_layers': 2, 'n_heads': 32, 'dim_feedforward': 431, 'batch_size': 37, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 19}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 29
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2201[0m   [32m0.7629[0m        [35m0.3387[0m       [31m0.9274[0m        [94m0.2281[0m     +  10.3607
      2   [36m0.3478[0m   [32m0.8419[0m        [35m0.2517[0m       0.9226        [94m0.2086[0m     +  10.4388
      3   [36m0.3745[0m   [32m0.8419[0m        [35m0.2431[0m       0.9226        0.2087        10.7302
      4   0.3735   [32m0.8495[0m        [35m0.2430[0m       0.9226        0.2104        10.9677
      5   0.3743   0.8494        [35m0.2404[0m       0.9214        0.2111        10.5815
      6   0.3489   0.8485        [35m0.2395[0m       0.9178        0.2143        10.7243
      7   0.3421   0.8490        [35m0.2368[0m       0.9154        0.2171        10.6085
      8   0.3507   [32m0.8500[0m        0.2393       0.9166        0.2167        10.6528
      9   0.3650   0.8471        0.2387       0.9178        0.2184        10.5793
     10   0.3491   [32m0.8503[0m        [35m0.2348[0m       0.9178        0.2163        10.5511
     11   0.3388   0.8493        0.2357       0.9178        0.2175        10.6208
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 04:50:53,702][0m Trial 275 finished with value: 0.20855162290016138 and parameters: {'lr': 0.000709115620693641, 'dropout': 0.598842843214, 'd_model_multiplier': 1, 'num_layers': 2, 'n_heads': 32, 'dim_feedforward': 421, 'batch_size': 32, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 29}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 33
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.1984[0m   [32m0.7007[0m        [35m0.3541[0m       [31m0.9299[0m        [94m0.2360[0m     +  8.9383
      2   [36m0.2650[0m   [32m0.7355[0m        [35m0.2487[0m       0.9214        0.2368        8.3873
      3   0.2556   0.7292        [35m0.2458[0m       0.9202        0.2465        9.0617
      4   [36m0.2775[0m   0.7301        [35m0.2427[0m       0.9141        0.2524        8.9462
      5   [36m0.2819[0m   [32m0.7373[0m        [35m0.2409[0m       0.9166        0.2496        9.0837
      6   0.2617   0.7331        0.2415       0.9154        0.2570        9.1710
      7   0.2593   0.7321        [35m0.2376[0m       0.9166        0.2553        8.8494
      8   0.2545   [32m0.7386[0m        0.2399       0.9129        0.2554        9.2885
      9   0.2607   [32m0.7416[0m        0.2378       0.9117        0.2634        9.0632
     10   0.2750   [32m0.7429[0m        [35m0.2372[0m       0.9190        0.2517        9.3423
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 04:52:33,376][0m Trial 276 finished with value: 0.23597909717346477 and parameters: {'lr': 0.00072269765353056, 'dropout': 0.6063046774015282, 'd_model_multiplier': 1, 'num_layers': 1, 'n_heads': 32, 'dim_feedforward': 423, 'batch_size': 35, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 33}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 27
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.4077[0m   [32m0.7891[0m        [35m0.3147[0m       [31m0.9274[0m        [94m0.3262[0m     +  10.7329
      2   [36m0.4291[0m   0.7872        [35m0.2620[0m       [31m0.9335[0m        [94m0.2724[0m     +  10.7669
      3   0.4247   0.7868        [35m0.2590[0m       0.9287        [94m0.2379[0m     +  10.9144
      4   0.3950   [32m0.7899[0m        [35m0.2559[0m       0.9262        [94m0.2318[0m     +  10.9752
      5   [36m0.4389[0m   [32m0.8012[0m        [35m0.2539[0m       0.9287        [94m0.2256[0m     +  10.7461
      6   0.4081   0.7961        [35m0.2461[0m       0.9238        [94m0.2230[0m     +  10.9656
      7   0.3886   [32m0.8053[0m        [35m0.2439[0m       0.9274        [94m0.2194[0m     +  10.6047
      8   0.4109   [32m0.8173[0m        [35m0.2377[0m       0.9238        0.2237        10.9265
      9   0.4123   0.8169        [35m0.2374[0m       0.9250        0.2235        10.9317
     10   0.4143   [32m0.8211[0m        [35m0.2361[0m       0.9250        0.2202        10.8269
     11   0.3988   [32m0.8228[0m        0.2364       0.9226        0.2248        10.8352
     12   0.4071   [32m0.8252[0m        [35m0.2312[0m       0.9238        0.2210        10.9604
     13   0.4279   0.8250        [35m0.2289[0m       0.9238        0.2236        10.9692
     14   0.4088   [32m0.8275[0m        0.2303       0.9238        0.2246        10.9293
     15   0.4042   [32m0.8301[0m        [35m0.2251[0m       0.9250        0.2283        11.3071
     16   0.4163   [32m0.8315[0m        0.2280       0.9250        0.2218        10.9265
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 04:55:39,267][0m Trial 277 finished with value: 0.2193733920425429 and parameters: {'lr': 0.0010763645693028622, 'dropout': 0.5741316625535248, 'd_model_multiplier': 1, 'num_layers': 2, 'n_heads': 32, 'dim_feedforward': 420, 'batch_size': 30, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 27}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 74
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2622[0m   [32m0.7349[0m        [35m0.3036[0m       [31m0.9154[0m        [94m0.2599[0m     +  10.4431
      2   0.2540   [32m0.7462[0m        [35m0.2366[0m       [31m0.9166[0m        [94m0.2546[0m     +  10.5660
      3   [36m0.2779[0m   [32m0.7540[0m        [35m0.2313[0m       [31m0.9178[0m        0.2553        10.6766
      4   0.2723   [32m0.7579[0m        [35m0.2297[0m       0.9105        0.2578        10.6465
      5   0.2669   [32m0.7644[0m        [35m0.2246[0m       0.9117        0.2621        10.8282
      6   0.2737   [32m0.7682[0m        [35m0.2234[0m       0.9081        0.2736        10.8095
      7   0.2732   0.7661        [35m0.2185[0m       0.9045        0.2723        10.6000
      8   0.2635   0.7583        0.2194       0.9069        0.2724        10.6456
      9   0.2746   0.7492        0.2198       0.9129        0.2729        10.9906
     10   0.2748   0.7543        [35m0.2168[0m       0.9093        0.2724        10.9651
     11   0.2686   0.7612        [35m0.2161[0m       0.9141        0.2721        10.9876
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 04:57:48,522][0m Trial 278 finished with value: 0.2545542315126114 and parameters: {'lr': 0.0013918681391676755, 'dropout': 0.4278768885010805, 'd_model_multiplier': 1, 'num_layers': 2, 'n_heads': 32, 'dim_feedforward': 407, 'batch_size': 43, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 74}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 16
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.2214[0m   [32m0.7298[0m        [35m0.3560[0m       [31m0.9093[0m        [94m0.3729[0m     +  8.6862
      2   [36m0.2615[0m   [32m0.7669[0m        [35m0.2545[0m       0.9045        [94m0.3670[0m     +  8.9748
      3   [36m0.2615[0m   [32m0.7750[0m        [35m0.2510[0m       0.9008        [94m0.3604[0m     +  8.9247
      4   [36m0.2746[0m   [32m0.7869[0m        0.2526       0.9021        0.3760        9.0586
      5   [36m0.2753[0m   [32m0.7949[0m        [35m0.2467[0m       0.9081        [94m0.3523[0m     +  9.0712
      6   0.2730   [32m0.8018[0m        [35m0.2459[0m       0.9069        0.3531        9.0173
      7   [36m0.2825[0m   [32m0.8061[0m        [35m0.2456[0m       0.9081        [94m0.3403[0m     +  9.1235
      8   [36m0.2843[0m   [32m0.8098[0m        [35m0.2434[0m       0.9069        [94m0.3181[0m     +  9.1758
      9   [36m0.2870[0m   [32m0.8159[0m        [35m0.2426[0m       0.9081        [94m0.3096[0m     +  9.2613
     10   0.2822   0.8091        0.2458       0.9081        [94m0.3012[0m     +  9.0946
     11   [36m0.2934[0m   0.8152        [35m0.2425[0m       [31m0.9129[0m        [94m0.2787[0m     +  9.1095
     12   0.2886   0.8130        [35m0.2398[0m       [31m0.9141[0m        [94m0.2599[0m     +  8.9941
     13   0.2872   0.8154        [35m0.2353[0m       0.9117        [94m0.2521[0m     +  8.9204
     14   0.2885   0.8151        [35m0.2334[0m       0.9141        [94m0.2467[0m     +  9.2567
     15   0.2880   0.8147        [35m0.2326[0m       [31m0.9154[0m        [94m0.2397[0m     +  9.1756
     16   [36m0.3016[0m   0.8131        [35m0.2323[0m       0.9154        [94m0.2381[0m     +  9.1799
     17   0.3014   0.8133        [35m0.2309[0m       [31m0.9178[0m        0.2395        9.1619
     18   0.2980   [32m0.8165[0m        [35m0.2299[0m       [31m0.9190[0m        [94m0.2352[0m     +  9.3892
     19   0.2981   0.8133        [35m0.2298[0m       0.9178        [94m0.2317[0m     +  9.1345
     20   0.3002   0.8156        [35m0.2288[0m       0.9178        [94m0.2286[0m     +  9.4916
     21   [36m0.3127[0m   0.8164        [35m0.2262[0m       0.9154        [94m0.2262[0m     +  9.1512
     22   [36m0.3232[0m   [32m0.8172[0m        [35m0.2260[0m       0.9154        [94m0.2252[0m     +  9.2885
     23   0.3070   [32m0.8184[0m        [35m0.2256[0m       0.9190        0.2307        8.9633
     24   0.3135   [32m0.8211[0m        [35m0.2242[0m       0.9178        0.2277        9.3101
     25   0.3209   [32m0.8234[0m        [35m0.2230[0m       [31m0.9202[0m        0.2262        9.3228
     26   0.3169   0.8224        0.2242       0.9202        [94m0.2251[0m     +  9.0001
     27   0.3196   [32m0.8243[0m        0.2251       [31m0.9214[0m        0.2267        9.2573
     28   [36m0.3294[0m   [32m0.8266[0m        [35m0.2215[0m       0.9214        [94m0.2239[0m     +  8.9070
     29   0.3271   [32m0.8279[0m        [35m0.2207[0m       [31m0.9226[0m        [94m0.2222[0m     +  9.4283
     30   [36m0.3314[0m   [32m0.8313[0m        [35m0.2192[0m       0.9214        0.2305        9.1163
     31   [36m0.3371[0m   0.8296        0.2201       [31m0.9238[0m        0.2234        9.3652
     32   0.3353   [32m0.8313[0m        [35m0.2185[0m       0.9238        0.2269        9.4196
     33   0.3275   0.8297        0.2192       0.9178        0.2279        9.3374
     34   0.3355   [32m0.8318[0m        0.2186       0.9190        0.2298        9.4884
     35   0.3292   0.8311        [35m0.2175[0m       0.9166        0.2244        8.9588
     36   [36m0.3453[0m   0.8317        [35m0.2158[0m       0.9202        [94m0.2221[0m     +  9.1234
     37   [36m0.3453[0m   [32m0.8333[0m        0.2169       0.9226        0.2269        9.0807
     38   0.3343   0.8314        0.2163       0.9190        0.2281        8.9021
     39   [36m0.3547[0m   [32m0.8338[0m        [35m0.2157[0m       [31m0.9250[0m        0.2303        9.2876
     40   0.3376   0.8321        0.2182       0.9190        0.2296        9.1997
     41   0.3480   0.8324        0.2165       0.9226        0.2317        9.2801
     42   0.3438   0.8320        [35m0.2151[0m       0.9202        0.2261        9.0646
     43   0.3439   0.8319        0.2160       0.9202        0.2272        9.3529
     44   0.3251   0.8308        0.2151       0.9154        0.2268        9.4781
     45   0.3341   0.8335        [35m0.2137[0m       0.9166        0.2277        9.3040
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 05:04:52,037][0m Trial 279 finished with value: 0.22214300101656723 and parameters: {'lr': 0.0006307585734103936, 'dropout': 0.6000157503353868, 'd_model_multiplier': 1, 'num_layers': 1, 'n_heads': 32, 'dim_feedforward': 416, 'batch_size': 40, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 16}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 16
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1146[0m   [32m0.4401[0m        [35m0.5599[0m       [31m0.9081[0m        [94m0.6760[0m     +  10.7176
      2   [36m0.3030[0m   [32m0.7692[0m        [35m0.3222[0m       0.9069        [94m0.6618[0m     +  10.7781
      3   [36m0.3389[0m   [32m0.8016[0m        [35m0.2763[0m       0.9045        0.6667        10.8980
      4   [36m0.3573[0m   0.7849        [35m0.2591[0m       [31m0.9141[0m        [94m0.6108[0m     +  10.8314
      5   0.3543   0.7743        [35m0.2562[0m       0.9105        0.6338        10.9981
      6   0.3533   0.7735        [35m0.2527[0m       0.9117        [94m0.6098[0m     +  10.8961
      7   0.3547   0.7800        [35m0.2509[0m       0.9105        [94m0.5654[0m     +  10.8802
      8   0.3489   0.7347        [35m0.2481[0m       0.9081        0.5805        10.9809
      9   [36m0.3603[0m   0.7729        0.2517       0.9093        [94m0.5551[0m     +  10.9789
     10   [36m0.3615[0m   0.7625        [35m0.2449[0m       0.9081        [94m0.5271[0m     +  10.8686
     11   0.3503   0.7385        0.2467       0.9081        [94m0.5196[0m     +  10.7329
     12   0.3492   0.7350        0.2462       0.9081        [94m0.4818[0m     +  10.8454
     13   0.3487   0.7366        [35m0.2442[0m       0.9081        [94m0.4787[0m     +  10.7934
     14   0.3417   0.7249        [35m0.2422[0m       0.9081        [94m0.4670[0m     +  10.8845
     15   0.3374   0.7258        0.2434       0.9081        0.4847        10.9044
     16   0.3209   0.7165        0.2438       0.9081        [94m0.4631[0m     +  10.8089
     17   0.3311   0.7269        0.2444       0.9081        [94m0.4508[0m     +  10.9547
     18   0.3323   0.7315        [35m0.2403[0m       0.9081        [94m0.4432[0m     +  10.9525
     19   0.3390   0.7380        0.2427       0.9081        [94m0.4349[0m     +  10.7749
     20   0.3492   0.7578        [35m0.2388[0m       0.9081        [94m0.4209[0m     +  10.9227
     21   0.3132   0.7296        0.2393       0.9081        [94m0.4208[0m     +  10.8770
     22   0.3472   0.7512        0.2391       0.9081        [94m0.3936[0m     +  10.7769
     23   0.3368   0.7444        0.2412       0.9081        0.4044        10.8246
     24   0.3307   0.7496        0.2401       0.9081        0.4064        10.9254
     25   0.2802   0.7251        [35m0.2380[0m       0.9081        0.4013        10.9236
     26   0.3567   0.7823        0.2384       0.9081        [94m0.3755[0m     +  10.7712
     27   [36m0.3642[0m   0.7876        0.2388       0.9081        [94m0.3755[0m     +  10.8770
     28   0.3562   0.7756        [35m0.2367[0m       0.9081        0.3781        11.0857
     29   0.3478   0.7762        [35m0.2359[0m       0.9081        [94m0.3581[0m     +  11.0259
     30   0.3513   0.7764        [35m0.2346[0m       0.9081        [94m0.3541[0m     +  10.9717
     31   [36m0.3662[0m   0.7898        0.2353       0.9081        [94m0.3462[0m     +  10.7647
     32   0.3426   0.7718        0.2359       0.9081        0.3497        11.0416
     33   0.3573   0.7852        [35m0.2345[0m       0.9081        [94m0.3452[0m     +  10.9248
     34   [36m0.3780[0m   0.7924        0.2347       0.9081        [94m0.3311[0m     +  10.9745
     35   0.3628   0.7909        0.2355       0.9081        [94m0.3307[0m     +  11.0832
     36   0.3464   0.7849        0.2366       0.9081        [94m0.3242[0m     +  10.8806
     37   0.3773   [32m0.8057[0m        [35m0.2312[0m       0.9081        [94m0.3198[0m     +  10.8001
     38   0.3719   0.8051        0.2354       0.9081        [94m0.3076[0m     +  11.1912
     39   0.3713   0.8026        0.2333       0.9081        0.3234        11.0219
     40   0.3540   0.7904        0.2330       0.9081        0.3258        10.8562
     41   0.3641   0.7954        0.2328       0.9081        0.3303        10.8431
     42   0.3706   0.7988        0.2331       0.9081        0.3144        10.9159
     43   [36m0.3811[0m   [32m0.8126[0m        0.2347       0.9081        0.3173        11.0150
     44   0.3702   [32m0.8151[0m        [35m0.2308[0m       0.9081        [94m0.2995[0m     +  10.8918
     45   [36m0.3869[0m   [32m0.8173[0m        0.2309       0.9081        0.3273        10.9039
     46   0.3657   0.8089        0.2322       0.9081        0.3288        11.1404
     47   0.3627   0.8093        0.2316       0.9081        0.3290        10.9128
     48   0.3600   0.8017        0.2337       0.9081        0.3225        10.8041
     49   0.3640   0.8058        [35m0.2298[0m       0.9081        0.3247        11.0197
     50   0.3848   [32m0.8219[0m        [35m0.2284[0m       0.9081        0.3069        10.9761
[32m[I 2023-05-03 05:14:00,098][0m Trial 280 finished with value: 0.29948405984523363 and parameters: {'lr': 6.825371597824312e-05, 'dropout': 0.598219563664121, 'd_model_multiplier': 1, 'num_layers': 2, 'n_heads': 32, 'dim_feedforward': 427, 'batch_size': 29, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 16}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 11
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1206[0m   [32m0.5914[0m        [35m0.4002[0m       [31m0.9359[0m        [94m0.2354[0m     +  15.9103
      2   [36m0.1546[0m   [32m0.6428[0m        [35m0.2691[0m       0.9359        [94m0.2227[0m     +  16.0509
      3   [36m0.1678[0m   [32m0.6710[0m        [35m0.2609[0m       0.9359        [94m0.2189[0m     +  15.9133
      4   0.1662   [32m0.6853[0m        [35m0.2600[0m       0.9359        [94m0.2172[0m     +  15.9193
      5   [36m0.1792[0m   [32m0.6922[0m        [35m0.2596[0m       0.9359        [94m0.2167[0m     +  16.0579
      6   [36m0.1797[0m   [32m0.6933[0m        [35m0.2569[0m       0.9359        0.2170        16.0083
      7   0.1784   [32m0.6958[0m        0.2571       0.9359        0.2169        16.0243
      8   [36m0.1815[0m   0.6953        0.2594       0.9347        0.2170        16.1213
      9   [36m0.1834[0m   [32m0.6998[0m        0.2573       0.9359        0.2176        15.9238
     10   0.1753   0.6997        0.2589       0.9347        0.2172        15.9156
     11   0.1670   0.6909        0.2582       0.9359        0.2189        15.9906
     12   0.1742   [32m0.7022[0m        0.2571       0.9347        0.2181        16.0681
     13   0.1734   [32m0.7051[0m        0.2570       0.9335        0.2180        15.9246
     14   0.1705   [32m0.7069[0m        0.2575       0.9335        0.2184        15.9174
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 05:18:00,355][0m Trial 281 finished with value: 0.21666137080869335 and parameters: {'lr': 0.0003743061718243855, 'dropout': 0.6733169955179608, 'd_model_multiplier': 1, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 384, 'batch_size': 34, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0, 'top_n_features': 11}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 30
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0835[0m   [32m0.4719[0m        [35m0.4894[0m       [31m0.9117[0m        [94m0.5392[0m     +  12.3798
      2   [36m0.3124[0m   [32m0.7347[0m        [35m0.3123[0m       0.9117        [94m0.4042[0m     +  12.8827
      3   [36m0.3460[0m   [32m0.7488[0m        [35m0.2839[0m       [31m0.9129[0m        [94m0.3431[0m     +  12.7782
      4   [36m0.3482[0m   0.7444        [35m0.2699[0m       [31m0.9141[0m        [94m0.3137[0m     +  12.7168
      5   [36m0.3518[0m   0.7445        [35m0.2674[0m       0.9141        [94m0.2886[0m     +  12.7958
      6   [36m0.3570[0m   [32m0.7552[0m        [35m0.2594[0m       [31m0.9178[0m        [94m0.2778[0m     +  12.7285
      7   0.3550   0.7489        0.2601       0.9166        [94m0.2698[0m     +  12.6654
      8   0.3518   0.7491        [35m0.2562[0m       0.9166        [94m0.2632[0m     +  12.7508
      9   0.3540   0.7488        [35m0.2542[0m       0.9166        [94m0.2586[0m     +  12.7170
     10   [36m0.3668[0m   [32m0.7604[0m        [35m0.2503[0m       0.9166        [94m0.2562[0m     +  12.4314
     11   [36m0.3679[0m   [32m0.7700[0m        [35m0.2465[0m       [31m0.9190[0m        [94m0.2497[0m     +  12.5463
     12   [36m0.3720[0m   [32m0.7737[0m        0.2499       0.9190        [94m0.2484[0m     +  12.7748
     13   [36m0.3776[0m   0.7736        [35m0.2417[0m       [31m0.9214[0m        [94m0.2477[0m     +  12.7649
     14   0.3754   0.7702        0.2446       0.9190        [94m0.2472[0m     +  12.5234
     15   [36m0.3813[0m   [32m0.7742[0m        [35m0.2397[0m       0.9166        [94m0.2468[0m     +  12.9178
     16   [36m0.3863[0m   [32m0.7767[0m        0.2405       0.9190        0.2472        12.5566
     17   [36m0.3893[0m   [32m0.7806[0m        0.2399       0.9178        [94m0.2466[0m     +  12.8535
     18   [36m0.3936[0m   [32m0.7851[0m        [35m0.2366[0m       0.9178        [94m0.2454[0m     +  12.6950
     19   0.3892   0.7851        [35m0.2349[0m       0.9190        0.2471        12.5241
     20   [36m0.3941[0m   [32m0.7853[0m        [35m0.2346[0m       0.9214        0.2459        12.3949
     21   0.3875   [32m0.7883[0m        [35m0.2322[0m       0.9190        [94m0.2454[0m     +  12.6500
     22   0.3890   [32m0.7887[0m        0.2324       0.9202        0.2480        12.7013
     23   0.3819   [32m0.7895[0m        0.2330       0.9178        0.2478        12.7914
     24   0.3843   [32m0.7913[0m        0.2350       0.9178        0.2490        12.6310
     25   0.3779   0.7900        [35m0.2313[0m       0.9178        0.2510        12.8121
     26   0.3730   0.7904        [35m0.2294[0m       0.9141        0.2506        12.7610
     27   0.3772   [32m0.7930[0m        [35m0.2279[0m       0.9117        0.2492        12.6573
     28   0.3645   [32m0.7942[0m        0.2285       0.9141        0.2495        12.6008
     29   0.3615   [32m0.7958[0m        0.2288       0.9154        0.2498        12.6803
     30   0.3576   0.7944        0.2283       0.9117        0.2547        12.2633
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 05:24:33,814][0m Trial 282 finished with value: 0.24536017927610312 and parameters: {'lr': 0.0009861093465878214, 'dropout': 0.6361684027755989, 'd_model_multiplier': 1, 'num_layers': 2, 'n_heads': 32, 'dim_feedforward': 435, 'batch_size': 126, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 30}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 24
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.2224[0m   [32m0.7392[0m        [35m0.3133[0m       [31m0.9323[0m        [94m0.2274[0m     +  8.1635
      2   [36m0.2269[0m   0.7380        [35m0.2493[0m       0.9287        0.2382        9.2850
      3   0.2209   0.7365        [35m0.2443[0m       0.9262        0.2473        9.0011
      4   [36m0.2271[0m   [32m0.7513[0m        0.2471       0.9274        0.2358        9.1147
      5   0.2224   0.7294        [35m0.2415[0m       0.9299        0.2509        9.1554
      6   [36m0.2557[0m   [32m0.7548[0m        [35m0.2389[0m       [31m0.9335[0m        0.2313        9.1081
      7   [36m0.2607[0m   [32m0.7580[0m        [35m0.2379[0m       0.9299        0.2280        9.2211
      8   0.2368   0.7370        [35m0.2316[0m       0.9299        0.2535        9.5530
      9   [36m0.2620[0m   [32m0.7624[0m        0.2321       0.9323        0.2362        9.4097
     10   [36m0.2753[0m   [32m0.7702[0m        0.2322       0.9335        [94m0.2271[0m     +  9.0753
     11   0.2686   [32m0.7722[0m        [35m0.2308[0m       0.9311        0.2320        8.8973
     12   0.2651   0.7693        [35m0.2306[0m       0.9287        [94m0.2256[0m     +  9.3714
     13   0.2450   0.7566        [35m0.2272[0m       0.9311        0.2366        9.1403
     14   0.2699   [32m0.7805[0m        0.2281       0.9311        [94m0.2245[0m     +  9.1022
     15   0.2638   0.7769        0.2273       0.9299        0.2284        9.2265
     16   [36m0.2764[0m   [32m0.7818[0m        [35m0.2264[0m       0.9311        0.2249        9.4360
     17   0.2714   0.7800        [35m0.2257[0m       0.9299        0.2259        9.6301
     18   [36m0.2795[0m   [32m0.7893[0m        [35m0.2254[0m       0.9311        [94m0.2225[0m     +  9.5024
     19   0.2661   0.7869        [35m0.2251[0m       0.9299        [94m0.2183[0m     +  9.0003
     20   0.2727   0.7890        0.2264       0.9274        0.2187        8.9215
     21   0.2645   0.7795        0.2259       0.9323        0.2267        9.5876
     22   0.2707   0.7846        [35m0.2233[0m       0.9311        0.2230        9.3162
     23   0.2715   0.7727        0.2246       0.9287        [94m0.2165[0m     +  9.3160
     24   0.2767   0.7852        0.2238       0.9299        0.2185        9.0003
     25   0.2671   0.7886        0.2244       0.9311        0.2261        9.2050
     26   [36m0.2882[0m   [32m0.7952[0m        [35m0.2230[0m       0.9287        [94m0.2086[0m     +  9.3358
     27   0.2690   0.7907        0.2231       0.9311        0.2191        9.3874
     28   0.2694   0.7855        0.2247       0.9299        0.2140        8.8296
     29   0.2773   0.7902        [35m0.2215[0m       0.9311        0.2223        8.9269
     30   0.2807   0.7889        0.2216       0.9287        0.2182        9.2254
     31   0.2592   0.7702        [35m0.2212[0m       0.9299        0.2313        9.1283
     32   0.2875   0.7808        0.2226       0.9323        0.2169        9.2365
     33   [36m0.2943[0m   0.7788        0.2237       0.9323        0.2278        8.9558
     34   0.2651   0.7477        [35m0.2195[0m       [31m0.9347[0m        0.2612        9.0933
     35   [36m0.3075[0m   [32m0.7966[0m        0.2250       0.9335        0.2213        9.2418
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 05:30:04,719][0m Trial 283 finished with value: 0.20857878667195445 and parameters: {'lr': 0.0019071453655629968, 'dropout': 0.5815261766266218, 'd_model_multiplier': 1, 'num_layers': 1, 'n_heads': 32, 'dim_feedforward': 410, 'batch_size': 39, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 24}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 44
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.0574[0m   [32m0.4200[0m        [35m0.4339[0m       [31m0.9323[0m        [94m0.2766[0m     +  9.0657
      2   [36m0.1375[0m   [32m0.6422[0m        [35m0.2904[0m       0.9323        [94m0.2382[0m     +  9.1431
      3   [36m0.1784[0m   [32m0.6956[0m        [35m0.2641[0m       0.9323        [94m0.2297[0m     +  9.7212
      4   [36m0.2193[0m   [32m0.7401[0m        [35m0.2600[0m       0.9311        [94m0.2219[0m     +  9.9023
      5   [36m0.2476[0m   [32m0.7704[0m        [35m0.2513[0m       0.9299        [94m0.2153[0m     +  9.6255
      6   [36m0.2674[0m   0.7536        [35m0.2450[0m       0.9311        0.2205        9.4589
      7   [36m0.2877[0m   [32m0.7809[0m        [35m0.2439[0m       0.9299        0.2164        9.8302
      8   [36m0.3221[0m   0.7739        [35m0.2406[0m       0.9287        0.2166        9.4060
      9   [36m0.3271[0m   [32m0.7932[0m        [35m0.2397[0m       0.9262        [94m0.2114[0m     +  9.7838
     10   0.3255   [32m0.7981[0m        [35m0.2385[0m       0.9287        [94m0.2109[0m     +  9.9960
     11   [36m0.3320[0m   0.7942        [35m0.2384[0m       0.9299        0.2123        10.8115
     12   [36m0.3374[0m   0.7968        0.2393       0.9299        0.2111        9.9029
     13   [36m0.3441[0m   [32m0.7989[0m        [35m0.2365[0m       0.9311        [94m0.2096[0m     +  10.1345
     14   [36m0.3473[0m   [32m0.8018[0m        [35m0.2345[0m       0.9311        [94m0.2089[0m     +  9.7194
     15   0.3370   0.7888        0.2347       0.9311        0.2141        10.1221
     16   0.3302   [32m0.8033[0m        0.2348       0.9287        0.2135        10.1030
     17   0.3367   0.7992        0.2366       0.9299        0.2134        9.7553
     18   0.3255   0.7967        [35m0.2339[0m       0.9287        0.2153        9.8026
     19   0.3363   0.8006        [35m0.2307[0m       0.9299        0.2120        10.0276
     20   0.3318   0.7921        0.2337       [31m0.9335[0m        0.2149        9.6034
     21   0.3287   0.7870        0.2313       0.9299        0.2180        9.5224
     22   0.3307   0.7905        0.2321       0.9323        0.2175        9.5568
     23   0.2992   0.7917        [35m0.2287[0m       0.9287        0.2232        9.8042
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 05:33:59,786][0m Trial 284 finished with value: 0.20891269827948517 and parameters: {'lr': 0.0019151281403670159, 'dropout': 0.5907437572970108, 'd_model_multiplier': 1, 'num_layers': 1, 'n_heads': 16, 'dim_feedforward': 398, 'batch_size': 135, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 44}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 45
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.2797[0m   [32m0.7045[0m        [35m0.3125[0m       [31m0.9081[0m        [94m0.2925[0m     +  8.9369
      2   [36m0.3038[0m   [32m0.7377[0m        [35m0.2397[0m       0.9045        0.3039        8.9122
      3   0.2966   [32m0.7465[0m        0.2399       0.9033        0.2994        9.3853
      4   [36m0.3101[0m   [32m0.7534[0m        [35m0.2362[0m       0.9045        0.3028        9.3304
      5   [36m0.3136[0m   [32m0.7548[0m        0.2389       [31m0.9093[0m        0.3078        9.1501
      6   0.3057   0.7488        [35m0.2357[0m       0.9093        0.3177        9.3633
      7   [36m0.3162[0m   0.7456        [35m0.2341[0m       [31m0.9129[0m        0.3238        9.4026
      8   0.3126   0.7513        [35m0.2328[0m       0.9069        0.3233        9.1751
      9   0.3151   [32m0.7587[0m        [35m0.2306[0m       0.9045        0.3155        9.0743
     10   0.3135   0.7544        [35m0.2276[0m       0.9081        0.3188        9.3042
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 05:35:41,404][0m Trial 285 finished with value: 0.29251243941657057 and parameters: {'lr': 0.0017401175356843187, 'dropout': 0.5916697230062891, 'd_model_multiplier': 1, 'num_layers': 1, 'n_heads': 32, 'dim_feedforward': 407, 'batch_size': 45, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 45}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 42
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.1163[0m   [32m0.5994[0m        [35m0.3971[0m       [31m0.9262[0m        [94m0.2611[0m     +  8.2160
      2   [36m0.2172[0m   [32m0.7261[0m        [35m0.2628[0m       0.9250        [94m0.2435[0m     +  9.4401
      3   [36m0.2453[0m   [32m0.7496[0m        [35m0.2531[0m       0.9250        [94m0.2435[0m     +  9.4240
      4   [36m0.2471[0m   0.7448        [35m0.2464[0m       0.9226        0.2478        9.4445
      5   [36m0.2525[0m   0.7450        [35m0.2414[0m       0.9166        0.2568        9.2787
      6   0.2287   0.7381        [35m0.2383[0m       0.9129        0.2669        9.4441
      7   0.2493   [32m0.7554[0m        0.2387       0.9238        0.2471        9.3861
      8   [36m0.2590[0m   [32m0.7645[0m        0.2405       0.9226        0.2470        9.1923
      9   [36m0.2775[0m   [32m0.7656[0m        0.2404       0.9214        0.2468        9.7920
     10   0.2743   [32m0.7683[0m        [35m0.2345[0m       0.9166        0.2537        9.4763
     11   0.2601   0.7681        [35m0.2340[0m       0.9178        0.2561        9.7074
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 05:37:33,463][0m Trial 286 finished with value: 0.24352872139011864 and parameters: {'lr': 0.0023304671043652614, 'dropout': 0.6181389721189603, 'd_model_multiplier': 1, 'num_layers': 1, 'n_heads': 16, 'dim_feedforward': 397, 'batch_size': 82, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 42}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 22
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.3569[0m   [32m0.7884[0m        [35m0.3055[0m       [31m0.9105[0m        [94m0.2627[0m     +  8.3076
      2   [36m0.4160[0m   [32m0.8206[0m        [35m0.2482[0m       [31m0.9250[0m        [94m0.2486[0m     +  9.1519
      3   [36m0.4324[0m   [32m0.8325[0m        [35m0.2456[0m       [31m0.9262[0m        [94m0.2425[0m     +  9.0626
      4   0.4310   [32m0.8350[0m        [35m0.2446[0m       0.9250        0.2433        8.7647
      5   [36m0.4402[0m   [32m0.8373[0m        0.2448       0.9214        0.2452        9.0477
      6   [36m0.4448[0m   [32m0.8410[0m        [35m0.2388[0m       0.9262        [94m0.2384[0m     +  9.3073
      7   [36m0.4705[0m   [32m0.8462[0m        [35m0.2379[0m       [31m0.9287[0m        [94m0.2380[0m     +  9.2248
      8   [36m0.4747[0m   [32m0.8482[0m        [35m0.2377[0m       0.9262        [94m0.2348[0m     +  9.0419
      9   [36m0.4962[0m   [32m0.8578[0m        [35m0.2336[0m       0.9262        [94m0.2285[0m     +  9.1652
     10   0.4932   0.8560        [35m0.2310[0m       0.9202        [94m0.2264[0m     +  9.1762
     11   0.4913   0.8562        [35m0.2296[0m       0.9202        [94m0.2241[0m     +  9.0113
     12   0.4950   0.8539        0.2304       0.9238        0.2242        9.3140
     13   [36m0.4988[0m   0.8540        0.2305       0.9250        0.2249        9.1134
     14   [36m0.4998[0m   [32m0.8593[0m        [35m0.2278[0m       0.9274        [94m0.2232[0m     +  8.9675
     15   [36m0.5292[0m   0.8579        [35m0.2256[0m       0.9226        0.2238        9.0625
     16   [36m0.5356[0m   0.8537        0.2281       0.9238        [94m0.2231[0m     +  9.1975
     17   [36m0.5405[0m   0.8489        0.2280       0.9226        [94m0.2222[0m     +  8.8976
     18   0.5074   0.8499        [35m0.2248[0m       0.9250        0.2269        9.0179
     19   [36m0.5591[0m   [32m0.8629[0m        [35m0.2241[0m       [31m0.9311[0m        [94m0.2184[0m     +  9.1637
     20   0.5545   0.8581        [35m0.2236[0m       0.9250        [94m0.2182[0m     +  9.1327
     21   0.5407   0.8583        0.2239       0.9262        0.2198        8.8635
     22   0.5124   0.8471        0.2236       0.9178        0.2335        9.0856
     23   0.5554   0.8605        0.2245       0.9250        0.2200        9.0895
     24   0.5207   0.8517        [35m0.2194[0m       0.9238        0.2243        8.7966
     25   0.5428   0.8563        0.2222       0.9299        0.2231        8.8706
     26   0.5307   0.8485        0.2247       0.9238        0.2290        9.1582
     27   0.5276   0.8538        0.2238       0.9262        0.2292        9.1947
     28   [36m0.5639[0m   [32m0.8671[0m        0.2227       0.9262        0.2195        9.3520
     29   0.5071   0.8537        0.2218       0.9238        0.2286        9.0550
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 05:42:05,843][0m Trial 287 finished with value: 0.218155175339354 and parameters: {'lr': 0.0019119016827503755, 'dropout': 0.5844538402310576, 'd_model_multiplier': 1, 'num_layers': 1, 'n_heads': 32, 'dim_feedforward': 411, 'batch_size': 40, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 22}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 60
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2348[0m   [32m0.8038[0m        [35m0.2826[0m       [31m0.9250[0m        [94m0.2215[0m     +  10.3996
      2   [36m0.2497[0m   0.8021        [35m0.2547[0m       [31m0.9287[0m        [94m0.2115[0m     +  10.4911
      3   0.2462   0.7756        0.2561       [31m0.9311[0m        0.2262        10.6921
      4   [36m0.2659[0m   0.7904        [35m0.2531[0m       0.9299        0.2217        10.6450
      5   [36m0.2778[0m   0.7843        [35m0.2499[0m       [31m0.9347[0m        [94m0.2087[0m     +  10.5631
      6   0.2626   0.7839        [35m0.2437[0m       0.9323        0.2169        10.6816
      7   0.2653   0.7966        [35m0.2431[0m       0.9347        0.2152        10.5273
      8   0.2609   0.7837        0.2434       0.9347        0.2136        10.6310
      9   0.2644   0.8012        [35m0.2390[0m       0.9311        0.2118        10.6586
     10   0.2516   0.7999        0.2391       0.9323        0.2138        10.9396
     11   0.2645   [32m0.8045[0m        [35m0.2388[0m       0.9335        [94m0.2060[0m     +  10.4035
     12   0.2675   0.8037        0.2403       0.9335        0.2070        10.7923
     13   0.2608   0.7988        [35m0.2369[0m       [31m0.9359[0m        0.2072        10.4879
     14   0.2654   0.8029        [35m0.2365[0m       0.9311        0.2092        10.5146
     15   0.2607   0.7944        0.2390       0.9311        0.2094        10.6689
     16   0.2531   0.7876        0.2375       0.9311        0.2148        10.6286
     17   0.2547   0.8044        0.2401       0.9299        0.2210        10.7507
     18   0.2589   0.7988        0.2405       0.9347        0.2125        10.6812
     19   0.2451   0.8006        0.2409       0.9311        [94m0.2047[0m     +  10.5332
     20   0.2560   0.7904        0.2395       0.9347        0.2091        10.7131
     21   0.2571   0.7816        0.2402       0.9359        [94m0.2031[0m     +  10.5851
     22   0.2710   [32m0.8055[0m        0.2392       0.9250        0.2217        10.5965
     23   0.2460   0.7762        0.2373       0.9311        0.2337        10.7497
     24   0.2568   0.7981        0.2400       0.9323        0.2076        10.6501
     25   0.2464   0.7841        0.2379       0.9323        0.2089        10.5262
     26   0.2638   [32m0.8068[0m        0.2371       0.9335        [94m0.2001[0m     +  10.8657
     27   0.2704   [32m0.8071[0m        0.2391       0.9359        0.2084        10.6060
     28   0.2626   [32m0.8110[0m        0.2406       0.9299        0.2073        10.8281
     29   0.2369   0.8005        0.2380       0.9311        0.2072        10.7006
     30   0.2664   0.8099        0.2394       0.9311        0.2001        10.7231
     31   0.2592   0.8023        0.2397       0.9311        0.2023        10.8822
     32   0.2569   0.7986        0.2373       0.9335        0.2014        11.3673
     33   0.2599   0.7967        0.2395       0.9323        0.2009        10.7044
     34   0.2575   0.7951        0.2402       0.9335        0.2100        10.9055
     35   0.2461   0.7928        0.2386       0.9287        0.2227        10.5844
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 05:48:31,044][0m Trial 288 finished with value: 0.2000684682302348 and parameters: {'lr': 0.002833319197583949, 'dropout': 0.5865107375455123, 'd_model_multiplier': 1, 'num_layers': 1, 'n_heads': 64, 'dim_feedforward': 414, 'batch_size': 49, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 60}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 62
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2619[0m   [32m0.7461[0m        [35m0.2845[0m       [31m0.9395[0m        [94m0.2056[0m     +  10.6448
      2   [36m0.2835[0m   [32m0.7600[0m        [35m0.2596[0m       0.9347        [94m0.2014[0m     +  10.8635
      3   0.2649   0.7349        [35m0.2595[0m       [31m0.9407[0m        0.2087        10.9005
      4   0.2741   0.7211        [35m0.2507[0m       [31m0.9420[0m        0.2198        10.8866
      5   0.2817   0.7413        [35m0.2463[0m       0.9407        0.2086        10.7188
      6   0.2805   0.7457        [35m0.2442[0m       [31m0.9444[0m        0.2015        10.4748
      7   [36m0.2986[0m   [32m0.7700[0m        0.2446       0.9444        [94m0.1989[0m     +  10.6258
      8   [36m0.3079[0m   0.7507        [35m0.2421[0m       0.9444        0.2023        10.3625
      9   0.2776   0.7504        [35m0.2410[0m       0.9420        0.2067        10.7602
     10   0.2880   0.7465        [35m0.2402[0m       0.9432        0.2119        10.8815
     11   0.2808   [32m0.7741[0m        [35m0.2385[0m       0.9432        0.2176        10.6281
     12   0.2807   0.7600        0.2414       0.9420        0.2006        10.7148
     13   0.2930   0.7598        0.2442       0.9420        0.2002        10.5915
     14   0.2745   0.7510        0.2401       0.9395        0.2057        10.6888
     15   0.2551   0.7566        0.2394       0.9407        0.2159        10.5824
     16   0.2353   0.7513        0.2419       0.9383        0.2017        10.5659
     17   0.2608   0.7692        0.2450       0.9395        [94m0.1966[0m     +  10.6970
     18   0.2647   0.7555        0.2412       0.9420        0.2032        11.0872
     19   0.2484   0.7543        0.2430       0.9395        0.1997        10.7501
     20   0.2685   0.7680        0.2420       0.9432        0.2084        10.7516
     21   0.2711   0.7699        0.2428       0.9432        [94m0.1965[0m     +  10.6156
     22   0.2682   0.7630        0.2410       0.9407        0.1975        10.9715
     23   0.2793   0.7713        0.2393       0.9383        0.2060        11.0015
     24   0.2564   0.7600        0.2423       0.9407        0.2038        10.7260
     25   0.2739   0.7680        0.2403       0.9407        0.1971        10.9199
     26   0.2569   0.7684        [35m0.2384[0m       0.9420        0.2012        10.7562
     27   0.2360   0.7488        0.2426       0.9407        0.2040        10.6066
     28   0.2418   0.7566        0.2425       0.9383        0.2127        10.8860
     29   0.2312   0.7599        0.2412       0.9395        0.2053        10.9793
     30   0.2339   0.7538        0.2394       0.9383        0.2028        11.1316
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 05:54:05,271][0m Trial 289 finished with value: 0.1965105299541711 and parameters: {'lr': 0.0033713188383357718, 'dropout': 0.5835511535642175, 'd_model_multiplier': 1, 'num_layers': 1, 'n_heads': 64, 'dim_feedforward': 417, 'batch_size': 50, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 62}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 55
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2695[0m   [32m0.7641[0m        [35m0.2754[0m       [31m0.9093[0m        [94m0.2806[0m     +  10.4098
      2   0.2654   [32m0.7676[0m        [35m0.2484[0m       [31m0.9105[0m        0.3062        10.7430
      3   0.2576   0.7657        [35m0.2477[0m       0.9105        0.3062        10.7464
      4   0.2524   [32m0.7709[0m        [35m0.2438[0m       0.9093        0.3302        11.0028
      5   0.2597   [32m0.7783[0m        [35m0.2392[0m       [31m0.9141[0m        0.3089        10.7664
      6   0.2594   [32m0.7861[0m        [35m0.2374[0m       0.9105        0.3086        11.0939
      7   0.2606   0.7841        0.2377       0.9141        0.3077        10.5299
      8   0.2605   0.7818        [35m0.2362[0m       0.9069        0.3189        10.5827
      9   0.2628   0.7796        0.2386       [31m0.9154[0m        0.3487        10.7820
     10   0.2538   0.7796        0.2386       0.9154        0.3139        10.6709
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 05:56:03,422][0m Trial 290 finished with value: 0.2805911397523142 and parameters: {'lr': 0.0034985885045431386, 'dropout': 0.607130350983162, 'd_model_multiplier': 1, 'num_layers': 1, 'n_heads': 64, 'dim_feedforward': 402, 'batch_size': 52, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 55}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 57
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.3488[0m   [32m0.7484[0m        [35m0.2692[0m       [31m0.9226[0m        [94m0.2723[0m     +  10.3282
      2   0.3219   0.7002        [35m0.2567[0m       [31m0.9238[0m        0.3391        10.6116
      3   0.3255   0.7274        [35m0.2486[0m       0.9117        0.3147        10.6109
      4   [36m0.3668[0m   [32m0.7538[0m        [35m0.2456[0m       0.9081        0.2924        10.5891
      5   [36m0.3680[0m   [32m0.7937[0m        0.2483       0.9129        [94m0.2642[0m     +  10.5831
      6   0.3046   0.7740        [35m0.2402[0m       0.9117        0.3038        10.3611
      7   0.3105   0.7913        0.2406       0.9117        0.2831        10.5213
      8   0.3583   [32m0.7987[0m        [35m0.2398[0m       0.9129        [94m0.2559[0m     +  10.5392
      9   0.3240   0.7752        [35m0.2391[0m       0.9129        0.2884        10.6425
     10   0.3337   0.7754        0.2406       0.8694        0.3430        10.7641
     11   0.3300   [32m0.8074[0m        0.2408       0.9141        [94m0.2416[0m     +  10.6859
     12   0.3251   0.7921        [35m0.2383[0m       0.8948        0.2672        10.6515
     13   0.3460   0.7914        0.2400       0.8489        0.3625        10.6487
     14   0.3191   0.7948        0.2402       0.9008        0.2672        10.5073
     15   0.3303   0.7857        0.2412       0.8851        0.2659        10.6725
     16   0.3279   0.8012        0.2417       0.9045        0.2477        10.7932
     17   0.3252   [32m0.8143[0m        0.2397       0.9057        0.2496        10.7598
     18   0.2890   0.7694        0.2406       0.8755        0.2949        10.5813
     19   0.3054   0.8042        0.2439       0.9008        0.2609        10.6469
     20   0.3600   0.8031        [35m0.2383[0m       0.9045        0.2644        10.5192
     21   0.3348   0.8138        0.2394       0.9129        [94m0.2412[0m     +  10.7178
     22   0.3141   0.7894        0.2416       0.9154        0.2460        10.4234
     23   0.3003   0.7893        0.2421       0.9045        0.2579        10.6596
     24   0.3252   0.8135        0.2384       0.9226        0.2552        10.3750
     25   0.3471   [32m0.8191[0m        [35m0.2375[0m       0.8960        0.2634        10.5844
     26   0.3237   0.7967        0.2393       0.9190        0.2429        10.5833
     27   0.3344   0.8073        0.2405       0.9105        0.2458        10.4531
     28   0.3288   0.8170        0.2386       0.9069        0.2462        10.6326
     29   0.3270   0.8018        0.2431       0.9093        0.2440        10.4765
     30   0.3378   0.8147        0.2395       0.9202        0.2566        10.3746
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 06:01:31,828][0m Trial 291 finished with value: 0.24115959986449728 and parameters: {'lr': 0.0056410651947194764, 'dropout': 0.6180509968447379, 'd_model_multiplier': 1, 'num_layers': 1, 'n_heads': 64, 'dim_feedforward': 416, 'batch_size': 46, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 57}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 62
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2211[0m   [32m0.7148[0m        [35m0.2768[0m       [31m0.9093[0m        [94m0.2953[0m     +  10.3627
      2   [36m0.2372[0m   0.7134        [35m0.2480[0m       0.9069        0.3103        10.6782
      3   [36m0.2514[0m   [32m0.7243[0m        [35m0.2454[0m       0.9033        0.3086        10.8321
      4   [36m0.2688[0m   [32m0.7313[0m        [35m0.2428[0m       [31m0.9105[0m        0.3071        10.6354
      5   [36m0.2733[0m   [32m0.7355[0m        [35m0.2387[0m       0.9105        0.3044        10.5273
      6   0.2657   0.7320        [35m0.2366[0m       0.9081        0.3017        10.7611
      7   [36m0.2828[0m   0.7296        [35m0.2339[0m       [31m0.9117[0m        0.3093        10.5990
      8   [36m0.2920[0m   [32m0.7393[0m        0.2355       [31m0.9129[0m        0.3025        10.6222
      9   0.2903   [32m0.7430[0m        [35m0.2322[0m       0.9105        0.2988        10.5958
     10   [36m0.2933[0m   [32m0.7456[0m        0.2327       0.9105        [94m0.2915[0m     +  10.8028
     11   0.2492   0.7319        0.2355       0.9008        0.2927        10.5081
     12   0.2772   [32m0.7475[0m        0.2331       0.9045        [94m0.2861[0m     +  10.6185
     13   0.2790   0.7430        [35m0.2314[0m       0.9057        0.2866        10.3952
     14   0.2756   0.7293        0.2325       0.9093        0.2890        10.8472
     15   0.2745   0.7345        0.2344       0.9069        [94m0.2821[0m     +  10.4000
     16   0.2843   0.7429        0.2346       0.9105        [94m0.2796[0m     +  10.7736
     17   0.2649   0.7381        0.2321       0.9057        0.2823        10.6862
     18   0.2832   0.7395        [35m0.2302[0m       0.9117        0.2821        10.9238
     19   0.2766   0.7442        0.2319       0.9105        0.2811        10.6211
     20   0.2764   0.7415        0.2354       [31m0.9141[0m        [94m0.2769[0m     +  10.7602
     21   0.2787   0.7446        0.2319       0.9093        [94m0.2736[0m     +  10.7909
     22   0.2658   0.7298        0.2326       0.9093        0.2843        10.5226
     23   0.2527   0.7327        0.2331       0.8924        0.2974        10.9686
     24   0.2637   0.7371        0.2341       0.9033        0.2859        10.8231
     25   0.2785   0.7463        0.2306       0.8912        0.3106        10.6783
     26   0.2793   0.7446        0.2331       0.9081        0.2775        10.6841
     27   0.2765   0.7472        0.2333       0.9141        [94m0.2733[0m     +  10.8635
     28   0.2589   0.7379        0.2321       0.9069        0.2833        10.8162
     29   0.2538   0.7318        0.2307       0.8924        0.2965        10.8137
     30   0.2734   0.7450        0.2303       0.9081        0.2749        10.5047
     31   0.2738   0.7388        0.2306       0.9069        0.2789        10.7452
     32   0.2550   0.7367        [35m0.2292[0m       0.8984        0.2985        10.9360
     33   0.2584   0.7352        0.2311       0.8972        0.2952        10.5067
     34   0.2717   0.7407        0.2317       0.8960        0.2952        10.8433
     35   0.2481   0.7249        0.2297       0.8888        0.3013        10.9265
     36   0.2511   0.7292        0.2305       0.8984        0.3014        10.9342
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 06:08:08,695][0m Trial 292 finished with value: 0.2732868107010981 and parameters: {'lr': 0.002991049906145737, 'dropout': 0.5871402904231904, 'd_model_multiplier': 1, 'num_layers': 1, 'n_heads': 64, 'dim_feedforward': 426, 'batch_size': 50, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 62}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 60
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2091[0m   [32m0.6935[0m        [35m0.3377[0m       [31m0.9154[0m        [94m0.2552[0m     +  12.7914
      2   [36m0.2671[0m   [32m0.7669[0m        [35m0.2495[0m       [31m0.9178[0m        [94m0.2430[0m     +  13.1639
      3   [36m0.2759[0m   [32m0.7832[0m        [35m0.2435[0m       0.9105        0.2567        13.1355
      4   0.2637   [32m0.7886[0m        [35m0.2430[0m       0.9105        0.2609        13.3699
      5   0.2627   0.7874        [35m0.2429[0m       0.9069        0.2682        13.5283
      6   0.2592   [32m0.7900[0m        [35m0.2406[0m       0.9033        0.2743        13.3181
      7   0.2657   [32m0.7982[0m        [35m0.2399[0m       0.9129        0.2551        13.5460
      8   0.2742   0.7972        [35m0.2399[0m       0.9178        0.2500        13.7149
      9   0.2572   0.7971        0.2405       0.9069        0.2809        13.4341
     10   0.2630   [32m0.8020[0m        [35m0.2334[0m       0.9093        0.2694        13.3567
     11   0.2632   0.7973        [35m0.2332[0m       0.9093        0.2720        13.7579
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 06:10:49,298][0m Trial 293 finished with value: 0.24296481413665388 and parameters: {'lr': 0.0027614928522033183, 'dropout': 0.5930539796597271, 'd_model_multiplier': 1, 'num_layers': 1, 'n_heads': 64, 'dim_feedforward': 416, 'batch_size': 163, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 60}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 50
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2701[0m   [32m0.7373[0m        [35m0.2941[0m       [31m0.8996[0m        [94m0.3016[0m     +  10.3765
      2   0.2677   0.7299        [35m0.2343[0m       [31m0.9008[0m        0.3191        10.8175
      3   [36m0.2762[0m   0.7352        0.2364       0.8996        0.3159        10.5134
      4   [36m0.2765[0m   0.7234        [35m0.2341[0m       0.9008        0.3234        10.9639
      5   0.2625   0.7056        0.2351       [31m0.9021[0m        0.3237        11.0027
      6   [36m0.2803[0m   [32m0.7452[0m        0.2364       0.9008        0.3174        11.0143
      7   0.2781   [32m0.7502[0m        [35m0.2314[0m       0.8996        0.3263        10.9555
      8   [36m0.2829[0m   0.7479        0.2328       0.9008        0.3230        10.7400
      9   [36m0.2860[0m   [32m0.7505[0m        [35m0.2288[0m       0.8960        0.3310        11.1367
     10   [36m0.2938[0m   [32m0.7658[0m        [35m0.2279[0m       0.9008        0.3167        10.9194
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 06:12:48,683][0m Trial 294 finished with value: 0.3016202369617003 and parameters: {'lr': 0.001962627972527803, 'dropout': 0.5790463897226619, 'd_model_multiplier': 1, 'num_layers': 1, 'n_heads': 64, 'dim_feedforward': 397, 'batch_size': 67, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 50}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 45
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.3140[0m   [32m0.7324[0m        [35m0.2783[0m       [31m0.9154[0m        [94m0.2594[0m     +  10.6512
      2   0.3041   0.7206        [35m0.2528[0m       0.9033        0.2753        10.8629
      3   0.3064   0.7142        [35m0.2465[0m       0.8984        0.2952        10.8329
      4   0.3117   0.7216        [35m0.2462[0m       0.8888        0.3119        10.6898
      5   0.3030   0.7240        [35m0.2425[0m       0.9033        0.2984        10.7569
      6   0.3053   0.7239        [35m0.2399[0m       0.8924        0.3289        10.7878
      7   0.3046   0.7281        [35m0.2378[0m       0.8888        0.3144        11.0047
      8   0.3124   [32m0.7392[0m        [35m0.2362[0m       0.8912        0.3048        10.7574
      9   0.3114   0.7360        [35m0.2355[0m       0.8996        0.2865        10.8588
     10   0.3045   0.7364        [35m0.2339[0m       0.9045        0.2948        10.9222
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 06:14:47,724][0m Trial 295 finished with value: 0.2593883417609813 and parameters: {'lr': 0.003932348582983592, 'dropout': 0.629145419721175, 'd_model_multiplier': 1, 'num_layers': 1, 'n_heads': 64, 'dim_feedforward': 408, 'batch_size': 57, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 45}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 36
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.3197[0m   [32m0.7767[0m        [35m0.2715[0m       [31m0.9141[0m        [94m0.3075[0m     +  10.6450
      2   [36m0.3215[0m   [32m0.7979[0m        [35m0.2566[0m       0.9081        [94m0.2752[0m     +  11.1748
      3   [36m0.3433[0m   [32m0.8012[0m        [35m0.2473[0m       0.9129        0.2946        11.0725
      4   [36m0.3613[0m   [32m0.8103[0m        [35m0.2415[0m       0.9117        0.2966        11.1008
      5   0.3548   0.7989        0.2420       0.9129        0.3080        11.0976
      6   0.3416   0.7863        [35m0.2390[0m       [31m0.9202[0m        0.3084        11.1115
      7   0.3515   0.8034        0.2394       0.9154        0.2869        11.0619
      8   0.3487   0.7952        [35m0.2350[0m       0.9141        0.2846        11.3268
      9   0.3395   0.7955        0.2358       0.9154        0.2764        11.1874
     10   0.3434   0.7981        [35m0.2348[0m       0.9154        0.2789        10.8925
     11   0.3360   0.7694        0.2361       0.9154        0.2773        11.3187
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 06:17:00,987][0m Trial 296 finished with value: 0.2751615298810576 and parameters: {'lr': 0.006633909042653335, 'dropout': 0.5976192436314506, 'd_model_multiplier': 1, 'num_layers': 1, 'n_heads': 64, 'dim_feedforward': 422, 'batch_size': 76, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 36}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 48
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2209[0m   [32m0.7524[0m        [35m0.3165[0m       [31m0.9335[0m        [94m0.2189[0m     +  10.4073
      2   [36m0.2308[0m   0.7414        [35m0.2494[0m       [31m0.9347[0m        0.2268        10.3233
      3   [36m0.2518[0m   0.7429        [35m0.2475[0m       0.9214        0.2326        10.6079
      4   0.2482   0.7425        [35m0.2429[0m       0.9141        0.2391        10.8234
      5   0.2475   0.7507        0.2468       0.9202        0.2412        10.4603
      6   [36m0.2703[0m   0.7449        0.2451       0.9262        0.2362        10.5768
      7   0.2577   0.7360        0.2465       [31m0.9383[0m        0.2361        10.7222
      8   0.2532   0.7421        0.2452       0.9323        0.2267        10.5814
      9   0.2665   0.7487        [35m0.2422[0m       0.9347        0.2286        10.6202
     10   0.2598   0.7500        [35m0.2408[0m       0.9299        0.2320        10.6374
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 06:18:57,961][0m Trial 297 finished with value: 0.21892620969063714 and parameters: {'lr': 0.0014175053224523464, 'dropout': 0.6084141172988501, 'd_model_multiplier': 1, 'num_layers': 1, 'n_heads': 64, 'dim_feedforward': 439, 'batch_size': 53, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.001, 'top_n_features': 48}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 51
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0512[0m   [32m0.3047[0m        [35m0.6846[0m       [31m0.9274[0m        [94m0.5569[0m     +  26.2692
      2   [36m0.0534[0m   [32m0.3303[0m        [35m0.4781[0m       0.9274        [94m0.3587[0m     +  26.7600
      3   [36m0.0929[0m   [32m0.5248[0m        [35m0.3501[0m       0.9274        [94m0.2831[0m     +  26.1782
      4   [36m0.1695[0m   [32m0.6397[0m        [35m0.2984[0m       0.9274        [94m0.2536[0m     +  26.2170
      5   [36m0.2097[0m   [32m0.6772[0m        [35m0.2748[0m       0.9274        [94m0.2429[0m     +  26.5879
      6   [36m0.2313[0m   [32m0.6935[0m        [35m0.2657[0m       0.9274        [94m0.2390[0m     +  26.5172
      7   [36m0.2454[0m   [32m0.7000[0m        [35m0.2583[0m       0.9274        [94m0.2376[0m     +  27.0958
      8   [36m0.2656[0m   [32m0.7027[0m        [35m0.2539[0m       0.9274        [94m0.2369[0m     +  26.4616
      9   [36m0.2701[0m   [32m0.7058[0m        [35m0.2526[0m       0.9274        [94m0.2358[0m     +  26.9464
     10   [36m0.2749[0m   [32m0.7116[0m        0.2528       0.9274        [94m0.2349[0m     +  26.5982
     11   [36m0.2840[0m   [32m0.7194[0m        [35m0.2491[0m       0.9274        [94m0.2338[0m     +  26.8240
     12   [36m0.2935[0m   [32m0.7307[0m        [35m0.2487[0m       [31m0.9287[0m        [94m0.2312[0m     +  26.7427
     13   [36m0.2999[0m   [32m0.7411[0m        [35m0.2469[0m       [31m0.9299[0m        [94m0.2291[0m     +  26.5657
     14   [36m0.3076[0m   [32m0.7526[0m        [35m0.2450[0m       0.9287        [94m0.2274[0m     +  26.3263
     15   [36m0.3129[0m   [32m0.7617[0m        [35m0.2441[0m       0.9299        [94m0.2270[0m     +  26.5605
     16   0.3123   [32m0.7658[0m        [35m0.2428[0m       0.9250        0.2275        26.4539
     17   0.3129   [32m0.7692[0m        [35m0.2401[0m       0.9178        0.2284        26.5598
     18   0.3108   [32m0.7722[0m        [35m0.2399[0m       0.9166        0.2292        26.2707
     19   0.3097   [32m0.7745[0m        [35m0.2380[0m       0.9214        0.2281        26.4012
     20   [36m0.3132[0m   [32m0.7760[0m        0.2390       0.9190        0.2286        26.8577
     21   [36m0.3166[0m   [32m0.7774[0m        0.2389       0.9190        0.2292        26.7877
     22   [36m0.3171[0m   0.7769        0.2394       0.9226        0.2280        26.7006
     23   0.3116   [32m0.7794[0m        [35m0.2379[0m       0.9226        0.2282        26.6378
     24   [36m0.3206[0m   [32m0.7795[0m        [35m0.2351[0m       0.9226        0.2280        26.5121
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 06:30:03,263][0m Trial 298 finished with value: 0.22697269603015433 and parameters: {'lr': 3.275030602152801e-05, 'dropout': 0.5700680874045432, 'd_model_multiplier': 1, 'num_layers': 4, 'n_heads': 64, 'dim_feedforward': 414, 'batch_size': 63, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 51}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 60
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.3089[0m   [32m0.8105[0m        [35m0.3020[0m       [31m0.9274[0m        [94m0.2136[0m     +  8.4736
      2   [36m0.3102[0m   0.8017        [35m0.2469[0m       [31m0.9287[0m        0.2328        9.2096
      3   [36m0.3269[0m   0.8044        [35m0.2465[0m       [31m0.9311[0m        0.2324        8.9021
      4   [36m0.3323[0m   0.7999        [35m0.2397[0m       [31m0.9347[0m        0.2414        9.4372
      5   0.3179   [32m0.8179[0m        0.2407       0.9323        0.2354        9.3970
      6   0.3261   0.7968        0.2442       0.9323        0.2501        9.8511
      7   0.3250   0.7856        0.2398       0.9299        0.2499        9.3473
      8   0.3302   0.8043        [35m0.2378[0m       0.9323        0.2375        9.6470
      9   [36m0.3624[0m   [32m0.8197[0m        [35m0.2349[0m       0.9323        0.2337        9.3629
     10   0.3479   0.7967        0.2353       0.9311        0.2470        9.1036
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 06:31:45,484][0m Trial 299 finished with value: 0.21362408660487367 and parameters: {'lr': 0.002713498096883307, 'dropout': 0.5806897853962472, 'd_model_multiplier': 1, 'num_layers': 1, 'n_heads': 32, 'dim_feedforward': 425, 'batch_size': 47, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 60}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 71
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
Warning, assumed runtime error: CUDA out of memory. Tried to allocate 860.00 MiB (GPU 0; 23.70 GiB total capacity; 21.91 GiB already allocated; 169.25 MiB free; 22.44 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[32m[I 2023-05-03 06:31:49,114][0m Trial 300 finished with value: 100.0 and parameters: {'lr': 0.0019340567072618417, 'dropout': 0.6165847780857204, 'd_model_multiplier': 1, 'num_layers': 12, 'n_heads': 32, 'dim_feedforward': 407, 'batch_size': 71, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0, 'top_n_features': 71}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 38
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2405[0m   [32m0.7506[0m        [35m0.2921[0m       [31m0.9359[0m        [94m0.2225[0m     +  26.0398
      2   [36m0.2738[0m   [32m0.7744[0m        [35m0.2489[0m       0.9347        [94m0.2098[0m     +  25.9719
      3   0.2676   [32m0.7843[0m        [35m0.2486[0m       0.9274        0.2188        26.0874
      4   0.2429   0.7744        [35m0.2441[0m       0.9335        0.2279        26.3246
      5   [36m0.2888[0m   [32m0.7892[0m        0.2451       0.9299        0.2224        25.9528
      6   0.2510   [32m0.7902[0m        0.2441       0.9335        0.2202        26.2786
      7   0.2550   [32m0.7959[0m        [35m0.2433[0m       0.9347        0.2248        26.2516
      8   0.2654   0.7927        [35m0.2416[0m       0.9311        0.2237        26.1670
      9   0.2606   0.7859        [35m0.2395[0m       0.9299        0.2311        26.2573
     10   [36m0.2972[0m   0.7936        [35m0.2346[0m       [31m0.9371[0m        0.2208        26.2590
     11   0.2565   0.7903        0.2373       0.9311        0.2379        26.1053
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 06:37:03,493][0m Trial 301 finished with value: 0.20979434938734215 and parameters: {'lr': 0.0015089708803951681, 'dropout': 0.5911928870516245, 'd_model_multiplier': 1, 'num_layers': 4, 'n_heads': 64, 'dim_feedforward': 394, 'batch_size': 38, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 38}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 40
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.3279[0m   [32m0.7541[0m        [35m0.2814[0m       [31m0.9287[0m        [94m0.2652[0m     +  26.1008
      2   0.2215   0.7140        [35m0.2605[0m       0.9287        0.3119        26.2857
      3   0.1833   0.7014        [35m0.2595[0m       0.9287        0.3162        25.9004
      4   [36m0.3366[0m   [32m0.7663[0m        [35m0.2564[0m       0.9287        0.2945        26.1916
      5   0.2717   0.7523        [35m0.2479[0m       0.9287        0.2890        26.1846
      6   0.2258   0.7399        0.2503       0.9287        0.2907        26.2355
      7   0.2214   0.7342        0.2520       0.9250        0.3023        26.2308
      8   0.2120   0.7046        0.2579       0.9287        0.2742        26.3143
      9   0.1746   0.6913        0.2605       0.9287        0.2773        26.4126
     10   0.2036   0.6995        0.2602       0.9287        0.2655        26.2334
     11   0.2080   0.7035        0.2577       0.9287        [94m0.2580[0m     +  26.1909
     12   0.2181   0.6960        0.2580       0.9287        [94m0.2490[0m     +  26.3067
     13   0.1803   0.6978        0.2590       0.9287        [94m0.2441[0m     +  26.4639
     14   0.2101   0.7028        0.2575       0.9287        0.2448        26.3224
     15   0.2051   0.6983        0.2587       0.9287        [94m0.2418[0m     +  26.2234
     16   0.2225   0.7043        0.2582       0.9287        0.2466        26.3465
     17   0.2147   0.7087        0.2578       0.9287        0.2451        26.2203
     18   0.2083   0.7056        0.2581       0.9287        0.2432        26.3135
     19   0.1868   0.7040        0.2548       0.9274        0.2449        26.0648
     20   0.2059   0.7047        0.2576       0.9262        0.2511        26.1376
     21   0.1853   0.7003        0.2590       0.9287        0.2500        26.1035
     22   0.1882   0.7004        0.2567       0.9287        0.2460        25.8943
     23   0.1844   0.7012        0.2561       0.9274        0.2437        26.0791
     24   0.2258   0.7067        0.2571       0.9287        [94m0.2409[0m     +  25.8792
     25   0.2053   0.7044        0.2579       0.9287        0.2455        25.9856
     26   0.2014   0.7023        0.2572       0.9287        0.2450        26.1657
     27   0.2180   0.7058        0.2573       0.9287        0.2427        26.1524
     28   0.2046   0.7074        0.2562       0.9287        0.2424        26.1517
     29   0.2013   0.7067        0.2575       0.9287        0.2425        26.0628
     30   0.2308   0.7042        0.2556       0.9287        0.2417        26.3582
     31   0.2068   0.7068        0.2562       0.9287        0.2421        26.1796
     32   0.2012   0.7045        0.2572       0.9287        0.2421        26.0447
     33   0.2032   0.7080        0.2560       0.9287        0.2427        26.0958
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 06:51:54,432][0m Trial 302 finished with value: 0.24089784906701858 and parameters: {'lr': 0.004770095394058925, 'dropout': 0.5884704058997843, 'd_model_multiplier': 1, 'num_layers': 4, 'n_heads': 64, 'dim_feedforward': 391, 'batch_size': 38, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 40}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 31
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2125[0m   [32m0.7257[0m        [35m0.2946[0m       [31m0.9178[0m        [94m0.2621[0m     +  25.8870
      2   [36m0.2357[0m   [32m0.7293[0m        [35m0.2714[0m       0.9105        0.2650        25.9434
      3   0.2183   [32m0.7331[0m        [35m0.2672[0m       0.9178        0.2909        26.1145
      4   [36m0.2382[0m   0.7314        0.2702       0.9178        0.2902        26.1094
      5   [36m0.2728[0m   [32m0.7568[0m        [35m0.2641[0m       0.9117        0.2637        25.9259
      6   0.2233   [32m0.7578[0m        0.2673       0.9178        0.2735        25.9943
      7   0.2389   0.7339        [35m0.2633[0m       0.9178        0.2655        26.1248
      8   0.2469   0.7501        [35m0.2617[0m       0.9105        [94m0.2539[0m     +  25.9562
      9   0.0822   0.5000        0.2784       0.9178        0.2849        26.0280
     10   0.0822   0.5000        0.2751       0.9178        0.2849        26.0678
     11   0.0822   0.5000        0.2751       0.9178        0.2849        25.9072
     12   0.0822   0.5000        0.2750       0.9178        0.2848        26.1924
     13   0.0822   0.5000        0.2750       0.9178        0.2848        26.0915
     14   0.0822   0.5000        0.2750       0.9178        0.2848        26.3271
     15   0.0822   0.5000        0.2749       0.9178        0.2848        26.2664
     16   0.0822   0.5000        0.2749       0.9178        0.2848        26.2834
     17   0.0822   0.5000        0.2749       0.9178        0.2848        25.8391
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 06:59:44,300][0m Trial 303 finished with value: 0.25387936095372654 and parameters: {'lr': 0.06650944203952948, 'dropout': 0.6021080417816258, 'd_model_multiplier': 1, 'num_layers': 4, 'n_heads': 64, 'dim_feedforward': 378, 'batch_size': 42, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 31}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 35
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2599[0m   [32m0.7332[0m        [35m0.2957[0m       [31m0.9117[0m        [94m0.2537[0m     +  26.0648
      2   0.2497   [32m0.7539[0m        [35m0.2450[0m       [31m0.9154[0m        0.2564        26.0273
      3   0.2491   0.7538        0.2467       0.9057        0.2614        26.2484
      4   0.2546   [32m0.7616[0m        [35m0.2402[0m       0.9141        0.2578        26.1315
      5   [36m0.2737[0m   [32m0.7722[0m        0.2419       [31m0.9214[0m        [94m0.2446[0m     +  26.2074
      6   [36m0.2764[0m   0.7681        [35m0.2399[0m       0.9166        [94m0.2446[0m     +  26.1588
      7   [36m0.2854[0m   0.7698        [35m0.2384[0m       0.9154        0.2515        26.1904
      8   [36m0.2985[0m   0.7714        [35m0.2358[0m       0.9214        [94m0.2435[0m     +  26.2803
      9   [36m0.3104[0m   [32m0.7731[0m        [35m0.2337[0m       0.9190        0.2485        26.2783
     10   [36m0.3147[0m   0.7729        0.2366       0.9190        0.2524        26.2981
     11   [36m0.3296[0m   [32m0.7757[0m        [35m0.2325[0m       [31m0.9262[0m        0.2541        26.1893
     12   0.3177   0.7729        0.2333       [31m0.9274[0m        0.2570        26.1469
     13   [36m0.3441[0m   [32m0.7836[0m        [35m0.2287[0m       [31m0.9287[0m        0.2516        26.4514
     14   [36m0.3498[0m   [32m0.7870[0m        0.2308       [31m0.9299[0m        [94m0.2431[0m     +  26.2476
     15   0.3406   0.7850        0.2298       0.9226        0.2472        25.9914
     16   [36m0.3527[0m   [32m0.7892[0m        0.2291       0.9238        [94m0.2386[0m     +  26.3793
     17   0.2894   0.7818        [35m0.2282[0m       0.9226        0.2479        26.2766
     18   0.2697   0.7682        0.2312       0.9057        0.2757        26.0237
     19   0.2809   0.7615        0.2311       0.9141        0.2597        26.2525
     20   0.2402   0.7598        0.2317       0.8936        0.2843        26.1345
     21   0.2526   0.7726        0.2343       0.8924        0.2694        26.1380
     22   0.2244   0.7468        0.2331       0.8948        0.2928        25.8688
     23   0.2913   0.7665        0.2328       0.9178        0.2706        26.3979
     24   0.2491   0.7677        0.2329       0.9008        0.2773        26.2736
     25   0.2584   0.7200        0.2365       0.9238        0.2817        26.1767
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 07:11:05,964][0m Trial 304 finished with value: 0.23856732630798144 and parameters: {'lr': 0.00159838437688448, 'dropout': 0.5830285355518191, 'd_model_multiplier': 1, 'num_layers': 4, 'n_heads': 64, 'dim_feedforward': 396, 'batch_size': 47, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 35}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 40
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2953[0m   [32m0.7759[0m        [35m0.2820[0m       [31m0.9202[0m        [94m0.2303[0m     +  25.8785
      2   0.2830   [32m0.8007[0m        [35m0.2525[0m       0.9190        0.2403        26.0441
      3   [36m0.3071[0m   0.7860        [35m0.2509[0m       [31m0.9250[0m        0.2391        26.1875
      4   0.2957   0.7773        [35m0.2488[0m       0.9214        0.2474        26.1162
      5   0.2764   0.7871        0.2492       0.9154        0.2431        26.3273
      6   0.2757   0.8002        [35m0.2417[0m       0.9214        0.2463        26.1508
      7   [36m0.3144[0m   0.7865        0.2434       0.9226        0.2416        26.2781
      8   0.3107   0.7908        0.2433       0.9250        0.2783        26.1236
      9   0.2459   0.7717        0.2478       0.9226        0.2901        26.3821
     10   0.2421   0.7791        0.2554       0.9202        0.2632        26.2121
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 07:15:53,958][0m Trial 305 finished with value: 0.2302847954984011 and parameters: {'lr': 0.0022642499985031533, 'dropout': 0.5711205267142319, 'd_model_multiplier': 1, 'num_layers': 4, 'n_heads': 64, 'dim_feedforward': 417, 'batch_size': 34, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0, 'top_n_features': 40}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 45
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
Warning, assumed runtime error: CUDA out of memory. Tried to allocate 4.38 GiB (GPU 0; 23.70 GiB total capacity; 14.65 GiB already allocated; 4.04 GiB free; 18.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[32m[I 2023-05-03 07:16:00,310][0m Trial 306 finished with value: 100.0 and parameters: {'lr': 0.0033585204041731687, 'dropout': 0.5933597898145848, 'd_model_multiplier': 1, 'num_layers': 4, 'n_heads': 64, 'dim_feedforward': 403, 'batch_size': 185, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 45}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 54
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2177[0m   [32m0.7626[0m        [35m0.2794[0m       [31m0.9178[0m        [94m0.2643[0m     +  15.8056
      2   0.2064   0.7552        [35m0.2664[0m       0.9166        [94m0.2641[0m     +  16.1290
      3   0.2132   [32m0.7668[0m        [35m0.2632[0m       0.9154        0.2676        16.2088
      4   0.2106   0.7640        [35m0.2606[0m       0.9166        [94m0.2591[0m     +  16.2739
      5   0.2065   0.7636        0.2610       0.9166        [94m0.2514[0m     +  16.2620
      6   0.2142   0.7620        [35m0.2578[0m       0.9166        0.2524        16.2877
      7   0.2119   0.7632        [35m0.2559[0m       0.9178        0.2530        16.0807
      8   0.2077   0.7637        0.2595       0.9166        0.2562        16.3223
      9   0.2100   0.7586        0.2602       0.9178        0.2576        16.1353
     10   0.2080   0.7631        0.2572       0.9178        0.2552        16.1747
     11   0.2114   0.7603        0.2594       0.9178        0.2636        16.1782
     12   0.2080   0.7576        0.2576       0.9178        0.2657        16.4594
     13   0.2156   0.7642        0.2581       0.9178        0.2552        16.4900
     14   0.2108   0.7453        0.2576       0.9166        0.2558        16.4574
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 07:20:04,410][0m Trial 307 finished with value: 0.25143427781066985 and parameters: {'lr': 0.01563828554317123, 'dropout': 0.6121996685974925, 'd_model_multiplier': 1, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 385, 'batch_size': 52, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 54}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 48
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
Warning, assumed runtime error: CUDA out of memory. Tried to allocate 2.56 GiB (GPU 0; 23.70 GiB total capacity; 18.54 GiB already allocated; 1.51 GiB free; 21.10 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[32m[I 2023-05-03 07:20:11,193][0m Trial 308 finished with value: 100.0 and parameters: {'lr': 0.0013433772164743905, 'dropout': 0.3322193374466158, 'd_model_multiplier': 4, 'num_layers': 3, 'n_heads': 32, 'dim_feedforward': 433, 'batch_size': 216, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 48}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 29
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2749[0m   [32m0.7560[0m        [35m0.3043[0m       [31m0.9117[0m        [94m0.2718[0m     +  25.8656
      2   [36m0.3042[0m   [32m0.7717[0m        [35m0.2412[0m       0.9093        [94m0.2698[0m     +  26.0465
      3   0.2664   0.7653        [35m0.2355[0m       0.8924        0.2785        26.4781
      4   0.2862   0.7698        0.2377       0.9057        0.2746        26.5018
      5   0.2938   [32m0.7739[0m        [35m0.2353[0m       0.9117        0.2755        26.2636
      6   0.2782   0.7684        [35m0.2331[0m       0.9093        0.2771        26.4982
      7   [36m0.3072[0m   0.7727        0.2346       [31m0.9129[0m        0.2734        26.5867
      8   0.2946   0.7715        [35m0.2327[0m       0.9105        0.2803        26.4908
      9   [36m0.3124[0m   [32m0.7774[0m        [35m0.2323[0m       0.9117        [94m0.2689[0m     +  26.4577
     10   0.3105   [32m0.7795[0m        [35m0.2299[0m       0.9117        0.2700        26.4625
     11   [36m0.3225[0m   [32m0.7806[0m        [35m0.2287[0m       0.9129        [94m0.2672[0m     +  26.6300
     12   [36m0.3281[0m   [32m0.7854[0m        0.2323       0.9129        [94m0.2648[0m     +  26.4633
     13   [36m0.3340[0m   [32m0.7882[0m        0.2292       0.9093        [94m0.2610[0m     +  26.6133
     14   0.3230   [32m0.7911[0m        [35m0.2278[0m       0.9105        0.2640        26.3980
     15   0.3309   [32m0.7912[0m        0.2285       0.9129        0.2679        26.4325
     16   [36m0.3368[0m   [32m0.7951[0m        [35m0.2274[0m       0.9129        [94m0.2602[0m     +  26.4520
     17   [36m0.3484[0m   [32m0.7955[0m        [35m0.2268[0m       0.9093        0.2662        26.1159
     18   [36m0.3503[0m   [32m0.7987[0m        [35m0.2243[0m       0.9093        [94m0.2578[0m     +  26.4527
     19   0.3462   0.7970        0.2269       0.9105        0.2591        26.6045
     20   [36m0.3553[0m   [32m0.8001[0m        [35m0.2230[0m       0.9033        [94m0.2553[0m     +  26.2893
     21   [36m0.3651[0m   [32m0.8066[0m        0.2239       0.9117        [94m0.2544[0m     +  26.5261
     22   [36m0.3786[0m   [32m0.8069[0m        0.2231       0.9105        [94m0.2531[0m     +  26.6730
     23   [36m0.3870[0m   0.8067        [35m0.2213[0m       0.9117        [94m0.2454[0m     +  26.4784
     24   0.3633   0.8038        0.2224       0.9093        0.2565        26.2055
     25   0.3835   0.8065        [35m0.2201[0m       [31m0.9202[0m        0.2518        26.3097
     26   0.3847   [32m0.8081[0m        0.2215       0.9105        0.2505        26.3009
     27   [36m0.3887[0m   [32m0.8110[0m        0.2207       0.9069        0.2483        26.3625
     28   0.3850   0.7995        0.2205       0.9141        0.2634        26.3256
     29   0.3802   0.8066        [35m0.2194[0m       0.9117        0.2548        26.5495
     30   0.3831   0.8078        0.2201       0.9117        0.2554        26.3497
     31   0.3848   0.8082        0.2194       0.9033        0.2521        26.4168
     32   0.3826   0.8086        [35m0.2169[0m       0.9057        0.2547        26.1850
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 07:34:43,352][0m Trial 309 finished with value: 0.24539300029987002 and parameters: {'lr': 0.0012150296669572052, 'dropout': 0.57130967796178, 'd_model_multiplier': 1, 'num_layers': 4, 'n_heads': 64, 'dim_feedforward': 426, 'batch_size': 58, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 29}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 35
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1510[0m   [32m0.6612[0m        [35m0.3040[0m       [31m0.9359[0m        [94m0.2307[0m     +  10.3380
      2   [36m0.1657[0m   [32m0.6618[0m        [35m0.2542[0m       0.9311        0.2562        10.4030
      3   [36m0.1717[0m   [32m0.7073[0m        [35m0.2493[0m       0.9323        0.2620        10.6737
      4   [36m0.1746[0m   [32m0.7100[0m        [35m0.2464[0m       0.9323        0.2604        10.5864
      5   [36m0.1748[0m   [32m0.7190[0m        [35m0.2442[0m       0.9335        0.2544        10.6641
      6   [36m0.1877[0m   [32m0.7341[0m        [35m0.2416[0m       0.9323        0.2461        10.6163
      7   0.1844   0.7243        [35m0.2400[0m       0.9323        0.2505        10.4603
      8   [36m0.1945[0m   0.7333        [35m0.2369[0m       0.9323        0.2560        10.6176
      9   [36m0.1962[0m   0.7332        [35m0.2366[0m       0.9299        0.2450        10.5272
     10   [36m0.1962[0m   [32m0.7364[0m        0.2367       0.9311        0.2529        10.5649
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 07:36:39,618][0m Trial 310 finished with value: 0.23070668173275952 and parameters: {'lr': 0.002093414193178641, 'dropout': 0.6023600823108911, 'd_model_multiplier': 1, 'num_layers': 2, 'n_heads': 32, 'dim_feedforward': 400, 'batch_size': 38, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 35}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 40
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0953[0m   [32m0.3478[0m        [35m0.6467[0m       [31m0.9117[0m        [94m0.5684[0m     +  16.1116
      2   0.0887   [32m0.3491[0m        [35m0.5513[0m       0.9117        [94m0.4686[0m     +  16.1871
      3   0.0768   [32m0.3538[0m        [35m0.4593[0m       0.9117        [94m0.3952[0m     +  16.8738
      4   0.0771   [32m0.3821[0m        [35m0.3889[0m       0.9117        [94m0.3496[0m     +  16.4360
      5   0.0839   [32m0.4470[0m        [35m0.3414[0m       0.9117        [94m0.3222[0m     +  16.4647
      6   [36m0.0974[0m   [32m0.5248[0m        [35m0.3114[0m       0.9117        [94m0.3056[0m     +  16.3441
      7   [36m0.1480[0m   [32m0.5929[0m        [35m0.2935[0m       0.9117        [94m0.2949[0m     +  16.4189
      8   [36m0.1835[0m   [32m0.6368[0m        [35m0.2809[0m       0.9117        [94m0.2881[0m     +  16.3760
      9   [36m0.2176[0m   [32m0.6637[0m        [35m0.2705[0m       0.9117        [94m0.2837[0m     +  16.1444
     10   [36m0.2280[0m   [32m0.6804[0m        [35m0.2647[0m       0.9117        [94m0.2806[0m     +  16.2986
     11   [36m0.2324[0m   [32m0.6939[0m        [35m0.2600[0m       0.9117        [94m0.2788[0m     +  16.2934
     12   [36m0.2409[0m   [32m0.7055[0m        [35m0.2556[0m       0.9117        [94m0.2769[0m     +  16.4468
     13   [36m0.2507[0m   [32m0.7144[0m        [35m0.2546[0m       0.9117        [94m0.2758[0m     +  16.3531
     14   [36m0.2616[0m   [32m0.7219[0m        [35m0.2506[0m       0.9117        [94m0.2751[0m     +  16.1561
     15   [36m0.2692[0m   [32m0.7295[0m        [35m0.2491[0m       0.9117        [94m0.2738[0m     +  16.3813
     16   [36m0.2746[0m   [32m0.7345[0m        [35m0.2470[0m       0.9117        [94m0.2734[0m     +  16.3245
     17   [36m0.2813[0m   [32m0.7395[0m        0.2471       0.9117        [94m0.2725[0m     +  16.5567
     18   [36m0.2886[0m   [32m0.7437[0m        [35m0.2456[0m       0.9117        [94m0.2721[0m     +  16.2911
     19   [36m0.2959[0m   [32m0.7472[0m        [35m0.2424[0m       0.9117        [94m0.2711[0m     +  16.2761
     20   [36m0.3029[0m   [32m0.7510[0m        [35m0.2418[0m       0.9105        [94m0.2703[0m     +  16.4476
     21   [36m0.3100[0m   [32m0.7541[0m        [35m0.2407[0m       0.9105        [94m0.2690[0m     +  16.4079
     22   0.3098   [32m0.7566[0m        [35m0.2396[0m       0.9117        [94m0.2686[0m     +  16.2556
     23   [36m0.3132[0m   [32m0.7593[0m        [35m0.2394[0m       0.9117        [94m0.2684[0m     +  16.1891
     24   0.3130   [32m0.7611[0m        [35m0.2373[0m       [31m0.9166[0m        [94m0.2679[0m     +  16.3790
     25   0.3116   [32m0.7630[0m        [35m0.2373[0m       [31m0.9190[0m        [94m0.2678[0m     +  16.3225
     26   0.3106   [32m0.7651[0m        [35m0.2356[0m       0.9141        [94m0.2672[0m     +  16.6167
     27   0.3093   [32m0.7655[0m        [35m0.2350[0m       0.9141        0.2679        16.3746
     28   0.3093   [32m0.7672[0m        0.2350       0.9141        0.2677        16.2028
     29   0.3086   [32m0.7681[0m        [35m0.2320[0m       0.9129        0.2683        16.3434
     30   0.3065   [32m0.7684[0m        0.2326       0.9117        0.2689        16.4499
     31   0.3065   [32m0.7685[0m        0.2325       0.9129        0.2697        16.3783
     32   0.3074   [32m0.7694[0m        [35m0.2319[0m       0.9129        0.2699        16.4018
     33   0.3077   [32m0.7698[0m        [35m0.2319[0m       0.9129        0.2707        16.2616
     34   0.3079   0.7696        [35m0.2307[0m       0.9129        0.2716        16.5106
     35   0.3086   [32m0.7701[0m        [35m0.2304[0m       0.9117        0.2722        16.2881
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 07:46:30,916][0m Trial 311 finished with value: 0.26722026956881406 and parameters: {'lr': 4.162638974857645e-06, 'dropout': 0.5602129745318699, 'd_model_multiplier': 4, 'num_layers': 3, 'n_heads': 32, 'dim_feedforward': 412, 'batch_size': 43, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0, 'top_n_features': 40}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 45
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2597[0m   [32m0.7250[0m        [35m0.2768[0m       [31m0.9117[0m        [94m0.2535[0m     +  15.9432
      2   0.2534   [32m0.7496[0m        [35m0.2617[0m       0.9093        0.2762        16.9340
      3   [36m0.2615[0m   [32m0.7540[0m        [35m0.2520[0m       0.9081        0.2856        16.5697
      4   [36m0.2620[0m   0.7476        [35m0.2499[0m       0.9093        0.2790        16.1559
      5   0.2366   0.7409        0.2542       0.9057        0.2762        16.2211
      6   0.1917   0.6986        0.2541       0.8609        0.3028        15.9231
      7   0.1901   0.6990        0.2559       0.8730        0.2707        16.1855
      8   0.1976   0.6935        0.2582       0.8984        0.2694        16.2190
      9   0.1834   0.6985        0.2558       [31m0.9226[0m        [94m0.2535[0m     +  16.2008
     10   0.1974   0.6966        0.2583       [31m0.9238[0m        0.2582        16.2038
     11   0.2000   0.6980        0.2576       [31m0.9262[0m        0.2578        16.4909
     12   0.1985   0.7021        0.2559       0.9262        [94m0.2512[0m     +  16.1367
     13   0.2001   0.7015        0.2559       [31m0.9274[0m        0.2580        16.3227
     14   0.2058   0.6996        0.2551       0.9274        0.2536        16.5082
     15   0.2038   0.7028        0.2554       0.9214        0.2594        16.3518
     16   0.2022   0.7023        0.2561       0.9262        0.2536        16.4710
     17   0.1903   0.6998        0.2579       0.9262        0.2611        16.2846
     18   0.1968   0.6979        0.2547       0.9274        0.2545        16.3770
     19   0.2179   0.6993        0.2571       0.9262        0.2544        16.5114
     20   0.1942   0.6929        0.2565       0.9262        0.2565        16.2828
     21   0.1919   0.6982        0.2552       0.9274        0.2539        16.2334
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 07:52:30,210][0m Trial 312 finished with value: 0.25115453868996274 and parameters: {'lr': 0.010675670781809867, 'dropout': 0.6562509547460538, 'd_model_multiplier': 1, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 421, 'batch_size': 64, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 45}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 23
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1775[0m   [32m0.5931[0m        [35m0.3091[0m       [31m0.8501[0m        [94m0.4711[0m     +  72.9205
      2   [36m0.1846[0m   [32m0.6500[0m        0.3193       [31m0.9105[0m        [94m0.3078[0m     +  73.0921
      3   0.1774   [32m0.6516[0m        0.3219       0.8767        0.3422        73.4010
      4   0.1822   0.6297        0.3116       0.8755        0.3250        73.2661
      5   0.1699   0.6389        0.3111       0.8815        0.3481        73.5019
      6   0.1650   0.6501        [35m0.2886[0m       0.8972        0.3291        73.3509
      7   0.1642   0.6466        [35m0.2800[0m       0.9033        0.3347        73.3508
      8   0.1623   [32m0.6535[0m        [35m0.2695[0m       0.9081        0.3333        73.2510
      9   0.1668   [32m0.6552[0m        [35m0.2596[0m       0.9081        0.3342        73.3067
     10   0.1609   0.6362        [35m0.2571[0m       0.9008        0.3321        73.5739
     11   0.1604   0.6288        [35m0.2538[0m       0.8803        0.3402        73.3917
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 08:07:13,611][0m Trial 313 finished with value: 0.30784585408442977 and parameters: {'lr': 0.000920231383921296, 'dropout': 0.4029439995297607, 'd_model_multiplier': 32, 'num_layers': 3, 'n_heads': 64, 'dim_feedforward': 406, 'batch_size': 51, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 23}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 57
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2403[0m   [32m0.7652[0m        [35m0.3732[0m       [31m0.9347[0m        [94m0.2279[0m     +  44.5651
      2   0.1571   0.6906        0.3848       0.9347        0.2697        45.0254
      3   0.1941   0.7112        [35m0.3661[0m       0.8222        0.6341        44.9101
      4   0.1989   0.7225        [35m0.3564[0m       0.8827        0.2784        44.9234
      5   0.1110   0.6035        [35m0.3346[0m       0.9347        0.3160        45.1964
      6   0.2127   0.7253        [35m0.3280[0m       0.8936        0.2866        44.9839
      7   0.1915   0.7119        [35m0.3124[0m       0.9323        0.3240        44.7458
      8   0.1719   0.7063        [35m0.2933[0m       0.9323        0.2809        45.0375
      9   0.1595   0.7044        [35m0.2809[0m       0.9347        0.3471        44.9709
     10   0.1841   0.7127        [35m0.2754[0m       0.9299        0.2830        44.6787
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 08:15:29,233][0m Trial 314 finished with value: 0.22790655250488775 and parameters: {'lr': 0.0015891510828208817, 'dropout': 0.6225258037141053, 'd_model_multiplier': 64, 'num_layers': 2, 'n_heads': 32, 'dim_feedforward': 441, 'batch_size': 57, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 57}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 67
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.3226[0m   [32m0.7746[0m        [35m0.3890[0m       [31m0.9311[0m        [94m0.2153[0m     +  23.5272
      2   [36m0.4594[0m   [32m0.8371[0m        [35m0.2502[0m       0.9311        [94m0.1910[0m     +  23.9263
      3   [36m0.4786[0m   [32m0.8559[0m        [35m0.2402[0m       [31m0.9359[0m        [94m0.1877[0m     +  23.9991
      4   0.4758   [32m0.8642[0m        [35m0.2372[0m       [31m0.9383[0m        [94m0.1862[0m     +  23.9555
      5   0.4608   0.8625        [35m0.2358[0m       0.9371        0.1879        23.8924
      6   0.4557   [32m0.8661[0m        [35m0.2345[0m       [31m0.9395[0m        [94m0.1853[0m     +  23.8589
      7   0.4372   [32m0.8699[0m        [35m0.2313[0m       0.9395        0.1866        23.9150
      8   0.4239   0.8656        [35m0.2296[0m       [31m0.9407[0m        0.1873        23.9282
      9   0.4119   0.8643        [35m0.2287[0m       0.9395        0.1895        23.7679
     10   0.3965   0.8653        [35m0.2256[0m       0.9371        0.1901        23.7541
     11   0.3715   0.8538        [35m0.2238[0m       0.9371        0.1946        23.7788
     12   0.3354   0.8492        [35m0.2232[0m       0.9323        0.1992        23.8233
     13   0.3402   0.8359        [35m0.2198[0m       0.9323        0.2003        23.9471
     14   0.3069   0.8377        [35m0.2174[0m       0.9299        0.2029        23.8360
     15   0.2843   0.8316        [35m0.2151[0m       0.9250        0.2087        23.8208
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 08:21:51,816][0m Trial 315 finished with value: 0.1852705024696319 and parameters: {'lr': 4.4401954403124515e-05, 'dropout': 0.32041321304899045, 'd_model_multiplier': 4, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 368, 'batch_size': 26, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 67}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 60
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1311[0m   [32m0.5466[0m        [35m0.3999[0m       [31m0.9238[0m        [94m0.2728[0m     +  23.4946
      2   [36m0.2338[0m   [32m0.7177[0m        [35m0.2558[0m       0.9238        [94m0.2552[0m     +  23.6073
      3   [36m0.2995[0m   [32m0.7533[0m        [35m0.2437[0m       [31m0.9250[0m        [94m0.2441[0m     +  24.1247
      4   [36m0.3062[0m   [32m0.7750[0m        [35m0.2403[0m       0.9250        [94m0.2371[0m     +  23.5948
      5   [36m0.3092[0m   [32m0.7911[0m        [35m0.2360[0m       0.9226        [94m0.2304[0m     +  23.6843
      6   [36m0.3161[0m   [32m0.7955[0m        [35m0.2357[0m       0.9238        [94m0.2292[0m     +  23.5270
      7   [36m0.3301[0m   [32m0.7968[0m        [35m0.2338[0m       0.9214        [94m0.2292[0m     +  23.7033
      8   0.3265   [32m0.7987[0m        0.2342       0.9226        [94m0.2269[0m     +  23.7794
      9   [36m0.3401[0m   [32m0.8031[0m        [35m0.2325[0m       0.9250        0.2279        23.5558
     10   0.3356   0.8029        0.2326       0.9226        [94m0.2255[0m     +  23.7407
     11   0.3286   0.8016        [35m0.2314[0m       0.9238        0.2290        23.6380
     12   0.3359   [32m0.8057[0m        0.2330       0.9250        0.2259        23.7205
     13   [36m0.3405[0m   [32m0.8072[0m        0.2317       0.9238        0.2260        23.7548
     14   0.3290   0.8023        [35m0.2292[0m       0.9238        0.2277        23.8085
     15   0.3146   0.8007        [35m0.2287[0m       0.9238        0.2313        23.7157
     16   0.3195   0.7993        0.2296       0.9226        0.2312        23.6359
     17   0.3175   0.8008        [35m0.2282[0m       0.9202        0.2313        23.4615
     18   0.3220   0.8027        [35m0.2281[0m       0.9214        0.2298        23.5400
     19   0.3144   0.7984        [35m0.2269[0m       0.9226        0.2327        23.6356
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 08:29:46,353][0m Trial 316 finished with value: 0.22549452362390643 and parameters: {'lr': 7.238407012128088e-05, 'dropout': 0.5825896420487507, 'd_model_multiplier': 4, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 353, 'batch_size': 33, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 60}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 55
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
Warning, assumed runtime error: CUDA out of memory. Tried to allocate 4.66 GiB (GPU 0; 23.70 GiB total capacity; 16.20 GiB already allocated; 4.06 GiB free; 18.55 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[32m[I 2023-05-03 08:29:53,564][0m Trial 317 finished with value: 100.0 and parameters: {'lr': 0.00012179208456329308, 'dropout': 0.30890312041622586, 'd_model_multiplier': 4, 'num_layers': 5, 'n_heads': 64, 'dim_feedforward': 362, 'batch_size': 197, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 55}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 65
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1181[0m   [32m0.5867[0m        [35m0.6691[0m       [31m0.8755[0m        [94m0.6391[0m     +  23.9946
      2   0.0943   0.4052        [35m0.6333[0m       [31m0.9335[0m        [94m0.5906[0m     +  24.2672
      3   0.0838   0.3381        [35m0.5875[0m       [31m0.9347[0m        [94m0.5365[0m     +  25.0997
      4   0.0585   0.3095        [35m0.5364[0m       0.9347        [94m0.4831[0m     +  25.0077
      5   0.0505   0.2964        [35m0.4872[0m       0.9347        [94m0.4340[0m     +  25.1828
      6   0.0481   0.2972        [35m0.4458[0m       0.9347        [94m0.3907[0m     +  25.3030
      7   0.0491   0.3241        [35m0.4075[0m       0.9347        [94m0.3538[0m     +  24.9833
      8   0.0517   0.3665        [35m0.3769[0m       0.9347        [94m0.3230[0m     +  25.7346
      9   0.0596   0.4294        [35m0.3506[0m       0.9347        [94m0.2981[0m     +  25.2200
     10   0.0938   0.4928        [35m0.3291[0m       0.9347        [94m0.2790[0m     +  24.9880
     11   [36m0.1374[0m   0.5494        [35m0.3149[0m       0.9347        [94m0.2642[0m     +  25.3083
     12   [36m0.1765[0m   [32m0.5929[0m        [35m0.3012[0m       0.9347        [94m0.2526[0m     +  25.2861
     13   [36m0.2108[0m   [32m0.6316[0m        [35m0.2922[0m       0.9347        [94m0.2433[0m     +  25.3064
     14   [36m0.2385[0m   [32m0.6633[0m        [35m0.2854[0m       0.9347        [94m0.2357[0m     +  25.6686
     15   [36m0.2683[0m   [32m0.6878[0m        [35m0.2769[0m       0.9347        [94m0.2293[0m     +  24.8890
     16   [36m0.2860[0m   [32m0.7078[0m        [35m0.2728[0m       0.9347        [94m0.2239[0m     +  25.1765
     17   [36m0.3057[0m   [32m0.7248[0m        [35m0.2681[0m       0.9347        [94m0.2192[0m     +  25.2693
     18   [36m0.3205[0m   [32m0.7416[0m        [35m0.2641[0m       [31m0.9359[0m        [94m0.2150[0m     +  25.2039
     19   [36m0.3347[0m   [32m0.7530[0m        [35m0.2597[0m       [31m0.9371[0m        [94m0.2114[0m     +  25.5107
     20   [36m0.3489[0m   [32m0.7615[0m        [35m0.2579[0m       [31m0.9383[0m        [94m0.2082[0m     +  25.0542
     21   [36m0.3573[0m   [32m0.7694[0m        [35m0.2550[0m       0.9371        [94m0.2054[0m     +  25.1577
     22   [36m0.3577[0m   [32m0.7756[0m        [35m0.2533[0m       [31m0.9395[0m        [94m0.2029[0m     +  25.4775
     23   [36m0.3668[0m   [32m0.7812[0m        [35m0.2504[0m       [31m0.9407[0m        [94m0.2008[0m     +  24.8780
     24   [36m0.3721[0m   [32m0.7845[0m        [35m0.2484[0m       [31m0.9420[0m        [94m0.1991[0m     +  25.0819
     25   [36m0.3738[0m   [32m0.7868[0m        [35m0.2480[0m       0.9420        [94m0.1975[0m     +  25.2062
     26   [36m0.3774[0m   [32m0.7905[0m        [35m0.2462[0m       [31m0.9444[0m        [94m0.1961[0m     +  25.1478
     27   [36m0.3819[0m   [32m0.7935[0m        0.2464       0.9444        [94m0.1948[0m     +  25.4351
     28   [36m0.3841[0m   [32m0.7957[0m        [35m0.2450[0m       0.9432        [94m0.1939[0m     +  25.0693
     29   [36m0.3870[0m   [32m0.7987[0m        [35m0.2449[0m       0.9420        [94m0.1929[0m     +  25.2076
     30   [36m0.3915[0m   [32m0.7999[0m        [35m0.2443[0m       0.9420        [94m0.1922[0m     +  25.1927
     31   [36m0.3937[0m   [32m0.8019[0m        [35m0.2430[0m       0.9432        [94m0.1915[0m     +  25.1625
     32   [36m0.3953[0m   [32m0.8040[0m        [35m0.2419[0m       0.9420        [94m0.1908[0m     +  25.0448
     33   0.3856   [32m0.8046[0m        [35m0.2417[0m       0.9420        [94m0.1903[0m     +  25.0530
     34   0.3871   [32m0.8062[0m        0.2424       0.9420        [94m0.1898[0m     +  25.5025
     35   0.3876   [32m0.8065[0m        [35m0.2402[0m       0.9420        [94m0.1895[0m     +  25.2717
     36   0.3848   [32m0.8079[0m        [35m0.2394[0m       0.9420        [94m0.1890[0m     +  25.2552
     37   0.3854   [32m0.8089[0m        0.2395       0.9420        [94m0.1886[0m     +  25.1907
     38   0.3853   [32m0.8096[0m        [35m0.2394[0m       0.9420        [94m0.1883[0m     +  25.3023
     39   0.3845   [32m0.8106[0m        [35m0.2392[0m       0.9420        [94m0.1880[0m     +  25.0345
     40   0.3855   [32m0.8111[0m        [35m0.2387[0m       0.9432        [94m0.1876[0m     +  25.3027
     41   0.3844   [32m0.8116[0m        [35m0.2382[0m       0.9432        [94m0.1874[0m     +  25.0943
     42   0.3836   [32m0.8124[0m        0.2390       0.9432        [94m0.1872[0m     +  25.0212
     43   0.3844   [32m0.8133[0m        0.2388       0.9432        [94m0.1869[0m     +  25.1381
     44   0.3852   [32m0.8136[0m        0.2387       0.9432        [94m0.1867[0m     +  25.0192
     45   0.3864   [32m0.8142[0m        [35m0.2371[0m       0.9420        [94m0.1865[0m     +  25.2688
     46   0.3855   [32m0.8144[0m        [35m0.2367[0m       0.9432        [94m0.1863[0m     +  25.1480
     47   0.3859   [32m0.8150[0m        0.2370       0.9407        0.1863        25.1156
     48   0.3867   0.8148        [35m0.2366[0m       0.9407        [94m0.1861[0m     +  25.0071
     49   0.3875   [32m0.8160[0m        [35m0.2365[0m       0.9407        [94m0.1861[0m     +  25.2825
     50   0.3879   0.8159        0.2371       0.9407        [94m0.1859[0m     +  25.1277
[32m[I 2023-05-03 08:50:56,152][0m Trial 318 finished with value: 0.18587597585683002 and parameters: {'lr': 4.15409846094464e-06, 'dropout': 0.40899120279949613, 'd_model_multiplier': 4, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 390, 'batch_size': 119, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 65}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 65
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2250[0m   [32m0.7495[0m        [35m0.3934[0m       [31m0.9395[0m        [94m0.2062[0m     +  27.5747
      2   [36m0.2810[0m   [32m0.8011[0m        [35m0.2510[0m       0.9395        [94m0.1914[0m     +  27.6382
      3   [36m0.3215[0m   [32m0.8093[0m        [35m0.2405[0m       [31m0.9456[0m        [94m0.1862[0m     +  27.5214
      4   0.3177   [32m0.8141[0m        [35m0.2388[0m       0.9444        [94m0.1847[0m     +  27.4168
      5   0.3199   [32m0.8158[0m        [35m0.2367[0m       0.9432        0.1849        27.5339
      6   0.3045   [32m0.8171[0m        [35m0.2339[0m       0.9432        0.1854        27.5288
      7   0.3060   [32m0.8194[0m        0.2340       0.9432        0.1855        27.5553
      8   0.2976   [32m0.8263[0m        [35m0.2322[0m       0.9432        [94m0.1837[0m     +  27.6124
      9   0.3092   0.8250        [35m0.2317[0m       0.9432        0.1843        27.6917
     10   0.2990   0.8245        [35m0.2289[0m       0.9432        0.1870        27.6698
     11   0.2969   0.8204        [35m0.2275[0m       0.9407        0.1869        27.7206
     12   0.2981   [32m0.8318[0m        [35m0.2270[0m       0.9395        0.1845        27.5539
     13   0.2878   0.8236        [35m0.2263[0m       0.9432        0.1888        27.6705
     14   0.2902   0.8289        [35m0.2257[0m       0.9395        0.1863        27.8022
     15   0.2963   0.8298        [35m0.2238[0m       0.9407        0.1859        27.6401
     16   0.2881   [32m0.8331[0m        [35m0.2226[0m       0.9407        0.1867        27.6765
     17   0.2783   0.8311        [35m0.2200[0m       0.9383        0.1886        27.7080
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 08:59:14,271][0m Trial 319 finished with value: 0.18372506434784835 and parameters: {'lr': 4.486903567018373e-05, 'dropout': 0.4235257174803894, 'd_model_multiplier': 4, 'num_layers': 6, 'n_heads': 32, 'dim_feedforward': 371, 'batch_size': 27, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 65}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 66
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
Warning, assumed runtime error: CUDA out of memory. Tried to allocate 1.72 GiB (GPU 0; 23.70 GiB total capacity; 20.80 GiB already allocated; 957.25 MiB free; 21.67 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[32m[I 2023-05-03 08:59:19,938][0m Trial 320 finished with value: 100.0 and parameters: {'lr': 3.0461533888517433e-05, 'dropout': 0.4099153153265866, 'd_model_multiplier': 4, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 385, 'batch_size': 145, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 66}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 62
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0490[0m   [32m0.2729[0m        [35m0.6107[0m       [31m0.9323[0m        [94m0.5062[0m     +  24.6881
      2   [36m0.0509[0m   [32m0.3104[0m        [35m0.4726[0m       0.9323        [94m0.3703[0m     +  24.7438
      3   [36m0.0675[0m   [32m0.4662[0m        [35m0.3650[0m       0.9323        [94m0.2860[0m     +  25.0073
      4   [36m0.1418[0m   [32m0.6034[0m        [35m0.3035[0m       0.9323        [94m0.2509[0m     +  25.2493
      5   [36m0.1841[0m   [32m0.6581[0m        [35m0.2776[0m       0.9323        [94m0.2365[0m     +  25.4502
      6   [36m0.2018[0m   [32m0.6872[0m        [35m0.2658[0m       0.9323        [94m0.2293[0m     +  25.4161
      7   [36m0.2336[0m   [32m0.7092[0m        [35m0.2595[0m       0.9323        [94m0.2248[0m     +  25.1211
      8   [36m0.2818[0m   [32m0.7310[0m        [35m0.2552[0m       0.9323        [94m0.2212[0m     +  24.8432
      9   [36m0.3034[0m   [32m0.7503[0m        [35m0.2512[0m       0.9323        [94m0.2172[0m     +  25.4971
     10   [36m0.3159[0m   [32m0.7633[0m        [35m0.2483[0m       0.9323        [94m0.2127[0m     +  24.9975
     11   [36m0.3355[0m   [32m0.7711[0m        [35m0.2430[0m       [31m0.9359[0m        [94m0.2089[0m     +  25.1828
     12   [36m0.3492[0m   [32m0.7741[0m        [35m0.2389[0m       [31m0.9407[0m        [94m0.2070[0m     +  25.1718
     13   [36m0.3543[0m   [32m0.7784[0m        [35m0.2377[0m       0.9407        [94m0.2059[0m     +  25.5099
     14   [36m0.3571[0m   [32m0.7827[0m        [35m0.2365[0m       0.9395        [94m0.2053[0m     +  25.1850
     15   [36m0.3620[0m   [32m0.7844[0m        [35m0.2348[0m       0.9395        [94m0.2044[0m     +  25.3387
     16   [36m0.3634[0m   [32m0.7883[0m        0.2353       0.9371        [94m0.2034[0m     +  25.1031
     17   [36m0.3671[0m   [32m0.7903[0m        [35m0.2337[0m       0.9383        [94m0.2028[0m     +  25.4167
     18   [36m0.3692[0m   [32m0.7925[0m        [35m0.2331[0m       0.9383        [94m0.2023[0m     +  25.4192
     19   0.3666   [32m0.7927[0m        0.2332       0.9383        [94m0.2019[0m     +  25.1863
     20   0.3629   [32m0.7936[0m        [35m0.2322[0m       0.9383        [94m0.2019[0m     +  25.1735
     21   0.3645   [32m0.7952[0m        [35m0.2313[0m       0.9359        [94m0.2017[0m     +  25.3715
     22   0.3652   [32m0.7965[0m        [35m0.2313[0m       0.9371        [94m0.2016[0m     +  25.2195
     23   0.3675   [32m0.7980[0m        [35m0.2310[0m       0.9371        [94m0.2012[0m     +  25.3787
     24   0.3662   [32m0.7987[0m        [35m0.2300[0m       0.9359        [94m0.2010[0m     +  25.5972
     25   0.3649   [32m0.8007[0m        0.2301       0.9359        [94m0.2005[0m     +  25.2198
     26   0.3661   0.8001        [35m0.2297[0m       0.9359        0.2009        25.0884
     27   0.3681   [32m0.8016[0m        [35m0.2284[0m       0.9359        [94m0.2004[0m     +  25.2096
     28   [36m0.3713[0m   [32m0.8033[0m        0.2287       0.9359        [94m0.2004[0m     +  25.0983
     29   [36m0.3718[0m   [32m0.8040[0m        0.2298       0.9371        [94m0.2003[0m     +  25.2536
     30   [36m0.3748[0m   [32m0.8045[0m        [35m0.2280[0m       0.9383        [94m0.2001[0m     +  25.3859
     31   0.3726   [32m0.8048[0m        [35m0.2274[0m       0.9383        [94m0.2001[0m     +  25.5123
     32   0.3729   [32m0.8061[0m        0.2277       0.9359        [94m0.1996[0m     +  25.4080
     33   0.3736   [32m0.8066[0m        [35m0.2266[0m       0.9359        0.1998        25.1277
     34   0.3719   0.8062        [35m0.2262[0m       0.9371        0.2005        25.0864
     35   0.3719   [32m0.8071[0m        0.2270       0.9371        0.2003        25.3504
     36   0.3667   [32m0.8083[0m        0.2264       0.9359        0.2000        25.4111
     37   0.3635   [32m0.8084[0m        [35m0.2248[0m       0.9359        0.2001        25.3877
     38   0.3630   [32m0.8087[0m        0.2256       0.9359        0.2002        25.4997
     39   0.3608   [32m0.8095[0m        0.2264       0.9359        [94m0.1996[0m     +  25.3864
     40   0.3597   0.8095        0.2255       0.9359        0.2001        25.2057
     41   0.3616   [32m0.8112[0m        [35m0.2229[0m       0.9359        [94m0.1993[0m     +  25.4637
     42   0.3618   [32m0.8113[0m        0.2238       0.9371        0.2000        25.4323
     43   0.3600   [32m0.8118[0m        0.2245       0.9347        0.1997        25.4005
     44   0.3600   [32m0.8131[0m        0.2237       0.9359        0.1994        25.3194
     45   0.3545   [32m0.8138[0m        0.2233       0.9359        [94m0.1989[0m     +  25.3703
     46   0.3520   [32m0.8146[0m        [35m0.2225[0m       0.9347        0.1995        25.3790
     47   0.3510   [32m0.8150[0m        [35m0.2218[0m       0.9371        0.1991        25.2445
     48   0.3512   0.8145        [35m0.2212[0m       0.9359        0.2001        25.2988
     49   0.3456   0.8140        0.2217       0.9347        0.2009        25.3439
     50   0.3483   0.8148        0.2220       0.9371        0.2002        25.2373
[32m[I 2023-05-03 09:20:26,681][0m Trial 321 finished with value: 0.19891628702289393 and parameters: {'lr': 2.716770422527114e-05, 'dropout': 0.4160724343501081, 'd_model_multiplier': 4, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 360, 'batch_size': 128, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 62}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 67
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2001[0m   [32m0.6501[0m        [35m0.4097[0m       [31m0.9226[0m        [94m0.2591[0m     +  23.7977
      2   [36m0.2753[0m   [32m0.7658[0m        [35m0.2524[0m       0.9202        [94m0.2355[0m     +  23.9024
      3   [36m0.2808[0m   [32m0.7869[0m        [35m0.2378[0m       [31m0.9238[0m        [94m0.2319[0m     +  23.8208
      4   0.2787   [32m0.7978[0m        [35m0.2330[0m       0.9202        [94m0.2306[0m     +  23.9243
      5   [36m0.2818[0m   [32m0.8046[0m        [35m0.2318[0m       0.9238        [94m0.2280[0m     +  24.0722
      6   0.2771   [32m0.8105[0m        [35m0.2284[0m       0.9226        0.2281        23.8613
      7   0.2780   [32m0.8136[0m        [35m0.2284[0m       0.9190        0.2288        23.8488
      8   [36m0.2823[0m   [32m0.8158[0m        [35m0.2282[0m       0.9190        [94m0.2274[0m     +  23.8239
      9   [36m0.2869[0m   [32m0.8201[0m        [35m0.2262[0m       0.9190        0.2274        23.9429
     10   0.2833   [32m0.8222[0m        [35m0.2248[0m       0.9214        [94m0.2265[0m     +  23.6367
     11   [36m0.2918[0m   0.8216        [35m0.2233[0m       0.9202        0.2267        23.7614
     12   0.2868   0.8178        [35m0.2219[0m       0.9190        0.2272        23.6446
     13   [36m0.2960[0m   0.8184        [35m0.2210[0m       0.9190        0.2276        23.8771
     14   0.2845   0.8211        0.2215       0.9178        0.2270        23.8212
     15   0.2858   0.8202        [35m0.2190[0m       0.9178        0.2277        23.9068
     16   0.2915   0.8169        [35m0.2177[0m       0.9226        0.2272        23.7719
     17   0.2930   0.8154        [35m0.2176[0m       0.9226        0.2284        23.8039
     18   0.2788   0.8066        [35m0.2139[0m       0.9178        0.2315        23.7550
     19   0.2868   0.8105        0.2149       0.9214        0.2301        23.7657
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 09:28:24,268][0m Trial 322 finished with value: 0.22653434169732098 and parameters: {'lr': 4.321930041045881e-05, 'dropout': 0.41539678962304494, 'd_model_multiplier': 4, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 372, 'batch_size': 30, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 67}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 64
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
Warning, assumed runtime error: CUDA out of memory. Tried to allocate 1.60 GiB (GPU 0; 23.70 GiB total capacity; 19.62 GiB already allocated; 957.25 MiB free; 21.67 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[32m[I 2023-05-03 09:28:29,497][0m Trial 323 finished with value: 100.0 and parameters: {'lr': 1.1321139483958599e-05, 'dropout': 0.42944495798471294, 'd_model_multiplier': 4, 'num_layers': 6, 'n_heads': 32, 'dim_feedforward': 364, 'batch_size': 135, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 64}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 64
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
Warning, assumed runtime error: CUDA out of memory. Tried to allocate 1.55 GiB (GPU 0; 23.70 GiB total capacity; 19.05 GiB already allocated; 957.25 MiB free; 21.67 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[32m[I 2023-05-03 09:28:34,132][0m Trial 324 finished with value: 100.0 and parameters: {'lr': 4.602408843694912e-05, 'dropout': 0.4106241425853908, 'd_model_multiplier': 4, 'num_layers': 6, 'n_heads': 32, 'dim_feedforward': 371, 'batch_size': 131, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 64}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 69
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
Warning, assumed runtime error: CUDA out of memory. Tried to allocate 1.44 GiB (GPU 0; 23.70 GiB total capacity; 21.24 GiB already allocated; 955.25 MiB free; 21.67 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[32m[I 2023-05-03 09:28:38,607][0m Trial 325 finished with value: 100.0 and parameters: {'lr': 2.7444914434248516e-07, 'dropout': 0.3939289908322148, 'd_model_multiplier': 4, 'num_layers': 7, 'n_heads': 32, 'dim_feedforward': 360, 'batch_size': 122, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 69}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 62
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
Warning, assumed runtime error: CUDA out of memory. Tried to allocate 1.53 GiB (GPU 0; 23.70 GiB total capacity; 18.76 GiB already allocated; 957.25 MiB free; 21.67 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[32m[I 2023-05-03 09:28:43,857][0m Trial 326 finished with value: 100.0 and parameters: {'lr': 2.1464152520900762e-05, 'dropout': 0.4251438427271946, 'd_model_multiplier': 4, 'num_layers': 6, 'n_heads': 32, 'dim_feedforward': 375, 'batch_size': 129, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 62}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 59
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
Warning, assumed runtime error: CUDA out of memory. Tried to allocate 1.64 GiB (GPU 0; 23.70 GiB total capacity; 19.88 GiB already allocated; 957.25 MiB free; 21.67 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[32m[I 2023-05-03 09:28:48,966][0m Trial 327 finished with value: 100.0 and parameters: {'lr': 5.9430984134758494e-05, 'dropout': 0.43747273057339797, 'd_model_multiplier': 4, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 344, 'batch_size': 139, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 59}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 72
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0776[0m   [32m0.3789[0m        [35m0.6975[0m       [31m0.2842[0m        [94m0.7085[0m     +  24.2296
      2   0.0754   0.3620        [35m0.6950[0m       [31m0.3495[0m        [94m0.7043[0m     +  24.5091
      3   0.0726   0.3419        [35m0.6890[0m       [31m0.4184[0m        [94m0.6988[0m     +  24.6930
      4   0.0701   0.3230        [35m0.6831[0m       [31m0.4921[0m        [94m0.6924[0m     +  24.5140
      5   0.0683   0.3072        [35m0.6765[0m       [31m0.5707[0m        [94m0.6853[0m     +  24.6697
      6   0.0673   0.2948        [35m0.6710[0m       [31m0.6433[0m        [94m0.6776[0m     +  24.6277
      7   0.0667   0.2852        [35m0.6608[0m       [31m0.7158[0m        [94m0.6694[0m     +  25.2674
      8   0.0664   0.2796        [35m0.6533[0m       [31m0.7618[0m        [94m0.6609[0m     +  25.1343
      9   0.0662   0.2750        [35m0.6439[0m       [31m0.7969[0m        [94m0.6520[0m     +  25.4541
     10   0.0662   0.2722        [35m0.6355[0m       [31m0.8392[0m        [94m0.6429[0m     +  25.3455
     11   0.0663   0.2708        [35m0.6271[0m       [31m0.8670[0m        [94m0.6337[0m     +  25.3683
     12   0.0663   0.2691        [35m0.6175[0m       [31m0.8827[0m        [94m0.6243[0m     +  25.1749
     13   0.0665   0.2687        [35m0.6069[0m       [31m0.8936[0m        [94m0.6148[0m     +  25.0184
     14   0.0667   0.2683        [35m0.5987[0m       [31m0.8984[0m        [94m0.6053[0m     +  25.2974
     15   0.0667   0.2677        [35m0.5875[0m       [31m0.8996[0m        [94m0.5958[0m     +  25.2435
     16   0.0668   0.2675        [35m0.5781[0m       0.8996        [94m0.5863[0m     +  24.9969
     17   0.0670   0.2677        [35m0.5682[0m       0.8996        [94m0.5769[0m     +  25.5004
     18   0.0673   0.2683        [35m0.5587[0m       0.8996        [94m0.5676[0m     +  25.4981
     19   0.0676   0.2684        [35m0.5488[0m       0.8996        [94m0.5584[0m     +  25.0395
     20   0.0679   0.2686        [35m0.5394[0m       0.8996        [94m0.5493[0m     +  25.1904
     21   0.0682   0.2690        [35m0.5310[0m       0.8996        [94m0.5404[0m     +  25.2762
     22   0.0690   0.2691        [35m0.5217[0m       0.8996        [94m0.5316[0m     +  25.1172
     23   0.0690   0.2693        [35m0.5130[0m       0.8996        [94m0.5231[0m     +  25.1317
     24   0.0691   0.2695        [35m0.5027[0m       0.8996        [94m0.5147[0m     +  25.1896
     25   0.0698   0.2698        [35m0.4942[0m       0.8996        [94m0.5065[0m     +  25.0961
     26   0.0698   0.2699        [35m0.4856[0m       0.8996        [94m0.4985[0m     +  25.2833
     27   0.0699   0.2707        [35m0.4775[0m       0.8996        [94m0.4908[0m     +  25.5715
     28   0.0700   0.2711        [35m0.4705[0m       0.8996        [94m0.4833[0m     +  25.1933
     29   0.0711   0.2717        [35m0.4612[0m       0.8996        [94m0.4759[0m     +  25.1456
     30   0.0712   0.2721        [35m0.4528[0m       0.8996        [94m0.4688[0m     +  25.2093
     31   0.0713   0.2727        [35m0.4460[0m       0.8996        [94m0.4619[0m     +  25.1830
     32   0.0715   0.2738        [35m0.4383[0m       0.8996        [94m0.4552[0m     +  24.8955
     33   0.0717   0.2748        [35m0.4319[0m       0.8996        [94m0.4488[0m     +  25.3312
     34   0.0719   0.2762        [35m0.4252[0m       0.8996        [94m0.4426[0m     +  25.0266
     35   0.0720   0.2781        [35m0.4182[0m       0.8996        [94m0.4366[0m     +  25.0704
     36   0.0722   0.2796        [35m0.4121[0m       0.8996        [94m0.4309[0m     +  25.0151
     37   0.0725   0.2817        [35m0.4061[0m       0.8996        [94m0.4254[0m     +  25.3931
     38   0.0726   0.2842        [35m0.3989[0m       0.8996        [94m0.4202[0m     +  25.5187
     39   0.0729   0.2876        [35m0.3931[0m       0.8996        [94m0.4151[0m     +  25.2376
     40   0.0732   0.2910        [35m0.3887[0m       0.8996        [94m0.4102[0m     +  25.2076
     41   0.0736   0.2954        [35m0.3814[0m       0.8996        [94m0.4056[0m     +  25.4022
     42   0.0741   0.3002        [35m0.3782[0m       0.8996        [94m0.4011[0m     +  25.3152
     43   0.0745   0.3058        [35m0.3722[0m       0.8996        [94m0.3969[0m     +  24.9024
     44   0.0750   0.3122        [35m0.3659[0m       0.8996        [94m0.3929[0m     +  25.2805
     45   0.0755   0.3180        [35m0.3611[0m       0.8996        [94m0.3890[0m     +  25.4879
     46   0.0763   0.3256        [35m0.3581[0m       0.8996        [94m0.3853[0m     +  25.3785
     47   0.0769   0.3331        [35m0.3528[0m       0.8996        [94m0.3818[0m     +  25.1922
     48   [36m0.0778[0m   0.3420        [35m0.3490[0m       0.8996        [94m0.3784[0m     +  25.2902
     49   [36m0.0787[0m   0.3507        [35m0.3456[0m       0.8996        [94m0.3752[0m     +  25.3183
     50   [36m0.0816[0m   0.3592        [35m0.3422[0m       0.8996        [94m0.3722[0m     +  25.0113
[32m[I 2023-05-03 09:49:51,292][0m Trial 328 finished with value: 0.37217228040729783 and parameters: {'lr': 3.7621328206235406e-07, 'dropout': 0.4194184044345339, 'd_model_multiplier': 4, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 379, 'batch_size': 123, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 72}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 69
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
Warning, assumed runtime error: CUDA out of memory. Tried to allocate 1.66 GiB (GPU 0; 23.70 GiB total capacity; 19.98 GiB already allocated; 957.25 MiB free; 21.67 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[32m[I 2023-05-03 09:49:57,342][0m Trial 329 finished with value: 100.0 and parameters: {'lr': 8.412933737080946e-05, 'dropout': 0.44413568807291914, 'd_model_multiplier': 4, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 316, 'batch_size': 140, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 69}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 75
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0851[0m   [32m0.5627[0m        [35m0.4441[0m       [31m0.9335[0m        [94m0.2521[0m     +  23.4159
      2   [36m0.1974[0m   [32m0.6781[0m        [35m0.2662[0m       0.9335        [94m0.2241[0m     +  23.6338
      3   [36m0.3042[0m   [32m0.7505[0m        [35m0.2465[0m       [31m0.9359[0m        [94m0.2088[0m     +  23.6922
      4   [36m0.3259[0m   [32m0.7779[0m        [35m0.2404[0m       [31m0.9371[0m        [94m0.2043[0m     +  23.7863
      5   [36m0.3426[0m   [32m0.7817[0m        [35m0.2360[0m       0.9359        [94m0.2025[0m     +  23.9023
      6   0.3369   [32m0.7845[0m        [35m0.2340[0m       0.9359        [94m0.2017[0m     +  23.7340
      7   0.3350   [32m0.7871[0m        [35m0.2324[0m       0.9347        [94m0.2010[0m     +  23.8005
      8   0.3336   [32m0.7899[0m        0.2334       0.9347        0.2012        23.9346
      9   0.3317   [32m0.7926[0m        [35m0.2317[0m       0.9347        [94m0.2008[0m     +  24.0220
     10   0.3256   [32m0.7936[0m        [35m0.2304[0m       0.9347        0.2014        23.9807
     11   0.3252   [32m0.7965[0m        0.2304       0.9250        0.2013        23.9524
     12   0.3255   [32m0.7977[0m        [35m0.2290[0m       0.9262        0.2018        23.9812
     13   0.3254   [32m0.7978[0m        [35m0.2284[0m       0.9323        0.2024        23.8587
     14   0.3272   [32m0.8022[0m        [35m0.2280[0m       0.9299        0.2017        23.8976
     15   0.3274   [32m0.8033[0m        [35m0.2273[0m       0.9287        0.2022        23.9010
     16   0.3247   [32m0.8043[0m        [35m0.2265[0m       0.9274        0.2021        23.8347
     17   0.3245   [32m0.8059[0m        [35m0.2256[0m       0.9287        0.2020        23.8182
     18   0.3152   0.8040        [35m0.2236[0m       0.9274        0.2046        23.9459
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 09:57:31,534][0m Trial 330 finished with value: 0.20077502397322597 and parameters: {'lr': 3.1000014200442066e-05, 'dropout': 0.45687200918408133, 'd_model_multiplier': 4, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 366, 'batch_size': 26, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.001, 'top_n_features': 75}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 63
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1842[0m   [32m0.7015[0m        [35m0.4131[0m       [31m0.9299[0m        [94m0.2389[0m     +  23.6666
      2   [36m0.2692[0m   [32m0.7633[0m        [35m0.2517[0m       0.9287        [94m0.2203[0m     +  23.9221
      3   [36m0.2869[0m   [32m0.7644[0m        [35m0.2399[0m       0.9299        0.2216        23.7329
      4   [36m0.2980[0m   [32m0.7719[0m        [35m0.2371[0m       0.9274        0.2223        23.9424
      5   0.2816   0.7716        [35m0.2325[0m       0.9274        0.2249        23.9479
      6   0.2747   [32m0.7749[0m        [35m0.2313[0m       0.9238        0.2285        23.8511
      7   0.2727   [32m0.7802[0m        [35m0.2304[0m       0.9226        0.2272        23.6832
      8   0.2787   [32m0.7826[0m        [35m0.2295[0m       0.9190        0.2298        24.8922
      9   0.2787   [32m0.7840[0m        [35m0.2292[0m       0.9262        0.2270        23.7833
     10   0.2762   [32m0.7876[0m        0.2293       0.9214        0.2286        23.8582
     11   0.2910   0.7864        [35m0.2276[0m       0.9274        0.2274        23.8815
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 10:02:18,979][0m Trial 331 finished with value: 0.22026065332947936 and parameters: {'lr': 4.056180375329768e-05, 'dropout': 0.44898744970169713, 'd_model_multiplier': 4, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 357, 'batch_size': 26, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.001, 'top_n_features': 63}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 77
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
Warning, assumed runtime error: CUDA out of memory. Tried to allocate 1.55 GiB (GPU 0; 23.70 GiB total capacity; 19.05 GiB already allocated; 957.25 MiB free; 21.67 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[32m[I 2023-05-03 10:02:24,396][0m Trial 332 finished with value: 100.0 and parameters: {'lr': 8.358268128371433e-05, 'dropout': 0.4582675150992949, 'd_model_multiplier': 4, 'num_layers': 6, 'n_heads': 32, 'dim_feedforward': 370, 'batch_size': 131, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.001, 'top_n_features': 77}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 84
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1070[0m   [32m0.5001[0m        [35m0.6811[0m       [31m0.8634[0m        [94m0.6376[0m     +  24.6027
      2   0.0794   0.3547        [35m0.5687[0m       [31m0.9008[0m        [94m0.5083[0m     +  24.1804
      3   0.0824   0.3584        [35m0.4491[0m       0.9008        [94m0.4086[0m     +  24.1164
      4   0.0923   0.4284        [35m0.3628[0m       0.9008        [94m0.3542[0m     +  24.5431
      5   [36m0.1285[0m   [32m0.5433[0m        [35m0.3115[0m       0.9008        [94m0.3277[0m     +  24.2342
      6   [36m0.1647[0m   [32m0.6151[0m        [35m0.2842[0m       0.9008        [94m0.3148[0m     +  24.7318
      7   [36m0.1928[0m   [32m0.6522[0m        [35m0.2696[0m       0.9008        [94m0.3082[0m     +  24.6160
      8   [36m0.2093[0m   [32m0.6724[0m        [35m0.2607[0m       0.9008        [94m0.3046[0m     +  24.7396
      9   [36m0.2159[0m   [32m0.6841[0m        [35m0.2543[0m       0.9008        [94m0.3024[0m     +  24.8397
     10   [36m0.2215[0m   [32m0.6942[0m        [35m0.2490[0m       0.9008        [94m0.3008[0m     +  24.8029
     11   [36m0.2281[0m   [32m0.7058[0m        [35m0.2464[0m       0.9008        [94m0.2997[0m     +  24.5818
     12   [36m0.2351[0m   [32m0.7139[0m        [35m0.2447[0m       0.9008        [94m0.2988[0m     +  24.5051
     13   [36m0.2406[0m   [32m0.7204[0m        [35m0.2415[0m       0.8996        [94m0.2984[0m     +  24.5902
     14   [36m0.2457[0m   [32m0.7256[0m        [35m0.2394[0m       0.8984        [94m0.2982[0m     +  24.8092
     15   [36m0.2512[0m   [32m0.7298[0m        [35m0.2361[0m       0.8996        0.2984        24.9998
     16   [36m0.2562[0m   [32m0.7321[0m        [35m0.2349[0m       0.8996        0.2993        24.7629
     17   [36m0.2594[0m   [32m0.7330[0m        [35m0.2335[0m       0.8960        0.3004        24.8954
     18   [36m0.2605[0m   0.7325        [35m0.2316[0m       0.8960        0.3016        24.7184
     19   [36m0.2652[0m   [32m0.7339[0m        [35m0.2291[0m       0.8996        0.3023        24.6058
     20   [36m0.2697[0m   [32m0.7361[0m        [35m0.2288[0m       0.8984        0.3026        24.7181
     21   [36m0.2713[0m   [32m0.7378[0m        [35m0.2275[0m       0.8972        0.3027        24.6868
     22   [36m0.2738[0m   [32m0.7388[0m        0.2289       0.8996        0.3035        24.5701
     23   [36m0.2749[0m   [32m0.7406[0m        0.2283       0.8984        0.3030        24.8827
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 10:12:17,505][0m Trial 333 finished with value: 0.2981507511136033 and parameters: {'lr': 1.1910564713980704e-05, 'dropout': 0.4286218384119692, 'd_model_multiplier': 4, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 348, 'batch_size': 108, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.001, 'top_n_features': 84}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 74
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1331[0m   [32m0.5680[0m        [35m0.4280[0m       [31m0.9045[0m        [94m0.3170[0m     +  23.7728
      2   [36m0.2700[0m   [32m0.7554[0m        [35m0.2549[0m       0.9045        [94m0.2862[0m     +  23.9796
      3   [36m0.3539[0m   [32m0.7921[0m        [35m0.2378[0m       [31m0.9069[0m        [94m0.2718[0m     +  24.3537
      4   [36m0.3669[0m   [32m0.7974[0m        [35m0.2328[0m       0.9045        [94m0.2702[0m     +  23.9934
      5   0.3658   [32m0.8070[0m        [35m0.2309[0m       0.9069        [94m0.2652[0m     +  23.8663
      6   0.3648   [32m0.8122[0m        [35m0.2279[0m       [31m0.9093[0m        [94m0.2642[0m     +  24.0536
      7   0.3604   [32m0.8169[0m        [35m0.2271[0m       [31m0.9105[0m        0.2650        23.8793
      8   0.3579   [32m0.8174[0m        [35m0.2260[0m       0.9081        [94m0.2628[0m     +  24.0310
      9   0.3549   [32m0.8198[0m        [35m0.2254[0m       0.9081        0.2643        24.0701
     10   0.3490   [32m0.8198[0m        [35m0.2248[0m       0.9045        0.2661        24.0375
     11   0.3444   [32m0.8222[0m        [35m0.2218[0m       0.9033        0.2657        24.1821
     12   0.3457   [32m0.8230[0m        0.2226       0.9069        0.2665        24.1654
     13   0.3398   0.8214        [35m0.2218[0m       0.9021        0.2700        24.0767
     14   0.3348   0.8192        [35m0.2199[0m       0.9045        0.2727        23.9904
     15   0.3276   0.8181        [35m0.2171[0m       0.9069        0.2745        23.9309
     16   0.3307   0.8172        0.2180       0.9021        0.2761        24.1200
     17   0.3309   0.8180        [35m0.2170[0m       0.9057        0.2753        24.0839
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 10:19:31,190][0m Trial 334 finished with value: 0.26280482288139184 and parameters: {'lr': 2.8546863635989452e-05, 'dropout': 0.4387614995920764, 'd_model_multiplier': 4, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 360, 'batch_size': 24, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.001, 'top_n_features': 74}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 53
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0551[0m   [32m0.2909[0m        [35m0.6351[0m       [31m0.9287[0m        [94m0.5861[0m     +  23.9384
      2   0.0520   [32m0.3013[0m        [35m0.5478[0m       0.9287        [94m0.4843[0m     +  23.8868
      3   0.0514   [32m0.3136[0m        [35m0.4587[0m       0.9287        [94m0.3962[0m     +  24.0556
      4   0.0531   [32m0.3403[0m        [35m0.3909[0m       0.9287        [94m0.3370[0m     +  23.9848
      5   [36m0.0581[0m   [32m0.4101[0m        [35m0.3433[0m       0.9287        [94m0.3009[0m     +  23.9674
      6   [36m0.0674[0m   [32m0.4838[0m        [35m0.3139[0m       0.9287        [94m0.2793[0m     +  24.0871
      7   [36m0.0909[0m   [32m0.5415[0m        [35m0.2947[0m       0.9287        [94m0.2661[0m     +  23.9690
      8   [36m0.1170[0m   [32m0.5800[0m        [35m0.2818[0m       0.9287        [94m0.2578[0m     +  23.9450
      9   [36m0.1346[0m   [32m0.6047[0m        [35m0.2741[0m       0.9287        [94m0.2525[0m     +  23.9323
     10   [36m0.1487[0m   [32m0.6222[0m        [35m0.2654[0m       0.9287        [94m0.2488[0m     +  23.8831
     11   [36m0.1620[0m   [32m0.6373[0m        [35m0.2617[0m       0.9287        [94m0.2462[0m     +  23.8827
     12   [36m0.1674[0m   [32m0.6482[0m        [35m0.2593[0m       0.9287        [94m0.2443[0m     +  24.0747
     13   [36m0.1730[0m   [32m0.6572[0m        [35m0.2547[0m       0.9287        [94m0.2428[0m     +  23.9080
     14   [36m0.1777[0m   [32m0.6652[0m        [35m0.2524[0m       0.9287        [94m0.2416[0m     +  24.1577
     15   [36m0.1779[0m   [32m0.6733[0m        [35m0.2505[0m       0.9274        [94m0.2406[0m     +  24.1009
     16   [36m0.1790[0m   [32m0.6805[0m        [35m0.2488[0m       0.9274        [94m0.2398[0m     +  24.0408
     17   [36m0.1791[0m   [32m0.6872[0m        [35m0.2465[0m       0.9274        [94m0.2391[0m     +  24.0736
     18   [36m0.1806[0m   [32m0.6923[0m        [35m0.2459[0m       0.9274        [94m0.2386[0m     +  24.0569
     19   [36m0.1830[0m   [32m0.6981[0m        [35m0.2443[0m       0.9274        [94m0.2382[0m     +  23.9377
     20   [36m0.1864[0m   [32m0.7026[0m        [35m0.2421[0m       0.9274        [94m0.2379[0m     +  23.8955
     21   [36m0.1890[0m   [32m0.7067[0m        [35m0.2395[0m       0.9274        [94m0.2379[0m     +  23.9844
     22   [36m0.1900[0m   [32m0.7099[0m        [35m0.2388[0m       0.9250        0.2379        24.0108
     23   [36m0.1943[0m   [32m0.7131[0m        0.2392       0.9250        0.2380        23.9649
     24   [36m0.1956[0m   [32m0.7154[0m        [35m0.2361[0m       0.9226        0.2382        23.8640
     25   [36m0.1968[0m   [32m0.7169[0m        0.2369       0.9226        0.2386        24.0094
     26   [36m0.2008[0m   [32m0.7182[0m        0.2365       0.9226        0.2389        23.8985
     27   [36m0.2030[0m   [32m0.7194[0m        [35m0.2356[0m       0.9214        0.2393        23.9789
     28   [36m0.2030[0m   [32m0.7197[0m        [35m0.2342[0m       0.9226        0.2397        23.8465
     29   [36m0.2035[0m   [32m0.7206[0m        [35m0.2331[0m       0.9226        0.2403        23.8462
     30   [36m0.2064[0m   [32m0.7219[0m        0.2336       0.9226        0.2407        24.0925
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 10:31:57,127][0m Trial 335 finished with value: 0.2378626114800302 and parameters: {'lr': 1.456275672852354e-06, 'dropout': 0.3971426323549794, 'd_model_multiplier': 4, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 385, 'batch_size': 26, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 53}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 68
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
Warning, assumed runtime error: CUDA out of memory. Tried to allocate 1.61 GiB (GPU 0; 23.70 GiB total capacity; 19.77 GiB already allocated; 957.25 MiB free; 21.67 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[32m[I 2023-05-03 10:32:02,281][0m Trial 336 finished with value: 100.0 and parameters: {'lr': 4.721374029369515e-05, 'dropout': 0.45821513816578646, 'd_model_multiplier': 4, 'num_layers': 6, 'n_heads': 32, 'dim_feedforward': 368, 'batch_size': 136, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.001, 'top_n_features': 68}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 74
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0660[0m   [32m0.2604[0m        [35m0.7418[0m       [31m0.4293[0m        [94m0.6848[0m     +  24.2556
      2   [36m0.0789[0m   [32m0.2786[0m        [35m0.6018[0m       [31m0.9045[0m        [94m0.5345[0m     +  24.6555
      3   0.0728   [32m0.2931[0m        [35m0.4614[0m       0.9045        [94m0.4161[0m     +  24.7398
      4   [36m0.0908[0m   [32m0.4277[0m        [35m0.3554[0m       0.9045        [94m0.3447[0m     +  24.8482
      5   [36m0.1504[0m   [32m0.5617[0m        [35m0.2964[0m       0.9045        [94m0.3166[0m     +  25.2511
      6   [36m0.1951[0m   [32m0.6310[0m        [35m0.2706[0m       0.9045        [94m0.3042[0m     +  24.9494
      7   [36m0.2205[0m   [32m0.6671[0m        [35m0.2588[0m       0.9045        [94m0.2976[0m     +  24.9645
      8   [36m0.2541[0m   [32m0.6983[0m        [35m0.2510[0m       0.9045        [94m0.2920[0m     +  25.2456
      9   [36m0.2842[0m   [32m0.7177[0m        [35m0.2462[0m       0.9045        [94m0.2874[0m     +  24.8257
     10   [36m0.3108[0m   [32m0.7376[0m        [35m0.2420[0m       [31m0.9057[0m        [94m0.2818[0m     +  25.3087
     11   [36m0.3274[0m   [32m0.7522[0m        [35m0.2396[0m       0.9057        [94m0.2764[0m     +  24.9654
     12   [36m0.3357[0m   [32m0.7624[0m        [35m0.2358[0m       [31m0.9105[0m        [94m0.2727[0m     +  25.4177
     13   [36m0.3435[0m   [32m0.7691[0m        [35m0.2337[0m       0.9093        [94m0.2708[0m     +  25.3112
     14   [36m0.3436[0m   [32m0.7744[0m        [35m0.2324[0m       0.9045        [94m0.2696[0m     +  25.2344
     15   [36m0.3507[0m   [32m0.7798[0m        [35m0.2307[0m       0.9057        [94m0.2683[0m     +  25.7558
     16   [36m0.3539[0m   [32m0.7820[0m        [35m0.2297[0m       0.9045        [94m0.2680[0m     +  25.2682
     17   [36m0.3580[0m   [32m0.7855[0m        [35m0.2276[0m       0.9069        [94m0.2666[0m     +  25.4437
     18   [36m0.3601[0m   [32m0.7871[0m        [35m0.2276[0m       0.9093        [94m0.2662[0m     +  25.0364
     19   [36m0.3659[0m   [32m0.7898[0m        [35m0.2271[0m       0.9105        [94m0.2650[0m     +  24.8574
     20   [36m0.3676[0m   [32m0.7916[0m        [35m0.2267[0m       0.9093        [94m0.2646[0m     +  25.0851
     21   0.3660   [32m0.7936[0m        [35m0.2256[0m       0.9093        [94m0.2642[0m     +  25.0087
     22   0.3668   [32m0.7951[0m        [35m0.2246[0m       0.9093        [94m0.2635[0m     +  25.3534
     23   [36m0.3676[0m   [32m0.7969[0m        0.2250       0.9093        [94m0.2632[0m     +  25.2146
     24   [36m0.3697[0m   [32m0.7987[0m        [35m0.2238[0m       0.9093        [94m0.2631[0m     +  25.3619
     25   [36m0.3704[0m   [32m0.7994[0m        0.2241       0.9093        [94m0.2628[0m     +  25.2878
     26   0.3699   [32m0.8007[0m        0.2244       0.9093        [94m0.2622[0m     +  25.2097
     27   [36m0.3736[0m   [32m0.8009[0m        [35m0.2238[0m       [31m0.9129[0m        0.2624        25.1339
     28   0.3726   [32m0.8014[0m        0.2241       0.9117        0.2623        25.1861
     29   0.3721   [32m0.8030[0m        [35m0.2226[0m       0.9129        [94m0.2615[0m     +  25.3190
     30   0.3693   [32m0.8042[0m        [35m0.2226[0m       0.9117        [94m0.2614[0m     +  24.7188
     31   0.3721   [32m0.8049[0m        [35m0.2209[0m       0.9117        0.2618        25.2433
     32   0.3720   [32m0.8060[0m        0.2218       0.9117        [94m0.2611[0m     +  25.1958
     33   0.3712   0.8059        [35m0.2208[0m       0.9117        [94m0.2609[0m     +  25.4924
     34   0.3718   [32m0.8063[0m        0.2215       0.9117        0.2610        25.1904
     35   0.3703   0.8062        [35m0.2208[0m       0.9117        [94m0.2606[0m     +  25.1327
     36   0.3719   [32m0.8070[0m        [35m0.2205[0m       0.9093        [94m0.2605[0m     +  25.1749
     37   0.3720   [32m0.8076[0m        0.2211       0.9093        [94m0.2597[0m     +  25.0066
     38   0.3715   [32m0.8078[0m        [35m0.2171[0m       0.9093        0.2599        25.2871
     39   0.3713   [32m0.8081[0m        0.2201       0.9105        0.2598        24.8509
     40   0.3706   [32m0.8084[0m        0.2175       0.9117        0.2600        24.7899
     41   0.3718   [32m0.8086[0m        0.2175       0.9105        0.2604        24.9420
     42   0.3697   0.8085        0.2201       0.9117        0.2601        25.1243
     43   0.3686   [32m0.8102[0m        0.2184       0.9129        [94m0.2594[0m     +  25.1112
     44   0.3682   [32m0.8103[0m        0.2187       0.9129        [94m0.2592[0m     +  25.2178
     45   0.3678   0.8092        0.2186       0.9105        0.2598        25.1695
     46   0.3690   0.8099        0.2175       0.9117        0.2601        25.1624
     47   0.3696   [32m0.8106[0m        0.2173       0.9117        0.2603        25.3075
     48   0.3685   [32m0.8108[0m        [35m0.2166[0m       0.9117        0.2602        25.3069
     49   0.3667   0.8100        [35m0.2151[0m       0.9105        0.2608        25.1407
     50   0.3696   [32m0.8111[0m        0.2153       0.9105        0.2607        25.1512
[32m[I 2023-05-03 10:53:01,954][0m Trial 337 finished with value: 0.2592127644417508 and parameters: {'lr': 1.8125238747832238e-05, 'dropout': 0.41769710428100415, 'd_model_multiplier': 4, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 337, 'batch_size': 121, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.001, 'top_n_features': 74}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 111
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0847[0m   [32m0.4626[0m        [35m0.4844[0m       [31m0.9129[0m        [94m0.3202[0m     +  23.9015
      2   [36m0.2867[0m   [32m0.7417[0m        [35m0.2813[0m       0.9129        [94m0.2628[0m     +  24.1291
      3   [36m0.3300[0m   [32m0.7731[0m        [35m0.2547[0m       0.9129        [94m0.2513[0m     +  24.0864
      4   [36m0.3584[0m   [32m0.7956[0m        [35m0.2485[0m       [31m0.9141[0m        [94m0.2431[0m     +  24.0401
      5   [36m0.3756[0m   [32m0.8051[0m        [35m0.2422[0m       0.9093        [94m0.2389[0m     +  23.9424
      6   [36m0.3772[0m   [32m0.8076[0m        [35m0.2376[0m       0.9093        [94m0.2371[0m     +  24.0125
      7   [36m0.3793[0m   [32m0.8091[0m        [35m0.2355[0m       [31m0.9154[0m        [94m0.2363[0m     +  24.1767
      8   [36m0.3814[0m   [32m0.8109[0m        [35m0.2343[0m       [31m0.9190[0m        [94m0.2354[0m     +  24.2086
      9   [36m0.3816[0m   0.8105        [35m0.2324[0m       0.9178        0.2359        24.1513
     10   [36m0.3819[0m   0.8097        [35m0.2311[0m       0.9154        0.2364        24.0752
     11   [36m0.3861[0m   [32m0.8112[0m        0.2316       0.9178        0.2366        24.2642
     12   0.3852   0.8095        [35m0.2290[0m       0.9166        0.2377        24.0203
     13   0.3857   0.8098        0.2293       0.9154        0.2381        24.0337
     14   [36m0.3911[0m   0.8096        [35m0.2270[0m       0.9166        0.2373        23.8736
     15   0.3885   0.8076        [35m0.2264[0m       0.9166        0.2387        24.0521
     16   [36m0.3976[0m   0.8089        0.2286       0.9190        0.2365        23.8677
     17   0.3895   0.8071        [35m0.2261[0m       0.9178        0.2381        23.9329
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 11:00:15,900][0m Trial 338 finished with value: 0.23538421866052672 and parameters: {'lr': 1.6886696824454682e-05, 'dropout': 0.4346903522008182, 'd_model_multiplier': 4, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 391, 'batch_size': 25, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 111}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 61
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0862[0m   [32m0.3962[0m        [35m0.4801[0m       [31m0.9117[0m        [94m0.3393[0m     +  23.5666
      2   [36m0.2856[0m   [32m0.6990[0m        [35m0.2842[0m       0.9117        [94m0.2700[0m     +  23.4352
      3   [36m0.3538[0m   [32m0.7671[0m        [35m0.2507[0m       [31m0.9141[0m        [94m0.2522[0m     +  23.9833
      4   [36m0.3574[0m   [32m0.7991[0m        [35m0.2396[0m       0.9141        [94m0.2444[0m     +  23.6901
      5   [36m0.3601[0m   [32m0.8073[0m        [35m0.2343[0m       [31m0.9178[0m        [94m0.2427[0m     +  23.6022
      6   0.3508   [32m0.8099[0m        [35m0.2315[0m       0.9178        [94m0.2424[0m     +  24.1303
      7   0.3574   [32m0.8113[0m        [35m0.2293[0m       [31m0.9190[0m        [94m0.2416[0m     +  23.9976
      8   0.3571   [32m0.8140[0m        [35m0.2275[0m       0.9190        [94m0.2410[0m     +  24.1467
      9   0.3532   [32m0.8160[0m        0.2286       [31m0.9214[0m        [94m0.2404[0m     +  23.8752
     10   [36m0.3682[0m   [32m0.8197[0m        0.2278       0.9214        [94m0.2389[0m     +  23.8145
     11   0.3640   [32m0.8206[0m        [35m0.2265[0m       0.9190        0.2400        23.8343
     12   0.3611   [32m0.8214[0m        0.2268       0.9214        0.2394        23.9527
     13   [36m0.3743[0m   [32m0.8233[0m        [35m0.2254[0m       0.9202        0.2389        24.2707
     14   0.3721   [32m0.8262[0m        [35m0.2235[0m       0.9178        [94m0.2382[0m     +  24.0933
     15   0.3658   [32m0.8280[0m        0.2248       0.9190        [94m0.2382[0m     +  23.9915
     16   [36m0.3802[0m   [32m0.8297[0m        [35m0.2231[0m       0.9214        [94m0.2364[0m     +  23.9953
     17   [36m0.3835[0m   [32m0.8313[0m        [35m0.2221[0m       0.9214        [94m0.2358[0m     +  23.9144
     18   [36m0.3876[0m   [32m0.8347[0m        [35m0.2212[0m       0.9166        [94m0.2353[0m     +  23.9074
     19   0.3855   0.8333        0.2215       0.9154        0.2370        23.9598
     20   0.3857   [32m0.8349[0m        [35m0.2205[0m       0.9178        0.2371        24.0305
     21   [36m0.3951[0m   [32m0.8357[0m        [35m0.2196[0m       0.9166        0.2354        24.0929
     22   0.3855   0.8357        0.2200       0.9178        0.2363        24.0099
     23   0.3892   [32m0.8385[0m        0.2202       0.9166        0.2356        24.0228
     24   [36m0.3955[0m   [32m0.8412[0m        [35m0.2179[0m       0.9178        [94m0.2333[0m     +  23.7004
     25   0.3817   0.8406        [35m0.2164[0m       0.9141        0.2356        23.5741
     26   [36m0.4094[0m   [32m0.8450[0m        0.2165       0.9166        [94m0.2319[0m     +  23.9242
     27   0.4047   0.8435        [35m0.2164[0m       0.9178        [94m0.2318[0m     +  23.8525
     28   [36m0.4145[0m   0.8444        [35m0.2137[0m       0.9178        0.2321        24.0854
     29   0.4102   [32m0.8474[0m        0.2142       0.9190        [94m0.2301[0m     +  24.3193
     30   [36m0.4170[0m   0.8454        [35m0.2123[0m       0.9178        0.2323        24.0218
     31   0.4052   0.8473        [35m0.2117[0m       0.9166        0.2325        23.9473
     32   0.4074   [32m0.8481[0m        [35m0.2111[0m       0.9154        0.2331        23.9227
     33   0.4002   0.8459        0.2114       0.9166        0.2332        23.8669
     34   0.3943   0.8480        [35m0.2092[0m       0.9178        0.2349        23.8885
     35   0.4008   0.8479        [35m0.2088[0m       0.9141        0.2329        24.0250
     36   0.3866   0.8466        [35m0.2087[0m       0.9117        0.2362        23.9631
     37   0.3943   0.8469        0.2091       0.9093        0.2347        23.8183
     38   0.3928   0.8445        [35m0.2072[0m       0.9117        0.2356        23.7475
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 11:15:50,894][0m Trial 339 finished with value: 0.23005077977098318 and parameters: {'lr': 5.2634919330828e-05, 'dropout': 0.44519995857509087, 'd_model_multiplier': 4, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 354, 'batch_size': 67, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 61}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 57
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0633[0m   [32m0.3409[0m        [35m0.6149[0m       [31m0.9250[0m        [94m0.4911[0m     +  19.6694
      2   0.0588   [32m0.3571[0m        [35m0.4402[0m       0.9250        [94m0.3452[0m     +  19.7898
      3   [36m0.0746[0m   [32m0.5032[0m        [35m0.3404[0m       0.9250        [94m0.2845[0m     +  19.9239
      4   [36m0.1594[0m   [32m0.6219[0m        [35m0.2955[0m       0.9250        [94m0.2601[0m     +  20.0614
      5   [36m0.2072[0m   [32m0.6670[0m        [35m0.2736[0m       0.9250        [94m0.2498[0m     +  20.0096
      6   [36m0.2361[0m   [32m0.6920[0m        [35m0.2653[0m       0.9250        [94m0.2447[0m     +  20.6254
      7   [36m0.2543[0m   [32m0.7089[0m        [35m0.2586[0m       0.9250        [94m0.2422[0m     +  20.0675
      8   [36m0.2752[0m   [32m0.7269[0m        [35m0.2550[0m       0.9250        [94m0.2398[0m     +  20.0214
      9   [36m0.2879[0m   [32m0.7372[0m        [35m0.2523[0m       0.9250        [94m0.2388[0m     +  19.9870
     10   [36m0.2990[0m   [32m0.7503[0m        [35m0.2496[0m       0.9250        [94m0.2370[0m     +  20.0164
     11   [36m0.3002[0m   [32m0.7590[0m        [35m0.2476[0m       0.9250        [94m0.2350[0m     +  19.9094
     12   [36m0.3049[0m   [32m0.7673[0m        [35m0.2467[0m       0.9250        [94m0.2326[0m     +  19.9106
     13   [36m0.3157[0m   [32m0.7726[0m        [35m0.2457[0m       [31m0.9262[0m        [94m0.2303[0m     +  20.0052
     14   [36m0.3227[0m   [32m0.7790[0m        [35m0.2423[0m       0.9250        [94m0.2275[0m     +  20.1274
     15   [36m0.3238[0m   [32m0.7850[0m        0.2425       [31m0.9274[0m        [94m0.2250[0m     +  20.1812
     16   [36m0.3307[0m   [32m0.7904[0m        [35m0.2406[0m       0.9274        [94m0.2236[0m     +  20.1643
     17   [36m0.3346[0m   [32m0.7948[0m        [35m0.2384[0m       [31m0.9299[0m        [94m0.2226[0m     +  19.9873
     18   [36m0.3466[0m   [32m0.7964[0m        [35m0.2381[0m       [31m0.9311[0m        [94m0.2224[0m     +  20.1086
     19   [36m0.3514[0m   [32m0.7989[0m        [35m0.2370[0m       [31m0.9335[0m        [94m0.2221[0m     +  19.9475
     20   0.3503   [32m0.7999[0m        0.2374       0.9335        [94m0.2218[0m     +  19.9587
     21   0.3508   [32m0.8003[0m        [35m0.2356[0m       [31m0.9347[0m        [94m0.2217[0m     +  20.1680
     22   [36m0.3521[0m   [32m0.8013[0m        0.2360       0.9347        [94m0.2214[0m     +  20.1320
     23   [36m0.3567[0m   [32m0.8014[0m        0.2366       0.9347        [94m0.2211[0m     +  19.9260
     24   [36m0.3567[0m   [32m0.8021[0m        [35m0.2348[0m       0.9347        [94m0.2210[0m     +  19.9931
     25   [36m0.3602[0m   [32m0.8028[0m        0.2360       0.9347        [94m0.2207[0m     +  20.0081
     26   0.3588   [32m0.8031[0m        [35m0.2335[0m       0.9347        [94m0.2207[0m     +  20.0714
     27   [36m0.3615[0m   [32m0.8038[0m        0.2346       0.9347        [94m0.2203[0m     +  20.1267
     28   [36m0.3644[0m   [32m0.8047[0m        0.2339       0.9311        [94m0.2201[0m     +  19.9746
     29   [36m0.3663[0m   [32m0.8049[0m        0.2350       0.9311        [94m0.2200[0m     +  20.0524
     30   [36m0.3680[0m   [32m0.8055[0m        [35m0.2325[0m       0.9311        [94m0.2199[0m     +  19.9231
     31   [36m0.3690[0m   [32m0.8061[0m        0.2331       0.9311        [94m0.2197[0m     +  20.1050
     32   0.3677   [32m0.8067[0m        0.2332       0.9311        [94m0.2195[0m     +  20.1189
     33   0.3655   [32m0.8078[0m        0.2330       0.9299        [94m0.2194[0m     +  20.0073
     34   0.3653   [32m0.8086[0m        [35m0.2317[0m       0.9287        0.2196        20.1270
     35   [36m0.3700[0m   [32m0.8091[0m        0.2321       0.9299        [94m0.2191[0m     +  20.0376
     36   [36m0.3726[0m   [32m0.8095[0m        0.2323       0.9287        0.2194        20.0657
     37   [36m0.3743[0m   [32m0.8098[0m        0.2317       0.9287        0.2192        20.0321
     38   0.3724   [32m0.8103[0m        [35m0.2310[0m       0.9287        [94m0.2190[0m     +  19.9378
     39   [36m0.3752[0m   [32m0.8107[0m        0.2331       0.9287        [94m0.2184[0m     +  19.9558
     40   0.3748   [32m0.8109[0m        0.2331       0.9287        0.2185        20.0991
     41   [36m0.3763[0m   [32m0.8116[0m        [35m0.2310[0m       0.9274        0.2187        19.9488
     42   0.3725   [32m0.8122[0m        0.2318       0.9274        0.2187        20.1276
     43   0.3684   [32m0.8130[0m        0.2318       0.9262        0.2185        20.1785
     44   0.3698   [32m0.8135[0m        0.2312       0.9262        [94m0.2182[0m     +  20.1110
     45   0.3733   [32m0.8142[0m        [35m0.2302[0m       0.9262        0.2185        20.1176
     46   0.3732   [32m0.8147[0m        0.2322       0.9262        0.2184        20.1052
     47   0.3740   [32m0.8154[0m        0.2315       0.9262        [94m0.2181[0m     +  19.9804
     48   0.3730   [32m0.8157[0m        0.2304       0.9262        [94m0.2179[0m     +  19.9684
     49   0.3741   [32m0.8164[0m        0.2306       0.9262        [94m0.2177[0m     +  19.8903
     50   0.3752   [32m0.8168[0m        [35m0.2301[0m       0.9262        0.2182        20.0481
[32m[I 2023-05-03 11:32:37,377][0m Trial 340 finished with value: 0.21770191897143765 and parameters: {'lr': 5.383265826859601e-06, 'dropout': 0.48555593816820053, 'd_model_multiplier': 4, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 376, 'batch_size': 30, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 57}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 121
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0509[0m   [32m0.3044[0m        [35m0.5684[0m       [31m0.9311[0m        [94m0.4742[0m     +  21.3056
      2   [36m0.0526[0m   [32m0.3158[0m        [35m0.4440[0m       0.9311        [94m0.3473[0m     +  21.1031
      3   [36m0.0724[0m   [32m0.4879[0m        [35m0.3465[0m       0.9311        [94m0.2779[0m     +  21.4982
      4   [36m0.1158[0m   [32m0.6306[0m        [35m0.2979[0m       0.9311        [94m0.2495[0m     +  21.5189
      5   [36m0.1748[0m   [32m0.6780[0m        [35m0.2753[0m       0.9311        [94m0.2367[0m     +  21.2960
      6   [36m0.1862[0m   [32m0.6876[0m        [35m0.2647[0m       0.9311        [94m0.2305[0m     +  21.5713
      7   [36m0.1904[0m   [32m0.6927[0m        [35m0.2581[0m       0.9311        [94m0.2275[0m     +  21.6577
      8   [36m0.2112[0m   [32m0.6995[0m        [35m0.2547[0m       0.9311        [94m0.2259[0m     +  21.4703
      9   [36m0.2283[0m   [32m0.7056[0m        [35m0.2530[0m       0.9311        [94m0.2249[0m     +  21.2940
     10   [36m0.2309[0m   [32m0.7135[0m        [35m0.2498[0m       0.9311        [94m0.2240[0m     +  21.0642
     11   [36m0.2418[0m   [32m0.7197[0m        [35m0.2493[0m       0.9311        [94m0.2230[0m     +  21.5078
     12   [36m0.2499[0m   [32m0.7237[0m        [35m0.2486[0m       0.9311        [94m0.2222[0m     +  21.6073
     13   [36m0.2555[0m   [32m0.7287[0m        [35m0.2462[0m       [31m0.9323[0m        [94m0.2215[0m     +  21.4987
     14   [36m0.2705[0m   [32m0.7340[0m        [35m0.2431[0m       [31m0.9335[0m        [94m0.2211[0m     +  21.5561
     15   [36m0.2705[0m   [32m0.7352[0m        [35m0.2414[0m       [31m0.9347[0m        0.2216        21.2313
     16   [36m0.2774[0m   [32m0.7384[0m        0.2414       0.9335        0.2225        21.4027
     17   [36m0.2784[0m   [32m0.7409[0m        [35m0.2399[0m       0.9323        0.2235        21.5989
     18   0.2773   [32m0.7437[0m        [35m0.2385[0m       0.9262        0.2245        21.3021
     19   0.2750   [32m0.7467[0m        0.2393       0.9238        0.2248        21.3536
     20   0.2775   [32m0.7490[0m        0.2388       0.9250        0.2264        21.1818
     21   0.2770   [32m0.7528[0m        [35m0.2363[0m       0.9250        0.2268        21.5108
     22   0.2777   [32m0.7548[0m        [35m0.2355[0m       0.9238        0.2276        21.7837
     23   [36m0.2793[0m   [32m0.7594[0m        0.2365       0.9226        0.2281        21.6158
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 11:41:13,103][0m Trial 341 finished with value: 0.22107779513030992 and parameters: {'lr': 2.8714617190710076e-05, 'dropout': 0.466467132049762, 'd_model_multiplier': 4, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 366, 'batch_size': 117, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 121}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 66
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0562[0m   [32m0.4011[0m        [35m0.4934[0m       [31m0.9323[0m        [94m0.3257[0m     +  23.4291
      2   [36m0.2247[0m   [32m0.6301[0m        [35m0.3028[0m       0.9323        [94m0.2404[0m     +  23.5586
      3   [36m0.2403[0m   [32m0.7155[0m        [35m0.2586[0m       [31m0.9347[0m        [94m0.2260[0m     +  23.8158
      4   [36m0.2638[0m   [32m0.7486[0m        [35m0.2444[0m       0.9262        [94m0.2201[0m     +  24.1721
      5   [36m0.2737[0m   [32m0.7612[0m        [35m0.2385[0m       0.9262        [94m0.2173[0m     +  23.8907
      6   0.2732   [32m0.7669[0m        [35m0.2355[0m       0.9250        [94m0.2156[0m     +  23.6974
      7   0.2689   [32m0.7696[0m        [35m0.2339[0m       0.9238        [94m0.2149[0m     +  23.7371
      8   0.2720   [32m0.7714[0m        [35m0.2323[0m       0.9238        [94m0.2145[0m     +  23.8792
      9   0.2714   [32m0.7731[0m        [35m0.2300[0m       0.9262        0.2148        24.0090
     10   0.2710   [32m0.7775[0m        0.2304       0.9250        [94m0.2143[0m     +  23.9537
     11   0.2692   [32m0.7812[0m        [35m0.2281[0m       0.9250        [94m0.2139[0m     +  23.8102
     12   0.2716   [32m0.7842[0m        [35m0.2276[0m       0.9250        [94m0.2136[0m     +  23.8339
     13   0.2722   [32m0.7881[0m        [35m0.2261[0m       0.9274        [94m0.2116[0m     +  24.0711
     14   [36m0.2778[0m   [32m0.7956[0m        0.2262       0.9299        [94m0.2103[0m     +  24.0215
     15   0.2722   [32m0.7975[0m        [35m0.2249[0m       0.9311        [94m0.2100[0m     +  23.9930
     16   0.2735   [32m0.8038[0m        [35m0.2234[0m       0.9299        [94m0.2093[0m     +  24.0565
     17   [36m0.2791[0m   [32m0.8104[0m        [35m0.2222[0m       0.9299        [94m0.2080[0m     +  24.0167
     18   [36m0.2837[0m   [32m0.8121[0m        [35m0.2216[0m       0.9299        [94m0.2075[0m     +  24.0786
     19   [36m0.2909[0m   [32m0.8169[0m        [35m0.2200[0m       0.9299        [94m0.2059[0m     +  23.8981
     20   [36m0.2931[0m   [32m0.8231[0m        [35m0.2189[0m       0.9311        0.2062        23.6718
     21   [36m0.3016[0m   [32m0.8258[0m        [35m0.2174[0m       0.9299        [94m0.2043[0m     +  23.8519
     22   [36m0.3037[0m   0.8251        0.2176       0.9311        0.2056        23.8864
     23   [36m0.3092[0m   0.8246        [35m0.2148[0m       0.9299        0.2045        24.2160
     24   [36m0.3138[0m   [32m0.8258[0m        0.2159       0.9299        0.2043        23.9959
     25   [36m0.3189[0m   0.8238        [35m0.2139[0m       0.9311        [94m0.2029[0m     +  24.0511
     26   [36m0.3228[0m   [32m0.8289[0m        [35m0.2124[0m       0.9311        0.2051        23.8491
     27   0.3037   0.8184        [35m0.2108[0m       0.9311        0.2086        24.0381
     28   0.3092   0.8163        [35m0.2103[0m       0.9335        0.2052        24.0734
     29   [36m0.3255[0m   0.8215        [35m0.2095[0m       0.9335        0.2051        23.8648
     30   [36m0.3329[0m   0.8231        [35m0.2078[0m       0.9335        0.2033        23.9204
     31   0.3112   0.8220        [35m0.2062[0m       0.9323        0.2057        23.7112
     32   0.3037   0.8157        [35m0.2062[0m       0.9323        0.2073        23.7934
     33   0.3245   0.8144        [35m0.2047[0m       0.9335        0.2066        24.2132
     34   0.3178   0.8179        [35m0.2036[0m       0.9311        0.2064        24.0225
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 11:55:12,419][0m Trial 342 finished with value: 0.20293092021342488 and parameters: {'lr': 4.0993108452000126e-05, 'dropout': 0.28008113926190453, 'd_model_multiplier': 4, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 328, 'batch_size': 70, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 66}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 94
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0386[0m   [32m0.2677[0m        [35m0.6189[0m       [31m0.9444[0m        [94m0.4550[0m     +  31.0011
      2   [36m0.0458[0m   [32m0.4157[0m        [35m0.3930[0m       0.9444        [94m0.2618[0m     +  30.8966
      3   [36m0.1602[0m   [32m0.6079[0m        [35m0.2965[0m       0.9444        [94m0.2161[0m     +  31.3675
      4   [36m0.2179[0m   [32m0.6665[0m        [35m0.2697[0m       0.9444        [94m0.2018[0m     +  31.2874
      5   [36m0.2453[0m   [32m0.7013[0m        [35m0.2575[0m       [31m0.9456[0m        [94m0.1930[0m     +  31.4673
      6   [36m0.2744[0m   [32m0.7245[0m        [35m0.2519[0m       0.9444        [94m0.1877[0m     +  31.3292
      7   [36m0.2835[0m   [32m0.7340[0m        [35m0.2464[0m       0.9444        [94m0.1855[0m     +  31.3468
      8   0.2810   [32m0.7358[0m        [35m0.2443[0m       0.9456        [94m0.1843[0m     +  31.5986
      9   0.2820   [32m0.7404[0m        [35m0.2410[0m       0.9456        [94m0.1835[0m     +  31.4497
     10   0.2778   [32m0.7422[0m        [35m0.2403[0m       0.9444        [94m0.1828[0m     +  31.5352
     11   [36m0.2842[0m   [32m0.7469[0m        [35m0.2374[0m       0.9456        [94m0.1820[0m     +  31.4900
     12   0.2820   [32m0.7474[0m        0.2384       0.9456        [94m0.1816[0m     +  31.5349
     13   [36m0.2913[0m   [32m0.7511[0m        [35m0.2355[0m       0.9456        [94m0.1813[0m     +  31.3365
     14   [36m0.2919[0m   [32m0.7525[0m        0.2365       0.9456        [94m0.1807[0m     +  31.3534
     15   [36m0.2937[0m   [32m0.7554[0m        [35m0.2328[0m       0.9456        0.1807        31.2617
     16   [36m0.3029[0m   [32m0.7593[0m        0.2338       0.9456        [94m0.1803[0m     +  31.5786
     17   0.2980   [32m0.7611[0m        0.2330       0.9456        [94m0.1800[0m     +  31.2288
     18   0.2983   [32m0.7630[0m        [35m0.2318[0m       [31m0.9468[0m        0.1803        31.3907
     19   0.3019   [32m0.7687[0m        [35m0.2295[0m       0.9444        [94m0.1799[0m     +  31.3962
     20   0.3009   [32m0.7717[0m        0.2297       0.9444        [94m0.1795[0m     +  31.4222
     21   0.3006   0.7694        [35m0.2288[0m       0.9456        0.1807        31.5257
     22   [36m0.3065[0m   0.7710        0.2294       0.9432        0.1795        31.5161
     23   [36m0.3109[0m   [32m0.7760[0m        [35m0.2266[0m       0.9456        0.1817        31.2834
     24   0.3079   0.7752        [35m0.2260[0m       0.9468        0.1816        31.4302
     25   0.3100   0.7728        [35m0.2260[0m       0.9456        0.1808        31.4838
     26   0.3051   0.7731        [35m0.2257[0m       0.9456        0.1806        31.5220
     27   [36m0.3239[0m   0.7691        [35m0.2247[0m       [31m0.9480[0m        0.1816        31.3753
     28   [36m0.3345[0m   [32m0.7792[0m        [35m0.2244[0m       0.9480        0.1797        31.4602
     29   0.3248   0.7720        0.2252       [31m0.9492[0m        0.1821        31.2330
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 12:10:56,035][0m Trial 343 finished with value: 0.1795279707268619 and parameters: {'lr': 2.2008095673154024e-05, 'dropout': 0.40960719043999105, 'd_model_multiplier': 4, 'num_layers': 7, 'n_heads': 32, 'dim_feedforward': 330, 'batch_size': 74, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.001, 'top_n_features': 94}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 96
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0683[0m   [32m0.3081[0m        [35m0.5681[0m       [31m0.9033[0m        [94m0.4349[0m     +  27.1709
      2   [36m0.1467[0m   [32m0.5792[0m        [35m0.3305[0m       0.9033        [94m0.3167[0m     +  27.8308
      3   [36m0.2203[0m   [32m0.6706[0m        [35m0.2609[0m       0.9033        [94m0.2998[0m     +  27.2901
      4   [36m0.2421[0m   [32m0.7033[0m        [35m0.2464[0m       0.9033        [94m0.2930[0m     +  27.6375
      5   [36m0.2611[0m   [32m0.7336[0m        [35m0.2380[0m       0.8996        [94m0.2867[0m     +  27.4796
      6   [36m0.2784[0m   [32m0.7452[0m        [35m0.2322[0m       0.8936        [94m0.2846[0m     +  27.8171
      7   [36m0.2911[0m   [32m0.7494[0m        [35m0.2300[0m       0.8900        [94m0.2832[0m     +  27.7763
      8   [36m0.2945[0m   [32m0.7532[0m        [35m0.2275[0m       0.8912        0.2838        27.9034
      9   [36m0.3019[0m   [32m0.7569[0m        [35m0.2274[0m       0.8924        0.2836        27.7848
     10   [36m0.3054[0m   [32m0.7580[0m        [35m0.2253[0m       0.8875        [94m0.2830[0m     +  27.4809
     11   0.3040   [32m0.7601[0m        [35m0.2237[0m       0.8888        [94m0.2828[0m     +  28.0206
     12   0.3037   [32m0.7615[0m        [35m0.2223[0m       0.8936        [94m0.2826[0m     +  27.8782
     13   [36m0.3056[0m   [32m0.7638[0m        [35m0.2210[0m       0.8972        [94m0.2821[0m     +  27.6229
     14   0.3029   [32m0.7661[0m        [35m0.2191[0m       0.8924        0.2822        27.4761
     15   0.3053   [32m0.7683[0m        0.2193       0.8936        0.2835        27.6184
     16   [36m0.3071[0m   [32m0.7697[0m        [35m0.2191[0m       0.8984        [94m0.2821[0m     +  27.7037
     17   [36m0.3109[0m   0.7695        [35m0.2186[0m       0.8996        0.2846        27.7411
     18   0.3054   [32m0.7702[0m        [35m0.2159[0m       0.9008        0.2848        28.2663
     19   0.3029   0.7685        [35m0.2149[0m       0.9021        0.2849        27.9764
     20   0.2970   0.7672        [35m0.2132[0m       0.9033        0.2869        27.8146
     21   0.2917   0.7652        [35m0.2121[0m       0.8984        0.2876        28.0891
     22   0.2911   0.7662        [35m0.2120[0m       0.8996        0.2894        27.7799
     23   0.2872   0.7654        [35m0.2107[0m       0.8984        0.2932        27.3256
     24   0.2820   0.7598        0.2113       0.9008        0.2948        27.6925
     25   0.2825   0.7602        0.2119       0.8996        0.2937        27.7406
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 12:22:57,886][0m Trial 344 finished with value: 0.28205069910370023 and parameters: {'lr': 3.362448107653732e-05, 'dropout': 0.30275957597189507, 'd_model_multiplier': 4, 'num_layers': 6, 'n_heads': 32, 'dim_feedforward': 306, 'batch_size': 74, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.001, 'top_n_features': 96}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 89
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0492[0m   [32m0.3298[0m        [35m0.6247[0m       [31m0.9347[0m        [94m0.4254[0m     +  30.7416
      2   [36m0.1601[0m   [32m0.6449[0m        [35m0.3533[0m       0.9347        [94m0.2527[0m     +  30.7618
      3   [36m0.2582[0m   [32m0.7594[0m        [35m0.2719[0m       0.9335        [94m0.2194[0m     +  31.3138
      4   [36m0.2797[0m   [32m0.7904[0m        [35m0.2513[0m       0.9323        [94m0.2105[0m     +  31.5019
      5   [36m0.2907[0m   [32m0.7986[0m        [35m0.2435[0m       0.9335        [94m0.2076[0m     +  30.9438
      6   [36m0.2957[0m   [32m0.8026[0m        [35m0.2395[0m       0.9299        [94m0.2063[0m     +  31.4932
      7   [36m0.2997[0m   [32m0.8035[0m        [35m0.2377[0m       0.9311        [94m0.2050[0m     +  31.4326
      8   [36m0.3095[0m   [32m0.8052[0m        [35m0.2351[0m       0.9311        0.2060        31.1108
      9   [36m0.3114[0m   [32m0.8076[0m        [35m0.2336[0m       0.9311        0.2069        31.3562
     10   [36m0.3219[0m   [32m0.8083[0m        [35m0.2329[0m       0.9335        [94m0.2042[0m     +  31.7288
     11   [36m0.3247[0m   0.8077        [35m0.2327[0m       0.9311        0.2059        31.2868
     12   0.3189   0.8067        [35m0.2300[0m       0.9311        0.2066        31.2603
     13   0.3145   0.8058        [35m0.2296[0m       0.9299        [94m0.2041[0m     +  31.4792
     14   0.3111   0.8041        [35m0.2280[0m       0.9299        0.2041        31.4030
     15   0.3093   0.8034        [35m0.2275[0m       0.9323        [94m0.2030[0m     +  31.3437
     16   0.3118   0.8038        [35m0.2265[0m       0.9311        0.2040        31.1296
     17   0.3109   0.8024        [35m0.2250[0m       0.9323        0.2032        31.4654
     18   0.3099   0.8020        0.2255       0.9347        [94m0.2029[0m     +  31.4306
     19   0.3139   0.8006        [35m0.2243[0m       0.9347        [94m0.2008[0m     +  31.3369
     20   0.3085   0.7978        [35m0.2220[0m       0.9323        0.2027        31.5181
     21   0.3052   0.7959        0.2226       0.9311        0.2043        31.3883
     22   0.3086   0.7954        0.2236       0.9323        0.2047        31.3025
     23   0.3060   0.7923        [35m0.2206[0m       0.9335        0.2041        31.4073
     24   0.3080   0.7930        [35m0.2194[0m       0.9311        0.2041        31.3503
     25   0.2956   0.7917        0.2207       0.9323        0.2055        31.2853
     26   0.3003   0.7899        [35m0.2171[0m       0.9299        0.2057        31.1274
     27   0.3066   0.7891        [35m0.2161[0m       0.9323        0.2046        31.3822
     28   0.2961   0.7878        [35m0.2157[0m       [31m0.9359[0m        0.2055        31.4086
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 12:38:07,779][0m Trial 345 finished with value: 0.2008238984080439 and parameters: {'lr': 2.51870353448516e-05, 'dropout': 0.26281034229496003, 'd_model_multiplier': 4, 'num_layers': 7, 'n_heads': 32, 'dim_feedforward': 332, 'batch_size': 71, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.001, 'top_n_features': 89}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 78
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0717[0m   [32m0.3315[0m        [35m0.5977[0m       [31m0.9093[0m        [94m0.4600[0m     +  27.2207
      2   [36m0.1236[0m   [32m0.5411[0m        [35m0.3691[0m       0.9093        [94m0.3174[0m     +  27.6790
      3   [36m0.2619[0m   [32m0.7082[0m        [35m0.2785[0m       0.9093        [94m0.2794[0m     +  27.7663
      4   [36m0.3434[0m   [32m0.7505[0m        [35m0.2512[0m       [31m0.9129[0m        [94m0.2637[0m     +  27.3746
      5   [36m0.3750[0m   [32m0.7655[0m        [35m0.2387[0m       [31m0.9166[0m        [94m0.2567[0m     +  27.8795
      6   [36m0.3872[0m   [32m0.7667[0m        [35m0.2334[0m       [31m0.9178[0m        [94m0.2548[0m     +  27.6761
      7   [36m0.3872[0m   0.7633        [35m0.2298[0m       0.9154        0.2556        27.4823
      8   0.3853   0.7622        [35m0.2287[0m       0.9154        0.2557        27.6217
      9   [36m0.3886[0m   0.7606        [35m0.2263[0m       0.9141        0.2556        27.4744
     10   0.3829   0.7590        [35m0.2240[0m       0.9154        0.2577        27.8712
     11   0.3783   0.7611        [35m0.2227[0m       0.9141        0.2575        27.8965
     12   0.3797   0.7615        0.2227       0.9141        0.2590        27.7142
     13   0.3759   0.7620        [35m0.2197[0m       0.9154        0.2606        27.4839
     14   0.3774   0.7632        0.2202       0.9141        0.2594        27.6113
     15   0.3729   0.7633        0.2199       0.9154        0.2616        27.8760
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 12:45:31,026][0m Trial 346 finished with value: 0.25481315198587534 and parameters: {'lr': 2.367329125221069e-05, 'dropout': 0.2965158776801595, 'd_model_multiplier': 4, 'num_layers': 6, 'n_heads': 32, 'dim_feedforward': 327, 'batch_size': 73, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.001, 'top_n_features': 78}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 86
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0449[0m   [32m0.2954[0m        [35m0.6041[0m       [31m0.9335[0m        [94m0.4084[0m     +  31.5006
      2   [36m0.0910[0m   [32m0.5033[0m        [35m0.3459[0m       0.9335        [94m0.2695[0m     +  31.3972
      3   [36m0.1208[0m   [32m0.6135[0m        [35m0.2696[0m       0.9335        [94m0.2438[0m     +  31.5656
      4   [36m0.1668[0m   [32m0.6849[0m        [35m0.2510[0m       0.9323        [94m0.2285[0m     +  31.4701
      5   [36m0.2151[0m   [32m0.7100[0m        [35m0.2411[0m       0.9323        [94m0.2213[0m     +  31.6885
      6   [36m0.2464[0m   [32m0.7255[0m        [35m0.2351[0m       0.9335        [94m0.2181[0m     +  31.5726
      7   [36m0.2614[0m   [32m0.7401[0m        [35m0.2325[0m       0.9335        [94m0.2157[0m     +  31.5595
      8   [36m0.2672[0m   [32m0.7461[0m        [35m0.2309[0m       [31m0.9347[0m        [94m0.2146[0m     +  31.4944
      9   0.2663   [32m0.7535[0m        [35m0.2291[0m       0.9347        [94m0.2139[0m     +  31.8782
     10   0.2670   [32m0.7606[0m        [35m0.2276[0m       0.9323        [94m0.2135[0m     +  31.6328
     11   0.2664   [32m0.7651[0m        [35m0.2262[0m       0.9323        0.2137        31.4384
     12   0.2640   [32m0.7669[0m        [35m0.2253[0m       0.9335        0.2142        31.5370
     13   [36m0.2675[0m   [32m0.7726[0m        [35m0.2237[0m       0.9323        0.2141        31.4721
     14   0.2608   [32m0.7774[0m        [35m0.2222[0m       0.9323        0.2142        32.2718
     15   0.2672   0.7761        [35m0.2205[0m       [31m0.9359[0m        0.2149        31.6820
     16   0.2645   [32m0.7813[0m        [35m0.2197[0m       0.9335        0.2155        31.3848
     17   0.2556   [32m0.7821[0m        [35m0.2185[0m       0.9359        0.2154        31.2629
     18   0.2566   [32m0.7870[0m        [35m0.2174[0m       0.9335        0.2144        31.7257
     19   0.2571   [32m0.7911[0m        0.2182       0.9311        0.2145        31.9458
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 12:56:04,754][0m Trial 347 finished with value: 0.21352458349738762 and parameters: {'lr': 3.821042736918238e-05, 'dropout': 0.2701552552434475, 'd_model_multiplier': 4, 'num_layers': 7, 'n_heads': 32, 'dim_feedforward': 334, 'batch_size': 91, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.001, 'top_n_features': 86}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 91
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0599[0m   [32m0.3441[0m        [35m0.6045[0m       [31m0.9359[0m        [94m0.4974[0m     +  30.8742
      2   [36m0.0656[0m   [32m0.3934[0m        [35m0.4348[0m       0.9359        [94m0.3318[0m     +  30.9559
      3   [36m0.1047[0m   [32m0.5492[0m        [35m0.3235[0m       0.9359        [94m0.2622[0m     +  31.4609
      4   [36m0.1398[0m   [32m0.6384[0m        [35m0.2770[0m       0.9359        [94m0.2385[0m     +  31.0629
      5   [36m0.1638[0m   [32m0.6887[0m        [35m0.2586[0m       0.9347        [94m0.2277[0m     +  31.4315
      6   [36m0.1666[0m   [32m0.7122[0m        [35m0.2489[0m       0.9323        [94m0.2236[0m     +  31.5159
      7   [36m0.1703[0m   [32m0.7207[0m        [35m0.2422[0m       0.9311        [94m0.2226[0m     +  31.3214
      8   [36m0.1712[0m   [32m0.7270[0m        [35m0.2386[0m       0.9323        [94m0.2224[0m     +  31.6765
      9   0.1705   [32m0.7343[0m        [35m0.2348[0m       0.9299        [94m0.2222[0m     +  31.5596
     10   0.1705   [32m0.7407[0m        [35m0.2345[0m       0.9274        [94m0.2213[0m     +  31.5360
     11   0.1703   [32m0.7449[0m        [35m0.2323[0m       0.9262        0.2217        31.4766
     12   [36m0.1713[0m   [32m0.7483[0m        [35m0.2309[0m       0.9262        0.2213        31.4312
     13   0.1705   [32m0.7516[0m        [35m0.2301[0m       0.9262        0.2221        31.2763
     14   0.1691   [32m0.7523[0m        [35m0.2287[0m       0.9262        0.2225        31.5013
     15   0.1707   [32m0.7541[0m        [35m0.2285[0m       0.9274        0.2218        31.6733
     16   [36m0.1718[0m   [32m0.7552[0m        [35m0.2261[0m       0.9274        0.2215        31.5215
     17   [36m0.1733[0m   [32m0.7566[0m        [35m0.2259[0m       0.9274        0.2216        31.5989
     18   [36m0.1742[0m   [32m0.7579[0m        [35m0.2244[0m       0.9262        0.2216        31.6927
     19   [36m0.1768[0m   [32m0.7581[0m        [35m0.2229[0m       0.9250        0.2221        31.6404
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 13:06:34,642][0m Trial 348 finished with value: 0.22129065663235076 and parameters: {'lr': 1.527964921127624e-05, 'dropout': 0.2512075036425189, 'd_model_multiplier': 4, 'num_layers': 7, 'n_heads': 32, 'dim_feedforward': 321, 'batch_size': 77, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.001, 'top_n_features': 91}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 99
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0744[0m   [32m0.3065[0m        [35m0.5538[0m       [31m0.9166[0m        [94m0.4177[0m     +  31.0231
      2   [36m0.1179[0m   [32m0.5998[0m        [35m0.3395[0m       0.9166        [94m0.2894[0m     +  31.0170
      3   [36m0.2476[0m   [32m0.7012[0m        [35m0.2707[0m       0.9166        [94m0.2626[0m     +  31.0117
      4   [36m0.3009[0m   [32m0.7266[0m        [35m0.2538[0m       0.9166        [94m0.2545[0m     +  31.5392
      5   [36m0.3136[0m   [32m0.7498[0m        [35m0.2471[0m       0.9154        [94m0.2479[0m     +  31.4749
      6   [36m0.3303[0m   [32m0.7679[0m        [35m0.2385[0m       [31m0.9190[0m        [94m0.2424[0m     +  31.4276
      7   [36m0.3321[0m   [32m0.7731[0m        [35m0.2349[0m       [31m0.9226[0m        [94m0.2413[0m     +  31.3779
      8   [36m0.3342[0m   [32m0.7788[0m        [35m0.2325[0m       [31m0.9238[0m        [94m0.2402[0m     +  31.4535
      9   0.3288   [32m0.7809[0m        [35m0.2302[0m       0.9226        [94m0.2394[0m     +  31.3243
     10   0.3283   [32m0.7827[0m        [35m0.2281[0m       0.9202        [94m0.2391[0m     +  31.0927
     11   0.3249   [32m0.7857[0m        [35m0.2276[0m       0.9190        [94m0.2388[0m     +  31.1135
     12   0.3226   0.7843        [35m0.2252[0m       0.9178        0.2396        31.2463
     13   0.3295   [32m0.7880[0m        0.2253       0.9202        [94m0.2384[0m     +  31.2532
     14   0.3269   0.7878        [35m0.2242[0m       0.9166        0.2389        31.0349
     15   0.3232   0.7861        [35m0.2234[0m       0.9166        0.2397        31.6828
     16   0.3190   0.7863        [35m0.2230[0m       0.9166        0.2398        31.1806
     17   0.3225   [32m0.7894[0m        [35m0.2220[0m       0.9166        0.2389        31.2228
     18   0.3107   0.7872        [35m0.2210[0m       0.9190        0.2409        31.3372
     19   0.3166   [32m0.7901[0m        [35m0.2203[0m       0.9190        0.2395        31.4872
     20   0.3180   0.7879        [35m0.2185[0m       0.9190        0.2407        31.3718
     21   0.3041   0.7859        [35m0.2182[0m       0.9178        0.2414        31.3952
     22   0.3069   0.7861        [35m0.2174[0m       0.9190        0.2420        31.3332
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 13:18:36,105][0m Trial 349 finished with value: 0.23836079535345933 and parameters: {'lr': 2.6243195611505512e-05, 'dropout': 0.28527947563874767, 'd_model_multiplier': 4, 'num_layers': 7, 'n_heads': 32, 'dim_feedforward': 339, 'batch_size': 70, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.001, 'top_n_features': 99}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 89
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0739[0m   [32m0.3589[0m        [35m0.5707[0m       [31m0.9178[0m        [94m0.4618[0m     +  27.6308
      2   [36m0.0826[0m   [32m0.4588[0m        [35m0.4041[0m       0.9178        [94m0.3315[0m     +  27.6844
      3   [36m0.1687[0m   [32m0.6539[0m        [35m0.3039[0m       0.9178        [94m0.2777[0m     +  27.6138
      4   [36m0.2084[0m   [32m0.7207[0m        [35m0.2683[0m       0.9178        [94m0.2600[0m     +  28.1434
      5   [36m0.2553[0m   [32m0.7547[0m        [35m0.2534[0m       0.9166        [94m0.2498[0m     +  28.1083
      6   [36m0.3004[0m   [32m0.7723[0m        [35m0.2450[0m       [31m0.9202[0m        [94m0.2424[0m     +  28.1221
      7   [36m0.3101[0m   [32m0.7798[0m        [35m0.2392[0m       0.9129        [94m0.2389[0m     +  27.7659
      8   [36m0.3161[0m   [32m0.7827[0m        [35m0.2356[0m       0.9093        [94m0.2379[0m     +  28.0037
      9   0.3139   [32m0.7832[0m        [35m0.2319[0m       0.9093        [94m0.2373[0m     +  27.9051
     10   0.3090   [32m0.7840[0m        [35m0.2311[0m       0.9081        [94m0.2371[0m     +  28.1414
     11   0.3093   [32m0.7853[0m        [35m0.2292[0m       0.9081        0.2372        28.2413
     12   0.3155   [32m0.7859[0m        [35m0.2284[0m       0.9081        [94m0.2368[0m     +  28.0609
     13   0.3154   [32m0.7876[0m        [35m0.2279[0m       0.9117        0.2373        27.8714
     14   0.3148   [32m0.7889[0m        [35m0.2252[0m       0.9129        0.2370        28.0362
     15   [36m0.3169[0m   [32m0.7900[0m        [35m0.2244[0m       0.9141        [94m0.2368[0m     +  28.4013
     16   0.3144   [32m0.7902[0m        0.2245       0.9166        0.2386        28.1994
     17   0.3142   0.7896        [35m0.2235[0m       0.9166        0.2389        27.8333
     18   0.3139   0.7901        [35m0.2235[0m       0.9178        0.2392        28.0193
     19   0.3153   [32m0.7913[0m        [35m0.2213[0m       0.9178        0.2404        28.3122
     20   0.3152   [32m0.7935[0m        [35m0.2206[0m       0.9154        0.2401        27.8002
     21   0.3156   [32m0.7943[0m        0.2209       0.9154        0.2418        28.2357
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 13:28:53,640][0m Trial 350 finished with value: 0.23676397902789686 and parameters: {'lr': 2.049530522154144e-05, 'dropout': 0.26621373677566407, 'd_model_multiplier': 4, 'num_layers': 6, 'n_heads': 32, 'dim_feedforward': 327, 'batch_size': 86, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.001, 'top_n_features': 89}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 102
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0547[0m   [32m0.3173[0m        [35m0.5322[0m       [31m0.9299[0m        [94m0.3451[0m     +  23.5703
      2   [36m0.1852[0m   [32m0.6268[0m        [35m0.3052[0m       0.9299        [94m0.2455[0m     +  23.9832
      3   [36m0.2092[0m   [32m0.6799[0m        [35m0.2607[0m       0.9299        [94m0.2338[0m     +  24.2262
      4   [36m0.2267[0m   [32m0.7114[0m        [35m0.2500[0m       0.9299        [94m0.2280[0m     +  23.9883
      5   [36m0.2668[0m   [32m0.7406[0m        [35m0.2439[0m       0.9287        [94m0.2216[0m     +  24.3088
      6   [36m0.2692[0m   [32m0.7445[0m        [35m0.2382[0m       0.9287        0.2237        23.9986
      7   [36m0.2717[0m   [32m0.7484[0m        [35m0.2348[0m       0.9262        0.2243        24.2545
      8   0.2683   [32m0.7542[0m        [35m0.2323[0m       0.9274        0.2255        24.0147
      9   0.2691   [32m0.7569[0m        [35m0.2311[0m       0.9262        0.2270        24.0645
     10   [36m0.2741[0m   [32m0.7614[0m        [35m0.2298[0m       0.9274        0.2264        24.4560
     11   0.2649   [32m0.7617[0m        [35m0.2286[0m       0.9262        0.2290        24.1836
     12   0.2737   [32m0.7646[0m        [35m0.2285[0m       0.9262        0.2277        24.2330
     13   0.2616   0.7645        0.2294       0.9274        0.2279        24.2521
     14   0.2705   0.7620        [35m0.2268[0m       0.9262        0.2301        24.3498
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 13:34:56,449][0m Trial 351 finished with value: 0.22159553977785837 and parameters: {'lr': 5.806473899547747e-05, 'dropout': 0.40352706453056825, 'd_model_multiplier': 4, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 316, 'batch_size': 78, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.001, 'top_n_features': 102}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 66
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0594[0m   [32m0.3602[0m        [35m0.5550[0m       [31m0.9250[0m        [94m0.3877[0m     +  23.5781
      2   [36m0.1368[0m   [32m0.6333[0m        [35m0.3238[0m       0.9250        [94m0.2623[0m     +  24.1075
      3   [36m0.2128[0m   [32m0.7338[0m        [35m0.2612[0m       0.9250        [94m0.2425[0m     +  23.8863
      4   [36m0.2440[0m   [32m0.7685[0m        [35m0.2448[0m       [31m0.9274[0m        [94m0.2365[0m     +  24.1383
      5   [36m0.2583[0m   [32m0.7806[0m        [35m0.2398[0m       0.9274        [94m0.2343[0m     +  24.0093
      6   [36m0.2667[0m   [32m0.7842[0m        [35m0.2348[0m       0.9250        [94m0.2340[0m     +  23.7409
      7   [36m0.2703[0m   [32m0.7867[0m        [35m0.2321[0m       0.9250        [94m0.2334[0m     +  23.9292
      8   [36m0.2730[0m   [32m0.7879[0m        [35m0.2304[0m       0.9250        [94m0.2322[0m     +  23.8828
      9   0.2725   0.7876        [35m0.2294[0m       0.9262        [94m0.2314[0m     +  24.0431
     10   0.2727   0.7859        [35m0.2267[0m       0.9274        0.2323        23.8163
     11   [36m0.2817[0m   0.7864        [35m0.2257[0m       0.9274        [94m0.2307[0m     +  24.1549
     12   [36m0.2845[0m   [32m0.7881[0m        0.2260       0.9274        [94m0.2300[0m     +  24.2426
     13   [36m0.2888[0m   [32m0.7887[0m        0.2265       0.9262        [94m0.2284[0m     +  24.0294
     14   [36m0.2945[0m   [32m0.7895[0m        [35m0.2249[0m       0.9262        0.2288        23.9999
     15   [36m0.3029[0m   [32m0.7907[0m        [35m0.2229[0m       0.9226        [94m0.2283[0m     +  24.4329
     16   [36m0.3083[0m   [32m0.7917[0m        0.2236       0.9226        [94m0.2277[0m     +  24.0070
     17   [36m0.3140[0m   [32m0.7933[0m        [35m0.2222[0m       0.9214        [94m0.2272[0m     +  24.1284
     18   [36m0.3152[0m   [32m0.7937[0m        0.2223       0.9226        [94m0.2266[0m     +  24.0323
     19   [36m0.3167[0m   [32m0.7952[0m        [35m0.2214[0m       0.9238        [94m0.2265[0m     +  24.0077
     20   [36m0.3185[0m   [32m0.7958[0m        [35m0.2200[0m       0.9238        [94m0.2260[0m     +  24.1342
     21   [36m0.3217[0m   [32m0.7965[0m        [35m0.2193[0m       0.9226        [94m0.2248[0m     +  23.8725
     22   [36m0.3222[0m   0.7951        [35m0.2168[0m       0.9226        [94m0.2247[0m     +  23.9338
     23   [36m0.3233[0m   0.7948        0.2178       0.9226        0.2255        24.3428
     24   0.3213   0.7959        0.2178       0.9214        0.2253        24.1284
     25   0.3139   0.7960        0.2171       0.9226        0.2254        23.9249
     26   0.3072   [32m0.7972[0m        [35m0.2143[0m       0.9238        0.2249        24.2478
     27   0.3069   0.7951        [35m0.2125[0m       0.9238        0.2262        23.9708
     28   0.3081   0.7968        0.2152       0.9238        0.2248        24.1529
     29   0.3114   0.7957        0.2127       0.9250        0.2248        23.9872
     30   0.3098   0.7942        0.2127       0.9262        0.2257        23.9753
     31   0.3089   0.7970        [35m0.2109[0m       0.9262        [94m0.2245[0m     +  23.9470
     32   0.3102   0.7968        0.2111       0.9262        0.2262        23.9961
     33   0.3115   0.7969        [35m0.2087[0m       0.9262        0.2267        23.8492
     34   0.3015   0.7950        0.2092       0.9262        0.2286        24.0388
     35   0.3073   0.7922        0.2087       0.9250        0.2303        23.9669
     36   0.3112   0.7912        [35m0.2055[0m       0.9250        0.2289        23.8231
     37   0.2992   0.7936        0.2068       0.9250        0.2324        23.8329
     38   0.3190   0.7955        0.2056       0.9274        0.2264        24.2668
     39   0.3052   0.7943        [35m0.2043[0m       0.9250        0.2300        23.9598
     40   0.3143   0.7949        0.2059       0.9238        0.2298        24.0667
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 13:51:23,230][0m Trial 352 finished with value: 0.224463079633707 and parameters: {'lr': 3.5144816590306775e-05, 'dropout': 0.3372738670011616, 'd_model_multiplier': 4, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 345, 'batch_size': 69, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.001, 'top_n_features': 66}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 71
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0490[0m   [32m0.3110[0m        [35m0.6413[0m       [31m0.9311[0m        [94m0.5157[0m     +  31.2947
      2   [36m0.0574[0m   [32m0.4160[0m        [35m0.4255[0m       0.9311        [94m0.3152[0m     +  31.2291
      3   [36m0.1779[0m   [32m0.6412[0m        [35m0.3017[0m       0.9311        [94m0.2481[0m     +  31.3297
      4   [36m0.2390[0m   [32m0.7118[0m        [35m0.2636[0m       [31m0.9323[0m        [94m0.2284[0m     +  31.4308
      5   [36m0.2674[0m   [32m0.7361[0m        [35m0.2492[0m       0.9299        [94m0.2209[0m     +  31.6809
      6   [36m0.2799[0m   [32m0.7462[0m        [35m0.2416[0m       0.9323        [94m0.2187[0m     +  31.2169
      7   [36m0.2926[0m   [32m0.7528[0m        [35m0.2380[0m       [31m0.9335[0m        [94m0.2172[0m     +  31.3939
      8   0.2840   [32m0.7576[0m        [35m0.2355[0m       0.9335        [94m0.2163[0m     +  31.4821
      9   0.2869   [32m0.7593[0m        [35m0.2343[0m       0.9335        [94m0.2159[0m     +  31.6667
     10   [36m0.2951[0m   [32m0.7601[0m        [35m0.2328[0m       0.9323        0.2163        31.4837
     11   0.2947   [32m0.7640[0m        [35m0.2325[0m       0.9311        [94m0.2156[0m     +  31.1809
     12   0.2929   [32m0.7645[0m        [35m0.2311[0m       0.9323        0.2157        31.4852
     13   0.2909   [32m0.7649[0m        [35m0.2293[0m       0.9323        0.2164        31.5680
     14   0.2933   [32m0.7674[0m        [35m0.2292[0m       [31m0.9347[0m        0.2157        31.4317
     15   0.2925   [32m0.7688[0m        0.2294       0.9347        0.2160        31.6190
     16   0.2898   [32m0.7694[0m        [35m0.2256[0m       0.9347        0.2165        31.3357
     17   0.2837   [32m0.7714[0m        0.2264       0.9323        0.2162        31.6085
     18   0.2787   [32m0.7735[0m        [35m0.2254[0m       0.9347        [94m0.2153[0m     +  31.3356
     19   0.2853   0.7735        [35m0.2244[0m       0.9347        0.2161        31.2558
     20   0.2889   [32m0.7744[0m        [35m0.2241[0m       0.9347        0.2163        31.4753
     21   0.2899   [32m0.7754[0m        [35m0.2220[0m       [31m0.9359[0m        0.2164        31.7064
     22   0.2903   [32m0.7762[0m        0.2232       0.9359        0.2168        31.4631
     23   0.2946   0.7735        [35m0.2215[0m       0.9347        0.2177        31.6946
     24   0.2884   [32m0.7766[0m        [35m0.2206[0m       0.9335        0.2169        31.5007
     25   0.2833   0.7738        [35m0.2183[0m       0.9347        0.2186        31.3834
     26   0.2853   0.7724        0.2184       0.9347        0.2186        31.7434
     27   0.2870   0.7740        0.2190       0.9311        0.2190        31.7078
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 14:06:06,031][0m Trial 353 finished with value: 0.2152804545787903 and parameters: {'lr': 2.149576330817143e-05, 'dropout': 0.31491807040099046, 'd_model_multiplier': 4, 'num_layers': 7, 'n_heads': 32, 'dim_feedforward': 330, 'batch_size': 82, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.001, 'top_n_features': 71}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 96
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0555[0m   [32m0.2508[0m        [35m0.7205[0m       [31m0.9129[0m        [94m0.6172[0m     +  26.9987
      2   [36m0.0636[0m   [32m0.3061[0m        [35m0.5017[0m       0.9129        [94m0.4127[0m     +  27.5482
      3   [36m0.0941[0m   [32m0.5307[0m        [35m0.3505[0m       0.9129        [94m0.3109[0m     +  27.2516
      4   [36m0.2910[0m   [32m0.6630[0m        [35m0.2872[0m       0.9129        [94m0.2782[0m     +  27.3701
      5   [36m0.3141[0m   [32m0.7007[0m        [35m0.2653[0m       0.9129        [94m0.2682[0m     +  27.4616
      6   0.3077   [32m0.7217[0m        [35m0.2550[0m       0.9129        [94m0.2645[0m     +  27.4782
      7   0.2982   [32m0.7387[0m        [35m0.2495[0m       0.9129        [94m0.2620[0m     +  27.1841
      8   0.2966   [32m0.7494[0m        [35m0.2448[0m       0.9129        [94m0.2598[0m     +  27.4618
      9   0.2995   [32m0.7582[0m        [35m0.2394[0m       [31m0.9141[0m        [94m0.2577[0m     +  27.2388
     10   0.2939   [32m0.7634[0m        [35m0.2359[0m       0.9141        [94m0.2569[0m     +  27.2976
     11   0.2914   [32m0.7662[0m        [35m0.2336[0m       0.9117        [94m0.2556[0m     +  27.4290
     12   0.2907   [32m0.7673[0m        [35m0.2327[0m       0.9105        0.2561        27.3443
     13   0.2879   [32m0.7691[0m        [35m0.2298[0m       0.9093        0.2561        27.3535
     14   0.2832   [32m0.7710[0m        0.2304       0.9081        0.2561        27.4907
     15   0.2892   [32m0.7717[0m        [35m0.2291[0m       0.9093        0.2563        27.4131
     16   0.2861   [32m0.7720[0m        [35m0.2268[0m       0.9093        0.2569        27.4753
     17   0.2865   [32m0.7725[0m        [35m0.2257[0m       0.9093        0.2570        27.4001
     18   0.2882   [32m0.7734[0m        [35m0.2254[0m       0.9093        0.2567        27.1125
     19   0.2893   [32m0.7745[0m        [35m0.2250[0m       0.9069        0.2567        27.1300
     20   0.2879   [32m0.7746[0m        [35m0.2234[0m       0.9045        0.2574        27.2601
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 14:15:41,232][0m Trial 354 finished with value: 0.2556418636329517 and parameters: {'lr': 1.2038769010422104e-05, 'dropout': 0.2806547865417379, 'd_model_multiplier': 4, 'num_layers': 6, 'n_heads': 32, 'dim_feedforward': 179, 'batch_size': 66, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.001, 'top_n_features': 96}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 108
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0621[0m   [32m0.2853[0m        [35m0.6091[0m       [31m0.9141[0m        [94m0.5089[0m     +  24.0483
      2   [36m0.0625[0m   0.2743        [35m0.4409[0m       0.9141        [94m0.3763[0m     +  24.0000
      3   [36m0.0744[0m   [32m0.4262[0m        [35m0.3330[0m       0.9141        [94m0.3172[0m     +  24.0069
      4   [36m0.1155[0m   [32m0.5829[0m        [35m0.2847[0m       0.9141        [94m0.2930[0m     +  24.0556
      5   [36m0.1656[0m   [32m0.6644[0m        [35m0.2647[0m       0.9141        [94m0.2809[0m     +  23.9980
      6   [36m0.2034[0m   [32m0.7081[0m        [35m0.2574[0m       0.9141        [94m0.2727[0m     +  24.0817
      7   [36m0.2307[0m   [32m0.7341[0m        [35m0.2516[0m       0.9141        [94m0.2673[0m     +  24.0751
      8   [36m0.2640[0m   [32m0.7576[0m        [35m0.2479[0m       0.9141        [94m0.2613[0m     +  24.1934
      9   [36m0.3058[0m   [32m0.7774[0m        [35m0.2432[0m       0.9141        [94m0.2555[0m     +  24.0906
     10   [36m0.3550[0m   [32m0.7925[0m        [35m0.2412[0m       0.9141        [94m0.2504[0m     +  24.2088
     11   [36m0.3708[0m   [32m0.8029[0m        [35m0.2372[0m       [31m0.9154[0m        [94m0.2460[0m     +  24.3000
     12   [36m0.3728[0m   [32m0.8092[0m        [35m0.2370[0m       0.9141        [94m0.2424[0m     +  23.9142
     13   [36m0.3830[0m   [32m0.8131[0m        [35m0.2346[0m       0.9141        [94m0.2409[0m     +  23.8561
     14   [36m0.3878[0m   [32m0.8176[0m        [35m0.2331[0m       0.9129        [94m0.2384[0m     +  24.3919
     15   [36m0.3942[0m   [32m0.8183[0m        0.2337       0.9141        [94m0.2375[0m     +  24.2292
     16   [36m0.3948[0m   [32m0.8199[0m        [35m0.2327[0m       0.9154        [94m0.2360[0m     +  24.0756
     17   [36m0.3964[0m   [32m0.8214[0m        [35m0.2323[0m       0.9154        [94m0.2346[0m     +  24.0513
     18   [36m0.3986[0m   [32m0.8227[0m        [35m0.2288[0m       0.9141        [94m0.2342[0m     +  24.0558
     19   0.3981   [32m0.8242[0m        0.2310       0.9129        [94m0.2327[0m     +  24.3432
     20   [36m0.3997[0m   [32m0.8243[0m        0.2301       [31m0.9166[0m        [94m0.2324[0m     +  24.0959
     21   [36m0.4001[0m   [32m0.8249[0m        0.2301       0.9166        0.2334        24.1473
     22   [36m0.4026[0m   [32m0.8265[0m        0.2294       [31m0.9178[0m        [94m0.2316[0m     +  24.0091
     23   0.4006   [32m0.8272[0m        [35m0.2287[0m       [31m0.9190[0m        0.2332        24.1480
     24   [36m0.4041[0m   [32m0.8293[0m        0.2288       [31m0.9202[0m        [94m0.2307[0m     +  24.0801
     25   0.3992   0.8284        0.2291       0.9190        0.2326        24.3497
     26   0.4040   [32m0.8311[0m        [35m0.2270[0m       [31m0.9214[0m        [94m0.2301[0m     +  23.9121
     27   0.3947   0.8285        [35m0.2269[0m       0.9190        0.2332        23.9102
     28   0.3999   0.8301        [35m0.2259[0m       0.9202        0.2316        24.1376
     29   0.3928   0.8309        0.2261       0.9202        0.2332        24.3557
     30   0.3952   [32m0.8312[0m        [35m0.2257[0m       0.9202        0.2337        24.0733
     31   0.3965   [32m0.8318[0m        [35m0.2250[0m       0.9214        0.2330        24.1975
     32   0.3969   [32m0.8325[0m        0.2261       0.9214        0.2317        24.4837
     33   0.3982   0.8322        [35m0.2237[0m       0.9214        0.2315        24.3295
     34   0.4005   [32m0.8336[0m        0.2243       0.9214        0.2313        24.2715
     35   [36m0.4057[0m   [32m0.8344[0m        [35m0.2235[0m       [31m0.9226[0m        0.2304        24.1830
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 14:30:12,655][0m Trial 355 finished with value: 0.23014432588702535 and parameters: {'lr': 1.5067474484438946e-05, 'dropout': 0.3826392515457203, 'd_model_multiplier': 4, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 337, 'batch_size': 73, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.001, 'top_n_features': 108}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 75
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0718[0m   [32m0.3815[0m        [35m0.5799[0m       [31m0.9202[0m        [94m0.4024[0m     +  23.8171
      2   [36m0.1387[0m   [32m0.5854[0m        [35m0.3321[0m       0.9202        [94m0.2816[0m     +  23.7889
      3   [36m0.1531[0m   [32m0.6503[0m        [35m0.2636[0m       0.9202        [94m0.2690[0m     +  24.0273
      4   [36m0.1663[0m   [32m0.6780[0m        [35m0.2485[0m       0.9202        [94m0.2661[0m     +  23.7210
      5   [36m0.1906[0m   [32m0.6925[0m        [35m0.2428[0m       0.9202        [94m0.2655[0m     +  24.0381
      6   [36m0.1983[0m   [32m0.7001[0m        [35m0.2373[0m       0.9154        0.2680        24.0798
      7   [36m0.2016[0m   [32m0.7094[0m        [35m0.2331[0m       0.9117        0.2687        24.1092
      8   0.2004   [32m0.7143[0m        [35m0.2316[0m       0.9141        0.2698        23.8786
      9   0.1993   [32m0.7182[0m        [35m0.2281[0m       0.9129        0.2722        24.0086
     10   0.1984   [32m0.7187[0m        [35m0.2275[0m       0.9154        0.2729        23.9989
     11   0.2012   [32m0.7206[0m        0.2285       0.9141        0.2712        24.0313
     12   [36m0.2038[0m   [32m0.7230[0m        [35m0.2263[0m       0.9141        0.2710        23.7629
     13   [36m0.2046[0m   [32m0.7243[0m        [35m0.2235[0m       0.9117        0.2719        23.9126
     14   0.1998   [32m0.7255[0m        [35m0.2232[0m       0.9129        0.2721        24.0564
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 14:36:12,722][0m Trial 356 finished with value: 0.2655250464341949 and parameters: {'lr': 3.7731356134391835e-05, 'dropout': 0.41110273019651883, 'd_model_multiplier': 4, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 294, 'batch_size': 70, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.001, 'top_n_features': 75}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 81
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1961[0m   [32m0.6903[0m        [35m0.3107[0m       [31m0.9202[0m        [94m0.2597[0m     +  22.7979
      2   [36m0.2469[0m   [32m0.7609[0m        [35m0.2411[0m       0.9141        [94m0.2516[0m     +  23.0050
      3   0.2406   [32m0.7669[0m        [35m0.2331[0m       0.9178        [94m0.2507[0m     +  23.1577
      4   [36m0.2500[0m   [32m0.7693[0m        [35m0.2285[0m       0.9178        [94m0.2504[0m     +  23.1523
      5   0.2476   [32m0.7725[0m        [35m0.2278[0m       0.9190        [94m0.2487[0m     +  22.8339
      6   [36m0.2556[0m   [32m0.7744[0m        [35m0.2248[0m       0.9190        0.2493        23.2450
      7   [36m0.2608[0m   [32m0.7805[0m        [35m0.2242[0m       0.9166        [94m0.2473[0m     +  23.2210
      8   [36m0.2754[0m   [32m0.7828[0m        [35m0.2216[0m       0.9178        [94m0.2453[0m     +  23.1794
      9   [36m0.2930[0m   [32m0.7900[0m        [35m0.2197[0m       0.9178        [94m0.2435[0m     +  23.1427
     10   0.2847   [32m0.7937[0m        0.2210       0.9190        [94m0.2422[0m     +  23.1867
     11   0.2881   [32m0.7961[0m        0.2206       0.9154        0.2440        23.0503
     12   0.2916   [32m0.7997[0m        [35m0.2179[0m       0.9154        0.2432        23.0896
     13   0.2888   [32m0.8029[0m        [35m0.2174[0m       0.9166        [94m0.2405[0m     +  23.3265
     14   [36m0.2965[0m   [32m0.8064[0m        [35m0.2156[0m       0.9166        [94m0.2369[0m     +  23.2399
     15   [36m0.3000[0m   [32m0.8092[0m        [35m0.2155[0m       0.9154        [94m0.2365[0m     +  23.4323
     16   [36m0.3066[0m   [32m0.8120[0m        [35m0.2128[0m       0.9190        0.2365        23.1220
     17   [36m0.3129[0m   0.8108        0.2135       0.9178        [94m0.2352[0m     +  23.0825
     18   0.2988   0.8116        [35m0.2112[0m       0.9166        [94m0.2335[0m     +  23.0830
     19   0.3045   0.8111        [35m0.2112[0m       0.9178        0.2348        22.8598
     20   0.3126   [32m0.8136[0m        [35m0.2096[0m       0.9166        [94m0.2325[0m     +  22.9856
     21   0.3058   0.8121        0.2100       0.9166        0.2346        23.2526
     22   0.3067   0.8117        [35m0.2061[0m       0.9190        0.2350        23.2669
     23   0.3068   0.8130        [35m0.2060[0m       0.9202        0.2338        23.1347
     24   0.3063   0.8085        [35m0.2041[0m       0.9190        0.2355        23.1378
     25   [36m0.3138[0m   0.8103        [35m0.2032[0m       [31m0.9238[0m        0.2332        23.1805
     26   [36m0.3145[0m   0.8082        [35m0.2025[0m       0.9214        0.2352        22.9537
     27   0.3042   0.8047        [35m0.2010[0m       0.9214        0.2370        23.1955
     28   0.2935   0.8013        [35m0.1975[0m       0.9190        0.2396        22.9034
     29   0.3016   0.8014        0.1984       0.9190        0.2399        23.2822
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 14:47:47,577][0m Trial 357 finished with value: 0.2324767783914829 and parameters: {'lr': 6.400804593160132e-05, 'dropout': 0.24936367534084325, 'd_model_multiplier': 2, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 314, 'batch_size': 61, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.001, 'top_n_features': 81}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 63
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0653[0m   [32m0.3090[0m        [35m0.6453[0m       [31m0.9105[0m        [94m0.4994[0m     +  31.2331
      2   [36m0.1047[0m   [32m0.5178[0m        [35m0.3882[0m       0.9105        [94m0.3185[0m     +  30.9880
      3   [36m0.2225[0m   [32m0.6975[0m        [35m0.2777[0m       0.9093        [94m0.2784[0m     +  31.6520
      4   [36m0.2785[0m   [32m0.7621[0m        [35m0.2510[0m       0.9081        [94m0.2636[0m     +  31.3001
      5   [36m0.2956[0m   [32m0.7860[0m        [35m0.2383[0m       0.9057        [94m0.2570[0m     +  31.3280
      6   [36m0.2995[0m   [32m0.7935[0m        [35m0.2329[0m       0.9105        [94m0.2548[0m     +  31.5022
      7   [36m0.3043[0m   [32m0.7984[0m        [35m0.2308[0m       0.9081        [94m0.2530[0m     +  31.6146
      8   [36m0.3111[0m   [32m0.8021[0m        [35m0.2273[0m       0.9081        [94m0.2522[0m     +  31.3913
      9   [36m0.3136[0m   [32m0.8034[0m        [35m0.2270[0m       0.9045        [94m0.2517[0m     +  31.1381
     10   0.3105   0.8031        0.2272       0.9057        0.2520        31.3390
     11   0.3127   [32m0.8045[0m        [35m0.2239[0m       0.9069        0.2525        31.5156
     12   [36m0.3137[0m   0.8033        [35m0.2230[0m       0.9069        0.2537        31.4509
     13   [36m0.3150[0m   0.8028        [35m0.2220[0m       0.9069        0.2536        31.4062
     14   [36m0.3151[0m   0.8024        [35m0.2218[0m       0.9069        0.2544        31.6934
     15   0.3134   0.7989        [35m0.2203[0m       0.9069        0.2561        31.7387
     16   0.3139   0.7975        [35m0.2189[0m       0.9081        0.2562        31.4044
     17   [36m0.3158[0m   0.7997        [35m0.2185[0m       0.9069        0.2545        31.8132
     18   [36m0.3194[0m   0.7958        [35m0.2173[0m       0.9081        0.2559        31.5186
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 14:57:46,448][0m Trial 358 finished with value: 0.2517000334979257 and parameters: {'lr': 2.7922831929816223e-05, 'dropout': 0.32235482444042984, 'd_model_multiplier': 4, 'num_layers': 7, 'n_heads': 32, 'dim_feedforward': 349, 'batch_size': 80, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 63}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 69
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0608[0m   [32m0.3476[0m        [35m0.6283[0m       [31m0.9190[0m        [94m0.5376[0m     +  34.2548
      2   [36m0.0671[0m   [32m0.3741[0m        [35m0.4827[0m       0.9190        [94m0.3932[0m     +  34.6883
      3   [36m0.0767[0m   [32m0.4702[0m        [35m0.3670[0m       0.9190        [94m0.3160[0m     +  34.8816
      4   [36m0.1256[0m   [32m0.6023[0m        [35m0.3035[0m       0.9190        [94m0.2833[0m     +  34.7358
      5   [36m0.1632[0m   [32m0.6572[0m        [35m0.2736[0m       0.9190        [94m0.2701[0m     +  34.7540
      6   [36m0.1862[0m   [32m0.6920[0m        [35m0.2588[0m       0.9178        [94m0.2627[0m     +  34.9311
      7   [36m0.2133[0m   [32m0.7132[0m        [35m0.2480[0m       0.9154        [94m0.2574[0m     +  35.0753
      8   [36m0.2260[0m   [32m0.7223[0m        [35m0.2424[0m       0.9141        [94m0.2552[0m     +  34.9215
      9   [36m0.2377[0m   [32m0.7270[0m        [35m0.2372[0m       0.9129        [94m0.2540[0m     +  34.6868
     10   [36m0.2514[0m   [32m0.7319[0m        [35m0.2342[0m       0.9154        [94m0.2535[0m     +  34.7130
     11   [36m0.2550[0m   [32m0.7353[0m        0.2352       0.9141        [94m0.2528[0m     +  34.8076
     12   [36m0.2555[0m   [32m0.7384[0m        [35m0.2313[0m       0.9166        [94m0.2523[0m     +  34.9272
     13   [36m0.2575[0m   [32m0.7394[0m        [35m0.2299[0m       0.9166        0.2524        34.7369
     14   0.2564   [32m0.7412[0m        0.2301       0.9166        0.2524        34.9451
     15   [36m0.2597[0m   [32m0.7429[0m        0.2305       0.9166        [94m0.2520[0m     +  35.0182
     16   [36m0.2637[0m   [32m0.7440[0m        0.2306       0.9178        [94m0.2515[0m     +  34.9450
     17   [36m0.2666[0m   [32m0.7452[0m        [35m0.2271[0m       0.9178        [94m0.2513[0m     +  35.1209
     18   [36m0.2696[0m   [32m0.7474[0m        0.2280       0.9178        0.2515        34.9904
     19   [36m0.2715[0m   [32m0.7480[0m        [35m0.2269[0m       0.9178        0.2516        34.7943
     20   0.2713   0.7479        0.2273       0.9178        0.2514        34.7460
     21   [36m0.2753[0m   [32m0.7490[0m        [35m0.2255[0m       0.9178        [94m0.2512[0m     +  34.5805
     22   [36m0.2762[0m   [32m0.7499[0m        0.2274       0.9178        [94m0.2507[0m     +  34.8537
     23   [36m0.2770[0m   [32m0.7507[0m        [35m0.2254[0m       0.9166        [94m0.2506[0m     +  34.8798
     24   [36m0.2772[0m   [32m0.7512[0m        0.2257       0.9190        [94m0.2504[0m     +  34.9333
     25   [36m0.2779[0m   [32m0.7512[0m        [35m0.2245[0m       0.9190        0.2507        34.5867
     26   [36m0.2785[0m   [32m0.7512[0m        0.2257       0.9190        [94m0.2504[0m     +  34.9532
     27   [36m0.2807[0m   [32m0.7534[0m        0.2256       0.9178        [94m0.2498[0m     +  34.6389
     28   [36m0.2814[0m   0.7533        0.2247       0.9190        [94m0.2495[0m     +  34.8785
     29   0.2809   [32m0.7535[0m        0.2246       0.9166        [94m0.2493[0m     +  34.8053
     30   0.2788   0.7525        [35m0.2233[0m       0.9166        0.2496        35.1932
     31   0.2779   0.7531        0.2233       0.9190        0.2493        34.7490
     32   0.2797   0.7531        0.2239       [31m0.9202[0m        [94m0.2492[0m     +  34.9490
     33   [36m0.2822[0m   0.7528        0.2237       [31m0.9214[0m        [94m0.2489[0m     +  34.4333
     34   [36m0.2839[0m   0.7523        [35m0.2229[0m       0.9190        0.2491        34.8610
     35   0.2831   0.7518        0.2240       0.9202        [94m0.2488[0m     +  34.6079
     36   [36m0.2839[0m   0.7515        [35m0.2223[0m       0.9202        0.2489        35.2813
     37   [36m0.2863[0m   0.7508        0.2239       0.9202        0.2489        34.8266
     38   0.2832   0.7500        0.2227       0.9202        [94m0.2486[0m     +  34.8231
     39   0.2833   0.7500        0.2235       0.9190        [94m0.2485[0m     +  34.8927
     40   0.2862   0.7498        [35m0.2199[0m       0.9190        [94m0.2483[0m     +  34.6127
     41   0.2842   0.7500        0.2225       0.9178        0.2485        35.0697
     42   0.2836   0.7496        0.2202       0.9178        [94m0.2482[0m     +  34.8113
     43   0.2828   0.7487        0.2207       0.9190        0.2486        34.7346
     44   0.2837   0.7488        0.2212       0.9154        [94m0.2479[0m     +  34.9229
     45   0.2809   0.7481        0.2214       0.9166        0.2482        34.8922
     46   0.2847   0.7488        0.2211       0.9166        [94m0.2478[0m     +  34.8169
     47   0.2833   0.7480        0.2212       0.9166        0.2479        34.8882
     48   0.2800   0.7470        [35m0.2196[0m       0.9154        0.2483        34.6197
     49   0.2823   0.7474        0.2208       0.9154        0.2483        34.9103
     50   0.2844   0.7479        [35m0.2190[0m       0.9154        0.2479        34.9058
[32m[I 2023-05-03 15:26:52,289][0m Trial 359 finished with value: 0.24781164433244063 and parameters: {'lr': 9.54780843075637e-06, 'dropout': 0.4248051580302336, 'd_model_multiplier': 4, 'num_layers': 8, 'n_heads': 32, 'dim_feedforward': 324, 'batch_size': 64, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 69}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 58
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.3368[0m   [32m0.7616[0m        [35m0.2983[0m       [31m0.9359[0m        [94m0.2034[0m     +  27.4574
      2   [36m0.3710[0m   [32m0.7676[0m        [35m0.2418[0m       [31m0.9395[0m        [94m0.1995[0m     +  27.6803
      3   0.3676   [32m0.7757[0m        [35m0.2366[0m       0.9395        0.1997        27.3382
      4   0.3547   [32m0.7784[0m        [35m0.2340[0m       [31m0.9407[0m        0.2003        27.4509
      5   0.3510   [32m0.7797[0m        0.2341       [31m0.9420[0m        0.2008        27.4455
      6   0.3505   0.7787        [35m0.2305[0m       0.9395        0.2018        27.6272
      7   0.3379   0.7785        [35m0.2302[0m       0.9407        0.2028        27.7082
      8   0.3328   0.7758        [35m0.2299[0m       0.9395        0.2039        27.5692
      9   0.3239   0.7764        [35m0.2279[0m       0.9407        0.2051        27.6415
     10   0.3357   0.7704        0.2284       0.9359        0.2071        27.5630
     11   0.3197   0.7679        [35m0.2242[0m       0.9347        0.2112        27.4675
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 15:32:23,123][0m Trial 360 finished with value: 0.19945212213059002 and parameters: {'lr': 4.348173755851842e-05, 'dropout': 0.3959119341575506, 'd_model_multiplier': 4, 'num_layers': 6, 'n_heads': 32, 'dim_feedforward': 329, 'batch_size': 56, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 58}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 59
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.3313[0m   [32m0.7707[0m        [35m0.2765[0m       [31m0.9141[0m        [94m0.2553[0m     +  34.6116
      2   [36m0.3423[0m   [32m0.7865[0m        [35m0.2388[0m       0.9141        [94m0.2476[0m     +  34.7157
      3   [36m0.3438[0m   [32m0.7929[0m        [35m0.2337[0m       [31m0.9154[0m        [94m0.2453[0m     +  34.7562
      4   [36m0.3547[0m   [32m0.8012[0m        [35m0.2324[0m       [31m0.9190[0m        [94m0.2416[0m     +  34.7857
      5   0.3525   [32m0.8035[0m        [35m0.2302[0m       0.9166        0.2422        34.6023
      6   0.3426   [32m0.8063[0m        [35m0.2279[0m       0.9129        [94m0.2389[0m     +  34.8418
      7   0.3414   0.8038        0.2286       0.9129        0.2416        34.6060
      8   [36m0.3612[0m   [32m0.8125[0m        [35m0.2263[0m       0.9141        [94m0.2377[0m     +  35.0987
      9   0.3525   [32m0.8145[0m        [35m0.2254[0m       0.9166        [94m0.2356[0m     +  34.9021
     10   0.3497   [32m0.8180[0m        [35m0.2238[0m       0.9154        0.2370        34.5803
     11   0.3454   [32m0.8206[0m        0.2240       0.9154        0.2365        34.9598
     12   [36m0.3616[0m   [32m0.8230[0m        [35m0.2224[0m       0.9166        [94m0.2332[0m     +  34.7362
     13   0.3551   [32m0.8257[0m        0.2230       0.9166        0.2339        35.1906
     14   [36m0.3745[0m   0.8246        [35m0.2222[0m       0.9178        [94m0.2331[0m     +  34.7144
     15   0.3698   0.8254        [35m0.2211[0m       0.9129        0.2343        34.7201
     16   0.3741   [32m0.8299[0m        [35m0.2189[0m       0.9178        [94m0.2322[0m     +  34.6369
     17   [36m0.3886[0m   0.8298        [35m0.2183[0m       [31m0.9202[0m        [94m0.2307[0m     +  34.6648
     18   0.3483   0.8238        [35m0.2167[0m       0.9166        0.2375        34.7665
     19   0.3614   0.8273        [35m0.2159[0m       0.9166        0.2347        34.9187
     20   0.3716   0.8282        0.2161       0.9166        0.2337        34.7223
     21   0.3713   0.8263        0.2165       0.9166        0.2334        35.0516
     22   0.3610   0.8279        [35m0.2134[0m       0.9166        0.2342        34.9015
     23   0.3617   0.8241        0.2137       0.9141        0.2354        34.8151
     24   0.3622   0.8276        [35m0.2099[0m       0.9154        0.2344        34.4926
     25   0.3571   0.8269        0.2129       0.9154        0.2360        34.7453
     26   0.3623   0.8271        [35m0.2097[0m       0.9178        0.2355        34.8761
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 15:48:03,910][0m Trial 361 finished with value: 0.23074643618394558 and parameters: {'lr': 6.43384262992757e-05, 'dropout': 0.38960010545712026, 'd_model_multiplier': 4, 'num_layers': 8, 'n_heads': 32, 'dim_feedforward': 332, 'batch_size': 58, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.001, 'top_n_features': 59}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 65
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
Warning, assumed runtime error: CUDA out of memory. Tried to allocate 1.30 GiB (GPU 0; 23.70 GiB total capacity; 19.58 GiB already allocated; 951.25 MiB free; 21.68 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[32m[I 2023-05-03 15:48:07,582][0m Trial 362 finished with value: 100.0 and parameters: {'lr': 4.7115194864299845e-05, 'dropout': 0.36651791662446587, 'd_model_multiplier': 4, 'num_layers': 6, 'n_heads': 64, 'dim_feedforward': 323, 'batch_size': 55, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 65}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 60
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2416[0m   [32m0.7507[0m        [35m0.2788[0m       [31m0.9202[0m        [94m0.2383[0m     +  31.2482
      2   [36m0.2586[0m   [32m0.7597[0m        [35m0.2368[0m       0.9045        0.2584        31.1570
      3   [36m0.2649[0m   [32m0.7646[0m        [35m0.2324[0m       0.9057        0.2515        31.4862
      4   [36m0.2733[0m   [32m0.7684[0m        [35m0.2304[0m       0.9081        0.2501        31.4109
      5   [36m0.2798[0m   [32m0.7700[0m        [35m0.2283[0m       0.9081        0.2578        30.8844
      6   0.2736   0.7697        [35m0.2270[0m       0.9045        0.2567        31.1818
      7   0.2639   0.7660        [35m0.2245[0m       0.9069        0.2535        31.3894
      8   0.2710   0.7677        0.2249       0.9069        0.2621        31.6236
      9   0.2685   0.7691        [35m0.2227[0m       0.9117        0.2561        31.1819
     10   0.2683   0.7691        [35m0.2209[0m       0.9105        0.2570        31.1565
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 15:53:51,656][0m Trial 363 finished with value: 0.23832730733605942 and parameters: {'lr': 9.365238160362097e-05, 'dropout': 0.34412089106548577, 'd_model_multiplier': 4, 'num_layers': 7, 'n_heads': 32, 'dim_feedforward': 341, 'batch_size': 68, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 60}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 81
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2969[0m   [32m0.6943[0m        [35m0.3294[0m       [31m0.9093[0m        [94m0.2763[0m     +  27.2481
      2   [36m0.3815[0m   [32m0.7698[0m        [35m0.2415[0m       [31m0.9105[0m        [94m0.2566[0m     +  27.4621
      3   [36m0.3935[0m   [32m0.7901[0m        [35m0.2341[0m       [31m0.9117[0m        [94m0.2494[0m     +  27.6936
      4   [36m0.4008[0m   [32m0.7992[0m        [35m0.2308[0m       0.9117        [94m0.2459[0m     +  27.7988
      5   [36m0.4155[0m   [32m0.8042[0m        [35m0.2287[0m       0.9093        [94m0.2450[0m     +  27.8661
      6   [36m0.4214[0m   [32m0.8094[0m        [35m0.2272[0m       [31m0.9141[0m        [94m0.2421[0m     +  27.2813
      7   0.4151   [32m0.8149[0m        [35m0.2241[0m       [31m0.9154[0m        [94m0.2394[0m     +  27.5552
      8   0.4189   [32m0.8167[0m        [35m0.2232[0m       0.9141        0.2399        27.6655
      9   0.4169   [32m0.8198[0m        [35m0.2215[0m       [31m0.9190[0m        [94m0.2383[0m     +  27.2215
     10   0.4162   [32m0.8202[0m        [35m0.2199[0m       0.9190        0.2385        27.6646
     11   0.4150   [32m0.8212[0m        [35m0.2188[0m       0.9166        0.2384        27.6459
     12   0.4168   [32m0.8223[0m        [35m0.2182[0m       0.9141        0.2385        27.5391
     13   0.4138   [32m0.8238[0m        [35m0.2143[0m       0.9166        [94m0.2372[0m     +  27.6376
     14   0.4092   [32m0.8245[0m        0.2146       0.9166        0.2388        27.6331
     15   0.4145   [32m0.8252[0m        [35m0.2130[0m       0.9141        0.2395        27.3568
     16   0.4022   0.8245        [35m0.2112[0m       0.9129        0.2406        27.6706
     17   0.4047   0.8244        [35m0.2099[0m       0.9141        0.2410        27.3988
     18   0.4123   [32m0.8254[0m        [35m0.2088[0m       0.9129        0.2413        27.3835
     19   0.4012   0.8252        [35m0.2082[0m       0.9141        0.2415        27.5176
     20   0.3930   0.8227        [35m0.2070[0m       0.9129        0.2450        27.5253
     21   0.3956   0.8232        [35m0.2026[0m       0.9129        0.2449        27.5329
     22   0.3809   0.8209        0.2047       0.9129        0.2480        27.5918
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 16:04:26,550][0m Trial 364 finished with value: 0.23721089432108358 and parameters: {'lr': 2.259580282446397e-05, 'dropout': 0.2302967961912518, 'd_model_multiplier': 4, 'num_layers': 6, 'n_heads': 32, 'dim_feedforward': 330, 'batch_size': 59, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0, 'top_n_features': 81}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 54
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
Warning, assumed runtime error: CUDA out of memory. Tried to allocate 90.00 MiB (GPU 0; 23.70 GiB total capacity; 22.38 GiB already allocated; 75.25 MiB free; 22.53 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[32m[I 2023-05-03 16:04:30,592][0m Trial 365 finished with value: 100.0 and parameters: {'lr': 3.0071686352436153e-05, 'dropout': 0.39667699958990693, 'd_model_multiplier': 32, 'num_layers': 8, 'n_heads': 32, 'dim_feedforward': 318, 'batch_size': 73, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 54}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 72
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.3606[0m   [32m0.8138[0m        [35m0.2735[0m       [31m0.9226[0m        [94m0.2247[0m     +  41.1456
      2   [36m0.3811[0m   [32m0.8229[0m        [35m0.2362[0m       0.9214        [94m0.2206[0m     +  41.7400
      3   [36m0.3874[0m   [32m0.8273[0m        [35m0.2322[0m       0.9190        [94m0.2193[0m     +  41.6427
      4   [36m0.3885[0m   [32m0.8316[0m        [35m0.2288[0m       0.9190        [94m0.2182[0m     +  41.8249
      5   0.3867   [32m0.8367[0m        [35m0.2257[0m       0.9202        [94m0.2161[0m     +  41.7368
      6   0.3837   [32m0.8384[0m        [35m0.2234[0m       0.9190        0.2167        41.7849
      7   0.3869   [32m0.8401[0m        [35m0.2204[0m       0.9178        0.2186        41.9810
      8   0.3865   0.8393        [35m0.2177[0m       0.9166        0.2198        41.9697
      9   0.3807   0.8371        [35m0.2162[0m       0.9214        0.2204        41.7574
     10   0.3788   0.8368        [35m0.2155[0m       0.9202        0.2223        41.7623
     11   0.3781   [32m0.8405[0m        [35m0.2142[0m       0.9166        0.2251        41.4997
     12   0.3625   0.8361        [35m0.2105[0m       0.9202        0.2272        41.4853
     13   0.3617   0.8379        [35m0.2097[0m       0.9178        0.2304        41.6193
     14   0.3682   0.8402        [35m0.2075[0m       0.9178        0.2281        41.6268
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 16:14:56,646][0m Trial 366 finished with value: 0.21606102293895552 and parameters: {'lr': 4.990688162927333e-05, 'dropout': 0.376991669699773, 'd_model_multiplier': 4, 'num_layers': 5, 'n_heads': 64, 'dim_feedforward': 311, 'batch_size': 64, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 72}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 65
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2908[0m   [32m0.8091[0m        [35m0.2911[0m       [31m0.9287[0m        [94m0.2079[0m     +  82.6702
      2   [36m0.2952[0m   [32m0.8123[0m        [35m0.2387[0m       [31m0.9299[0m        [94m0.2061[0m     +  82.6730
      3   0.2950   [32m0.8134[0m        [35m0.2342[0m       0.9287        [94m0.2058[0m     +  82.7008
      4   [36m0.2993[0m   [32m0.8167[0m        [35m0.2322[0m       0.9274        [94m0.2053[0m     +  82.5158
      5   [36m0.3097[0m   [32m0.8172[0m        [35m0.2301[0m       0.9299        0.2057        83.0698
      6   [36m0.3175[0m   [32m0.8189[0m        [35m0.2277[0m       0.9274        0.2064        82.8090
      7   0.3164   0.8180        [35m0.2263[0m       0.9299        0.2062        82.8162
      8   0.3089   [32m0.8190[0m        [35m0.2257[0m       [31m0.9311[0m        0.2071        82.6244
      9   [36m0.3233[0m   [32m0.8207[0m        [35m0.2234[0m       0.9299        0.2057        82.8211
     10   [36m0.3241[0m   [32m0.8216[0m        [35m0.2216[0m       0.9311        0.2063        82.8030
     11   [36m0.3250[0m   0.8209        [35m0.2210[0m       0.9311        0.2076        82.8090
     12   0.3215   0.8181        [35m0.2191[0m       0.9311        0.2075        83.0280
     13   0.3205   0.8168        [35m0.2176[0m       [31m0.9323[0m        0.2095        82.8760
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 16:34:24,184][0m Trial 367 finished with value: 0.20530544076353635 and parameters: {'lr': 3.2064173182685584e-06, 'dropout': 0.3226868019157047, 'd_model_multiplier': 64, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 334, 'batch_size': 53, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 65}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 66
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
Warning, assumed runtime error: CUDA out of memory. Tried to allocate 126.00 MiB (GPU 0; 23.70 GiB total capacity; 22.18 GiB already allocated; 75.25 MiB free; 22.53 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[32m[I 2023-05-03 16:34:27,801][0m Trial 368 finished with value: 100.0 and parameters: {'lr': 6.345300041603792e-06, 'dropout': 0.35144664702450173, 'd_model_multiplier': 64, 'num_layers': 9, 'n_heads': 32, 'dim_feedforward': 335, 'batch_size': 51, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 66}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 65
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp       dur
-------  -------  -------  ------------  -----------  ------------  ----  --------
      1   [36m0.4458[0m   [32m0.8502[0m        [35m0.2750[0m       [31m0.9274[0m        [94m0.2128[0m     +  120.1678
      2   [36m0.4537[0m   0.8431        [35m0.2364[0m       0.9262        0.2146        120.3576
      3   [36m0.4636[0m   0.8444        [35m0.2324[0m       [31m0.9299[0m        [94m0.2119[0m     +  120.7258
      4   [36m0.4693[0m   0.8442        [35m0.2297[0m       0.9299        [94m0.2111[0m     +  120.7307
      5   0.4682   0.8453        [35m0.2286[0m       0.9299        [94m0.2098[0m     +  120.5585
      6   [36m0.4704[0m   0.8435        [35m0.2267[0m       [31m0.9311[0m        0.2108        120.4460
      7   0.4694   0.8431        [35m0.2253[0m       0.9274        0.2103        120.4795
      8   0.4604   0.8431        [35m0.2237[0m       0.9250        0.2109        120.6833
      9   0.4543   0.8434        [35m0.2209[0m       [31m0.9323[0m        [94m0.2082[0m     +  120.6792
     10   [36m0.4777[0m   0.8458        [35m0.2191[0m       0.9311        [94m0.2060[0m     +  120.7352
     11   0.4709   0.8447        [35m0.2180[0m       0.9287        0.2078        120.5252
     12   0.4774   0.8453        [35m0.2155[0m       0.9323        [94m0.2055[0m     +  121.0078
     13   [36m0.4796[0m   0.8461        [35m0.2153[0m       0.9311        0.2071        120.7769
     14   0.4744   0.8467        [35m0.2126[0m       [31m0.9335[0m        0.2089        120.6825
     15   0.4749   0.8462        [35m0.2117[0m       [31m0.9371[0m        [94m0.2046[0m     +  120.9677
     16   0.4756   0.8463        [35m0.2101[0m       [31m0.9383[0m        0.2076        120.3432
     17   [36m0.4820[0m   0.8452        [35m0.2093[0m       0.9347        0.2086        120.5571
     18   0.4769   0.8440        [35m0.2067[0m       0.9359        0.2075        120.4305
     19   0.4713   0.8412        [35m0.2058[0m       0.9359        0.2071        120.8988
     20   0.4807   0.8418        [35m0.2045[0m       0.9371        0.2097        120.6724
     21   0.4697   0.8379        [35m0.2026[0m       0.9323        0.2118        120.7132
     22   0.4761   0.8383        [35m0.2017[0m       0.9323        0.2106        120.9947
     23   0.4799   0.8357        [35m0.2003[0m       0.9359        0.2118        120.9744
     24   0.4637   0.8323        [35m0.1993[0m       0.9347        0.2142        120.8546
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 17:25:08,349][0m Trial 369 finished with value: 0.2046491935975318 and parameters: {'lr': 3.168286202811326e-06, 'dropout': 0.310442394972708, 'd_model_multiplier': 64, 'num_layers': 6, 'n_heads': 32, 'dim_feedforward': 351, 'batch_size': 54, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.001, 'top_n_features': 65}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 64
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp       dur
-------  -------  -------  ------------  -----------  ------------  ----  --------
      1   [36m0.2704[0m   [32m0.7761[0m        [35m0.2891[0m       [31m0.9081[0m        [94m0.2656[0m     +  120.7110
      2   [36m0.2770[0m   [32m0.7818[0m        [35m0.2326[0m       0.9045        0.2660        120.9533
      3   [36m0.2774[0m   [32m0.7833[0m        [35m0.2287[0m       0.9021        0.2680        120.9580
      4   0.2773   0.7833        [35m0.2256[0m       0.9045        0.2681        121.2157
      5   0.2766   [32m0.7852[0m        [35m0.2242[0m       0.9069        0.2689        120.9178
      6   0.2766   [32m0.7855[0m        [35m0.2223[0m       0.9057        0.2692        121.0895
      7   0.2756   [32m0.7864[0m        [35m0.2196[0m       0.9069        0.2711        121.0496
      8   0.2725   0.7846        0.2200       0.9081        0.2723        121.0846
      9   0.2729   0.7844        [35m0.2185[0m       0.9081        0.2713        120.8144
     10   0.2723   0.7845        [35m0.2162[0m       0.9069        0.2728        120.9755
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 17:47:22,650][0m Trial 370 finished with value: 0.26564694611308365 and parameters: {'lr': 2.024992613612739e-06, 'dropout': 0.3186976375820365, 'd_model_multiplier': 64, 'num_layers': 6, 'n_heads': 32, 'dim_feedforward': 352, 'batch_size': 55, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.001, 'top_n_features': 64}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 70
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp       dur
-------  -------  -------  ------------  -----------  ------------  ----  --------
      1   [36m0.2857[0m   [32m0.7541[0m        [35m0.2660[0m       [31m0.9287[0m        [94m0.2335[0m     +  122.0631
      2   [36m0.3102[0m   [32m0.7718[0m        [35m0.2320[0m       0.9262        [94m0.2302[0m     +  122.0729
      3   [36m0.3114[0m   [32m0.7811[0m        [35m0.2295[0m       0.9226        [94m0.2288[0m     +  122.0755
      4   [36m0.3146[0m   [32m0.7887[0m        [35m0.2255[0m       0.9226        [94m0.2273[0m     +  122.1120
      5   [36m0.3216[0m   [32m0.7914[0m        [35m0.2227[0m       0.9226        [94m0.2264[0m     +  122.1105
      6   0.3194   [32m0.7918[0m        [35m0.2218[0m       0.9226        0.2269        122.2196
      7   0.3129   0.7916        [35m0.2191[0m       0.9214        0.2294        122.0709
      8   0.3141   0.7911        [35m0.2167[0m       0.9214        0.2302        122.3068
      9   0.3157   0.7904        [35m0.2150[0m       0.9202        0.2293        122.3032
     10   0.3132   0.7839        [35m0.2122[0m       0.9214        0.2320        122.4075
     11   0.3120   0.7882        [35m0.2108[0m       0.9214        0.2313        122.5748
     12   0.3126   0.7881        [35m0.2099[0m       0.9202        0.2340        122.2712
     13   0.3121   0.7833        [35m0.2080[0m       0.9190        0.2353        122.4178
     14   0.3131   0.7839        [35m0.2060[0m       0.9190        0.2360        122.2351
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 18:18:10,711][0m Trial 371 finished with value: 0.22638560242347303 and parameters: {'lr': 3.2014565371305917e-06, 'dropout': 0.2990201445583437, 'd_model_multiplier': 64, 'num_layers': 6, 'n_heads': 32, 'dim_feedforward': 341, 'batch_size': 50, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.001, 'top_n_features': 70}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 59
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp       dur
-------  -------  -------  ------------  -----------  ------------  ----  --------
      1   [36m0.3192[0m   [32m0.7532[0m        [35m0.2697[0m       [31m0.9190[0m        [94m0.2565[0m     +  121.2613
      2   [36m0.3323[0m   [32m0.7745[0m        [35m0.2317[0m       [31m0.9226[0m        [94m0.2514[0m     +  121.6162
      3   [36m0.3419[0m   [32m0.7853[0m        [35m0.2289[0m       0.9214        [94m0.2462[0m     +  121.3269
      4   [36m0.3510[0m   [32m0.7900[0m        [35m0.2264[0m       0.9202        [94m0.2439[0m     +  121.1735
      5   [36m0.3623[0m   [32m0.7941[0m        0.2266       0.9214        [94m0.2423[0m     +  121.3349
      6   [36m0.3688[0m   [32m0.7989[0m        [35m0.2249[0m       0.9214        [94m0.2403[0m     +  121.1477
      7   [36m0.3768[0m   [32m0.8022[0m        [35m0.2238[0m       0.9214        [94m0.2379[0m     +  121.3350
      8   [36m0.3841[0m   [32m0.8040[0m        [35m0.2213[0m       0.9226        [94m0.2363[0m     +  121.4732
      9   [36m0.3909[0m   [32m0.8076[0m        [35m0.2200[0m       0.9214        [94m0.2341[0m     +  121.7661
     10   [36m0.3954[0m   [32m0.8108[0m        [35m0.2188[0m       [31m0.9238[0m        [94m0.2325[0m     +  121.7256
     11   [36m0.4037[0m   [32m0.8123[0m        [35m0.2186[0m       0.9202        [94m0.2311[0m     +  121.7701
     12   [36m0.4054[0m   [32m0.8152[0m        [35m0.2172[0m       0.9202        [94m0.2297[0m     +  121.8601
     13   [36m0.4059[0m   [32m0.8161[0m        [35m0.2160[0m       0.9190        [94m0.2288[0m     +  121.9502
     14   [36m0.4178[0m   [32m0.8199[0m        [35m0.2143[0m       0.9202        [94m0.2251[0m     +  121.6819
     15   [36m0.4231[0m   [32m0.8216[0m        [35m0.2138[0m       0.9214        [94m0.2244[0m     +  121.2588
     16   0.4164   [32m0.8248[0m        [35m0.2127[0m       0.9202        [94m0.2233[0m     +  121.6439
     17   0.4169   [32m0.8253[0m        [35m0.2112[0m       0.9202        0.2236        121.4852
     18   0.4188   [32m0.8273[0m        [35m0.2099[0m       0.9190        [94m0.2224[0m     +  121.6614
     19   0.4201   [32m0.8299[0m        [35m0.2099[0m       0.9202        [94m0.2210[0m     +  121.1740
     20   0.4194   [32m0.8320[0m        [35m0.2076[0m       0.9226        [94m0.2206[0m     +  121.3994
     21   0.4198   [32m0.8322[0m        [35m0.2066[0m       0.9214        0.2213        121.6892
     22   0.4199   [32m0.8325[0m        0.2069       0.9202        [94m0.2201[0m     +  121.7725
     23   0.4196   [32m0.8343[0m        [35m0.2046[0m       0.9214        [94m0.2200[0m     +  121.3387
     24   0.4106   0.8329        [35m0.2038[0m       0.9190        0.2205        121.5798
     25   0.4118   [32m0.8351[0m        0.2044       0.9214        0.2206        122.0507
     26   0.4076   0.8347        [35m0.2029[0m       0.9190        0.2212        121.6037
     27   0.4095   [32m0.8368[0m        [35m0.2027[0m       0.9190        0.2207        121.8084
     28   0.4095   0.8365        [35m0.2004[0m       0.9190        0.2222        121.7133
     29   0.4023   0.8359        [35m0.1987[0m       0.9214        0.2229        121.8517
     30   0.3970   0.8350        0.1996       0.9190        0.2244        121.7173
     31   0.3948   0.8348        [35m0.1983[0m       0.9214        0.2250        122.0690
     32   0.3896   0.8346        [35m0.1970[0m       0.9190        0.2255        121.5530
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 19:26:05,131][0m Trial 372 finished with value: 0.22003212318436988 and parameters: {'lr': 2.5012269308705643e-06, 'dropout': 0.3144171618079782, 'd_model_multiplier': 64, 'num_layers': 6, 'n_heads': 32, 'dim_feedforward': 357, 'batch_size': 53, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.001, 'top_n_features': 59}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 62
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
Warning, assumed runtime error: CUDA out of memory. Tried to allocate 752.00 MiB (GPU 0; 23.70 GiB total capacity; 21.33 GiB already allocated; 75.25 MiB free; 22.53 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[32m[I 2023-05-03 19:26:09,682][0m Trial 373 finished with value: 100.0 and parameters: {'lr': 3.981464492558105e-06, 'dropout': 0.32693906244013954, 'd_model_multiplier': 64, 'num_layers': 7, 'n_heads': 32, 'dim_feedforward': 347, 'batch_size': 62, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.001, 'top_n_features': 62}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 66
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp       dur
-------  -------  -------  ------------  -----------  ------------  ----  --------
      1   [36m0.3387[0m   [32m0.8073[0m        [35m0.2575[0m       [31m0.9214[0m        [94m0.2314[0m     +  121.2701
      2   [36m0.3496[0m   [32m0.8094[0m        [35m0.2418[0m       [31m0.9250[0m        [94m0.2294[0m     +  121.6332
      3   0.3413   [32m0.8180[0m        [35m0.2373[0m       0.9238        0.2296        121.4384
      4   0.3386   [32m0.8184[0m        [35m0.2336[0m       0.9226        0.2356        121.2307
      5   0.3322   [32m0.8217[0m        [35m0.2299[0m       0.9238        0.2338        121.3031
      6   0.3345   0.8215        [35m0.2263[0m       0.9214        0.2384        121.4105
      7   0.3479   [32m0.8301[0m        [35m0.2217[0m       0.9202        0.2323        121.4243
      8   0.3261   0.8277        [35m0.2200[0m       0.9166        0.2418        121.2989
      9   0.3397   [32m0.8320[0m        [35m0.2158[0m       0.9202        0.2414        121.5039
     10   0.3478   [32m0.8400[0m        [35m0.2138[0m       0.9202        0.2431        121.5483
     11   0.3362   0.8340        [35m0.2111[0m       0.9214        0.2422        121.6320
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 19:50:33,303][0m Trial 374 finished with value: 0.22939993182091356 and parameters: {'lr': 1.7159539454200077e-05, 'dropout': 0.33294823225226866, 'd_model_multiplier': 64, 'num_layers': 6, 'n_heads': 32, 'dim_feedforward': 331, 'batch_size': 59, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.001, 'top_n_features': 66}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 55
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp       dur
-------  -------  -------  ------------  -----------  ------------  ----  --------
      1   [36m0.2674[0m   [32m0.7654[0m        [35m0.2971[0m       [31m0.9045[0m        [94m0.2724[0m     +  104.2072
      2   0.2644   [32m0.7701[0m        [35m0.2321[0m       0.9021        0.2754        104.2403
      3   0.2620   0.7667        [35m0.2306[0m       0.9008        0.2764        104.0469
      4   0.2630   0.7634        [35m0.2294[0m       0.9033        0.2778        104.3192
      5   [36m0.2750[0m   0.7624        [35m0.2290[0m       [31m0.9057[0m        0.2769        104.2837
      6   [36m0.2761[0m   0.7667        [35m0.2263[0m       0.9057        0.2751        104.3852
      7   [36m0.2828[0m   0.7610        [35m0.2241[0m       0.9057        0.2773        103.9879
      8   0.2800   0.7607        [35m0.2235[0m       0.9057        0.2761        104.3738
      9   0.2804   0.7559        [35m0.2223[0m       0.9021        0.2793        104.1872
     10   [36m0.2858[0m   0.7501        [35m0.2188[0m       0.9021        0.2803        104.1096
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 20:09:43,036][0m Trial 375 finished with value: 0.2723743405909319 and parameters: {'lr': 7.910899331267951e-06, 'dropout': 0.3056288731235214, 'd_model_multiplier': 64, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 325, 'batch_size': 21, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.001, 'top_n_features': 55}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 68
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp       dur
-------  -------  -------  ------------  -----------  ------------  ----  --------
      1   [36m0.2999[0m   [32m0.7865[0m        [35m0.2800[0m       [31m0.9202[0m        [94m0.2338[0m     +  102.3509
      2   [36m0.3333[0m   [32m0.8048[0m        [35m0.2336[0m       [31m0.9238[0m        [94m0.2283[0m     +  102.5268
      3   0.3297   [32m0.8099[0m        [35m0.2304[0m       [31m0.9250[0m        [94m0.2246[0m     +  102.8579
      4   [36m0.3426[0m   [32m0.8160[0m        [35m0.2273[0m       0.9250        [94m0.2233[0m     +  102.9034
      5   [36m0.3435[0m   [32m0.8199[0m        [35m0.2258[0m       0.9250        [94m0.2218[0m     +  102.3988
      6   [36m0.3452[0m   [32m0.8240[0m        [35m0.2244[0m       0.9238        [94m0.2209[0m     +  102.9136
      7   [36m0.3485[0m   [32m0.8282[0m        [35m0.2232[0m       0.9250        [94m0.2193[0m     +  102.5531
      8   0.3385   [32m0.8285[0m        [35m0.2217[0m       0.9238        [94m0.2187[0m     +  102.7378
      9   0.3477   [32m0.8308[0m        [35m0.2199[0m       0.9238        [94m0.2182[0m     +  102.6573
     10   [36m0.3505[0m   [32m0.8338[0m        [35m0.2180[0m       0.9202        [94m0.2174[0m     +  102.7280
     11   [36m0.3509[0m   [32m0.8347[0m        [35m0.2164[0m       0.9238        0.2176        102.5840
     12   0.3466   [32m0.8362[0m        [35m0.2154[0m       0.9226        [94m0.2171[0m     +  102.5315
     13   0.3498   [32m0.8384[0m        [35m0.2138[0m       0.9214        0.2174        102.8191
     14   0.3477   [32m0.8395[0m        [35m0.2119[0m       0.9226        [94m0.2164[0m     +  102.8434
     15   0.3445   [32m0.8400[0m        [35m0.2100[0m       0.9214        0.2169        102.5543
     16   0.3453   [32m0.8403[0m        [35m0.2099[0m       0.9226        0.2166        102.9020
     17   0.3444   [32m0.8422[0m        [35m0.2073[0m       0.9190        [94m0.2162[0m     +  102.9068
     18   0.3281   0.8417        0.2074       0.9190        0.2178        102.5286
     19   0.3328   [32m0.8424[0m        [35m0.2056[0m       0.9190        0.2173        102.8468
     20   0.3294   0.8413        [35m0.2038[0m       0.9202        0.2188        102.8645
     21   0.3298   0.8397        0.2038       0.9178        0.2192        102.6898
     22   0.3310   0.8421        [35m0.2015[0m       0.9154        0.2193        103.1202
     23   0.3355   0.8393        [35m0.2004[0m       0.9154        0.2197        103.1821
     24   0.3319   0.8390        [35m0.2002[0m       0.9154        0.2208        102.9697
     25   0.3290   0.8355        [35m0.1982[0m       0.9141        0.2225        102.9893
     26   0.3305   0.8395        [35m0.1967[0m       0.9166        0.2221        102.9645
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 20:56:28,845][0m Trial 376 finished with value: 0.21615294868885535 and parameters: {'lr': 2.7959828630489947e-06, 'dropout': 0.2768602459051122, 'd_model_multiplier': 64, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 364, 'batch_size': 57, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 68}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 58
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
Warning, assumed runtime error: CUDA out of memory. Tried to allocate 1.09 GiB (GPU 0; 23.70 GiB total capacity; 21.31 GiB already allocated; 75.25 MiB free; 22.53 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[32m[I 2023-05-03 20:56:33,143][0m Trial 377 finished with value: 100.0 and parameters: {'lr': 4.095251691495414e-06, 'dropout': 0.3016471174057487, 'd_model_multiplier': 64, 'num_layers': 6, 'n_heads': 64, 'dim_feedforward': 342, 'batch_size': 46, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 58}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 75
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2692[0m   [32m0.7886[0m        [35m0.2773[0m       [31m0.9262[0m        [94m0.2227[0m     +  82.8264
      2   [36m0.2772[0m   0.7757        [35m0.2372[0m       0.9262        0.2327        82.7280
      3   [36m0.2789[0m   0.7790        [35m0.2331[0m       0.9190        0.2345        82.7015
      4   0.2768   0.7795        [35m0.2317[0m       0.9166        0.2378        83.1169
      5   0.2726   0.7819        [35m0.2282[0m       0.9093        0.2429        82.8783
      6   0.2674   0.7817        [35m0.2265[0m       0.9105        0.2436        82.9059
      7   0.2689   0.7810        [35m0.2250[0m       0.9105        0.2440        82.7900
      8   0.2701   0.7806        [35m0.2226[0m       0.9081        0.2474        82.9103
      9   0.2755   0.7800        [35m0.2222[0m       0.9093        0.2477        82.7338
     10   0.2698   0.7776        [35m0.2214[0m       0.9045        0.2468        82.6657
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 21:11:46,961][0m Trial 378 finished with value: 0.22273093651182851 and parameters: {'lr': 5.6961702318667865e-06, 'dropout': 0.41156529772666506, 'd_model_multiplier': 64, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 336, 'batch_size': 66, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.001, 'top_n_features': 75}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 88
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0712[0m   [32m0.3084[0m        [35m0.6499[0m       [31m0.9190[0m        [94m0.5764[0m     +  31.4922
      2   0.0643   0.3021        [35m0.5193[0m       0.9190        [94m0.4476[0m     +  31.7831
      3   [36m0.0794[0m   [32m0.3535[0m        [35m0.4073[0m       0.9190        [94m0.3589[0m     +  32.0116
      4   [36m0.1294[0m   [32m0.4856[0m        [35m0.3346[0m       0.9190        [94m0.3066[0m     +  31.8884
      5   [36m0.1594[0m   [32m0.5759[0m        [35m0.2930[0m       0.9190        [94m0.2824[0m     +  31.8598
      6   [36m0.1679[0m   [32m0.6308[0m        [35m0.2738[0m       0.9190        [94m0.2712[0m     +  32.1162
      7   [36m0.1829[0m   [32m0.6653[0m        [35m0.2642[0m       0.9190        [94m0.2651[0m     +  32.1328
      8   [36m0.1879[0m   [32m0.6857[0m        [35m0.2573[0m       0.9190        [94m0.2611[0m     +  32.1492
      9   [36m0.1968[0m   [32m0.7067[0m        [35m0.2537[0m       0.9190        [94m0.2577[0m     +  32.1677
     10   [36m0.2195[0m   [32m0.7298[0m        [35m0.2491[0m       0.9190        [94m0.2539[0m     +  32.1564
     11   [36m0.2565[0m   [32m0.7609[0m        [35m0.2454[0m       0.9190        [94m0.2480[0m     +  31.8688
     12   [36m0.2950[0m   [32m0.7818[0m        [35m0.2408[0m       0.9178        [94m0.2422[0m     +  31.8922
     13   [36m0.3114[0m   [32m0.7862[0m        [35m0.2365[0m       0.9190        [94m0.2405[0m     +  32.1320
     14   [36m0.3251[0m   [32m0.7900[0m        [35m0.2348[0m       [31m0.9202[0m        [94m0.2380[0m     +  31.8234
     15   [36m0.3335[0m   [32m0.7921[0m        [35m0.2340[0m       [31m0.9226[0m        [94m0.2358[0m     +  31.8152
     16   [36m0.3389[0m   [32m0.7931[0m        [35m0.2321[0m       0.9226        0.2360        31.9464
     17   [36m0.3454[0m   [32m0.7958[0m        [35m0.2300[0m       [31m0.9238[0m        [94m0.2348[0m     +  32.0031
     18   [36m0.3544[0m   [32m0.7988[0m        [35m0.2296[0m       0.9238        [94m0.2327[0m     +  32.2535
     19   [36m0.3651[0m   [32m0.8000[0m        [35m0.2292[0m       0.9226        0.2329        32.1575
     20   [36m0.3685[0m   [32m0.8025[0m        [35m0.2292[0m       [31m0.9262[0m        [94m0.2310[0m     +  32.2321
     21   0.3679   [32m0.8036[0m        [35m0.2284[0m       0.9262        0.2320        31.9204
     22   [36m0.3690[0m   [32m0.8041[0m        0.2286       0.9250        0.2317        31.9390
     23   [36m0.3729[0m   [32m0.8048[0m        [35m0.2275[0m       0.9238        0.2319        32.0039
     24   [36m0.3766[0m   [32m0.8058[0m        [35m0.2271[0m       0.9262        0.2313        32.1366
     25   [36m0.3768[0m   [32m0.8077[0m        [35m0.2261[0m       0.9250        [94m0.2306[0m     +  31.8389
     26   [36m0.3780[0m   [32m0.8079[0m        [35m0.2258[0m       0.9250        0.2312        31.3130
     27   [36m0.3791[0m   0.8078        0.2261       0.9238        [94m0.2304[0m     +  32.0971
     28   0.3765   [32m0.8097[0m        0.2258       0.9238        0.2314        31.9001
     29   [36m0.3838[0m   [32m0.8122[0m        [35m0.2239[0m       0.9238        [94m0.2299[0m     +  31.7168
     30   [36m0.3883[0m   [32m0.8137[0m        0.2241       0.9226        0.2303        32.2025
     31   0.3820   [32m0.8148[0m        0.2259       0.9238        [94m0.2291[0m     +  31.9750
     32   0.3831   [32m0.8156[0m        0.2248       0.9214        0.2295        32.1974
     33   0.3853   [32m0.8161[0m        [35m0.2231[0m       0.9226        0.2299        31.6373
     34   0.3799   [32m0.8186[0m        [35m0.2228[0m       0.9226        [94m0.2290[0m     +  32.0834
     35   0.3835   0.8174        [35m0.2223[0m       0.9226        0.2294        32.1071
     36   0.3785   [32m0.8191[0m        [35m0.2216[0m       0.9214        0.2301        31.5024
     37   0.3801   0.8190        [35m0.2212[0m       0.9214        0.2290        32.4652
     38   0.3819   [32m0.8206[0m        0.2218       0.9226        [94m0.2274[0m     +  31.9732
     39   0.3816   [32m0.8210[0m        [35m0.2205[0m       0.9214        0.2286        31.5090
     40   0.3855   [32m0.8233[0m        [35m0.2202[0m       0.9226        0.2284        31.9812
     41   0.3850   0.8232        [35m0.2200[0m       0.9226        0.2275        32.5671
     42   0.3829   0.8227        [35m0.2197[0m       0.9226        [94m0.2271[0m     +  32.1211
     43   0.3806   0.8229        [35m0.2191[0m       0.9226        [94m0.2270[0m     +  32.5565
     44   0.3830   [32m0.8244[0m        [35m0.2177[0m       0.9226        0.2271        32.6306
     45   0.3791   [32m0.8253[0m        [35m0.2177[0m       0.9226        0.2273        32.0933
     46   0.3758   0.8253        0.2187       0.9226        0.2279        31.8768
     47   0.3753   0.8248        0.2182       0.9226        [94m0.2268[0m     +  32.0658
     48   0.3800   [32m0.8264[0m        0.2185       0.9226        [94m0.2258[0m     +  31.9932
     49   0.3716   [32m0.8278[0m        [35m0.2166[0m       0.9226        0.2271        31.6509
     50   0.3726   0.8250        0.2172       0.9226        0.2269        31.9499
[32m[I 2023-05-03 21:38:30,524][0m Trial 379 finished with value: 0.2258078725118879 and parameters: {'lr': 1.246908227653394e-05, 'dropout': 0.3299572465911491, 'd_model_multiplier': 4, 'num_layers': 7, 'n_heads': 32, 'dim_feedforward': 350, 'batch_size': 95, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 88}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 63
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0575[0m   [32m0.4294[0m        [35m0.5021[0m       [31m0.9311[0m        [94m0.2874[0m     +  23.5166
      2   [36m0.1422[0m   [32m0.6556[0m        [35m0.2717[0m       0.9299        [94m0.2433[0m     +  23.7112
      3   [36m0.1692[0m   [32m0.7144[0m        [35m0.2462[0m       0.9262        [94m0.2373[0m     +  23.4667
      4   [36m0.1797[0m   [32m0.7357[0m        [35m0.2369[0m       0.9190        0.2379        23.3356
      5   [36m0.1862[0m   [32m0.7463[0m        [35m0.2327[0m       0.9214        0.2406        23.9490
      6   [36m0.1892[0m   [32m0.7542[0m        [35m0.2310[0m       0.9178        0.2377        23.7521
      7   [36m0.1920[0m   [32m0.7618[0m        [35m0.2284[0m       0.9178        0.2413        23.7989
      8   [36m0.1977[0m   [32m0.7683[0m        [35m0.2283[0m       0.9202        0.2394        23.9862
      9   [36m0.1977[0m   [32m0.7737[0m        [35m0.2257[0m       0.9190        0.2413        23.9085
     10   [36m0.1991[0m   [32m0.7764[0m        [35m0.2248[0m       0.9190        0.2390        23.8243
     11   [36m0.2012[0m   0.7761        [35m0.2238[0m       0.9190        0.2418        23.8961
     12   0.2005   [32m0.7777[0m        [35m0.2222[0m       0.9166        0.2423        23.7722
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 21:43:39,801][0m Trial 380 finished with value: 0.2373172324038186 and parameters: {'lr': 3.516872937672117e-05, 'dropout': 0.26296832062180014, 'd_model_multiplier': 4, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 359, 'batch_size': 48, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0, 'top_n_features': 63}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 52
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp       dur
-------  -------  -------  ------------  -----------  ------------  ----  --------
      1   [36m0.3274[0m   [32m0.7497[0m        [35m0.2697[0m       [31m0.9238[0m        [94m0.2434[0m     +  101.5516
      2   [36m0.3378[0m   [32m0.7561[0m        [35m0.2340[0m       0.9238        [94m0.2416[0m     +  101.8432
      3   [36m0.3442[0m   [32m0.7613[0m        [35m0.2310[0m       0.9214        [94m0.2407[0m     +  101.9964
      4   0.3311   [32m0.7632[0m        [35m0.2290[0m       0.9226        0.2416        102.1425
      5   0.3350   [32m0.7650[0m        [35m0.2263[0m       0.9190        0.2422        101.8295
      6   0.3370   [32m0.7675[0m        [35m0.2256[0m       0.9202        0.2425        101.8207
      7   0.3370   [32m0.7723[0m        [35m0.2227[0m       0.9226        0.2423        101.8939
      8   0.3323   [32m0.7760[0m        [35m0.2225[0m       0.9238        0.2425        101.8478
      9   0.3344   [32m0.7761[0m        [35m0.2204[0m       0.9214        0.2430        101.9567
     10   0.3401   [32m0.7811[0m        [35m0.2183[0m       0.9202        0.2430        101.9374
     11   [36m0.3447[0m   [32m0.7859[0m        [35m0.2170[0m       0.9202        0.2408        102.0794
     12   0.3447   [32m0.7885[0m        [35m0.2156[0m       0.9214        [94m0.2401[0m     +  101.9679
     13   [36m0.3473[0m   0.7880        [35m0.2143[0m       0.9214        0.2416        101.9783
     14   0.3465   [32m0.7890[0m        [35m0.2131[0m       0.9202        0.2425        102.1400
     15   0.3461   0.7864        [35m0.2112[0m       0.9238        0.2414        101.8080
     16   0.3406   0.7845        [35m0.2104[0m       0.9226        0.2428        102.1240
     17   0.3398   0.7852        [35m0.2087[0m       0.9190        0.2444        102.0497
     18   0.3353   0.7832        [35m0.2076[0m       0.9190        0.2469        101.7165
     19   0.3359   0.7833        [35m0.2067[0m       0.9190        0.2487        101.8107
     20   0.3309   0.7811        [35m0.2052[0m       0.9214        0.2503        101.9091
     21   0.3396   0.7863        [35m0.2049[0m       0.9202        0.2502        101.8261
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 22:21:11,797][0m Trial 381 finished with value: 0.24006040814133336 and parameters: {'lr': 5.0137890361558e-06, 'dropout': 0.3419376703063544, 'd_model_multiplier': 64, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 317, 'batch_size': 60, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 52}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 71
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0818[0m   [32m0.3411[0m        [35m0.5396[0m       [31m0.9105[0m        [94m0.3962[0m     +  34.4115
      2   [36m0.1385[0m   [32m0.5704[0m        [35m0.3235[0m       0.9105        [94m0.3012[0m     +  34.3927
      3   [36m0.2416[0m   [32m0.6740[0m        [35m0.2640[0m       0.9105        [94m0.2823[0m     +  34.5761
      4   [36m0.2948[0m   [32m0.7002[0m        [35m0.2476[0m       0.9105        [94m0.2744[0m     +  34.6532
      5   [36m0.3182[0m   [32m0.7107[0m        [35m0.2397[0m       0.9093        [94m0.2698[0m     +  34.4730
      6   [36m0.3347[0m   [32m0.7182[0m        [35m0.2360[0m       [31m0.9129[0m        [94m0.2666[0m     +  34.4722
      7   [36m0.3381[0m   [32m0.7227[0m        [35m0.2341[0m       0.9117        [94m0.2658[0m     +  34.4322
      8   0.3371   [32m0.7256[0m        [35m0.2319[0m       [31m0.9166[0m        [94m0.2649[0m     +  34.2420
      9   0.3379   [32m0.7292[0m        [35m0.2317[0m       0.9154        0.2649        34.4968
     10   [36m0.3401[0m   [32m0.7316[0m        [35m0.2307[0m       0.9154        [94m0.2647[0m     +  34.6971
     11   [36m0.3407[0m   [32m0.7344[0m        [35m0.2296[0m       0.9166        [94m0.2646[0m     +  34.8303
     12   [36m0.3411[0m   [32m0.7369[0m        [35m0.2286[0m       [31m0.9178[0m        [94m0.2645[0m     +  34.4132
     13   0.3394   [32m0.7390[0m        [35m0.2264[0m       [31m0.9190[0m        [94m0.2644[0m     +  34.6408
     14   0.3375   [32m0.7410[0m        0.2266       [31m0.9202[0m        0.2645        34.8517
     15   0.3367   [32m0.7438[0m        0.2266       0.9178        [94m0.2641[0m     +  34.5808
     16   0.3371   [32m0.7459[0m        [35m0.2252[0m       0.9166        0.2642        34.5711
     17   0.3400   [32m0.7485[0m        [35m0.2246[0m       0.9166        0.2641        34.4980
     18   0.3378   [32m0.7501[0m        0.2252       0.9166        0.2642        34.4583
     19   [36m0.3419[0m   [32m0.7513[0m        [35m0.2240[0m       0.9166        0.2642        34.7658
     20   [36m0.3434[0m   [32m0.7529[0m        [35m0.2224[0m       0.9166        0.2644        34.5888
     21   0.3417   [32m0.7540[0m        0.2231       0.9166        0.2645        34.3765
     22   0.3370   [32m0.7550[0m        [35m0.2224[0m       0.9141        0.2645        34.4576
     23   0.3420   [32m0.7555[0m        [35m0.2224[0m       0.9141        0.2650        34.5844
     24   0.3412   [32m0.7570[0m        [35m0.2217[0m       0.9129        0.2644        34.5338
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 22:35:37,736][0m Trial 382 finished with value: 0.2640528954552011 and parameters: {'lr': 3.024804308562991e-06, 'dropout': 0.31836801475115, 'd_model_multiplier': 4, 'num_layers': 4, 'n_heads': 64, 'dim_feedforward': 323, 'batch_size': 19, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 71}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 58
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0690[0m   [32m0.3325[0m        [35m0.5981[0m       [31m0.9178[0m        [94m0.5234[0m     +  27.6757
      2   0.0676   0.3234        [35m0.4797[0m       0.9178        [94m0.4257[0m     +  27.4404
      3   [36m0.0696[0m   [32m0.3450[0m        [35m0.4033[0m       0.9178        [94m0.3665[0m     +  27.1698
      4   [36m0.0767[0m   [32m0.4096[0m        [35m0.3559[0m       0.9178        [94m0.3300[0m     +  27.3682
      5   [36m0.0842[0m   [32m0.4818[0m        [35m0.3267[0m       0.9178        [94m0.3073[0m     +  27.6329
      6   [36m0.1063[0m   [32m0.5517[0m        [35m0.3067[0m       0.9178        [94m0.2927[0m     +  27.6152
      7   [36m0.1622[0m   [32m0.6084[0m        [35m0.2938[0m       0.9178        [94m0.2820[0m     +  27.3862
      8   [36m0.1872[0m   [32m0.6455[0m        [35m0.2850[0m       0.9178        [94m0.2742[0m     +  27.4384
      9   [36m0.2161[0m   [32m0.6775[0m        [35m0.2767[0m       0.9178        [94m0.2679[0m     +  27.2555
     10   [36m0.2437[0m   [32m0.7030[0m        [35m0.2711[0m       0.9178        [94m0.2629[0m     +  27.3740
     11   [36m0.2742[0m   [32m0.7221[0m        [35m0.2658[0m       0.9178        [94m0.2585[0m     +  27.6397
     12   [36m0.2986[0m   [32m0.7390[0m        [35m0.2631[0m       0.9178        [94m0.2547[0m     +  27.1815
     13   [36m0.3202[0m   [32m0.7499[0m        [35m0.2599[0m       0.9178        [94m0.2517[0m     +  27.6393
     14   [36m0.3432[0m   [32m0.7599[0m        [35m0.2563[0m       0.9178        [94m0.2488[0m     +  27.6166
     15   [36m0.3637[0m   [32m0.7671[0m        [35m0.2551[0m       0.9178        [94m0.2461[0m     +  27.1440
     16   [36m0.3733[0m   [32m0.7748[0m        [35m0.2527[0m       0.9178        [94m0.2432[0m     +  27.4308
     17   [36m0.3913[0m   [32m0.7824[0m        [35m0.2497[0m       [31m0.9190[0m        [94m0.2406[0m     +  27.5735
     18   [36m0.4001[0m   [32m0.7870[0m        [35m0.2492[0m       [31m0.9202[0m        [94m0.2387[0m     +  27.5765
     19   [36m0.4051[0m   [32m0.7921[0m        [35m0.2471[0m       0.9190        [94m0.2367[0m     +  27.6838
     20   [36m0.4072[0m   [32m0.7958[0m        [35m0.2468[0m       0.9202        [94m0.2349[0m     +  27.2881
     21   [36m0.4111[0m   [32m0.7993[0m        [35m0.2431[0m       [31m0.9214[0m        [94m0.2335[0m     +  27.5963
     22   [36m0.4115[0m   [32m0.8013[0m        0.2431       [31m0.9226[0m        [94m0.2322[0m     +  27.5666
     23   [36m0.4156[0m   [32m0.8031[0m        [35m0.2407[0m       0.9226        [94m0.2312[0m     +  27.3376
     24   [36m0.4197[0m   [32m0.8049[0m        0.2409       0.9226        [94m0.2299[0m     +  27.5509
     25   0.4191   [32m0.8054[0m        [35m0.2399[0m       [31m0.9238[0m        [94m0.2290[0m     +  27.5181
     26   0.4178   0.8054        [35m0.2395[0m       0.9238        [94m0.2283[0m     +  27.0693
     27   0.4180   [32m0.8062[0m        [35m0.2380[0m       0.9238        [94m0.2277[0m     +  27.5879
     28   0.4186   [32m0.8066[0m        0.2393       0.9226        [94m0.2272[0m     +  27.2635
     29   0.4189   [32m0.8067[0m        [35m0.2374[0m       0.9226        [94m0.2268[0m     +  27.4096
     30   [36m0.4223[0m   [32m0.8071[0m        0.2382       0.9226        [94m0.2265[0m     +  27.4926
     31   0.4206   [32m0.8076[0m        [35m0.2368[0m       0.9238        [94m0.2260[0m     +  27.6168
     32   0.4193   [32m0.8077[0m        [35m0.2356[0m       [31m0.9250[0m        [94m0.2256[0m     +  27.7548
     33   [36m0.4255[0m   [32m0.8080[0m        0.2363       0.9250        [94m0.2256[0m     +  27.6432
     34   0.4250   0.8079        [35m0.2356[0m       [31m0.9262[0m        [94m0.2253[0m     +  27.7526
     35   [36m0.4258[0m   [32m0.8082[0m        [35m0.2348[0m       0.9262        [94m0.2250[0m     +  27.6864
     36   [36m0.4269[0m   0.8082        [35m0.2344[0m       [31m0.9274[0m        [94m0.2247[0m     +  27.3268
     37   [36m0.4280[0m   [32m0.8088[0m        0.2354       0.9274        [94m0.2244[0m     +  27.4140
     38   [36m0.4285[0m   [32m0.8090[0m        [35m0.2341[0m       [31m0.9287[0m        [94m0.2243[0m     +  27.4784
     39   0.4281   [32m0.8093[0m        [35m0.2334[0m       0.9287        [94m0.2242[0m     +  27.9467
     40   0.4262   [32m0.8095[0m        0.2345       0.9287        [94m0.2239[0m     +  27.6348
     41   0.4269   [32m0.8098[0m        0.2353       0.9274        [94m0.2239[0m     +  27.6337
     42   0.4265   [32m0.8102[0m        [35m0.2314[0m       0.9274        [94m0.2236[0m     +  27.4939
     43   0.4282   [32m0.8106[0m        0.2350       0.9274        [94m0.2234[0m     +  27.7743
     44   0.4262   [32m0.8110[0m        0.2324       0.9274        [94m0.2233[0m     +  27.3836
     45   0.4268   [32m0.8117[0m        0.2332       0.9274        [94m0.2233[0m     +  27.4674
     46   0.4271   [32m0.8121[0m        0.2341       0.9274        [94m0.2232[0m     +  27.6335
     47   0.4278   [32m0.8125[0m        0.2321       0.9274        [94m0.2232[0m     +  27.8024
     48   0.4281   [32m0.8128[0m        0.2323       0.9274        [94m0.2229[0m     +  27.9424
     49   [36m0.4290[0m   [32m0.8132[0m        0.2330       0.9274        [94m0.2229[0m     +  27.6595
     50   [36m0.4295[0m   [32m0.8135[0m        0.2324       0.9274        [94m0.2226[0m     +  27.8734
[32m[I 2023-05-03 22:58:40,092][0m Trial 383 finished with value: 0.2225827102649601 and parameters: {'lr': 1.7437070525952137e-06, 'dropout': 0.42447429927238084, 'd_model_multiplier': 4, 'num_layers': 6, 'n_heads': 32, 'dim_feedforward': 336, 'batch_size': 65, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 58}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 66
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0810[0m   [32m0.3567[0m        [35m0.6163[0m       [31m0.9105[0m        [94m0.5387[0m     +  19.9987
      2   0.0805   0.3525        [35m0.4627[0m       0.9105        [94m0.3963[0m     +  19.9477
      3   [36m0.0972[0m   [32m0.4847[0m        [35m0.3477[0m       0.9105        [94m0.3233[0m     +  19.9819
      4   [36m0.1922[0m   [32m0.6039[0m        [35m0.2929[0m       0.9105        [94m0.2945[0m     +  20.0425
      5   [36m0.2293[0m   [32m0.6586[0m        [35m0.2680[0m       [31m0.9117[0m        [94m0.2828[0m     +  20.2266
      6   [36m0.2552[0m   [32m0.6914[0m        [35m0.2544[0m       [31m0.9129[0m        [94m0.2764[0m     +  19.9189
      7   [36m0.2744[0m   [32m0.7078[0m        [35m0.2464[0m       0.9105        [94m0.2730[0m     +  19.9590
      8   [36m0.2836[0m   [32m0.7153[0m        [35m0.2410[0m       0.9117        [94m0.2716[0m     +  20.0233
      9   [36m0.2908[0m   [32m0.7210[0m        [35m0.2378[0m       0.9105        [94m0.2713[0m     +  19.9647
     10   [36m0.2960[0m   [32m0.7235[0m        [35m0.2354[0m       0.9105        [94m0.2712[0m     +  20.3555
     11   [36m0.2998[0m   [32m0.7276[0m        [35m0.2334[0m       0.9117        [94m0.2708[0m     +  20.2511
     12   [36m0.3067[0m   [32m0.7323[0m        [35m0.2318[0m       0.9129        [94m0.2705[0m     +  20.3692
     13   [36m0.3111[0m   [32m0.7355[0m        [35m0.2308[0m       0.9117        [94m0.2702[0m     +  20.2580
     14   [36m0.3116[0m   [32m0.7384[0m        0.2311       0.9129        [94m0.2702[0m     +  19.8588
     15   [36m0.3122[0m   [32m0.7421[0m        [35m0.2296[0m       [31m0.9141[0m        [94m0.2697[0m     +  20.0898
     16   [36m0.3248[0m   [32m0.7447[0m        [35m0.2284[0m       0.9141        [94m0.2691[0m     +  20.3091
     17   [36m0.3257[0m   [32m0.7474[0m        [35m0.2279[0m       0.9129        [94m0.2688[0m     +  20.1269
     18   0.3224   [32m0.7496[0m        [35m0.2270[0m       0.9117        [94m0.2688[0m     +  19.8166
     19   0.3223   [32m0.7507[0m        [35m0.2254[0m       0.9117        0.2688        20.3589
     20   0.3227   [32m0.7521[0m        0.2270       0.9129        [94m0.2687[0m     +  20.1950
     21   0.3220   [32m0.7530[0m        [35m0.2250[0m       0.9141        [94m0.2683[0m     +  20.3708
     22   0.3214   [32m0.7539[0m        [35m0.2237[0m       0.9129        0.2684        20.4907
     23   0.3252   [32m0.7562[0m        [35m0.2233[0m       0.9129        [94m0.2679[0m     +  20.3253
     24   0.3247   [32m0.7568[0m        0.2246       0.9129        [94m0.2676[0m     +  20.4174
     25   [36m0.3276[0m   [32m0.7573[0m        0.2243       0.9129        [94m0.2676[0m     +  20.4112
     26   [36m0.3276[0m   [32m0.7581[0m        [35m0.2223[0m       0.9129        0.2678        20.2222
     27   0.3260   [32m0.7591[0m        0.2227       0.9141        0.2681        20.1942
     28   0.3245   [32m0.7603[0m        [35m0.2220[0m       [31m0.9154[0m        0.2680        20.4742
     29   0.3265   [32m0.7612[0m        [35m0.2212[0m       0.9154        0.2676        19.9506
     30   0.3260   [32m0.7617[0m        [35m0.2206[0m       0.9141        0.2677        19.9991
     31   0.3263   [32m0.7624[0m        0.2213       0.9141        [94m0.2672[0m     +  20.3121
     32   0.3272   [32m0.7627[0m        0.2209       0.9141        0.2673        20.1949
     33   0.3274   [32m0.7638[0m        0.2213       0.9154        [94m0.2671[0m     +  20.1522
     34   [36m0.3301[0m   [32m0.7640[0m        0.2208       0.9154        [94m0.2670[0m     +  20.1338
     35   0.3294   [32m0.7644[0m        0.2209       0.9154        0.2672        20.1778
     36   [36m0.3315[0m   [32m0.7652[0m        [35m0.2190[0m       [31m0.9166[0m        [94m0.2669[0m     +  20.1326
     37   0.3293   [32m0.7661[0m        0.2201       0.9154        [94m0.2666[0m     +  20.3585
     38   0.3302   [32m0.7664[0m        [35m0.2188[0m       0.9154        [94m0.2665[0m     +  20.3881
     39   [36m0.3336[0m   0.7664        0.2195       0.9166        [94m0.2664[0m     +  20.6189
     40   0.3308   [32m0.7677[0m        0.2198       0.9141        0.2667        20.3765
     41   0.3323   0.7663        [35m0.2175[0m       0.9154        0.2668        20.2302
     42   [36m0.3342[0m   [32m0.7680[0m        [35m0.2175[0m       0.9166        0.2668        20.4531
     43   0.3312   [32m0.7702[0m        [35m0.2172[0m       0.9141        [94m0.2660[0m     +  20.5099
     44   0.3257   [32m0.7703[0m        [35m0.2166[0m       0.9166        0.2664        20.4146
     45   0.3270   [32m0.7715[0m        [35m0.2152[0m       0.9166        0.2660        20.4890
     46   0.3286   [32m0.7730[0m        0.2171       0.9154        [94m0.2657[0m     +  20.3221
     47   0.3283   [32m0.7735[0m        0.2175       0.9154        [94m0.2656[0m     +  20.2703
     48   0.3280   0.7730        0.2163       0.9141        [94m0.2651[0m     +  20.3748
     49   0.3276   [32m0.7744[0m        0.2154       0.9141        0.2653        20.2833
     50   0.3280   0.7744        0.2164       0.9129        0.2654        20.3304
[32m[I 2023-05-03 23:15:34,801][0m Trial 384 finished with value: 0.26514495572271196 and parameters: {'lr': 8.176725728069647e-06, 'dropout': 0.38455571325968335, 'd_model_multiplier': 4, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 374, 'batch_size': 53, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.001, 'top_n_features': 66}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 61
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0599[0m   [32m0.3286[0m        [35m0.6517[0m       [31m0.9214[0m        [94m0.6120[0m     +  19.9967
      2   0.0565   0.2867        [35m0.5834[0m       0.9214        [94m0.5499[0m     +  20.1023
      3   0.0566   0.2814        [35m0.5268[0m       0.9214        [94m0.4983[0m     +  20.6601
      4   0.0564   0.2814        [35m0.4794[0m       0.9214        [94m0.4550[0m     +  20.3547
      5   0.0560   0.2825        [35m0.4398[0m       0.9214        [94m0.4189[0m     +  20.1346
      6   0.0558   0.2881        [35m0.4069[0m       0.9214        [94m0.3891[0m     +  20.1621
      7   0.0573   0.3123        [35m0.3795[0m       0.9214        [94m0.3649[0m     +  20.1951
      8   0.0593   [32m0.3437[0m        [35m0.3576[0m       0.9214        [94m0.3454[0m     +  20.6445
      9   [36m0.0622[0m   [32m0.3809[0m        [35m0.3386[0m       0.9214        [94m0.3300[0m     +  20.3901
     10   [36m0.0668[0m   [32m0.4221[0m        [35m0.3240[0m       0.9214        [94m0.3179[0m     +  20.3306
     11   [36m0.0719[0m   [32m0.4552[0m        [35m0.3123[0m       0.9214        [94m0.3083[0m     +  20.2468
     12   [36m0.0788[0m   [32m0.4841[0m        [35m0.3035[0m       0.9214        [94m0.3006[0m     +  20.5158
     13   [36m0.0882[0m   [32m0.5076[0m        [35m0.2958[0m       0.9214        [94m0.2943[0m     +  20.0366
     14   [36m0.1010[0m   [32m0.5306[0m        [35m0.2900[0m       0.9214        [94m0.2891[0m     +  20.4609
     15   [36m0.1118[0m   [32m0.5516[0m        [35m0.2841[0m       0.9214        [94m0.2846[0m     +  20.5660
     16   [36m0.1218[0m   [32m0.5708[0m        [35m0.2798[0m       0.9214        [94m0.2808[0m     +  20.4044
     17   [36m0.1293[0m   [32m0.5866[0m        [35m0.2764[0m       0.9214        [94m0.2774[0m     +  20.2928
     18   [36m0.1348[0m   [32m0.6010[0m        [35m0.2737[0m       0.9214        [94m0.2743[0m     +  20.6233
     19   [36m0.1405[0m   [32m0.6146[0m        [35m0.2707[0m       0.9214        [94m0.2717[0m     +  20.6162
     20   [36m0.1447[0m   [32m0.6272[0m        [35m0.2676[0m       0.9214        [94m0.2692[0m     +  20.3247
     21   [36m0.1490[0m   [32m0.6390[0m        [35m0.2649[0m       0.9214        [94m0.2670[0m     +  20.1025
     22   [36m0.1562[0m   [32m0.6504[0m        [35m0.2627[0m       0.9214        [94m0.2649[0m     +  20.8556
     23   [36m0.1613[0m   [32m0.6595[0m        [35m0.2617[0m       0.9214        [94m0.2630[0m     +  20.2202
     24   [36m0.1658[0m   [32m0.6687[0m        [35m0.2589[0m       0.9214        [94m0.2612[0m     +  20.8525
     25   [36m0.1700[0m   [32m0.6758[0m        [35m0.2580[0m       0.9214        [94m0.2596[0m     +  20.5354
     26   [36m0.1739[0m   [32m0.6829[0m        [35m0.2562[0m       0.9214        [94m0.2581[0m     +  20.3981
     27   [36m0.1811[0m   [32m0.6924[0m        [35m0.2556[0m       0.9214        [94m0.2566[0m     +  20.5852
     28   [36m0.1869[0m   [32m0.6995[0m        [35m0.2532[0m       0.9214        [94m0.2553[0m     +  20.4154
     29   [36m0.1935[0m   [32m0.7067[0m        [35m0.2526[0m       0.9214        [94m0.2540[0m     +  20.5978
     30   [36m0.1989[0m   [32m0.7134[0m        [35m0.2519[0m       0.9214        [94m0.2528[0m     +  20.6655
     31   [36m0.2042[0m   [32m0.7192[0m        [35m0.2506[0m       0.9202        [94m0.2517[0m     +  20.7697
     32   [36m0.2077[0m   [32m0.7244[0m        [35m0.2494[0m       0.9202        [94m0.2506[0m     +  19.9707
     33   [36m0.2104[0m   [32m0.7294[0m        [35m0.2480[0m       0.9202        [94m0.2497[0m     +  20.7346
     34   [36m0.2159[0m   [32m0.7345[0m        [35m0.2480[0m       0.9190        [94m0.2487[0m     +  20.5552
     35   [36m0.2181[0m   [32m0.7376[0m        [35m0.2464[0m       0.9178        [94m0.2479[0m     +  20.4061
     36   [36m0.2212[0m   [32m0.7406[0m        0.2475       0.9190        [94m0.2470[0m     +  20.3804
     37   [36m0.2254[0m   [32m0.7445[0m        [35m0.2454[0m       0.9190        [94m0.2462[0m     +  20.4307
     38   [36m0.2292[0m   [32m0.7467[0m        [35m0.2444[0m       0.9178        [94m0.2455[0m     +  20.7666
     39   [36m0.2345[0m   [32m0.7487[0m        [35m0.2441[0m       0.9178        [94m0.2448[0m     +  20.0199
     40   [36m0.2378[0m   [32m0.7508[0m        [35m0.2431[0m       0.9178        [94m0.2442[0m     +  20.8955
     41   [36m0.2403[0m   [32m0.7528[0m        0.2433       0.9178        [94m0.2436[0m     +  20.2864
     42   [36m0.2515[0m   [32m0.7555[0m        [35m0.2425[0m       0.9166        [94m0.2430[0m     +  20.3767
     43   [36m0.2549[0m   [32m0.7575[0m        [35m0.2413[0m       0.9166        [94m0.2425[0m     +  20.7828
     44   [36m0.2559[0m   [32m0.7586[0m        [35m0.2407[0m       0.9178        [94m0.2421[0m     +  20.0970
     45   [36m0.2587[0m   [32m0.7602[0m        [35m0.2406[0m       0.9178        [94m0.2417[0m     +  20.5384
     46   [36m0.2595[0m   [32m0.7611[0m        [35m0.2393[0m       0.9178        [94m0.2413[0m     +  20.2855
     47   [36m0.2613[0m   [32m0.7630[0m        [35m0.2391[0m       0.9178        [94m0.2409[0m     +  20.4625
     48   [36m0.2626[0m   [32m0.7644[0m        0.2391       0.9178        [94m0.2405[0m     +  20.7714
     49   [36m0.2647[0m   [32m0.7658[0m        [35m0.2390[0m       0.9178        [94m0.2402[0m     +  20.4077
     50   [36m0.2656[0m   [32m0.7665[0m        [35m0.2375[0m       0.9178        [94m0.2399[0m     +  20.3332
[32m[I 2023-05-03 23:32:41,226][0m Trial 385 finished with value: 0.2398675502319578 and parameters: {'lr': 1.0209744215814438e-06, 'dropout': 0.2948022218034038, 'd_model_multiplier': 4, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 354, 'batch_size': 75, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 61}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 51
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
Warning, assumed runtime error: CUDA out of memory. Tried to allocate 1.63 GiB (GPU 0; 23.70 GiB total capacity; 21.83 GiB already allocated; 75.25 MiB free; 22.53 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[32m[I 2023-05-03 23:32:45,365][0m Trial 386 finished with value: 100.0 and parameters: {'lr': 8.904671894258844e-06, 'dropout': 0.28729267199228253, 'd_model_multiplier': 32, 'num_layers': 5, 'n_heads': 64, 'dim_feedforward': 363, 'batch_size': 69, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 51}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 81
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0754[0m   [32m0.3557[0m        [35m0.6882[0m       [31m0.9154[0m        [94m0.6259[0m     +  27.1862
      2   0.0638   0.3038        [35m0.5476[0m       [31m0.9166[0m        [94m0.4542[0m     +  27.6492
      3   0.0626   0.3002        [35m0.4203[0m       0.9166        [94m0.3582[0m     +  27.6779
      4   0.0688   [32m0.3864[0m        [35m0.3456[0m       0.9166        [94m0.3145[0m     +  27.7439
      5   [36m0.0930[0m   [32m0.5141[0m        [35m0.3049[0m       0.9166        [94m0.2953[0m     +  27.4543
      6   [36m0.1211[0m   [32m0.5852[0m        [35m0.2837[0m       0.9166        [94m0.2861[0m     +  28.2033
      7   [36m0.1446[0m   [32m0.6253[0m        [35m0.2731[0m       0.9166        [94m0.2813[0m     +  27.5456
      8   [36m0.1614[0m   [32m0.6497[0m        [35m0.2663[0m       0.9166        [94m0.2787[0m     +  27.5918
      9   [36m0.1758[0m   [32m0.6681[0m        [35m0.2604[0m       0.9166        [94m0.2764[0m     +  28.0252
     10   [36m0.1824[0m   [32m0.6820[0m        [35m0.2576[0m       0.9166        [94m0.2750[0m     +  27.7136
     11   [36m0.1897[0m   [32m0.6929[0m        [35m0.2559[0m       0.9166        [94m0.2737[0m     +  28.1803
     12   [36m0.1937[0m   [32m0.7024[0m        [35m0.2540[0m       0.9166        [94m0.2728[0m     +  27.7961
     13   [36m0.2015[0m   [32m0.7113[0m        [35m0.2536[0m       0.9166        [94m0.2722[0m     +  27.9424
     14   [36m0.2064[0m   [32m0.7196[0m        [35m0.2528[0m       0.9166        [94m0.2713[0m     +  28.1744
     15   [36m0.2109[0m   [32m0.7273[0m        [35m0.2527[0m       0.9166        [94m0.2701[0m     +  27.6667
     16   [36m0.2208[0m   [32m0.7329[0m        [35m0.2509[0m       0.9166        [94m0.2699[0m     +  27.9256
     17   [36m0.2279[0m   [32m0.7389[0m        [35m0.2507[0m       0.9166        [94m0.2691[0m     +  27.7406
     18   [36m0.2354[0m   [32m0.7456[0m        0.2512       0.9166        [94m0.2678[0m     +  28.0385
     19   [36m0.2434[0m   [32m0.7508[0m        0.2511       0.9166        [94m0.2661[0m     +  27.7720
     20   [36m0.2646[0m   [32m0.7573[0m        [35m0.2497[0m       0.9166        [94m0.2647[0m     +  28.1910
     21   [36m0.2792[0m   [32m0.7643[0m        [35m0.2480[0m       0.9166        [94m0.2629[0m     +  28.5644
     22   [36m0.2834[0m   [32m0.7692[0m        [35m0.2465[0m       0.9166        [94m0.2602[0m     +  27.5339
     23   [36m0.3059[0m   [32m0.7785[0m        [35m0.2464[0m       0.9166        [94m0.2560[0m     +  27.9448
     24   [36m0.3265[0m   [32m0.7899[0m        [35m0.2445[0m       [31m0.9178[0m        [94m0.2493[0m     +  27.7361
     25   [36m0.3549[0m   [32m0.8025[0m        [35m0.2441[0m       [31m0.9202[0m        [94m0.2424[0m     +  28.1147
     26   [36m0.3716[0m   [32m0.8084[0m        [35m0.2431[0m       [31m0.9214[0m        [94m0.2366[0m     +  27.7685
     27   [36m0.3760[0m   [32m0.8117[0m        [35m0.2422[0m       0.9202        [94m0.2346[0m     +  28.1882
     28   [36m0.3787[0m   [32m0.8125[0m        [35m0.2398[0m       0.9202        [94m0.2335[0m     +  28.2418
     29   [36m0.3855[0m   [32m0.8143[0m        [35m0.2387[0m       0.9190        0.2337        28.2032
     30   [36m0.3966[0m   [32m0.8153[0m        [35m0.2351[0m       0.9178        0.2336        27.7985
     31   0.3879   [32m0.8164[0m        0.2370       0.9141        0.2348        27.9886
     32   0.3921   [32m0.8176[0m        0.2356       0.9141        0.2351        28.0467
     33   [36m0.4000[0m   [32m0.8189[0m        0.2361       0.9141        0.2349        28.4136
     34   0.3874   0.8189        [35m0.2332[0m       0.9141        0.2358        27.6114
     35   0.3947   [32m0.8202[0m        0.2343       0.9129        0.2356        27.7051
     36   [36m0.4034[0m   [32m0.8215[0m        0.2345       0.9166        0.2349        28.3572
     37   0.3999   [32m0.8217[0m        0.2337       0.9117        0.2357        27.9054
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 23:50:28,434][0m Trial 387 finished with value: 0.23350674787844827 and parameters: {'lr': 1.5388373932090955e-05, 'dropout': 0.6447215562046248, 'd_model_multiplier': 4, 'num_layers': 6, 'n_heads': 32, 'dim_feedforward': 346, 'batch_size': 87, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 81}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 70
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1644[0m   [32m0.6079[0m        [35m0.3438[0m       [31m0.9166[0m        [94m0.2820[0m     +  30.9273
      2   [36m0.2418[0m   [32m0.6812[0m        [35m0.2449[0m       [31m0.9190[0m        [94m0.2703[0m     +  30.9617
      3   [36m0.2519[0m   [32m0.7038[0m        [35m0.2331[0m       0.9190        [94m0.2692[0m     +  31.0677
      4   [36m0.2666[0m   [32m0.7193[0m        [35m0.2310[0m       0.9190        [94m0.2663[0m     +  30.8435
      5   0.2606   [32m0.7270[0m        [35m0.2289[0m       0.9166        [94m0.2659[0m     +  30.9813
      6   0.2651   [32m0.7293[0m        [35m0.2262[0m       0.9166        0.2674        30.8961
      7   0.2636   [32m0.7357[0m        0.2262       0.9166        [94m0.2655[0m     +  30.8928
      8   0.2623   [32m0.7400[0m        [35m0.2256[0m       0.9166        0.2659        30.7967
      9   0.2598   0.7366        [35m0.2252[0m       0.9166        0.2672        30.7842
     10   0.2589   [32m0.7417[0m        [35m0.2236[0m       0.9178        0.2665        31.0279
     11   0.2587   [32m0.7435[0m        [35m0.2228[0m       0.9178        0.2675        31.2666
     12   0.2550   [32m0.7440[0m        [35m0.2224[0m       0.9166        0.2686        30.9032
     13   0.2530   0.7438        [35m0.2219[0m       0.9154        0.2693        31.1222
     14   0.2550   [32m0.7468[0m        [35m0.2206[0m       0.9154        0.2701        31.2626
     15   0.2533   [32m0.7507[0m        0.2207       0.9166        0.2678        31.3631
     16   0.2487   0.7483        [35m0.2184[0m       0.9166        0.2706        30.9341
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-03 23:59:16,880][0m Trial 388 finished with value: 0.26549586355506005 and parameters: {'lr': 2.214563030130047e-05, 'dropout': 0.4115329357883834, 'd_model_multiplier': 4, 'num_layers': 7, 'n_heads': 32, 'dim_feedforward': 332, 'batch_size': 61, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0, 'top_n_features': 70}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 55
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp       dur
-------  -------  -------  ------------  -----------  ------------  ----  --------
      1   [36m0.2414[0m   [32m0.7828[0m        [35m0.3045[0m       [31m0.9008[0m        [94m0.2602[0m     +  101.8608
      2   [36m0.2825[0m   [32m0.7932[0m        [35m0.2433[0m       [31m0.9057[0m        0.2717        102.5203
      3   [36m0.2958[0m   [32m0.8022[0m        [35m0.2431[0m       [31m0.9166[0m        [94m0.2473[0m     +  101.6398
      4   [36m0.3091[0m   [32m0.8134[0m        [35m0.2402[0m       [31m0.9178[0m        0.2545        101.9344
      5   0.2879   [32m0.8266[0m        [35m0.2399[0m       0.9154        [94m0.2436[0m     +  101.9921
      6   0.2981   0.8252        [35m0.2346[0m       0.9178        [94m0.2318[0m     +  101.8757
      7   [36m0.3151[0m   [32m0.8344[0m        0.2358       [31m0.9238[0m        0.2404        101.9608
      8   0.2853   0.8245        0.2375       0.8972        0.2806        101.8161
      9   0.3002   0.8294        0.2407       0.8972        0.2695        101.8200
     10   0.3052   0.8299        0.2350       0.8948        0.2948        102.5569
     11   0.3060   0.8331        0.2415       0.9069        0.2639        102.2951
     12   0.2884   0.8340        0.2352       0.9190        0.2356        102.0711
     13   0.2816   0.8331        0.2359       0.9081        0.2550        102.1764
     14   0.2913   0.8307        0.2397       0.9129        0.2352        102.0280
     15   0.2771   0.8261        0.2378       0.9141        [94m0.2310[0m     +  102.0479
     16   0.2891   [32m0.8355[0m        0.2371       0.8996        0.2584        102.0450
     17   0.2615   0.8159        0.2365       0.9045        0.2570        102.0736
     18   0.2926   0.8092        0.2451       0.8960        0.3031        102.0236
     19   0.2929   0.7860        0.2402       0.9214        0.2363        102.0240
     20   0.2794   0.8143        0.2357       0.8972        0.2829        101.7182
     21   0.2991   0.8116        0.2389       0.9166        0.2564        101.8949
     22   0.2971   0.8060        [35m0.2332[0m       0.9093        0.2578        101.9552
     23   0.2768   0.8091        0.2366       0.8851        0.3335        101.6718
     24   0.2696   0.8006        0.2382       0.9166        0.2316        101.4562
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 00:41:58,765][0m Trial 389 finished with value: 0.2309726057382708 and parameters: {'lr': 9.046198853639571e-05, 'dropout': 0.40170619637095895, 'd_model_multiplier': 64, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 329, 'batch_size': 78, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 55}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 63
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.3071[0m   [32m0.7497[0m        [35m0.3477[0m       [31m0.9214[0m        [94m0.2400[0m     +  21.6436
      2   [36m0.3890[0m   [32m0.7868[0m        [35m0.2417[0m       [31m0.9311[0m        [94m0.2259[0m     +  21.4819
      3   [36m0.3936[0m   [32m0.7960[0m        [35m0.2358[0m       0.9274        [94m0.2241[0m     +  21.6269
      4   [36m0.3939[0m   [32m0.8030[0m        [35m0.2323[0m       0.9262        [94m0.2233[0m     +  21.2864
      5   0.3883   [32m0.8057[0m        [35m0.2298[0m       0.9250        0.2235        21.0280
      6   0.3866   [32m0.8101[0m        [35m0.2269[0m       0.9274        0.2235        21.0661
      7   0.3867   [32m0.8139[0m        [35m0.2263[0m       0.9238        [94m0.2225[0m     +  21.3423
      8   0.3894   [32m0.8157[0m        [35m0.2244[0m       0.9262        0.2225        21.0908
      9   0.3831   [32m0.8180[0m        [35m0.2217[0m       0.9238        0.2226        21.2125
     10   0.3821   [32m0.8188[0m        [35m0.2211[0m       0.9226        [94m0.2218[0m     +  21.0895
     11   0.3790   [32m0.8212[0m        [35m0.2200[0m       0.9214        0.2221        21.1143
     12   0.3665   [32m0.8212[0m        [35m0.2174[0m       0.9214        0.2234        21.2120
     13   0.3760   [32m0.8221[0m        [35m0.2171[0m       0.9226        0.2228        21.3092
     14   0.3709   [32m0.8240[0m        [35m0.2158[0m       0.9202        0.2232        20.8422
     15   0.3716   0.8232        [35m0.2138[0m       0.9238        0.2224        21.0903
     16   0.3722   0.8235        0.2142       0.9226        0.2227        21.0198
     17   0.3574   0.8235        [35m0.2113[0m       0.9214        0.2232        21.0892
     18   0.3688   0.8226        [35m0.2109[0m       0.9214        0.2232        21.1468
     19   0.3693   [32m0.8247[0m        [35m0.2105[0m       0.9226        0.2225        21.1349
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 00:49:03,413][0m Trial 390 finished with value: 0.22182342552432413 and parameters: {'lr': 4.315808071204189e-05, 'dropout': 0.27709756905920857, 'd_model_multiplier': 4, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 310, 'batch_size': 113, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.001, 'top_n_features': 63}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 93
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0684[0m   [32m0.4317[0m        [35m0.5724[0m       [31m0.9335[0m        [94m0.4087[0m     +  41.7613
      2   0.0678   [32m0.4489[0m        [35m0.3810[0m       0.9335        [94m0.2837[0m     +  41.8154
      3   [36m0.0757[0m   [32m0.5336[0m        [35m0.3005[0m       0.9335        [94m0.2522[0m     +  41.9142
      4   [36m0.0997[0m   [32m0.5928[0m        [35m0.2735[0m       0.9335        [94m0.2437[0m     +  41.9801
      5   [36m0.1145[0m   [32m0.6233[0m        [35m0.2605[0m       0.9335        [94m0.2403[0m     +  41.9099
      6   [36m0.1277[0m   [32m0.6423[0m        [35m0.2525[0m       0.9335        [94m0.2381[0m     +  41.8997
      7   [36m0.1516[0m   [32m0.6553[0m        [35m0.2488[0m       0.9335        [94m0.2363[0m     +  41.8942
      8   [36m0.1715[0m   [32m0.6679[0m        [35m0.2447[0m       0.9335        [94m0.2343[0m     +  41.8978
      9   [36m0.1889[0m   [32m0.6767[0m        [35m0.2439[0m       0.9335        [94m0.2325[0m     +  41.8945
     10   [36m0.1954[0m   [32m0.6849[0m        [35m0.2404[0m       0.9335        [94m0.2315[0m     +  41.8770
     11   [36m0.2022[0m   [32m0.6910[0m        [35m0.2382[0m       0.9323        0.2316        41.7177
     12   0.2021   [32m0.6944[0m        [35m0.2374[0m       0.9323        0.2323        41.7829
     13   0.1985   [32m0.6964[0m        [35m0.2362[0m       0.9311        0.2331        41.8650
     14   [36m0.2024[0m   [32m0.6994[0m        [35m0.2350[0m       0.9323        0.2340        41.8349
     15   [36m0.2032[0m   [32m0.7014[0m        [35m0.2350[0m       0.9262        0.2347        41.8157
     16   0.1980   [32m0.7034[0m        [35m0.2342[0m       0.9262        0.2357        41.9186
     17   0.1937   [32m0.7048[0m        [35m0.2339[0m       0.9262        0.2363        41.8386
     18   0.1907   [32m0.7069[0m        [35m0.2337[0m       0.9274        0.2367        41.9431
     19   0.1907   [32m0.7081[0m        0.2337       0.9287        0.2373        41.8726
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 01:03:03,117][0m Trial 391 finished with value: 0.23147523124334024 and parameters: {'lr': 2.4584451980423367e-06, 'dropout': 0.4356839891468732, 'd_model_multiplier': 4, 'num_layers': 5, 'n_heads': 64, 'dim_feedforward': 381, 'batch_size': 22, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 93}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 74
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0669[0m   [32m0.3741[0m        [35m0.7082[0m       [31m0.7521[0m        [94m0.6589[0m     +  19.7916
      2   0.0653   0.3191        [35m0.6259[0m       [31m0.9190[0m        [94m0.5626[0m     +  20.1517
      3   0.0655   0.3079        [35m0.5316[0m       0.9190        [94m0.4719[0m     +  19.8763
      4   0.0665   0.3159        [35m0.4500[0m       0.9190        [94m0.3990[0m     +  20.0789
      5   [36m0.0692[0m   0.3605        [35m0.3833[0m       0.9190        [94m0.3467[0m     +  20.1817
      6   [36m0.0841[0m   [32m0.4464[0m        [35m0.3361[0m       0.9190        [94m0.3131[0m     +  20.0429
      7   [36m0.1213[0m   [32m0.5202[0m        [35m0.3073[0m       0.9190        [94m0.2932[0m     +  20.1979
      8   [36m0.1525[0m   [32m0.5739[0m        [35m0.2885[0m       0.9190        [94m0.2809[0m     +  20.0055
      9   [36m0.1805[0m   [32m0.6116[0m        [35m0.2761[0m       0.9190        [94m0.2727[0m     +  19.9641
     10   [36m0.2083[0m   [32m0.6416[0m        [35m0.2682[0m       0.9190        [94m0.2669[0m     +  19.8605
     11   [36m0.2332[0m   [32m0.6647[0m        [35m0.2615[0m       0.9190        [94m0.2627[0m     +  19.6679
     12   [36m0.2481[0m   [32m0.6805[0m        [35m0.2579[0m       0.9190        [94m0.2594[0m     +  19.8344
     13   [36m0.2630[0m   [32m0.6949[0m        [35m0.2542[0m       0.9190        [94m0.2566[0m     +  20.0372
     14   [36m0.2631[0m   [32m0.7041[0m        [35m0.2524[0m       0.9190        [94m0.2545[0m     +  20.1509
     15   [36m0.2739[0m   [32m0.7143[0m        [35m0.2479[0m       0.9190        [94m0.2523[0m     +  20.3894
     16   [36m0.2789[0m   [32m0.7232[0m        [35m0.2459[0m       0.9178        [94m0.2505[0m     +  20.0117
     17   [36m0.2830[0m   [32m0.7281[0m        [35m0.2443[0m       0.9178        [94m0.2491[0m     +  19.9854
     18   [36m0.2850[0m   [32m0.7325[0m        [35m0.2426[0m       0.9190        [94m0.2479[0m     +  20.1320
     19   [36m0.2886[0m   [32m0.7370[0m        [35m0.2411[0m       0.9178        [94m0.2469[0m     +  20.1404
     20   [36m0.2897[0m   [32m0.7411[0m        [35m0.2389[0m       [31m0.9202[0m        [94m0.2462[0m     +  20.3221
     21   [36m0.2913[0m   [32m0.7441[0m        [35m0.2381[0m       0.9202        [94m0.2458[0m     +  20.0294
     22   [36m0.2922[0m   [32m0.7460[0m        [35m0.2362[0m       0.9202        [94m0.2454[0m     +  20.2447
     23   0.2915   [32m0.7474[0m        [35m0.2356[0m       0.9202        0.2454        20.0579
     24   0.2909   [32m0.7485[0m        [35m0.2342[0m       0.9202        0.2456        20.3545
     25   0.2904   [32m0.7496[0m        [35m0.2339[0m       0.9202        0.2456        20.0971
     26   [36m0.2987[0m   [32m0.7504[0m        [35m0.2338[0m       0.9202        0.2455        20.0632
     27   0.2900   [32m0.7505[0m        [35m0.2333[0m       [31m0.9214[0m        0.2455        20.0577
     28   0.2974   [32m0.7512[0m        [35m0.2325[0m       0.9214        0.2458        20.1230
     29   0.2967   [32m0.7522[0m        [35m0.2322[0m       0.9214        0.2455        19.8779
     30   0.2975   [32m0.7530[0m        [35m0.2320[0m       0.9214        0.2457        19.9439
     31   0.2975   [32m0.7538[0m        [35m0.2314[0m       0.9202        0.2457        19.9396
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 01:13:46,995][0m Trial 392 finished with value: 0.2453758924244537 and parameters: {'lr': 3.406806682220793e-06, 'dropout': 0.30932169224878875, 'd_model_multiplier': 4, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 321, 'batch_size': 57, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 74}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 57
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2136[0m   [32m0.7023[0m        [35m0.3112[0m       [31m0.9299[0m        [94m0.2271[0m     +  27.0603
      2   [36m0.2646[0m   [32m0.7485[0m        [35m0.2406[0m       0.9287        [94m0.2168[0m     +  27.1447
      3   [36m0.3008[0m   [32m0.7616[0m        [35m0.2346[0m       [31m0.9347[0m        [94m0.2138[0m     +  27.1578
      4   [36m0.3060[0m   [32m0.7712[0m        [35m0.2313[0m       0.9347        [94m0.2123[0m     +  27.2073
      5   [36m0.3186[0m   [32m0.7754[0m        [35m0.2307[0m       0.9311        [94m0.2119[0m     +  27.1847
      6   0.3135   [32m0.7825[0m        [35m0.2285[0m       0.9323        [94m0.2114[0m     +  27.4066
      7   0.3085   [32m0.7844[0m        0.2301       0.9323        [94m0.2113[0m     +  27.2612
      8   0.3077   [32m0.7873[0m        [35m0.2265[0m       0.9323        [94m0.2112[0m     +  27.2807
      9   0.3178   [32m0.7915[0m        0.2270       0.9311        [94m0.2101[0m     +  27.1807
     10   0.3010   [32m0.7924[0m        0.2267       0.9311        0.2105        27.5885
     11   0.3083   [32m0.7941[0m        [35m0.2260[0m       0.9299        0.2103        27.4995
     12   0.3091   [32m0.7987[0m        [35m0.2245[0m       0.9299        [94m0.2093[0m     +  27.6246
     13   0.3122   [32m0.8015[0m        [35m0.2232[0m       0.9323        [94m0.2084[0m     +  27.2186
     14   0.3072   [32m0.8030[0m        [35m0.2225[0m       0.9311        0.2090        27.3511
     15   [36m0.3263[0m   [32m0.8037[0m        [35m0.2217[0m       0.9311        [94m0.2078[0m     +  27.3127
     16   [36m0.3282[0m   [32m0.8062[0m        [35m0.2202[0m       0.9311        [94m0.2075[0m     +  27.3622
     17   0.3258   [32m0.8082[0m        [35m0.2192[0m       0.9299        0.2086        27.4703
     18   0.3258   [32m0.8101[0m        [35m0.2173[0m       0.9335        [94m0.2075[0m     +  27.2275
     19   0.3260   [32m0.8119[0m        0.2182       0.9311        0.2075        27.3156
     20   [36m0.3288[0m   [32m0.8119[0m        [35m0.2166[0m       0.9323        [94m0.2067[0m     +  27.5451
     21   0.3285   0.8109        0.2166       0.9323        [94m0.2059[0m     +  27.1884
     22   [36m0.3301[0m   [32m0.8148[0m        0.2170       0.9323        [94m0.2057[0m     +  27.3540
     23   0.3201   [32m0.8162[0m        [35m0.2165[0m       0.9323        0.2059        27.3265
     24   0.3291   [32m0.8175[0m        [35m0.2142[0m       0.9347        [94m0.2039[0m     +  27.3622
     25   0.3252   0.8152        0.2143       0.9311        0.2047        27.1334
     26   [36m0.3336[0m   0.8158        0.2151       0.9335        0.2047        27.1565
     27   [36m0.3352[0m   0.8139        [35m0.2130[0m       0.9347        0.2039        27.5346
     28   [36m0.3397[0m   0.8120        [35m0.2122[0m       0.9347        [94m0.2039[0m     +  27.2602
     29   0.3358   0.8127        [35m0.2114[0m       0.9335        0.2046        27.3771
     30   0.3267   0.8118        [35m0.2096[0m       0.9335        0.2057        27.4553
     31   0.3337   0.8127        0.2098       0.9335        0.2044        27.3260
     32   0.3274   0.8126        0.2106       0.9335        0.2048        27.4513
     33   0.3365   0.8114        [35m0.2093[0m       0.9335        0.2050        27.3383
     34   0.3379   0.8152        [35m0.2077[0m       0.9323        [94m0.2028[0m     +  27.4070
     35   0.3348   0.8117        0.2080       0.9335        0.2045        27.3818
     36   0.3381   0.8121        [35m0.2062[0m       [31m0.9359[0m        [94m0.2027[0m     +  27.2949
     37   [36m0.3585[0m   0.8149        0.2066       0.9359        [94m0.2014[0m     +  27.3894
     38   0.3380   0.8103        [35m0.2038[0m       0.9347        0.2043        27.4261
     39   0.3540   0.8098        0.2074       [31m0.9383[0m        0.2027        27.3218
     40   0.3551   0.8065        0.2043       0.9359        0.2034        27.3301
     41   0.3375   0.8061        0.2062       0.9359        0.2038        27.6906
     42   0.3413   0.8113        [35m0.2035[0m       0.9335        0.2038        27.1614
     43   0.3386   0.8093        [35m0.2034[0m       0.9335        0.2046        27.0362
     44   0.3374   0.8074        0.2037       0.9335        0.2044        27.1330
     45   0.3396   0.8053        [35m0.2013[0m       0.9347        0.2052        27.2340
     46   0.3361   0.8010        0.2026       0.9335        0.2065        27.2966
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 01:35:14,029][0m Trial 393 finished with value: 0.2013731892552889 and parameters: {'lr': 2.6483737039536313e-05, 'dropout': 0.3562865553324489, 'd_model_multiplier': 4, 'num_layers': 6, 'n_heads': 32, 'dim_feedforward': 340, 'batch_size': 49, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 57}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 57
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0729[0m   [32m0.4142[0m        [35m0.5041[0m       [31m0.9178[0m        [94m0.3306[0m     +  27.0917
      2   [36m0.1621[0m   [32m0.6488[0m        [35m0.2880[0m       0.9178        [94m0.2725[0m     +  27.1948
      3   [36m0.2222[0m   [32m0.6961[0m        [35m0.2498[0m       0.9178        [94m0.2643[0m     +  27.3557
      4   [36m0.2403[0m   [32m0.6996[0m        [35m0.2358[0m       [31m0.9202[0m        0.2658        27.4692
      5   [36m0.2466[0m   [32m0.7084[0m        [35m0.2305[0m       [31m0.9214[0m        0.2652        27.5579
      6   0.2457   [32m0.7181[0m        [35m0.2282[0m       0.9214        0.2643        27.4199
      7   0.2452   [32m0.7246[0m        [35m0.2264[0m       0.9202        0.2648        27.4931
      8   0.2388   [32m0.7313[0m        [35m0.2262[0m       0.9190        0.2647        27.4174
      9   0.2258   0.7312        [35m0.2243[0m       0.9141        0.2679        27.4940
     10   0.2299   [32m0.7399[0m        0.2245       0.9154        0.2656        27.3608
     11   0.2286   [32m0.7422[0m        [35m0.2221[0m       0.9129        0.2678        27.3855
     12   0.2298   [32m0.7459[0m        [35m0.2217[0m       0.9129        0.2662        27.4251
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 01:41:10,601][0m Trial 394 finished with value: 0.2642746649529366 and parameters: {'lr': 2.6275912140884347e-05, 'dropout': 0.36289084033581787, 'd_model_multiplier': 4, 'num_layers': 6, 'n_heads': 32, 'dim_feedforward': 345, 'batch_size': 44, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 57}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 65
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2303[0m   [32m0.7524[0m        [35m0.3069[0m       [31m0.9323[0m        [94m0.2242[0m     +  30.8458
      2   [36m0.2336[0m   [32m0.7635[0m        [35m0.2386[0m       0.9190        0.2304        31.0878
      3   0.2271   [32m0.7699[0m        [35m0.2342[0m       0.9214        0.2258        31.3787
      4   0.2244   [32m0.7773[0m        [35m0.2330[0m       0.9166        0.2277        31.1675
      5   0.2300   [32m0.7809[0m        [35m0.2312[0m       0.9250        [94m0.2221[0m     +  31.0307
      6   [36m0.2352[0m   [32m0.7848[0m        [35m0.2297[0m       0.9250        [94m0.2190[0m     +  30.9545
      7   0.2335   0.7834        [35m0.2288[0m       0.9250        0.2211        31.1834
      8   0.2336   0.7842        [35m0.2279[0m       0.9262        [94m0.2178[0m     +  31.1332
      9   0.2298   0.7767        [35m0.2254[0m       0.9250        0.2211        31.8183
     10   0.2335   0.7772        0.2263       0.9299        0.2195        30.9283
     11   [36m0.2392[0m   0.7822        [35m0.2242[0m       0.9250        0.2185        30.9747
     12   0.2391   0.7826        0.2243       0.9238        0.2198        30.9792
     13   [36m0.2476[0m   0.7838        [35m0.2226[0m       0.9250        [94m0.2157[0m     +  31.0312
     14   [36m0.2531[0m   [32m0.7849[0m        [35m0.2198[0m       0.9287        [94m0.2147[0m     +  31.0319
     15   [36m0.2596[0m   [32m0.7902[0m        [35m0.2196[0m       0.9274        0.2150        31.1159
     16   0.2550   0.7878        [35m0.2164[0m       0.9226        0.2171        31.0617
     17   0.2523   0.7858        0.2167       0.9226        0.2207        31.2420
     18   0.2565   0.7860        0.2183       0.9238        0.2172        31.4442
     19   [36m0.2601[0m   0.7854        0.2170       0.9274        0.2152        31.0225
     20   [36m0.2603[0m   0.7877        [35m0.2151[0m       0.9250        0.2180        31.3736
     21   [36m0.2618[0m   [32m0.7905[0m        0.2151       0.9262        0.2157        31.1808
     22   [36m0.2677[0m   [32m0.7919[0m        [35m0.2143[0m       0.9262        [94m0.2139[0m     +  31.0567
     23   [36m0.2721[0m   [32m0.7932[0m        [35m0.2124[0m       0.9287        0.2145        31.1803
     24   0.2677   [32m0.7935[0m        0.2134       0.9262        0.2177        31.2161
     25   0.2680   0.7911        [35m0.2119[0m       0.9262        0.2176        31.0339
     26   0.2543   0.7823        [35m0.2100[0m       0.9250        0.2221        30.9685
     27   0.2715   0.7917        [35m0.2090[0m       0.9238        0.2169        31.3659
     28   0.2671   0.7868        0.2106       0.9250        0.2200        31.1183
     29   0.2667   0.7910        [35m0.2087[0m       0.9274        0.2168        31.1867
     30   0.2672   0.7880        [35m0.2061[0m       0.9238        0.2197        30.9890
     31   0.2656   0.7894        [35m0.2056[0m       0.9262        0.2186        31.0762
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 01:57:48,126][0m Trial 395 finished with value: 0.21393712628024666 and parameters: {'lr': 3.396281769753168e-05, 'dropout': 0.3386430347790712, 'd_model_multiplier': 4, 'num_layers': 7, 'n_heads': 32, 'dim_feedforward': 342, 'batch_size': 51, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.001, 'top_n_features': 65}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 52
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
Warning, assumed runtime error: CUDA out of memory. Tried to allocate 724.00 MiB (GPU 0; 23.70 GiB total capacity; 20.25 GiB already allocated; 125.25 MiB free; 22.48 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[32m[I 2023-05-04 01:57:52,320][0m Trial 396 finished with value: 100.0 and parameters: {'lr': 4.790816791214357e-05, 'dropout': 0.3882878394531471, 'd_model_multiplier': 64, 'num_layers': 5, 'n_heads': 64, 'dim_feedforward': 353, 'batch_size': 49, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 52}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 59
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0558[0m   [32m0.3300[0m        [35m0.5608[0m       [31m0.9274[0m        [94m0.4113[0m     +  27.2147
      2   [36m0.1898[0m   [32m0.6569[0m        [35m0.3508[0m       0.9274        [94m0.2660[0m     +  27.1294
      3   [36m0.2695[0m   [32m0.7248[0m        [35m0.2722[0m       [31m0.9287[0m        [94m0.2355[0m     +  27.1940
      4   [36m0.2843[0m   [32m0.7432[0m        [35m0.2489[0m       [31m0.9299[0m        [94m0.2290[0m     +  27.2364
      5   0.2784   [32m0.7486[0m        [35m0.2399[0m       0.9299        [94m0.2274[0m     +  27.5015
      6   [36m0.2870[0m   [32m0.7519[0m        [35m0.2360[0m       0.9250        [94m0.2264[0m     +  27.3955
      7   [36m0.2928[0m   [32m0.7552[0m        [35m0.2343[0m       0.9274        [94m0.2255[0m     +  27.3685
      8   [36m0.2952[0m   [32m0.7578[0m        [35m0.2325[0m       0.9287        [94m0.2248[0m     +  27.1575
      9   0.2883   [32m0.7590[0m        [35m0.2311[0m       [31m0.9311[0m        [94m0.2243[0m     +  27.4377
     10   0.2920   [32m0.7617[0m        [35m0.2292[0m       0.9299        0.2244        27.4769
     11   0.2937   [32m0.7631[0m        0.2294       0.9311        [94m0.2241[0m     +  27.2106
     12   0.2876   [32m0.7645[0m        [35m0.2284[0m       0.9262        0.2243        27.2526
     13   0.2877   [32m0.7666[0m        0.2285       0.9250        0.2242        27.5656
     14   0.2865   [32m0.7674[0m        [35m0.2282[0m       0.9262        0.2246        27.4114
     15   0.2883   [32m0.7678[0m        [35m0.2264[0m       0.9274        0.2241        27.8478
     16   0.2880   [32m0.7699[0m        0.2273       0.9262        0.2253        27.2882
     17   0.2852   [32m0.7704[0m        0.2267       0.9274        0.2242        27.1092
     18   0.2832   [32m0.7711[0m        [35m0.2254[0m       0.9262        0.2250        27.3380
     19   0.2821   0.7697        [35m0.2239[0m       0.9238        0.2263        27.2801
     20   0.2844   [32m0.7715[0m        0.2244       0.9262        0.2249        27.3081
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 02:07:27,803][0m Trial 397 finished with value: 0.2240698867203823 and parameters: {'lr': 1.7987612896397404e-05, 'dropout': 0.33080274711446517, 'd_model_multiplier': 4, 'num_layers': 6, 'n_heads': 32, 'dim_feedforward': 339, 'batch_size': 55, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0, 'top_n_features': 59}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 68
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0567[0m   [32m0.3472[0m        [35m0.4677[0m       [31m0.9202[0m        [94m0.3361[0m     +  26.0517
      2   [36m0.1720[0m   [32m0.6196[0m        [35m0.3071[0m       0.9202        [94m0.2737[0m     +  26.5890
      3   [36m0.2552[0m   [32m0.6948[0m        [35m0.2725[0m       0.9202        [94m0.2553[0m     +  26.4169
      4   [36m0.3046[0m   [32m0.7334[0m        [35m0.2607[0m       0.9202        [94m0.2459[0m     +  26.2948
      5   [36m0.3416[0m   [32m0.7665[0m        [35m0.2521[0m       0.9202        [94m0.2375[0m     +  26.4241
      6   [36m0.3535[0m   [32m0.7833[0m        [35m0.2441[0m       0.9190        [94m0.2317[0m     +  26.3405
      7   0.3474   [32m0.7916[0m        [35m0.2399[0m       [31m0.9226[0m        [94m0.2296[0m     +  26.4685
      8   0.3396   [32m0.7938[0m        [35m0.2366[0m       0.9202        [94m0.2296[0m     +  26.3692
      9   0.3308   [32m0.7976[0m        [35m0.2352[0m       0.9214        [94m0.2287[0m     +  26.3321
     10   0.3278   [32m0.7983[0m        [35m0.2333[0m       0.9190        0.2292        26.3114
     11   0.3245   [32m0.8015[0m        [35m0.2325[0m       0.9190        [94m0.2287[0m     +  26.3669
     12   0.3240   [32m0.8033[0m        [35m0.2319[0m       0.9166        0.2289        26.3714
     13   0.3196   [32m0.8047[0m        [35m0.2318[0m       0.9154        0.2291        26.4637
     14   0.3181   [32m0.8048[0m        0.2319       0.9141        0.2295        26.4414
     15   0.3153   0.8043        [35m0.2307[0m       0.9141        0.2300        26.4028
     16   0.3152   [32m0.8049[0m        [35m0.2302[0m       0.9141        0.2303        26.6083
     17   0.3129   0.8044        [35m0.2291[0m       0.9141        0.2308        26.6750
     18   0.3113   0.8045        0.2292       0.9141        0.2309        26.3682
     19   0.3102   [32m0.8049[0m        [35m0.2278[0m       0.9154        0.2311        26.7027
     20   0.3113   [32m0.8052[0m        [35m0.2275[0m       0.9141        0.2310        26.2676
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 02:16:43,660][0m Trial 398 finished with value: 0.22872181603417 and parameters: {'lr': 1.1542560123100077e-05, 'dropout': 0.3554075640206065, 'd_model_multiplier': 2, 'num_layers': 6, 'n_heads': 32, 'dim_feedforward': 334, 'batch_size': 47, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 68}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 55
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2384[0m   [32m0.7210[0m        [35m0.3438[0m       [31m0.9154[0m        [94m0.2684[0m     +  27.5005
      2   [36m0.2967[0m   [32m0.7477[0m        [35m0.2377[0m       [31m0.9178[0m        [94m0.2676[0m     +  27.6715
      3   [36m0.3044[0m   [32m0.7540[0m        [35m0.2337[0m       0.9166        [94m0.2651[0m     +  27.5875
      4   0.3019   [32m0.7580[0m        [35m0.2309[0m       0.9178        [94m0.2639[0m     +  27.6774
      5   [36m0.3064[0m   [32m0.7593[0m        [35m0.2301[0m       0.9166        [94m0.2592[0m     +  27.6867
      6   [36m0.3185[0m   0.7576        [35m0.2281[0m       0.9178        [94m0.2584[0m     +  27.4765
      7   [36m0.3377[0m   0.7509        [35m0.2234[0m       [31m0.9190[0m        [94m0.2566[0m     +  27.5271
      8   0.3346   0.7569        0.2243       0.9166        [94m0.2544[0m     +  27.5898
      9   0.3213   0.7500        [35m0.2229[0m       0.9166        [94m0.2543[0m     +  27.6375
     10   0.3201   0.7545        [35m0.2212[0m       0.9190        0.2543        27.7345
     11   0.3277   0.7383        0.2213       0.9178        [94m0.2532[0m     +  27.7730
     12   0.3275   0.7501        [35m0.2188[0m       0.9190        [94m0.2512[0m     +  27.5315
     13   0.3215   0.7450        [35m0.2174[0m       0.9178        0.2528        27.5538
     14   0.3204   [32m0.7604[0m        [35m0.2163[0m       0.9190        [94m0.2503[0m     +  27.7128
     15   0.3119   0.7507        [35m0.2132[0m       [31m0.9202[0m        0.2531        27.7413
     16   0.3208   0.7571        [35m0.2115[0m       [31m0.9214[0m        0.2518        27.6134
     17   0.3161   0.7480        0.2120       0.9202        0.2507        27.4609
     18   0.3205   [32m0.7667[0m        [35m0.2113[0m       0.9214        [94m0.2482[0m     +  27.7941
     19   0.3067   0.7598        [35m0.2076[0m       0.9214        0.2500        27.6501
     20   0.2989   0.7639        0.2076       0.9190        0.2515        27.6461
     21   0.2960   0.7597        [35m0.2062[0m       0.9178        0.2532        27.5255
     22   0.2923   0.7507        [35m0.2040[0m       0.9178        0.2550        27.6574
     23   0.3049   0.7469        [35m0.2033[0m       0.9214        0.2562        27.7797
     24   0.2855   0.7560        [35m0.2004[0m       0.9202        0.2541        27.5933
     25   0.2929   0.7590        0.2008       0.9202        0.2548        27.7434
     26   0.2772   0.7463        [35m0.1974[0m       0.9166        0.2605        27.7145
     27   0.2810   0.7527        0.1981       0.9166        0.2589        28.3372
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 02:29:40,012][0m Trial 399 finished with value: 0.24816494845439474 and parameters: {'lr': 6.424949898765184e-05, 'dropout': 0.34683575220269525, 'd_model_multiplier': 4, 'num_layers': 6, 'n_heads': 32, 'dim_feedforward': 326, 'batch_size': 27, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 55}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 61
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0554[0m   [32m0.3292[0m        [35m0.6333[0m       [31m0.9226[0m        [94m0.5678[0m     +  27.0620
      2   [36m0.0591[0m   [32m0.3522[0m        [35m0.5163[0m       0.9226        [94m0.4250[0m     +  27.3565
      3   [36m0.0669[0m   [32m0.3768[0m        [35m0.4094[0m       0.9226        [94m0.3454[0m     +  27.3799
      4   [36m0.0728[0m   [32m0.4372[0m        [35m0.3444[0m       0.9226        [94m0.3064[0m     +  27.6908
      5   [36m0.0971[0m   [32m0.5104[0m        [35m0.3048[0m       0.9226        [94m0.2858[0m     +  27.3734
      6   [36m0.1188[0m   [32m0.5657[0m        [35m0.2824[0m       0.9226        [94m0.2749[0m     +  27.4649
      7   [36m0.1404[0m   [32m0.6049[0m        [35m0.2707[0m       0.9226        [94m0.2691[0m     +  27.5148
      8   [36m0.1593[0m   [32m0.6332[0m        [35m0.2619[0m       0.9214        [94m0.2652[0m     +  27.3164
      9   [36m0.1769[0m   [32m0.6559[0m        [35m0.2545[0m       0.9214        [94m0.2627[0m     +  27.3791
     10   [36m0.1866[0m   [32m0.6714[0m        [35m0.2516[0m       0.9226        [94m0.2606[0m     +  27.3207
     11   [36m0.1956[0m   [32m0.6852[0m        [35m0.2470[0m       0.9226        [94m0.2588[0m     +  27.2799
     12   [36m0.2017[0m   [32m0.6949[0m        [35m0.2437[0m       [31m0.9250[0m        [94m0.2577[0m     +  27.3358
     13   [36m0.2080[0m   [32m0.7033[0m        [35m0.2418[0m       0.9226        [94m0.2569[0m     +  27.4451
     14   [36m0.2096[0m   [32m0.7080[0m        [35m0.2390[0m       0.9166        [94m0.2564[0m     +  27.2971
     15   [36m0.2125[0m   [32m0.7114[0m        [35m0.2385[0m       0.9166        [94m0.2561[0m     +  27.6057
     16   [36m0.2167[0m   [32m0.7154[0m        [35m0.2374[0m       0.9141        [94m0.2556[0m     +  27.4402
     17   [36m0.2169[0m   [32m0.7189[0m        [35m0.2362[0m       0.9141        [94m0.2550[0m     +  27.4989
     18   0.2161   [32m0.7217[0m        [35m0.2355[0m       0.9129        [94m0.2547[0m     +  27.4115
     19   [36m0.2181[0m   [32m0.7246[0m        [35m0.2348[0m       0.9117        [94m0.2541[0m     +  27.4603
     20   [36m0.2191[0m   [32m0.7276[0m        [35m0.2339[0m       0.9117        [94m0.2537[0m     +  27.5007
     21   [36m0.2193[0m   [32m0.7300[0m        0.2347       0.9093        [94m0.2534[0m     +  27.5537
     22   [36m0.2202[0m   [32m0.7316[0m        [35m0.2326[0m       0.9081        [94m0.2532[0m     +  27.2511
     23   [36m0.2217[0m   [32m0.7338[0m        [35m0.2318[0m       0.9081        0.2532        27.4560
     24   0.2190   [32m0.7356[0m        0.2323       0.9081        [94m0.2530[0m     +  27.7596
     25   0.2205   [32m0.7376[0m        0.2326       0.9093        0.2532        27.4950
     26   0.2189   [32m0.7386[0m        0.2318       0.9093        [94m0.2529[0m     +  27.4329
     27   0.2206   [32m0.7401[0m        [35m0.2312[0m       0.9117        [94m0.2528[0m     +  27.5223
     28   0.2188   [32m0.7408[0m        [35m0.2298[0m       0.9081        0.2538        27.6275
     29   0.2200   [32m0.7413[0m        [35m0.2294[0m       0.9093        0.2534        27.5478
     30   0.2200   [32m0.7418[0m        [35m0.2292[0m       0.9081        0.2534        27.4578
     31   [36m0.2224[0m   [32m0.7434[0m        [35m0.2287[0m       0.9081        0.2534        27.6347
     32   [36m0.2229[0m   [32m0.7440[0m        0.2305       0.9093        0.2534        27.6602
     33   0.2215   [32m0.7443[0m        0.2294       0.9081        0.2534        27.3683
     34   0.2212   [32m0.7454[0m        [35m0.2277[0m       0.9081        0.2531        27.2095
     35   0.2213   0.7452        [35m0.2277[0m       0.9117        0.2535        27.3182
     36   0.2200   [32m0.7459[0m        0.2287       0.9093        0.2539        27.3269
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 02:46:38,531][0m Trial 400 finished with value: 0.25276195622773384 and parameters: {'lr': 5.759418775944202e-06, 'dropout': 0.492112234050635, 'd_model_multiplier': 4, 'num_layers': 6, 'n_heads': 32, 'dim_feedforward': 366, 'batch_size': 54, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 61}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 83
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp       dur
-------  -------  -------  ------------  -----------  ------------  ----  --------
      1   [36m0.2783[0m   [32m0.7976[0m        [35m0.2898[0m       [31m0.9335[0m        [94m0.2164[0m     +  118.4394
      2   [36m0.3141[0m   0.7959        [35m0.2754[0m       0.9335        [94m0.2139[0m     +  118.5809
      3   0.2968   [32m0.8049[0m        [35m0.2708[0m       0.9335        [94m0.2074[0m     +  118.6006
      4   0.3080   [32m0.8093[0m        [35m0.2662[0m       0.9335        [94m0.2071[0m     +  118.7697
      5   0.3082   0.8047        [35m0.2638[0m       0.9335        0.2109        118.6389
      6   0.3082   0.8075        [35m0.2616[0m       0.9335        [94m0.2068[0m     +  118.6206
      7   0.2983   0.8028        [35m0.2609[0m       0.9335        0.2118        118.5070
      8   0.3050   0.8056        [35m0.2593[0m       0.9335        0.2119        119.0072
      9   0.3108   [32m0.8134[0m        [35m0.2586[0m       0.9335        0.2104        119.0793
     10   0.3020   0.8102        0.2600       0.9335        0.2107        119.0938
     11   0.3048   0.8124        [35m0.2574[0m       0.9335        0.2131        118.5944
     12   0.2959   0.8059        [35m0.2569[0m       0.9335        0.2161        119.0487
     13   0.3027   0.8074        [35m0.2565[0m       0.9335        0.2150        118.6730
     14   0.3089   0.8115        [35m0.2560[0m       0.9335        0.2148        118.7402
     15   0.2869   0.8109        [35m0.2555[0m       0.9335        0.2183        118.6230
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 03:18:20,926][0m Trial 401 finished with value: 0.20683532402351804 and parameters: {'lr': 0.00015309052415420988, 'dropout': 0.37291175050728304, 'd_model_multiplier': 4, 'num_layers': 15, 'n_heads': 64, 'dim_feedforward': 303, 'batch_size': 17, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.001, 'top_n_features': 83}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 78
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp       dur
-------  -------  -------  ------------  -----------  ------------  ----  --------
      1   [36m0.2842[0m   [32m0.7844[0m        [35m0.3022[0m       [31m0.9190[0m        [94m0.2376[0m     +  101.8449
      2   0.2710   [32m0.7878[0m        [35m0.2381[0m       0.9081        0.2484        101.5478
      3   0.2705   0.7869        0.2383       0.9117        0.2565        101.9085
      4   0.2647   0.7855        [35m0.2375[0m       0.9069        0.2636        101.9747
      5   0.2779   0.7869        [35m0.2355[0m       0.9117        0.2648        101.9124
      6   0.2808   [32m0.7893[0m        [35m0.2328[0m       0.9105        0.2672        101.9218
      7   [36m0.3082[0m   [32m0.7931[0m        [35m0.2300[0m       0.9166        0.2561        101.9027
      8   0.2749   0.7855        [35m0.2282[0m       0.9069        0.2725        101.9141
      9   0.2773   0.7863        [35m0.2252[0m       0.9081        0.2746        101.6450
     10   0.3009   [32m0.8019[0m        [35m0.2249[0m       0.9105        0.2618        101.9031
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 03:37:04,419][0m Trial 402 finished with value: 0.23760348677346899 and parameters: {'lr': 2.7737990833386183e-05, 'dropout': 0.3977159441370237, 'd_model_multiplier': 64, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 349, 'batch_size': 43, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 78}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 64
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0584[0m   [32m0.3264[0m        [35m0.6716[0m       [31m0.9250[0m        [94m0.5840[0m     +  27.5513
      2   0.0544   0.3227        [35m0.5153[0m       0.9250        [94m0.4158[0m     +  26.9735
      3   [36m0.0649[0m   [32m0.4431[0m        [35m0.3756[0m       0.9250        [94m0.3110[0m     +  27.3080
      4   [36m0.1809[0m   [32m0.6013[0m        [35m0.3036[0m       0.9250        [94m0.2702[0m     +  27.3818
      5   [36m0.2353[0m   [32m0.6762[0m        [35m0.2748[0m       0.9250        [94m0.2521[0m     +  27.5144
      6   [36m0.2560[0m   [32m0.7192[0m        [35m0.2617[0m       0.9250        [94m0.2419[0m     +  27.6633
      7   [36m0.2744[0m   [32m0.7511[0m        [35m0.2513[0m       [31m0.9299[0m        [94m0.2351[0m     +  27.4788
      8   [36m0.2833[0m   [32m0.7689[0m        [35m0.2454[0m       0.9262        [94m0.2308[0m     +  27.9719
      9   0.2828   [32m0.7775[0m        [35m0.2402[0m       0.9238        [94m0.2284[0m     +  27.3950
     10   [36m0.2865[0m   [32m0.7848[0m        [35m0.2382[0m       0.9250        [94m0.2266[0m     +  27.7586
     11   0.2816   [32m0.7914[0m        [35m0.2362[0m       0.9238        [94m0.2254[0m     +  27.6822
     12   0.2856   [32m0.7952[0m        [35m0.2343[0m       0.9262        [94m0.2241[0m     +  27.3361
     13   [36m0.2899[0m   [32m0.7980[0m        [35m0.2334[0m       0.9250        [94m0.2231[0m     +  27.3850
     14   [36m0.2939[0m   [32m0.8007[0m        0.2335       0.9226        [94m0.2223[0m     +  27.2270
     15   [36m0.2943[0m   [32m0.8022[0m        [35m0.2319[0m       0.9238        [94m0.2219[0m     +  27.3555
     16   [36m0.2956[0m   [32m0.8035[0m        [35m0.2309[0m       0.9226        [94m0.2213[0m     +  27.3636
     17   [36m0.2997[0m   [32m0.8049[0m        [35m0.2303[0m       0.9214        [94m0.2210[0m     +  27.5461
     18   0.2964   [32m0.8054[0m        0.2308       0.9202        [94m0.2206[0m     +  27.4943
     19   0.2993   [32m0.8059[0m        [35m0.2287[0m       0.9214        [94m0.2204[0m     +  27.2646
     20   0.2993   [32m0.8067[0m        0.2305       0.9226        [94m0.2200[0m     +  27.7127
     21   0.2972   0.8067        [35m0.2285[0m       0.9226        [94m0.2198[0m     +  27.2785
     22   0.2985   0.8063        [35m0.2282[0m       0.9226        [94m0.2197[0m     +  27.3729
     23   [36m0.2998[0m   0.8065        0.2285       0.9238        [94m0.2195[0m     +  27.5352
     24   [36m0.3011[0m   0.8065        [35m0.2271[0m       0.9214        0.2197        27.2223
     25   [36m0.3035[0m   [32m0.8071[0m        0.2271       0.9226        [94m0.2194[0m     +  27.3701
     26   0.3020   0.8063        [35m0.2264[0m       0.9214        0.2197        27.4834
     27   [36m0.3073[0m   0.8070        [35m0.2259[0m       0.9214        0.2194        27.5091
     28   [36m0.3106[0m   [32m0.8071[0m        0.2262       0.9202        [94m0.2194[0m     +  27.2248
     29   0.3092   0.8071        [35m0.2244[0m       0.9202        [94m0.2192[0m     +  27.4394
     30   0.3078   0.8069        0.2260       0.9202        0.2193        27.2261
     31   0.3073   0.8065        0.2252       0.9214        0.2195        27.4487
     32   0.3098   0.8067        [35m0.2242[0m       0.9190        0.2194        27.3852
     33   0.3095   0.8071        [35m0.2230[0m       0.9190        0.2194        27.3983
     34   0.3074   0.8067        [35m0.2225[0m       0.9190        0.2194        27.4471
     35   0.3094   0.8064        0.2227       0.9190        0.2193        27.3732
     36   0.3059   0.8067        0.2245       0.9190        0.2192        27.2151
     37   0.3079   0.8068        0.2227       0.9190        0.2192        27.4352
     38   0.3063   0.8068        [35m0.2214[0m       0.9190        0.2193        27.1772
     39   0.3061   0.8068        0.2226       0.9190        [94m0.2191[0m     +  27.3203
     40   0.3062   [32m0.8072[0m        [35m0.2210[0m       0.9190        0.2192        27.5144
     41   0.3067   [32m0.8073[0m        0.2218       0.9190        0.2195        27.5532
     42   0.3073   0.8073        0.2212       0.9190        0.2194        27.3676
     43   0.3051   [32m0.8080[0m        0.2210       0.9190        0.2193        27.4233
     44   0.3047   [32m0.8083[0m        0.2211       0.9190        [94m0.2189[0m     +  27.1399
     45   0.3048   [32m0.8083[0m        [35m0.2203[0m       0.9202        0.2190        27.2123
     46   0.3048   0.8083        [35m0.2197[0m       0.9214        0.2191        26.9853
     47   0.3038   [32m0.8086[0m        0.2200       0.9202        0.2195        27.4178
     48   0.3015   0.8075        [35m0.2195[0m       0.9214        0.2194        27.3281
     49   0.3020   0.8075        0.2206       0.9202        0.2195        27.3490
     50   0.3035   0.8075        0.2197       0.9226        0.2194        27.3191
[32m[I 2023-05-04 03:59:57,635][0m Trial 403 finished with value: 0.21893260397350342 and parameters: {'lr': 7.929537192841937e-06, 'dropout': 0.31630406543814954, 'd_model_multiplier': 4, 'num_layers': 6, 'n_heads': 32, 'dim_feedforward': 356, 'batch_size': 61, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 64}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 57
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
Warning, assumed runtime error: CUDA out of memory. Tried to allocate 1.85 GiB (GPU 0; 23.70 GiB total capacity; 22.04 GiB already allocated; 329.25 MiB free; 22.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[32m[I 2023-05-04 04:00:03,498][0m Trial 404 finished with value: 100.0 and parameters: {'lr': 3.607291329060511e-05, 'dropout': 0.42121842324186937, 'd_model_multiplier': 2, 'num_layers': 7, 'n_heads': 32, 'dim_feedforward': 331, 'batch_size': 156, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 57}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 72
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2256[0m   [32m0.7900[0m        [35m0.2799[0m       [31m0.8984[0m        [94m0.3044[0m     +  45.9018
      2   [36m0.2350[0m   0.7854        [35m0.2567[0m       0.8948        [94m0.2952[0m     +  46.2955
      3   [36m0.2534[0m   0.7863        [35m0.2499[0m       [31m0.9008[0m        [94m0.2768[0m     +  46.0889
      4   0.2415   0.7819        [35m0.2464[0m       0.8996        [94m0.2690[0m     +  46.1965
      5   [36m0.3093[0m   0.7827        [35m0.2443[0m       [31m0.9154[0m        [94m0.2300[0m     +  46.1728
      6   0.2482   0.7789        [35m0.2435[0m       0.9021        0.2650        46.3877
      7   0.2668   0.7890        [35m0.2401[0m       0.9057        0.2483        46.2964
      8   0.2979   [32m0.7922[0m        [35m0.2366[0m       0.9033        0.2512        46.2851
      9   0.2496   0.7891        [35m0.2351[0m       0.9093        0.2489        46.2651
     10   0.2687   [32m0.7942[0m        0.2426       0.9129        0.2392        46.3449
     11   [36m0.3137[0m   [32m0.8106[0m        0.2352       0.9093        0.2476        46.4994
     12   [36m0.3386[0m   [32m0.8163[0m        [35m0.2332[0m       [31m0.9214[0m        [94m0.2193[0m     +  46.2686
     13   0.3278   0.7926        [35m0.2328[0m       [31m0.9274[0m        [94m0.2183[0m     +  46.6025
     14   0.3231   0.7976        0.2335       [31m0.9311[0m        0.2277        46.4033
     15   0.2941   0.7941        [35m0.2309[0m       0.9154        0.2318        46.6193
     16   0.2824   0.7944        0.2324       0.9299        0.2305        46.6847
     17   0.3143   0.7934        [35m0.2273[0m       0.9226        0.2249        46.5269
     18   0.3156   0.7947        0.2337       0.9250        0.2266        46.3173
     19   0.2921   0.7861        0.2285       0.9238        0.2254        46.6017
     20   0.3021   0.7977        0.2285       0.9238        0.2292        46.7377
     21   0.2965   0.7977        0.2275       0.9299        0.2506        47.2692
     22   0.2975   0.7951        [35m0.2246[0m       0.9238        0.2402        46.5845
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 04:17:56,968][0m Trial 405 finished with value: 0.2182545665529936 and parameters: {'lr': 0.00011406010040180565, 'dropout': 0.5075593138485005, 'd_model_multiplier': 32, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 370, 'batch_size': 48, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0, 'top_n_features': 72}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 50
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
Warning, assumed runtime error: CUDA out of memory. Tried to allocate 1.33 GiB (GPU 0; 23.70 GiB total capacity; 19.89 GiB already allocated; 1.05 GiB free; 21.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[32m[I 2023-05-04 04:18:00,427][0m Trial 406 finished with value: 100.0 and parameters: {'lr': 2.0860770689400546e-06, 'dropout': 0.16268291278472316, 'd_model_multiplier': 16, 'num_layers': 10, 'n_heads': 64, 'dim_feedforward': 343, 'batch_size': 56, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 50}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 60
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0756[0m   [32m0.4762[0m        [35m0.6822[0m       [31m0.8888[0m        [94m0.6391[0m     +  27.3647
      2   0.0585   0.3342        [35m0.5988[0m       [31m0.9311[0m        [94m0.5353[0m     +  27.5244
      3   0.0598   0.3130        [35m0.5048[0m       0.9311        [94m0.4354[0m     +  27.5908
      4   0.0556   0.3514        [35m0.4187[0m       0.9311        [94m0.3539[0m     +  27.2691
      5   0.0671   [32m0.4839[0m        [35m0.3555[0m       0.9311        [94m0.3003[0m     +  27.5318
      6   [36m0.1135[0m   [32m0.5908[0m        [35m0.3162[0m       0.9311        [94m0.2699[0m     +  27.5957
      7   [36m0.1957[0m   [32m0.6508[0m        [35m0.2938[0m       0.9311        [94m0.2527[0m     +  27.8824
      8   [36m0.2381[0m   [32m0.6813[0m        [35m0.2803[0m       0.9311        [94m0.2421[0m     +  27.4958
      9   [36m0.2494[0m   [32m0.7026[0m        [35m0.2702[0m       0.9311        [94m0.2351[0m     +  27.4398
     10   [36m0.2685[0m   [32m0.7189[0m        [35m0.2640[0m       0.9311        [94m0.2302[0m     +  27.6376
     11   [36m0.2823[0m   [32m0.7318[0m        [35m0.2595[0m       0.9311        [94m0.2263[0m     +  27.4025
     12   [36m0.2899[0m   [32m0.7434[0m        [35m0.2558[0m       0.9311        [94m0.2231[0m     +  27.3857
     13   [36m0.2946[0m   [32m0.7520[0m        [35m0.2521[0m       0.9311        [94m0.2205[0m     +  27.8309
     14   0.2929   [32m0.7599[0m        [35m0.2487[0m       0.9311        [94m0.2179[0m     +  27.3648
     15   0.2938   [32m0.7683[0m        [35m0.2458[0m       [31m0.9323[0m        [94m0.2155[0m     +  27.5767
     16   [36m0.3094[0m   [32m0.7752[0m        [35m0.2434[0m       [31m0.9335[0m        [94m0.2133[0m     +  27.6754
     17   [36m0.3186[0m   [32m0.7805[0m        [35m0.2410[0m       0.9335        [94m0.2115[0m     +  27.5106
     18   [36m0.3231[0m   [32m0.7847[0m        [35m0.2396[0m       [31m0.9347[0m        [94m0.2101[0m     +  27.4314
     19   0.3175   [32m0.7873[0m        [35m0.2382[0m       0.9335        [94m0.2090[0m     +  27.2140
     20   0.3181   [32m0.7897[0m        [35m0.2366[0m       0.9299        [94m0.2083[0m     +  27.3256
     21   0.3229   [32m0.7926[0m        [35m0.2363[0m       0.9287        [94m0.2077[0m     +  27.4062
     22   0.3184   [32m0.7933[0m        [35m0.2360[0m       0.9274        [94m0.2072[0m     +  27.5881
     23   0.3184   [32m0.7937[0m        [35m0.2356[0m       0.9287        [94m0.2068[0m     +  27.9007
     24   0.3169   [32m0.7949[0m        [35m0.2333[0m       0.9274        [94m0.2064[0m     +  27.4849
     25   0.3146   [32m0.7952[0m        0.2334       0.9274        [94m0.2061[0m     +  27.4446
     26   0.3063   [32m0.7957[0m        [35m0.2327[0m       0.9262        [94m0.2059[0m     +  27.1476
     27   0.3060   [32m0.7960[0m        [35m0.2326[0m       0.9262        [94m0.2058[0m     +  27.5043
     28   0.3050   [32m0.7961[0m        [35m0.2315[0m       0.9262        [94m0.2057[0m     +  27.8240
     29   0.3032   [32m0.7961[0m        [35m0.2311[0m       0.9262        0.2057        27.5262
     30   0.3038   [32m0.7966[0m        [35m0.2310[0m       0.9287        0.2057        27.6314
     31   0.3033   [32m0.7973[0m        [35m0.2302[0m       0.9274        [94m0.2056[0m     +  27.5292
     32   0.3036   [32m0.7974[0m        [35m0.2296[0m       0.9274        [94m0.2055[0m     +  27.8470
     33   0.3046   [32m0.7979[0m        0.2300       0.9274        [94m0.2055[0m     +  27.5603
     34   0.3029   [32m0.7983[0m        0.2309       0.9274        [94m0.2053[0m     +  27.2304
     35   0.2989   0.7978        [35m0.2284[0m       0.9274        0.2054        27.2788
     36   0.3000   0.7983        0.2302       0.9274        [94m0.2052[0m     +  27.3060
     37   0.2972   0.7979        0.2288       0.9262        0.2052        27.4960
     38   0.2976   0.7982        [35m0.2278[0m       0.9274        0.2055        27.6513
     39   0.2943   [32m0.7984[0m        0.2284       0.9262        0.2056        27.8148
     40   0.2959   [32m0.7992[0m        [35m0.2271[0m       0.9262        0.2054        27.7360
     41   0.2960   [32m0.7997[0m        0.2274       0.9250        0.2055        27.8580
     42   0.2958   [32m0.7999[0m        [35m0.2269[0m       0.9250        0.2054        27.4248
     43   0.2965   [32m0.8006[0m        0.2276       0.9262        0.2055        27.6270
     44   0.2948   [32m0.8008[0m        [35m0.2264[0m       0.9262        0.2054        27.3762
     45   0.2949   [32m0.8010[0m        [35m0.2263[0m       0.9250        0.2055        27.2876
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 04:39:10,199][0m Trial 407 finished with value: 0.20520897483522976 and parameters: {'lr': 4.030131061228686e-06, 'dropout': 0.29208908865018673, 'd_model_multiplier': 4, 'num_layers': 6, 'n_heads': 32, 'dim_feedforward': 324, 'batch_size': 66, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.001, 'top_n_features': 60}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 67
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0966[0m   [32m0.5537[0m        [35m0.6482[0m       [31m0.9141[0m        [94m0.6013[0m     +  27.1827
      2   0.0716   0.4096        [35m0.5772[0m       [31m0.9250[0m        [94m0.5160[0m     +  27.0132
      3   0.0696   0.3771        [35m0.4963[0m       0.9250        [94m0.4327[0m     +  27.6668
      4   0.0687   0.3919        [35m0.4248[0m       0.9250        [94m0.3706[0m     +  27.3798
      5   0.0747   0.4631        [35m0.3730[0m       0.9250        [94m0.3289[0m     +  27.2384
      6   0.0857   0.5390        [35m0.3363[0m       0.9250        [94m0.3007[0m     +  27.2527
      7   [36m0.1098[0m   [32m0.6053[0m        [35m0.3115[0m       0.9250        [94m0.2814[0m     +  27.3260
      8   [36m0.1467[0m   [32m0.6533[0m        [35m0.2945[0m       0.9250        [94m0.2685[0m     +  27.7332
      9   [36m0.1716[0m   [32m0.6807[0m        [35m0.2823[0m       0.9250        [94m0.2597[0m     +  27.6026
     10   [36m0.1940[0m   [32m0.6973[0m        [35m0.2732[0m       0.9250        [94m0.2535[0m     +  27.6615
     11   [36m0.2017[0m   [32m0.7115[0m        [35m0.2681[0m       0.9250        [94m0.2489[0m     +  27.3293
     12   [36m0.2077[0m   [32m0.7203[0m        [35m0.2634[0m       0.9250        [94m0.2453[0m     +  27.5955
     13   [36m0.2155[0m   [32m0.7282[0m        [35m0.2579[0m       0.9250        [94m0.2422[0m     +  27.6094
     14   [36m0.2255[0m   [32m0.7369[0m        [35m0.2541[0m       0.9250        [94m0.2394[0m     +  27.4121
     15   [36m0.2350[0m   [32m0.7486[0m        [35m0.2510[0m       0.9238        [94m0.2368[0m     +  27.5035
     16   [36m0.2442[0m   [32m0.7563[0m        [35m0.2479[0m       0.9226        [94m0.2347[0m     +  27.7280
     17   [36m0.2487[0m   [32m0.7634[0m        [35m0.2458[0m       0.9226        [94m0.2330[0m     +  27.1514
     18   [36m0.2496[0m   [32m0.7682[0m        [35m0.2433[0m       0.9226        [94m0.2318[0m     +  27.4077
     19   [36m0.2535[0m   [32m0.7712[0m        [35m0.2406[0m       0.9226        [94m0.2310[0m     +  27.4997
     20   [36m0.2636[0m   [32m0.7730[0m        [35m0.2383[0m       0.9238        [94m0.2305[0m     +  27.3488
     21   [36m0.2652[0m   [32m0.7740[0m        [35m0.2380[0m       0.9238        [94m0.2303[0m     +  27.5804
     22   [36m0.2672[0m   [32m0.7742[0m        [35m0.2368[0m       [31m0.9262[0m        [94m0.2301[0m     +  27.6152
     23   [36m0.2685[0m   [32m0.7756[0m        0.2369       0.9262        [94m0.2297[0m     +  27.5712
     24   [36m0.2689[0m   0.7752        [35m0.2354[0m       0.9262        0.2297        27.4354
     25   [36m0.2706[0m   0.7755        [35m0.2349[0m       0.9262        [94m0.2295[0m     +  28.0008
     26   0.2705   0.7756        [35m0.2346[0m       0.9250        [94m0.2293[0m     +  27.3253
     27   [36m0.2710[0m   0.7756        [35m0.2331[0m       0.9250        [94m0.2293[0m     +  27.2400
     28   0.2710   0.7748        [35m0.2323[0m       0.9250        0.2294        27.6777
     29   [36m0.2713[0m   0.7744        0.2332       0.9250        0.2294        27.3665
     30   [36m0.2720[0m   0.7741        [35m0.2323[0m       0.9250        [94m0.2292[0m     +  27.6585
     31   [36m0.2727[0m   0.7740        [35m0.2311[0m       0.9250        0.2293        27.3444
     32   [36m0.2738[0m   0.7729        [35m0.2309[0m       0.9262        0.2294        27.4824
     33   0.2732   0.7733        0.2318       0.9238        [94m0.2292[0m     +  27.7601
     34   0.2735   0.7732        [35m0.2303[0m       0.9238        [94m0.2292[0m     +  27.5927
     35   [36m0.2744[0m   0.7728        [35m0.2294[0m       0.9250        0.2293        27.5084
     36   [36m0.2756[0m   0.7722        0.2305       0.9250        0.2293        27.2853
     37   0.2750   0.7727        [35m0.2291[0m       0.9238        [94m0.2291[0m     +  27.1806
     38   0.2752   0.7725        0.2295       0.9238        0.2291        27.3369
     39   0.2753   0.7707        0.2300       0.9250        0.2292        27.8616
     40   0.2753   0.7696        [35m0.2291[0m       0.9250        0.2294        27.7047
     41   [36m0.2775[0m   0.7694        [35m0.2282[0m       0.9238        0.2293        27.4481
     42   [36m0.2780[0m   0.7688        0.2283       0.9238        0.2295        27.5561
     43   [36m0.2790[0m   0.7686        [35m0.2276[0m       0.9238        0.2294        27.9277
     44   [36m0.2819[0m   0.7692        0.2281       0.9238        0.2293        27.3983
     45   [36m0.2828[0m   0.7679        [35m0.2276[0m       0.9238        0.2297        27.6821
     46   [36m0.2833[0m   0.7674        0.2283       0.9238        0.2295        27.8616
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 05:00:45,873][0m Trial 408 finished with value: 0.22908015853692715 and parameters: {'lr': 3.3755132681560145e-06, 'dropout': 0.2908050052865012, 'd_model_multiplier': 4, 'num_layers': 6, 'n_heads': 32, 'dim_feedforward': 320, 'batch_size': 67, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.001, 'top_n_features': 67}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 62
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1164[0m   [32m0.5774[0m        [35m0.4233[0m       [31m0.9166[0m        [94m0.2879[0m     +  27.7481
      2   [36m0.3070[0m   [32m0.7609[0m        [35m0.2578[0m       0.9166        [94m0.2489[0m     +  28.1721
      3   [36m0.3077[0m   [32m0.7687[0m        [35m0.2395[0m       [31m0.9178[0m        [94m0.2451[0m     +  27.8530
      4   0.2999   [32m0.7714[0m        [35m0.2324[0m       0.9141        0.2460        27.5046
      5   0.2910   [32m0.7738[0m        [35m0.2289[0m       0.9117        0.2473        27.8364
      6   0.2902   [32m0.7745[0m        0.2293       0.9129        0.2480        27.3623
      7   0.2904   [32m0.7772[0m        [35m0.2268[0m       0.9129        0.2493        27.5537
      8   0.2829   [32m0.7791[0m        [35m0.2253[0m       0.9117        0.2497        27.5997
      9   0.2805   0.7772        0.2257       0.9141        0.2518        27.6775
     10   0.2815   0.7766        [35m0.2236[0m       0.9154        0.2513        27.8278
     11   0.2800   0.7748        [35m0.2227[0m       0.9154        0.2526        27.6108
     12   0.2774   [32m0.7803[0m        [35m0.2212[0m       0.9129        0.2507        27.6285
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 05:06:46,489][0m Trial 409 finished with value: 0.24512295108716461 and parameters: {'lr': 1.7059393607309852e-05, 'dropout': 0.2988914558960807, 'd_model_multiplier': 4, 'num_layers': 6, 'n_heads': 32, 'dim_feedforward': 327, 'batch_size': 22, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.001, 'top_n_features': 62}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 59
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0721[0m   [32m0.3985[0m        [35m0.6125[0m       [31m0.9262[0m        [94m0.5270[0m     +  27.0031
      2   0.0597   0.3426        [35m0.4908[0m       0.9262        [94m0.4076[0m     +  27.0270
      3   0.0618   0.3959        [35m0.3892[0m       0.9262        [94m0.3294[0m     +  27.3315
      4   [36m0.1071[0m   [32m0.5204[0m        [35m0.3267[0m       0.9262        [94m0.2853[0m     +  27.3225
      5   [36m0.1551[0m   [32m0.6069[0m        [35m0.2894[0m       0.9262        [94m0.2625[0m     +  27.6063
      6   [36m0.1929[0m   [32m0.6561[0m        [35m0.2719[0m       0.9262        [94m0.2508[0m     +  27.2942
      7   [36m0.2107[0m   [32m0.6899[0m        [35m0.2607[0m       0.9262        [94m0.2435[0m     +  27.3093
      8   [36m0.2280[0m   [32m0.7124[0m        [35m0.2536[0m       0.9262        [94m0.2383[0m     +  27.4138
      9   [36m0.2447[0m   [32m0.7299[0m        [35m0.2480[0m       [31m0.9274[0m        [94m0.2340[0m     +  27.3280
     10   [36m0.2652[0m   [32m0.7423[0m        [35m0.2444[0m       0.9262        [94m0.2307[0m     +  27.4853
     11   [36m0.2849[0m   [32m0.7496[0m        [35m0.2414[0m       0.9274        [94m0.2282[0m     +  27.6470
     12   [36m0.3038[0m   [32m0.7556[0m        [35m0.2384[0m       [31m0.9299[0m        [94m0.2267[0m     +  27.1033
     13   [36m0.3137[0m   [32m0.7615[0m        [35m0.2365[0m       0.9287        [94m0.2252[0m     +  27.1268
     14   [36m0.3228[0m   [32m0.7656[0m        0.2367       0.9299        [94m0.2240[0m     +  27.4983
     15   [36m0.3392[0m   [32m0.7675[0m        [35m0.2346[0m       [31m0.9335[0m        [94m0.2230[0m     +  27.3396
     16   [36m0.3428[0m   [32m0.7696[0m        [35m0.2333[0m       0.9335        [94m0.2222[0m     +  27.3025
     17   [36m0.3516[0m   [32m0.7698[0m        [35m0.2333[0m       0.9335        [94m0.2218[0m     +  27.3716
     18   [36m0.3526[0m   [32m0.7715[0m        [35m0.2317[0m       [31m0.9347[0m        [94m0.2212[0m     +  27.4013
     19   [36m0.3575[0m   [32m0.7745[0m        [35m0.2314[0m       0.9347        [94m0.2207[0m     +  27.5780
     20   [36m0.3579[0m   [32m0.7753[0m        0.2319       0.9335        [94m0.2203[0m     +  27.3528
     21   [36m0.3588[0m   [32m0.7761[0m        [35m0.2297[0m       0.9347        [94m0.2201[0m     +  27.3965
     22   0.3565   [32m0.7772[0m        0.2304       0.9347        [94m0.2197[0m     +  27.4131
     23   0.3558   [32m0.7773[0m        0.2303       0.9311        [94m0.2196[0m     +  27.8530
     24   0.3559   [32m0.7778[0m        [35m0.2293[0m       0.9335        [94m0.2195[0m     +  27.5612
     25   0.3560   [32m0.7787[0m        [35m0.2290[0m       0.9323        [94m0.2193[0m     +  27.3010
     26   0.3569   [32m0.7799[0m        [35m0.2282[0m       0.9311        [94m0.2190[0m     +  27.2777
     27   0.3577   [32m0.7808[0m        0.2291       0.9335        [94m0.2187[0m     +  27.4086
     28   0.3577   [32m0.7812[0m        [35m0.2274[0m       0.9311        [94m0.2187[0m     +  27.2462
     29   0.3562   [32m0.7825[0m        0.2275       0.9311        [94m0.2184[0m     +  27.2653
     30   0.3548   [32m0.7831[0m        0.2283       0.9311        [94m0.2182[0m     +  27.5257
     31   0.3554   [32m0.7842[0m        0.2278       0.9311        [94m0.2179[0m     +  27.3527
     32   0.3563   [32m0.7848[0m        [35m0.2258[0m       0.9299        0.2179        27.4660
     33   0.3573   [32m0.7856[0m        0.2272       0.9299        [94m0.2179[0m     +  27.4611
     34   0.3580   [32m0.7868[0m        0.2266       0.9299        [94m0.2176[0m     +  27.3990
     35   0.3572   [32m0.7878[0m        [35m0.2251[0m       0.9287        [94m0.2175[0m     +  27.2462
     36   0.3573   [32m0.7884[0m        0.2253       0.9287        [94m0.2174[0m     +  27.4309
     37   0.3583   [32m0.7894[0m        0.2264       0.9287        [94m0.2172[0m     +  27.7020
     38   0.3574   [32m0.7902[0m        [35m0.2251[0m       0.9274        [94m0.2171[0m     +  27.2704
     39   [36m0.3617[0m   [32m0.7916[0m        [35m0.2248[0m       0.9287        [94m0.2169[0m     +  27.4286
     40   [36m0.3628[0m   [32m0.7918[0m        [35m0.2247[0m       0.9287        [94m0.2167[0m     +  27.4559
     41   0.3603   [32m0.7923[0m        0.2259       0.9274        0.2167        27.2809
     42   0.3602   [32m0.7929[0m        [35m0.2238[0m       0.9274        0.2167        27.3374
     43   0.3604   [32m0.7943[0m        [35m0.2233[0m       0.9287        [94m0.2164[0m     +  27.4058
     44   0.3591   [32m0.7949[0m        [35m0.2232[0m       0.9287        [94m0.2163[0m     +  27.5802
     45   0.3585   [32m0.7953[0m        0.2234       0.9274        0.2165        27.1950
     46   0.3587   [32m0.7962[0m        0.2236       0.9274        [94m0.2162[0m     +  27.3931
     47   0.3590   [32m0.7968[0m        0.2233       0.9274        [94m0.2160[0m     +  27.5215
     48   0.3598   [32m0.7974[0m        [35m0.2229[0m       0.9274        [94m0.2159[0m     +  27.3658
     49   0.3576   [32m0.7979[0m        0.2229       0.9287        [94m0.2158[0m     +  27.2282
     50   0.3586   [32m0.7990[0m        [35m0.2215[0m       0.9274        [94m0.2157[0m     +  27.6479
[32m[I 2023-05-04 05:29:41,117][0m Trial 410 finished with value: 0.21570423879675305 and parameters: {'lr': 7.1871088918259236e-06, 'dropout': 0.32131775720721, 'd_model_multiplier': 4, 'num_layers': 6, 'n_heads': 32, 'dim_feedforward': 318, 'batch_size': 63, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.001, 'top_n_features': 59}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 115
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0969[0m   [32m0.3469[0m        [35m0.6358[0m       [31m0.9166[0m        [94m0.4929[0m     +  34.5977
      2   0.0919   [32m0.3727[0m        [35m0.4152[0m       0.9166        [94m0.3462[0m     +  34.9050
      3   [36m0.1006[0m   [32m0.4921[0m        [35m0.3142[0m       0.9166        [94m0.3009[0m     +  34.9526
      4   [36m0.1400[0m   [32m0.5687[0m        [35m0.2749[0m       0.9166        [94m0.2861[0m     +  34.8371
      5   [36m0.1588[0m   [32m0.6120[0m        [35m0.2591[0m       0.9166        [94m0.2797[0m     +  35.0286
      6   [36m0.2048[0m   [32m0.6471[0m        [35m0.2511[0m       0.9166        [94m0.2745[0m     +  35.0466
      7   [36m0.2352[0m   [32m0.6835[0m        [35m0.2458[0m       0.9166        [94m0.2675[0m     +  35.1346
      8   [36m0.2624[0m   [32m0.7161[0m        [35m0.2396[0m       [31m0.9190[0m        [94m0.2607[0m     +  34.9657
      9   [36m0.2741[0m   [32m0.7313[0m        [35m0.2351[0m       [31m0.9214[0m        [94m0.2577[0m     +  34.9945
     10   [36m0.2816[0m   [32m0.7401[0m        [35m0.2324[0m       0.9202        [94m0.2566[0m     +  35.1147
     11   [36m0.2907[0m   [32m0.7474[0m        [35m0.2314[0m       0.9214        [94m0.2551[0m     +  34.8432
     12   0.2869   [32m0.7538[0m        [35m0.2306[0m       [31m0.9238[0m        [94m0.2531[0m     +  35.0175
     13   0.2864   [32m0.7563[0m        [35m0.2285[0m       0.9202        [94m0.2523[0m     +  35.0545
     14   0.2895   [32m0.7608[0m        [35m0.2277[0m       0.9190        [94m0.2510[0m     +  35.2069
     15   [36m0.2913[0m   [32m0.7615[0m        [35m0.2269[0m       0.9190        [94m0.2509[0m     +  35.1590
     16   [36m0.2995[0m   [32m0.7679[0m        [35m0.2267[0m       0.9214        [94m0.2488[0m     +  35.0293
     17   [36m0.3032[0m   [32m0.7705[0m        [35m0.2253[0m       0.9238        [94m0.2480[0m     +  35.0809
     18   [36m0.3066[0m   [32m0.7727[0m        [35m0.2241[0m       0.9238        [94m0.2475[0m     +  35.2478
     19   [36m0.3071[0m   [32m0.7756[0m        0.2255       0.9238        [94m0.2464[0m     +  35.2041
     20   [36m0.3080[0m   [32m0.7770[0m        [35m0.2241[0m       0.9226        0.2465        35.3067
     21   0.3072   [32m0.7770[0m        [35m0.2238[0m       0.9214        0.2467        35.1073
     22   [36m0.3088[0m   [32m0.7794[0m        [35m0.2218[0m       0.9226        [94m0.2457[0m     +  35.1220
     23   [36m0.3122[0m   [32m0.7801[0m        [35m0.2214[0m       0.9214        [94m0.2451[0m     +  35.1563
     24   [36m0.3164[0m   [32m0.7806[0m        0.2225       0.9214        [94m0.2449[0m     +  35.1241
     25   0.3137   [32m0.7812[0m        0.2216       0.9214        [94m0.2449[0m     +  35.0086
     26   0.3163   [32m0.7823[0m        [35m0.2194[0m       0.9214        [94m0.2444[0m     +  35.1611
     27   [36m0.3196[0m   [32m0.7837[0m        0.2207       0.9214        [94m0.2439[0m     +  35.1874
     28   0.3187   0.7832        0.2197       0.9214        0.2441        34.9457
     29   [36m0.3210[0m   [32m0.7845[0m        [35m0.2183[0m       0.9214        0.2439        35.2918
     30   0.3177   0.7844        0.2184       0.9214        [94m0.2438[0m     +  35.0464
     31   0.3164   0.7841        0.2192       0.9214        0.2442        35.1372
     32   [36m0.3225[0m   0.7844        0.2187       0.9226        0.2439        35.1182
     33   [36m0.3229[0m   [32m0.7847[0m        [35m0.2175[0m       0.9214        0.2445        34.9427
     34   0.3218   [32m0.7868[0m        0.2187       0.9214        [94m0.2435[0m     +  35.1702
     35   [36m0.3266[0m   0.7850        0.2188       0.9214        0.2443        35.1728
     36   0.3260   [32m0.7871[0m        [35m0.2164[0m       0.9214        [94m0.2434[0m     +  35.2214
     37   0.3252   0.7867        0.2166       0.9214        0.2442        35.1013
     38   0.3243   0.7850        0.2165       0.9226        0.2446        35.2430
     39   0.3245   0.7859        0.2167       0.9214        0.2439        34.9789
     40   0.3254   0.7857        [35m0.2149[0m       0.9226        0.2439        35.2797
     41   0.3252   0.7865        [35m0.2144[0m       0.9226        0.2443        35.1931
     42   0.3204   [32m0.7872[0m        0.2151       0.9214        0.2443        35.1993
     43   0.3213   [32m0.7873[0m        [35m0.2144[0m       0.9226        0.2447        34.9257
     44   0.3201   [32m0.7881[0m        [35m0.2143[0m       0.9202        0.2445        35.2532
     45   0.3190   0.7879        [35m0.2142[0m       0.9214        0.2450        35.1750
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 05:56:39,204][0m Trial 411 finished with value: 0.24335347339420826 and parameters: {'lr': 5.499823815920271e-06, 'dropout': 0.2698486828433612, 'd_model_multiplier': 4, 'num_layers': 8, 'n_heads': 32, 'dim_feedforward': 337, 'batch_size': 34, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.001, 'top_n_features': 115}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 70
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0850[0m   [32m0.4001[0m        [35m0.6680[0m       [31m0.9287[0m        [94m0.5727[0m     +  27.2360
      2   0.0733   0.3946        [35m0.5789[0m       0.9287        [94m0.4679[0m     +  27.0614
      3   0.0670   0.3944        [35m0.4865[0m       0.9287        [94m0.3827[0m     +  27.4794
      4   0.0684   [32m0.4095[0m        [35m0.4129[0m       0.9287        [94m0.3259[0m     +  27.9843
      5   0.0736   [32m0.4548[0m        [35m0.3628[0m       0.9287        [94m0.2914[0m     +  27.4417
      6   [36m0.0857[0m   [32m0.5183[0m        [35m0.3284[0m       0.9287        [94m0.2708[0m     +  27.3244
      7   [36m0.0967[0m   [32m0.5747[0m        [35m0.3056[0m       0.9287        [94m0.2590[0m     +  27.5271
      8   [36m0.1467[0m   [32m0.6117[0m        [35m0.2911[0m       0.9287        [94m0.2519[0m     +  27.4562
      9   [36m0.1807[0m   [32m0.6385[0m        [35m0.2814[0m       0.9287        [94m0.2476[0m     +  27.4737
     10   [36m0.1839[0m   [32m0.6501[0m        [35m0.2747[0m       0.9287        [94m0.2456[0m     +  27.2302
     11   0.1789   [32m0.6576[0m        [35m0.2707[0m       0.9287        [94m0.2446[0m     +  27.2751
     12   0.1761   [32m0.6630[0m        [35m0.2659[0m       0.9287        [94m0.2443[0m     +  27.1987
     13   0.1740   [32m0.6666[0m        [35m0.2614[0m       0.9287        0.2444        27.2011
     14   0.1757   [32m0.6693[0m        [35m0.2588[0m       0.9287        0.2450        27.3679
     15   0.1720   [32m0.6708[0m        [35m0.2578[0m       0.9287        0.2452        27.4116
     16   0.1716   [32m0.6732[0m        [35m0.2574[0m       0.9287        0.2456        27.4871
     17   0.1716   [32m0.6734[0m        [35m0.2535[0m       0.9287        0.2462        27.1623
     18   0.1745   [32m0.6758[0m        [35m0.2525[0m       0.9287        0.2464        27.2733
     19   0.1744   [32m0.6769[0m        [35m0.2522[0m       0.9287        0.2470        27.2466
     20   0.1757   [32m0.6777[0m        0.2526       0.9287        0.2473        27.5532
     21   0.1741   [32m0.6799[0m        [35m0.2500[0m       0.9287        0.2477        27.3648
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 06:06:43,004][0m Trial 412 finished with value: 0.24430120711134884 and parameters: {'lr': 4.771818833206851e-06, 'dropout': 0.627990306878206, 'd_model_multiplier': 4, 'num_layers': 6, 'n_heads': 32, 'dim_feedforward': 331, 'batch_size': 53, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.001, 'top_n_features': 70}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 65
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0538[0m   [32m0.3137[0m        [35m0.6754[0m       [31m0.9214[0m        [94m0.6192[0m     +  30.9333
      2   0.0518   0.2978        [35m0.6215[0m       [31m0.9226[0m        [94m0.5501[0m     +  30.9488
      3   0.0519   0.2962        [35m0.5519[0m       0.9226        [94m0.4814[0m     +  30.9045
      4   0.0525   0.2982        [35m0.4901[0m       0.9226        [94m0.4235[0m     +  31.1165
      5   0.0529   0.3024        [35m0.4338[0m       0.9226        [94m0.3790[0m     +  31.4314
      6   [36m0.0539[0m   [32m0.3187[0m        [35m0.3883[0m       0.9226        [94m0.3461[0m     +  31.0821
      7   [36m0.0566[0m   [32m0.3564[0m        [35m0.3571[0m       0.9226        [94m0.3228[0m     +  30.9923
      8   [36m0.0621[0m   [32m0.4126[0m        [35m0.3314[0m       0.9226        [94m0.3055[0m     +  30.9235
      9   [36m0.0865[0m   [32m0.4620[0m        [35m0.3132[0m       0.9226        [94m0.2927[0m     +  30.8287
     10   [36m0.1125[0m   [32m0.5024[0m        [35m0.2996[0m       0.9226        [94m0.2836[0m     +  31.2188
     11   [36m0.1292[0m   [32m0.5360[0m        [35m0.2904[0m       0.9226        [94m0.2769[0m     +  30.9159
     12   [36m0.1436[0m   [32m0.5608[0m        [35m0.2816[0m       0.9226        [94m0.2724[0m     +  30.9574
     13   [36m0.1586[0m   [32m0.5827[0m        [35m0.2761[0m       0.9226        [94m0.2690[0m     +  31.0691
     14   [36m0.1671[0m   [32m0.5967[0m        [35m0.2724[0m       0.9226        [94m0.2666[0m     +  31.0361
     15   [36m0.1715[0m   [32m0.6093[0m        [35m0.2688[0m       0.9226        [94m0.2648[0m     +  31.0405
     16   0.1684   [32m0.6197[0m        [35m0.2648[0m       0.9226        [94m0.2634[0m     +  31.3980
     17   [36m0.1751[0m   [32m0.6285[0m        [35m0.2629[0m       0.9226        [94m0.2626[0m     +  30.8383
     18   [36m0.1785[0m   [32m0.6356[0m        [35m0.2598[0m       0.9226        [94m0.2619[0m     +  30.8334
     19   [36m0.1835[0m   [32m0.6428[0m        [35m0.2582[0m       0.9226        [94m0.2613[0m     +  31.2022
     20   [36m0.1862[0m   [32m0.6483[0m        [35m0.2579[0m       0.9226        [94m0.2608[0m     +  31.0366
     21   [36m0.1880[0m   [32m0.6512[0m        0.2590       0.9226        [94m0.2606[0m     +  31.3327
     22   [36m0.1911[0m   [32m0.6549[0m        [35m0.2554[0m       0.9226        [94m0.2603[0m     +  31.1571
     23   [36m0.1939[0m   [32m0.6593[0m        [35m0.2532[0m       0.9226        [94m0.2599[0m     +  31.2608
     24   [36m0.1995[0m   [32m0.6617[0m        0.2544       0.9226        [94m0.2596[0m     +  30.9925
     25   [36m0.2002[0m   [32m0.6658[0m        [35m0.2515[0m       0.9226        [94m0.2588[0m     +  30.9945
     26   [36m0.2023[0m   [32m0.6690[0m        0.2519       0.9214        [94m0.2585[0m     +  31.4132
     27   [36m0.2042[0m   [32m0.6738[0m        [35m0.2499[0m       0.9202        [94m0.2579[0m     +  31.5913
     28   [36m0.2077[0m   [32m0.6766[0m        0.2527       0.9202        0.2582        31.4706
     29   [36m0.2115[0m   [32m0.6817[0m        [35m0.2494[0m       0.9202        [94m0.2574[0m     +  31.1227
     30   [36m0.2133[0m   [32m0.6849[0m        [35m0.2482[0m       0.9202        [94m0.2571[0m     +  31.2416
     31   [36m0.2151[0m   [32m0.6890[0m        0.2495       0.9214        [94m0.2567[0m     +  31.3451
     32   [36m0.2189[0m   [32m0.6930[0m        [35m0.2479[0m       0.9226        [94m0.2567[0m     +  31.3721
     33   [36m0.2211[0m   [32m0.6956[0m        [35m0.2471[0m       [31m0.9238[0m        0.2568        31.2628
     34   [36m0.2239[0m   [32m0.7013[0m        [35m0.2435[0m       [31m0.9250[0m        [94m0.2560[0m     +  31.0932
     35   [36m0.2252[0m   [32m0.7058[0m        0.2453       0.9250        [94m0.2553[0m     +  31.1483
     36   [36m0.2290[0m   [32m0.7084[0m        0.2449       0.9250        0.2559        31.1733
     37   0.2284   [32m0.7100[0m        0.2443       0.9238        0.2566        30.8955
     38   0.2274   [32m0.7117[0m        [35m0.2419[0m       0.9238        0.2573        31.1102
     39   [36m0.2301[0m   [32m0.7158[0m        [35m0.2401[0m       0.9202        0.2570        31.1092
     40   0.2297   [32m0.7163[0m        0.2412       0.9214        0.2576        31.1399
     41   [36m0.2309[0m   [32m0.7181[0m        0.2411       0.9178        0.2580        31.2719
     42   [36m0.2324[0m   [32m0.7192[0m        0.2411       0.9154        0.2585        31.6461
     43   0.2324   [32m0.7196[0m        0.2411       0.9154        0.2589        31.2247
     44   0.2321   [32m0.7206[0m        [35m0.2388[0m       0.9166        0.2595        31.0081
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 06:30:08,658][0m Trial 413 finished with value: 0.25534220239848004 and parameters: {'lr': 4.406933521205439e-06, 'dropout': 0.6530322712993666, 'd_model_multiplier': 4, 'num_layers': 7, 'n_heads': 32, 'dim_feedforward': 316, 'batch_size': 73, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.001, 'top_n_features': 65}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 76
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2153[0m   [32m0.7000[0m        [35m0.3772[0m       [31m0.9238[0m        [94m0.2457[0m     +  23.7049
      2   [36m0.2474[0m   [32m0.7385[0m        [35m0.2391[0m       [31m0.9250[0m        [94m0.2428[0m     +  23.6330
      3   [36m0.2680[0m   [32m0.7527[0m        [35m0.2345[0m       0.9238        [94m0.2396[0m     +  23.7537
      4   [36m0.2741[0m   [32m0.7555[0m        [35m0.2305[0m       0.9202        0.2417        23.6909
      5   [36m0.2750[0m   [32m0.7673[0m        0.2306       0.9190        [94m0.2383[0m     +  23.8910
      6   0.2678   [32m0.7722[0m        [35m0.2274[0m       0.9190        [94m0.2376[0m     +  23.7254
      7   0.2716   [32m0.7759[0m        [35m0.2260[0m       0.9129        0.2413        23.5880
      8   [36m0.2787[0m   0.7719        [35m0.2245[0m       0.9141        0.2402        23.6426
      9   [36m0.2832[0m   0.7732        [35m0.2219[0m       0.9154        0.2383        23.6773
     10   [36m0.2901[0m   [32m0.7826[0m        [35m0.2195[0m       0.9166        [94m0.2369[0m     +  23.7590
     11   [36m0.3119[0m   0.7799        [35m0.2189[0m       0.9154        [94m0.2363[0m     +  23.6705
     12   [36m0.3145[0m   [32m0.7847[0m        [35m0.2180[0m       0.9166        [94m0.2326[0m     +  23.8257
     13   [36m0.3236[0m   [32m0.7864[0m        [35m0.2143[0m       0.9214        [94m0.2323[0m     +  23.8806
     14   0.3196   [32m0.7943[0m        [35m0.2132[0m       0.9238        [94m0.2319[0m     +  23.9629
     15   0.3066   0.7928        [35m0.2104[0m       0.9190        0.2326        23.8200
     16   0.2973   [32m0.7965[0m        [35m0.2075[0m       0.9178        0.2321        23.6572
     17   0.3129   [32m0.7987[0m        [35m0.2066[0m       0.9214        [94m0.2293[0m     +  23.5936
     18   0.3074   0.7949        [35m0.2048[0m       0.9238        0.2318        23.8011
     19   0.2999   0.7938        [35m0.2038[0m       0.9166        0.2349        23.6501
     20   0.2979   [32m0.8033[0m        [35m0.1983[0m       0.9117        0.2353        23.8576
     21   0.2903   0.7940        [35m0.1973[0m       0.9202        0.2327        23.8214
     22   0.3030   0.7991        [35m0.1952[0m       0.9141        0.2337        23.9546
     23   0.2938   0.7915        [35m0.1938[0m       0.9190        0.2349        23.8972
     24   [36m0.3320[0m   0.7941        [35m0.1914[0m       0.9214        0.2325        24.0436
     25   0.3085   0.7853        [35m0.1888[0m       0.9214        0.2395        24.0714
     26   0.2923   0.7819        [35m0.1868[0m       0.9238        0.2417        23.9687
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 06:40:52,362][0m Trial 414 finished with value: 0.22930717707905546 and parameters: {'lr': 7.195830762082294e-05, 'dropout': 0.30920770567196504, 'd_model_multiplier': 4, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 339, 'batch_size': 28, 'pos_encoding': 'learnable', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.001, 'top_n_features': 76}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 53
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
Warning, assumed runtime error: CUDA out of memory. Tried to allocate 1.18 GiB (GPU 0; 23.70 GiB total capacity; 17.16 GiB already allocated; 595.25 MiB free; 22.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[32m[I 2023-05-04 06:40:56,615][0m Trial 415 finished with value: 100.0 and parameters: {'lr': 0.020954600559132543, 'dropout': 0.6967331885334803, 'd_model_multiplier': 4, 'num_layers': 6, 'n_heads': 32, 'dim_feedforward': 325, 'batch_size': 100, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0, 'top_n_features': 53}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 60
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0711[0m   [32m0.3651[0m        [35m0.5987[0m       [31m0.9166[0m        [94m0.5075[0m     +  23.9340
      2   0.0677   [32m0.3732[0m        [35m0.4590[0m       0.9166        [94m0.4009[0m     +  23.4695
      3   [36m0.0754[0m   [32m0.4594[0m        [35m0.3745[0m       0.9166        [94m0.3375[0m     +  24.2763
      4   [36m0.1225[0m   [32m0.5640[0m        [35m0.3257[0m       0.9166        [94m0.3040[0m     +  24.7057
      5   [36m0.1564[0m   [32m0.6265[0m        [35m0.2977[0m       0.9166        [94m0.2866[0m     +  24.3090
      6   [36m0.1958[0m   [32m0.6651[0m        [35m0.2817[0m       0.9166        [94m0.2763[0m     +  24.1339
      7   [36m0.2287[0m   [32m0.6849[0m        [35m0.2707[0m       0.9166        [94m0.2693[0m     +  24.0382
      8   [36m0.2418[0m   [32m0.6964[0m        [35m0.2634[0m       0.9166        [94m0.2643[0m     +  23.7494
      9   [36m0.2495[0m   [32m0.7083[0m        [35m0.2577[0m       0.9154        [94m0.2603[0m     +  24.1021
     10   [36m0.2606[0m   [32m0.7197[0m        [35m0.2525[0m       0.9166        [94m0.2570[0m     +  24.2168
     11   [36m0.2684[0m   [32m0.7294[0m        [35m0.2493[0m       [31m0.9178[0m        [94m0.2544[0m     +  24.4658
     12   [36m0.2767[0m   [32m0.7375[0m        [35m0.2452[0m       0.9178        [94m0.2522[0m     +  24.0328
     13   [36m0.2804[0m   [32m0.7466[0m        [35m0.2425[0m       [31m0.9190[0m        [94m0.2502[0m     +  24.4674
     14   [36m0.2897[0m   [32m0.7535[0m        [35m0.2408[0m       0.9190        [94m0.2486[0m     +  24.3267
     15   [36m0.2926[0m   [32m0.7590[0m        [35m0.2389[0m       0.9190        [94m0.2472[0m     +  24.4298
     16   [36m0.2986[0m   [32m0.7632[0m        [35m0.2376[0m       0.9178        [94m0.2463[0m     +  24.4598
     17   [36m0.3025[0m   [32m0.7663[0m        [35m0.2360[0m       0.9190        [94m0.2455[0m     +  24.5746
     18   [36m0.3082[0m   [32m0.7691[0m        [35m0.2351[0m       0.9190        [94m0.2449[0m     +  24.2805
     19   [36m0.3107[0m   [32m0.7720[0m        [35m0.2344[0m       [31m0.9202[0m        [94m0.2444[0m     +  24.1265
     20   [36m0.3133[0m   [32m0.7740[0m        [35m0.2336[0m       0.9202        [94m0.2438[0m     +  24.1938
     21   0.3128   [32m0.7752[0m        [35m0.2330[0m       0.9202        [94m0.2434[0m     +  24.3756
     22   [36m0.3143[0m   [32m0.7763[0m        [35m0.2325[0m       [31m0.9214[0m        [94m0.2432[0m     +  24.0459
     23   0.3121   [32m0.7773[0m        [35m0.2313[0m       0.9214        [94m0.2429[0m     +  24.2268
     24   0.3137   [32m0.7785[0m        0.2318       0.9214        [94m0.2428[0m     +  24.7464
     25   [36m0.3144[0m   [32m0.7794[0m        0.2315       0.9214        [94m0.2427[0m     +  23.9963
     26   [36m0.3146[0m   [32m0.7799[0m        [35m0.2310[0m       0.9202        [94m0.2425[0m     +  24.1086
     27   0.3138   [32m0.7811[0m        [35m0.2295[0m       0.9202        [94m0.2423[0m     +  24.5749
     28   0.3138   [32m0.7813[0m        [35m0.2292[0m       0.9202        0.2424        24.2766
     29   [36m0.3148[0m   [32m0.7821[0m        0.2300       0.9202        [94m0.2423[0m     +  24.5823
     30   [36m0.3154[0m   [32m0.7829[0m        0.2296       0.9202        [94m0.2421[0m     +  24.5599
     31   [36m0.3162[0m   [32m0.7834[0m        [35m0.2284[0m       0.9202        [94m0.2420[0m     +  24.1822
     32   [36m0.3170[0m   [32m0.7839[0m        [35m0.2280[0m       0.9202        [94m0.2419[0m     +  24.1365
     33   0.3159   [32m0.7846[0m        [35m0.2272[0m       0.9202        [94m0.2417[0m     +  24.4667
     34   0.3157   [32m0.7849[0m        0.2275       0.9202        [94m0.2417[0m     +  24.3668
     35   0.3164   [32m0.7858[0m        0.2284       0.9190        [94m0.2414[0m     +  24.2556
     36   [36m0.3171[0m   [32m0.7863[0m        0.2276       0.9190        0.2415        24.1858
     37   0.3164   [32m0.7866[0m        0.2276       0.9190        0.2414        24.3303
     38   0.3163   [32m0.7874[0m        0.2276       0.9202        [94m0.2412[0m     +  24.3854
     39   0.3158   [32m0.7877[0m        [35m0.2267[0m       0.9202        0.2413        24.6353
     40   0.3152   [32m0.7877[0m        0.2281       0.9202        0.2414        24.2983
     41   [36m0.3172[0m   [32m0.7882[0m        [35m0.2258[0m       0.9202        [94m0.2412[0m     +  24.5190
     42   [36m0.3179[0m   [32m0.7891[0m        [35m0.2258[0m       0.9202        [94m0.2411[0m     +  24.2820
     43   [36m0.3185[0m   [32m0.7894[0m        0.2266       0.9202        [94m0.2410[0m     +  24.3579
     44   [36m0.3210[0m   [32m0.7899[0m        [35m0.2258[0m       0.9202        [94m0.2409[0m     +  24.6743
     45   [36m0.3220[0m   [32m0.7901[0m        [35m0.2254[0m       0.9202        0.2409        24.2625
     46   [36m0.3224[0m   [32m0.7907[0m        [35m0.2249[0m       0.9202        [94m0.2408[0m     +  24.3352
     47   [36m0.3277[0m   0.7904        0.2256       0.9190        [94m0.2408[0m     +  24.2538
     48   [36m0.3292[0m   [32m0.7913[0m        0.2257       0.9190        [94m0.2406[0m     +  24.3057
     49   0.3281   [32m0.7915[0m        0.2253       0.9178        0.2406        24.3409
     50   0.3276   [32m0.7918[0m        0.2253       0.9178        [94m0.2404[0m     +  24.2393
[32m[I 2023-05-04 07:01:15,639][0m Trial 416 finished with value: 0.24043148323587235 and parameters: {'lr': 2.6620357380593464e-06, 'dropout': 0.28694498064562163, 'd_model_multiplier': 4, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 347, 'batch_size': 81, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.001, 'top_n_features': 60}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 98
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp       dur
-------  -------  -------  ------------  -----------  ------------  ----  --------
      1   [36m0.0606[0m   [32m0.3540[0m        [35m0.5566[0m       [31m0.9166[0m        [94m0.3363[0m     +  121.1456
      2   [36m0.2165[0m   [32m0.6572[0m        [35m0.2896[0m       0.9166        [94m0.2712[0m     +  121.2316
      3   [36m0.2807[0m   [32m0.6987[0m        [35m0.2525[0m       0.9166        [94m0.2637[0m     +  121.4598
      4   [36m0.3173[0m   [32m0.7221[0m        [35m0.2444[0m       0.9166        [94m0.2594[0m     +  121.6306
      5   [36m0.3319[0m   [32m0.7429[0m        [35m0.2405[0m       [31m0.9178[0m        [94m0.2546[0m     +  121.6079
      6   [36m0.3421[0m   [32m0.7571[0m        [35m0.2371[0m       [31m0.9190[0m        [94m0.2497[0m     +  121.4907
      7   [36m0.3570[0m   [32m0.7692[0m        [35m0.2346[0m       [31m0.9202[0m        [94m0.2455[0m     +  121.3618
      8   [36m0.3685[0m   [32m0.7773[0m        [35m0.2326[0m       [31m0.9214[0m        [94m0.2432[0m     +  121.5592
      9   0.3671   [32m0.7819[0m        [35m0.2317[0m       0.9202        [94m0.2416[0m     +  121.4232
     10   [36m0.3694[0m   [32m0.7853[0m        [35m0.2300[0m       0.9214        [94m0.2409[0m     +  121.4922
     11   [36m0.3731[0m   [32m0.7882[0m        [35m0.2277[0m       0.9214        [94m0.2391[0m     +  122.0303
     12   0.3694   [32m0.7890[0m        0.2282       0.9202        [94m0.2391[0m     +  121.7310
     13   0.3714   [32m0.7911[0m        [35m0.2259[0m       0.9202        [94m0.2389[0m     +  121.6756
     14   [36m0.3765[0m   [32m0.7934[0m        [35m0.2258[0m       0.9202        [94m0.2381[0m     +  121.5338
     15   0.3750   [32m0.7949[0m        [35m0.2257[0m       0.9202        [94m0.2379[0m     +  121.4515
     16   0.3734   [32m0.7963[0m        [35m0.2249[0m       0.9190        [94m0.2374[0m     +  121.7568
     17   0.3717   [32m0.7969[0m        [35m0.2240[0m       0.9190        [94m0.2373[0m     +  121.5018
     18   [36m0.3779[0m   [32m0.7984[0m        [35m0.2222[0m       0.9166        [94m0.2370[0m     +  122.0658
     19   0.3776   [32m0.7997[0m        [35m0.2215[0m       0.9166        [94m0.2369[0m     +  121.4261
     20   0.3703   0.7992        0.2215       0.9166        0.2377        121.7934
     21   0.3695   [32m0.8000[0m        [35m0.2199[0m       0.9154        0.2377        121.7052
     22   0.3724   0.7997        0.2211       0.9166        0.2383        121.4774
     23   0.3710   [32m0.8004[0m        [35m0.2194[0m       0.9154        0.2387        121.8919
     24   0.3673   0.7998        [35m0.2191[0m       0.9190        0.2386        121.9048
     25   0.3694   [32m0.8019[0m        [35m0.2181[0m       0.9154        0.2383        121.6356
     26   0.3690   [32m0.8039[0m        0.2191       0.9178        0.2371        122.1363
     27   0.3681   0.8019        [35m0.2171[0m       0.9154        0.2380        121.7592
     28   0.3651   0.8024        [35m0.2166[0m       0.9166        0.2386        121.9448
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 08:00:57,726][0m Trial 417 finished with value: 0.23691020132084134 and parameters: {'lr': 1.5750769377575952e-06, 'dropout': 0.4074328030365082, 'd_model_multiplier': 64, 'num_layers': 6, 'n_heads': 32, 'dim_feedforward': 360, 'batch_size': 49, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.001, 'top_n_features': 98}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 104
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0823[0m   [32m0.3448[0m        [35m0.5341[0m       [31m0.9141[0m        [94m0.4075[0m     +  25.0628
      2   0.0795   [32m0.4141[0m        [35m0.3581[0m       0.9141        [94m0.3257[0m     +  24.9001
      3   [36m0.0912[0m   [32m0.5016[0m        [35m0.3026[0m       0.9141        [94m0.3030[0m     +  24.6582
      4   [36m0.1087[0m   [32m0.5648[0m        [35m0.2807[0m       0.9141        [94m0.2939[0m     +  25.0380
      5   [36m0.1291[0m   [32m0.6141[0m        [35m0.2689[0m       0.9141        [94m0.2883[0m     +  25.1723
      6   [36m0.1606[0m   [32m0.6475[0m        [35m0.2628[0m       0.9141        [94m0.2845[0m     +  24.7448
      7   [36m0.1774[0m   [32m0.6671[0m        [35m0.2577[0m       0.9141        [94m0.2815[0m     +  24.7351
      8   [36m0.1916[0m   [32m0.6819[0m        [35m0.2543[0m       0.9141        [94m0.2794[0m     +  24.8582
      9   [36m0.2003[0m   [32m0.6935[0m        [35m0.2510[0m       0.9141        [94m0.2773[0m     +  24.8758
     10   [36m0.2132[0m   [32m0.7030[0m        [35m0.2491[0m       0.9141        [94m0.2754[0m     +  24.7735
     11   [36m0.2245[0m   [32m0.7122[0m        [35m0.2484[0m       0.9141        [94m0.2736[0m     +  24.8806
     12   [36m0.2406[0m   [32m0.7222[0m        [35m0.2461[0m       0.9141        [94m0.2719[0m     +  24.8902
     13   [36m0.2474[0m   [32m0.7292[0m        [35m0.2444[0m       0.9141        [94m0.2701[0m     +  25.0297
     14   [36m0.2528[0m   [32m0.7351[0m        [35m0.2426[0m       0.9141        [94m0.2689[0m     +  24.8012
     15   [36m0.2603[0m   [32m0.7403[0m        [35m0.2415[0m       0.9141        [94m0.2673[0m     +  24.7910
     16   [36m0.2657[0m   [32m0.7452[0m        [35m0.2405[0m       0.9141        [94m0.2661[0m     +  24.8260
     17   [36m0.2701[0m   [32m0.7484[0m        [35m0.2389[0m       0.9141        [94m0.2654[0m     +  25.0335
     18   [36m0.2725[0m   [32m0.7512[0m        [35m0.2388[0m       0.9129        [94m0.2646[0m     +  25.0255
     19   [36m0.2760[0m   [32m0.7537[0m        [35m0.2380[0m       0.9129        [94m0.2640[0m     +  24.8229
     20   [36m0.2777[0m   [32m0.7551[0m        [35m0.2361[0m       0.9129        [94m0.2638[0m     +  24.8959
     21   0.2762   [32m0.7570[0m        0.2366       0.9129        [94m0.2633[0m     +  24.8516
     22   [36m0.2805[0m   [32m0.7586[0m        0.2368       0.9141        [94m0.2629[0m     +  24.8514
     23   [36m0.2843[0m   [32m0.7598[0m        [35m0.2356[0m       0.9141        [94m0.2627[0m     +  24.9136
     24   [36m0.2860[0m   [32m0.7610[0m        0.2357       0.9141        [94m0.2627[0m     +  24.7016
     25   [36m0.2870[0m   [32m0.7619[0m        [35m0.2355[0m       0.9129        [94m0.2623[0m     +  24.8453
     26   0.2869   [32m0.7627[0m        [35m0.2340[0m       0.9129        0.2628        25.0039
     27   [36m0.2873[0m   [32m0.7644[0m        0.2345       0.9129        [94m0.2622[0m     +  25.1279
     28   0.2862   [32m0.7651[0m        0.2351       0.9129        0.2623        24.9240
     29   0.2869   [32m0.7657[0m        [35m0.2329[0m       0.9129        [94m0.2621[0m     +  24.7552
     30   [36m0.2876[0m   [32m0.7665[0m        0.2341       0.9129        0.2621        24.7958
     31   [36m0.2886[0m   [32m0.7672[0m        0.2335       0.9129        [94m0.2621[0m     +  24.7747
     32   [36m0.2886[0m   [32m0.7684[0m        0.2343       0.9117        [94m0.2617[0m     +  25.0006
     33   [36m0.2889[0m   [32m0.7690[0m        0.2334       0.9141        [94m0.2616[0m     +  24.8421
     34   [36m0.2916[0m   [32m0.7697[0m        [35m0.2325[0m       0.9141        [94m0.2614[0m     +  24.9358
     35   [36m0.2934[0m   [32m0.7700[0m        [35m0.2315[0m       0.9141        0.2616        24.8385
     36   0.2927   [32m0.7709[0m        0.2322       0.9141        [94m0.2611[0m     +  24.8377
     37   0.2927   [32m0.7714[0m        0.2321       0.9141        0.2611        24.7864
     38   0.2925   [32m0.7720[0m        [35m0.2312[0m       0.9141        0.2612        25.0604
     39   0.2920   [32m0.7727[0m        0.2315       0.9141        [94m0.2610[0m     +  24.8126
     40   0.2910   [32m0.7731[0m        0.2317       0.9141        0.2611        24.7854
     41   0.2914   [32m0.7735[0m        [35m0.2306[0m       0.9141        0.2612        24.7923
     42   0.2924   [32m0.7739[0m        [35m0.2306[0m       0.9141        0.2610        24.9049
     43   0.2917   [32m0.7742[0m        0.2319       0.9141        [94m0.2607[0m     +  25.0512
     44   0.2921   [32m0.7744[0m        0.2307       0.9141        0.2609        24.7877
     45   0.2926   [32m0.7753[0m        0.2306       0.9141        0.2608        24.9309
     46   0.2923   [32m0.7756[0m        0.2314       0.9141        0.2608        24.8811
     47   0.2926   [32m0.7760[0m        [35m0.2305[0m       0.9141        0.2608        24.6983
     48   0.2908   [32m0.7763[0m        0.2314       0.9141        0.2610        24.8565
     49   0.2927   [32m0.7770[0m        [35m0.2302[0m       0.9141        [94m0.2604[0m     +  25.0961
     50   0.2915   [32m0.7772[0m        0.2308       [31m0.9154[0m        [94m0.2603[0m     +  24.9671
[32m[I 2023-05-04 08:21:48,326][0m Trial 418 finished with value: 0.26034602723754446 and parameters: {'lr': 1.272640161388567e-06, 'dropout': 0.3260007403554191, 'd_model_multiplier': 4, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 379, 'batch_size': 15, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 104}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 69
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0567[0m   [32m0.3371[0m        [35m0.7164[0m       [31m0.4027[0m        [94m0.6929[0m     +  31.0305
      2   0.0557   0.3198        [35m0.6864[0m       [31m0.7328[0m        [94m0.6513[0m     +  30.9303
      3   [36m0.0591[0m   0.3106        [35m0.6461[0m       [31m0.9105[0m        [94m0.6034[0m     +  31.1040
      4   0.0587   0.3054        [35m0.5994[0m       [31m0.9250[0m        [94m0.5542[0m     +  30.9786
      5   0.0572   0.3000        [35m0.5519[0m       0.9250        [94m0.5070[0m     +  31.0310
      6   0.0554   0.2962        [35m0.5082[0m       0.9250        [94m0.4638[0m     +  31.3502
      7   0.0540   0.2986        [35m0.4660[0m       0.9250        [94m0.4249[0m     +  31.3083
      8   0.0540   0.3094        [35m0.4297[0m       0.9250        [94m0.3909[0m     +  31.3261
      9   0.0550   0.3265        [35m0.3970[0m       0.9250        [94m0.3629[0m     +  31.0953
     10   0.0565   [32m0.3507[0m        [35m0.3710[0m       0.9250        [94m0.3411[0m     +  31.5603
     11   0.0579   [32m0.3753[0m        [35m0.3491[0m       0.9250        [94m0.3243[0m     +  31.2907
     12   [36m0.0604[0m   [32m0.4085[0m        [35m0.3324[0m       0.9250        [94m0.3112[0m     +  31.3344
     13   [36m0.0639[0m   [32m0.4436[0m        [35m0.3202[0m       0.9250        [94m0.3010[0m     +  31.1958
     14   [36m0.0678[0m   [32m0.4732[0m        [35m0.3099[0m       0.9250        [94m0.2927[0m     +  31.2251
     15   [36m0.0729[0m   [32m0.5034[0m        [35m0.3006[0m       0.9250        [94m0.2858[0m     +  31.3716
     16   [36m0.0807[0m   [32m0.5318[0m        [35m0.2941[0m       0.9250        [94m0.2801[0m     +  31.1563
     17   [36m0.0901[0m   [32m0.5534[0m        [35m0.2878[0m       0.9250        [94m0.2754[0m     +  31.1287
     18   [36m0.0994[0m   [32m0.5763[0m        [35m0.2827[0m       0.9250        [94m0.2713[0m     +  31.1274
     19   [36m0.1084[0m   [32m0.5953[0m        [35m0.2779[0m       0.9250        [94m0.2678[0m     +  31.2602
     20   [36m0.1139[0m   [32m0.6099[0m        [35m0.2739[0m       0.9250        [94m0.2648[0m     +  31.2992
     21   [36m0.1243[0m   [32m0.6268[0m        [35m0.2712[0m       0.9250        [94m0.2622[0m     +  31.3118
     22   [36m0.1354[0m   [32m0.6402[0m        [35m0.2680[0m       0.9238        [94m0.2599[0m     +  31.2838
     23   [36m0.1440[0m   [32m0.6508[0m        [35m0.2655[0m       0.9238        [94m0.2579[0m     +  31.2543
     24   [36m0.1483[0m   [32m0.6585[0m        [35m0.2634[0m       0.9238        [94m0.2562[0m     +  31.3164
     25   [36m0.1551[0m   [32m0.6680[0m        [35m0.2603[0m       0.9238        [94m0.2546[0m     +  31.1614
     26   [36m0.1597[0m   [32m0.6758[0m        [35m0.2582[0m       0.9238        [94m0.2533[0m     +  31.1374
     27   [36m0.1633[0m   [32m0.6814[0m        [35m0.2556[0m       0.9238        [94m0.2521[0m     +  31.0511
     28   [36m0.1671[0m   [32m0.6875[0m        [35m0.2548[0m       0.9238        [94m0.2510[0m     +  31.1951
     29   [36m0.1710[0m   [32m0.6932[0m        [35m0.2530[0m       0.9238        [94m0.2500[0m     +  31.3093
     30   [36m0.1750[0m   [32m0.6988[0m        [35m0.2527[0m       0.9214        [94m0.2490[0m     +  31.0420
     31   [36m0.1772[0m   [32m0.7028[0m        [35m0.2505[0m       0.9202        [94m0.2481[0m     +  31.1234
     32   [36m0.1803[0m   [32m0.7066[0m        [35m0.2487[0m       0.9202        [94m0.2475[0m     +  31.3057
     33   [36m0.1822[0m   [32m0.7100[0m        [35m0.2480[0m       0.9202        [94m0.2469[0m     +  31.3904
     34   [36m0.1845[0m   [32m0.7131[0m        [35m0.2472[0m       0.9202        [94m0.2463[0m     +  30.9373
     35   [36m0.1868[0m   [32m0.7164[0m        [35m0.2466[0m       0.9202        [94m0.2457[0m     +  31.2860
     36   [36m0.1882[0m   [32m0.7192[0m        [35m0.2464[0m       0.9202        [94m0.2451[0m     +  31.2130
     37   [36m0.1891[0m   [32m0.7214[0m        [35m0.2452[0m       0.9202        [94m0.2448[0m     +  31.1939
     38   [36m0.1918[0m   [32m0.7239[0m        0.2452       0.9202        [94m0.2443[0m     +  31.1422
     39   [36m0.1942[0m   [32m0.7263[0m        [35m0.2440[0m       0.9202        [94m0.2438[0m     +  31.2862
     40   [36m0.1968[0m   [32m0.7289[0m        [35m0.2431[0m       0.9202        [94m0.2435[0m     +  31.3665
     41   [36m0.1994[0m   [32m0.7312[0m        [35m0.2428[0m       0.9202        [94m0.2430[0m     +  31.0369
     42   [36m0.2013[0m   [32m0.7331[0m        [35m0.2425[0m       0.9202        [94m0.2426[0m     +  31.2053
     43   [36m0.2033[0m   [32m0.7350[0m        [35m0.2412[0m       0.9202        [94m0.2424[0m     +  31.3046
     44   [36m0.2036[0m   [32m0.7358[0m        0.2413       0.9214        [94m0.2420[0m     +  31.1588
     45   [36m0.2043[0m   [32m0.7367[0m        [35m0.2406[0m       0.9214        [94m0.2418[0m     +  31.2173
     46   [36m0.2049[0m   [32m0.7384[0m        [35m0.2397[0m       0.9214        [94m0.2414[0m     +  31.3754
     47   [36m0.2064[0m   [32m0.7399[0m        0.2404       0.9214        [94m0.2410[0m     +  31.0794
     48   [36m0.2070[0m   [32m0.7406[0m        0.2400       0.9214        [94m0.2409[0m     +  31.3099
     49   [36m0.2078[0m   [32m0.7415[0m        [35m0.2393[0m       0.9214        [94m0.2406[0m     +  31.2538
     50   [36m0.2099[0m   [32m0.7430[0m        [35m0.2383[0m       0.9214        [94m0.2403[0m     +  31.6939
[32m[I 2023-05-04 08:47:56,528][0m Trial 419 finished with value: 0.24030651775568254 and parameters: {'lr': 7.117156976940228e-07, 'dropout': 0.36102896676540136, 'd_model_multiplier': 4, 'num_layers': 7, 'n_heads': 32, 'dim_feedforward': 334, 'batch_size': 42, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.001, 'top_n_features': 69}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 90
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0623[0m   [32m0.3160[0m        [35m0.5968[0m       [31m0.9141[0m        [94m0.5024[0m     +  27.6165
      2   [36m0.0645[0m   [32m0.3294[0m        [35m0.4587[0m       0.9141        [94m0.3986[0m     +  27.4732
      3   [36m0.0735[0m   [32m0.4185[0m        [35m0.3772[0m       0.9141        [94m0.3431[0m     +  27.2715
      4   [36m0.0920[0m   [32m0.5207[0m        [35m0.3290[0m       0.9141        [94m0.3134[0m     +  27.5891
      5   [36m0.1308[0m   [32m0.5869[0m        [35m0.3049[0m       0.9141        [94m0.2967[0m     +  27.5697
      6   [36m0.1878[0m   [32m0.6229[0m        [35m0.2887[0m       0.9141        [94m0.2865[0m     +  27.5783
      7   [36m0.2165[0m   [32m0.6426[0m        [35m0.2795[0m       0.9141        [94m0.2798[0m     +  28.1713
      8   [36m0.2328[0m   [32m0.6562[0m        [35m0.2722[0m       0.9141        [94m0.2752[0m     +  27.7786
      9   [36m0.2546[0m   [32m0.6663[0m        [35m0.2663[0m       0.9141        [94m0.2718[0m     +  27.3733
     10   [36m0.2660[0m   [32m0.6761[0m        [35m0.2613[0m       0.9141        [94m0.2691[0m     +  27.4387
     11   [36m0.2776[0m   [32m0.6828[0m        [35m0.2584[0m       0.9141        [94m0.2671[0m     +  27.5735
     12   [36m0.2862[0m   [32m0.6893[0m        [35m0.2559[0m       0.9141        [94m0.2654[0m     +  27.5470
     13   [36m0.2959[0m   [32m0.6957[0m        [35m0.2522[0m       0.9141        [94m0.2638[0m     +  27.5754
     14   [36m0.3051[0m   [32m0.7012[0m        [35m0.2510[0m       0.9141        [94m0.2622[0m     +  27.5827
     15   [36m0.3182[0m   [32m0.7076[0m        [35m0.2495[0m       0.9141        [94m0.2611[0m     +  27.4903
     16   0.3168   [32m0.7137[0m        [35m0.2474[0m       0.9141        [94m0.2596[0m     +  27.5095
     17   [36m0.3208[0m   [32m0.7192[0m        [35m0.2472[0m       0.9141        [94m0.2583[0m     +  27.6352
     18   [36m0.3390[0m   [32m0.7248[0m        [35m0.2443[0m       0.9141        [94m0.2568[0m     +  27.7567
     19   [36m0.3431[0m   [32m0.7292[0m        [35m0.2441[0m       0.9141        [94m0.2560[0m     +  27.4184
     20   [36m0.3461[0m   [32m0.7339[0m        [35m0.2428[0m       0.9141        [94m0.2548[0m     +  27.4648
     21   [36m0.3474[0m   [32m0.7380[0m        [35m0.2419[0m       0.9141        [94m0.2538[0m     +  27.3237
     22   [36m0.3486[0m   [32m0.7417[0m        [35m0.2410[0m       [31m0.9154[0m        [94m0.2529[0m     +  27.4067
     23   [36m0.3517[0m   [32m0.7460[0m        [35m0.2386[0m       0.9154        [94m0.2519[0m     +  27.5392
     24   0.3510   [32m0.7483[0m        0.2386       0.9154        [94m0.2512[0m     +  27.5370
     25   [36m0.3536[0m   [32m0.7511[0m        0.2388       0.9141        [94m0.2505[0m     +  27.8156
     26   [36m0.3561[0m   [32m0.7539[0m        [35m0.2378[0m       [31m0.9178[0m        [94m0.2499[0m     +  27.3554
     27   0.3558   [32m0.7562[0m        [35m0.2375[0m       0.9166        [94m0.2493[0m     +  27.7346
     28   0.3561   [32m0.7576[0m        0.2377       0.9166        [94m0.2492[0m     +  27.6373
     29   [36m0.3585[0m   [32m0.7596[0m        [35m0.2366[0m       0.9166        [94m0.2488[0m     +  27.3878
     30   0.3513   [32m0.7611[0m        [35m0.2358[0m       [31m0.9190[0m        [94m0.2483[0m     +  27.4505
     31   0.3503   [32m0.7622[0m        [35m0.2352[0m       0.9190        [94m0.2479[0m     +  27.6585
     32   0.3536   [32m0.7642[0m        [35m0.2350[0m       [31m0.9214[0m        [94m0.2476[0m     +  27.4989
     33   0.3509   [32m0.7652[0m        0.2360       0.9214        [94m0.2475[0m     +  27.4613
     34   0.3498   [32m0.7667[0m        [35m0.2332[0m       0.9214        [94m0.2474[0m     +  27.6872
     35   0.3566   [32m0.7669[0m        0.2344       0.9214        [94m0.2473[0m     +  27.5409
     36   0.3572   [32m0.7671[0m        0.2341       0.9214        0.2474        27.4961
     37   [36m0.3602[0m   [32m0.7681[0m        0.2336       0.9214        [94m0.2471[0m     +  27.6138
     38   0.3601   [32m0.7681[0m        [35m0.2329[0m       0.9214        [94m0.2470[0m     +  27.7381
     39   [36m0.3608[0m   [32m0.7689[0m        [35m0.2322[0m       0.9214        [94m0.2470[0m     +  27.6648
     40   0.3602   0.7685        0.2327       0.9214        0.2474        27.5751
     41   0.3537   [32m0.7694[0m        [35m0.2320[0m       0.9214        0.2472        27.7524
     42   0.3547   [32m0.7708[0m        0.2326       [31m0.9226[0m        [94m0.2466[0m     +  27.5077
     43   0.3543   [32m0.7710[0m        0.2320       0.9226        0.2469        27.2417
     44   0.3551   [32m0.7720[0m        [35m0.2312[0m       0.9226        [94m0.2466[0m     +  27.6711
     45   0.3554   0.7718        [35m0.2311[0m       0.9226        0.2470        27.9004
     46   0.3550   [32m0.7723[0m        0.2317       0.9214        0.2470        27.5257
     47   0.3548   [32m0.7734[0m        0.2314       0.9214        0.2466        27.4741
     48   0.3554   [32m0.7737[0m        [35m0.2302[0m       0.9214        0.2468        27.6119
     49   0.3568   [32m0.7741[0m        0.2309       0.9214        0.2467        27.3979
     50   0.3582   [32m0.7746[0m        0.2307       0.9214        0.2467        27.8844
[32m[I 2023-05-04 09:10:59,607][0m Trial 420 finished with value: 0.2466259626164906 and parameters: {'lr': 1.9981693823896037e-06, 'dropout': 0.3878474146529713, 'd_model_multiplier': 4, 'num_layers': 6, 'n_heads': 32, 'dim_feedforward': 300, 'batch_size': 58, 'pos_encoding': 'learnable', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 90}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 56
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0595[0m   [32m0.2857[0m        [35m0.5986[0m       [31m0.9093[0m        [94m0.4645[0m     +  23.6370
      2   [36m0.0669[0m   [32m0.3722[0m        [35m0.3897[0m       0.9093        [94m0.3405[0m     +  23.7051
      3   [36m0.1478[0m   [32m0.5916[0m        [35m0.2939[0m       0.9093        [94m0.3013[0m     +  24.1830
      4   [36m0.2173[0m   [32m0.6691[0m        [35m0.2645[0m       0.9081        [94m0.2888[0m     +  24.2631
      5   [36m0.2509[0m   [32m0.7068[0m        [35m0.2515[0m       0.9093        [94m0.2811[0m     +  23.9383
      6   [36m0.2831[0m   [32m0.7361[0m        [35m0.2444[0m       [31m0.9105[0m        [94m0.2740[0m     +  24.0079
      7   [36m0.2955[0m   [32m0.7555[0m        [35m0.2394[0m       0.9069        [94m0.2693[0m     +  24.0975
      8   [36m0.3045[0m   [32m0.7649[0m        [35m0.2371[0m       0.9081        [94m0.2663[0m     +  24.0948
      9   [36m0.3075[0m   [32m0.7707[0m        [35m0.2347[0m       0.9081        [94m0.2649[0m     +  24.0140
     10   [36m0.3180[0m   [32m0.7745[0m        [35m0.2326[0m       0.9081        [94m0.2639[0m     +  24.1632
     11   [36m0.3254[0m   [32m0.7765[0m        [35m0.2326[0m       0.9081        [94m0.2635[0m     +  24.2279
     12   [36m0.3274[0m   [32m0.7801[0m        [35m0.2318[0m       0.9081        [94m0.2620[0m     +  24.1334
     13   [36m0.3323[0m   [32m0.7825[0m        [35m0.2299[0m       0.9093        [94m0.2612[0m     +  23.8501
     14   [36m0.3353[0m   [32m0.7835[0m        [35m0.2296[0m       0.9093        [94m0.2605[0m     +  24.0004
     15   [36m0.3362[0m   [32m0.7850[0m        0.2296       0.9093        [94m0.2602[0m     +  24.1224
     16   [36m0.3404[0m   [32m0.7867[0m        [35m0.2286[0m       0.9105        [94m0.2600[0m     +  24.2306
     17   0.3384   [32m0.7873[0m        0.2300       0.9105        0.2602        24.2241
     18   0.3372   [32m0.7882[0m        [35m0.2273[0m       0.9105        0.2608        24.0813
     19   0.3375   [32m0.7891[0m        [35m0.2264[0m       0.9105        [94m0.2597[0m     +  24.0786
     20   0.3390   [32m0.7909[0m        0.2276       0.9093        0.2598        24.2561
     21   0.3403   [32m0.7915[0m        0.2280       0.9093        0.2598        23.9121
     22   [36m0.3560[0m   [32m0.7933[0m        [35m0.2256[0m       0.9105        0.2598        23.8626
     23   [36m0.3592[0m   [32m0.7948[0m        0.2278       0.9093        [94m0.2582[0m     +  24.0097
     24   [36m0.3592[0m   [32m0.7963[0m        0.2262       0.9105        0.2585        23.7916
     25   [36m0.3615[0m   [32m0.7980[0m        0.2277       0.9093        [94m0.2568[0m     +  23.9770
     26   [36m0.3622[0m   [32m0.7997[0m        [35m0.2251[0m       0.9093        [94m0.2565[0m     +  23.9784
     27   [36m0.3689[0m   [32m0.8016[0m        0.2272       0.9093        [94m0.2556[0m     +  23.9839
     28   [36m0.3712[0m   [32m0.8030[0m        0.2252       0.9093        0.2573        24.0682
     29   [36m0.3718[0m   [32m0.8031[0m        0.2251       0.9081        0.2562        23.8611
     30   0.3711   [32m0.8035[0m        0.2252       0.9093        0.2559        24.4089
     31   0.3716   [32m0.8052[0m        [35m0.2245[0m       0.9081        [94m0.2549[0m     +  24.1418
     32   [36m0.3774[0m   [32m0.8065[0m        [35m0.2238[0m       0.9093        0.2559        24.2605
     33   [36m0.3795[0m   [32m0.8070[0m        [35m0.2226[0m       0.9069        0.2575        24.0490
     34   [36m0.3797[0m   [32m0.8074[0m        0.2239       0.9045        0.2575        24.2806
     35   0.3759   [32m0.8089[0m        0.2246       0.9081        0.2560        24.3200
     36   [36m0.3808[0m   [32m0.8106[0m        0.2244       0.9057        0.2561        24.0695
     37   [36m0.3818[0m   [32m0.8107[0m        [35m0.2219[0m       0.9057        0.2558        23.9459
     38   0.3808   0.8105        0.2225       0.9057        0.2550        24.1215
     39   [36m0.3831[0m   [32m0.8114[0m        0.2240       0.9057        0.2557        24.0570
     40   [36m0.3850[0m   [32m0.8119[0m        [35m0.2208[0m       0.9045        0.2564        24.1805
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 09:27:29,088][0m Trial 421 finished with value: 0.2549315859105307 and parameters: {'lr': 2.505801496528287e-05, 'dropout': 0.5469997760564393, 'd_model_multiplier': 4, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 306, 'batch_size': 68, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 56}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 64
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0733[0m   [32m0.4314[0m        [35m0.4763[0m       [31m0.9154[0m        [94m0.3160[0m     +  23.9213
      2   [36m0.1923[0m   [32m0.6695[0m        [35m0.2751[0m       0.9154        [94m0.2758[0m     +  23.3129
      3   [36m0.2119[0m   [32m0.7333[0m        [35m0.2475[0m       [31m0.9166[0m        [94m0.2721[0m     +  23.9611
      4   [36m0.2137[0m   [32m0.7545[0m        [35m0.2354[0m       0.9093        0.2723        24.1823
      5   [36m0.2200[0m   [32m0.7663[0m        [35m0.2323[0m       0.9033        [94m0.2686[0m     +  24.0521
      6   [36m0.2275[0m   [32m0.7732[0m        [35m0.2282[0m       0.8984        [94m0.2668[0m     +  23.9591
      7   0.2217   [32m0.7763[0m        [35m0.2277[0m       0.8972        [94m0.2645[0m     +  24.1422
      8   0.2259   [32m0.7810[0m        [35m0.2259[0m       0.8984        [94m0.2627[0m     +  23.7491
      9   [36m0.2299[0m   [32m0.7852[0m        0.2260       0.8900        0.2643        23.6596
     10   0.2261   [32m0.7859[0m        [35m0.2249[0m       0.8960        [94m0.2626[0m     +  23.9652
     11   0.2217   [32m0.7860[0m        [35m0.2245[0m       0.8936        [94m0.2623[0m     +  23.9888
     12   0.2282   [32m0.7923[0m        0.2245       0.8960        [94m0.2610[0m     +  23.8031
     13   0.2239   [32m0.7928[0m        [35m0.2240[0m       0.8948        0.2620        23.7075
     14   [36m0.2301[0m   [32m0.7939[0m        [35m0.2219[0m       0.8960        [94m0.2596[0m     +  23.7779
     15   0.2267   [32m0.7958[0m        0.2223       0.8960        [94m0.2595[0m     +  23.8192
     16   0.2273   [32m0.7967[0m        [35m0.2215[0m       0.8984        0.2604        23.9063
     17   0.2264   0.7960        [35m0.2189[0m       0.8948        0.2608        23.8311
     18   0.2259   [32m0.7975[0m        0.2194       0.8960        0.2632        24.0090
     19   0.2270   [32m0.7995[0m        [35m0.2177[0m       0.8948        0.2631        24.1817
     20   0.2207   0.7934        0.2177       0.8948        0.2649        24.5023
     21   0.2225   0.7936        [35m0.2175[0m       0.8972        0.2628        23.9951
     22   0.2220   0.7900        [35m0.2162[0m       0.9008        0.2642        23.8893
     23   0.2180   0.7877        0.2164       0.9008        0.2628        23.8490
     24   0.2217   0.7912        [35m0.2144[0m       0.9008        0.2643        23.8664
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 09:37:28,419][0m Trial 422 finished with value: 0.25948905572745645 and parameters: {'lr': 5.4958073804327814e-05, 'dropout': 0.41799130334644014, 'd_model_multiplier': 4, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 351, 'batch_size': 59, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0, 'top_n_features': 64}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 73
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
Warning, assumed runtime error: CUDA out of memory. Tried to allocate 158.00 MiB (GPU 0; 23.70 GiB total capacity; 22.33 GiB already allocated; 125.25 MiB free; 22.48 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[32m[I 2023-05-04 09:37:32,388][0m Trial 423 finished with value: 100.0 and parameters: {'lr': 1.2051234622898379e-05, 'dropout': 0.34859570948596735, 'd_model_multiplier': 64, 'num_layers': 7, 'n_heads': 32, 'dim_feedforward': 313, 'batch_size': 64, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 73}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 61
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0748[0m   [32m0.3596[0m        [35m0.5761[0m       [31m0.9154[0m        [94m0.3970[0m     +  34.3862
      2   [36m0.1714[0m   [32m0.6384[0m        [35m0.3113[0m       0.9154        [94m0.2813[0m     +  34.5237
      3   [36m0.1997[0m   [32m0.6928[0m        [35m0.2542[0m       0.9105        [94m0.2703[0m     +  34.3103
      4   [36m0.2016[0m   [32m0.6988[0m        [35m0.2379[0m       0.9093        0.2756        34.2924
      5   [36m0.2030[0m   [32m0.7098[0m        [35m0.2295[0m       0.9057        0.2778        34.6828
      6   [36m0.2077[0m   [32m0.7108[0m        [35m0.2258[0m       0.9057        0.2795        34.5795
      7   [36m0.2122[0m   [32m0.7174[0m        [35m0.2256[0m       0.9069        0.2777        34.8098
      8   [36m0.2146[0m   [32m0.7192[0m        [35m0.2233[0m       0.9105        0.2782        35.0640
      9   [36m0.2195[0m   [32m0.7241[0m        [35m0.2232[0m       0.9081        0.2787        34.8811
     10   [36m0.2209[0m   [32m0.7251[0m        [35m0.2215[0m       0.9117        0.2766        35.2080
     11   [36m0.2235[0m   [32m0.7279[0m        [35m0.2208[0m       0.9093        0.2752        34.7694
     12   [36m0.2249[0m   [32m0.7308[0m        [35m0.2199[0m       0.9069        0.2754        34.9525
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 09:45:04,381][0m Trial 424 finished with value: 0.270277843834267 and parameters: {'lr': 4.135307241362399e-05, 'dropout': 0.302094602453015, 'd_model_multiplier': 4, 'num_layers': 8, 'n_heads': 32, 'dim_feedforward': 325, 'batch_size': 76, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.001, 'top_n_features': 61}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 56
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.0696[0m   [32m0.4059[0m        [35m0.6578[0m       [31m0.8791[0m        [94m0.6228[0m     +  9.5011
      2   0.0629   0.3608        [35m0.6167[0m       [31m0.9287[0m        [94m0.5763[0m     +  9.0522
      3   0.0618   0.3439        [35m0.5807[0m       0.9287        [94m0.5373[0m     +  9.6884
      4   0.0694   0.3315        [35m0.5522[0m       0.9287        [94m0.5041[0m     +  9.7717
      5   0.0689   0.3248        [35m0.5237[0m       0.9287        [94m0.4759[0m     +  9.7703
      6   0.0688   0.3233        [35m0.4990[0m       0.9287        [94m0.4522[0m     +  9.5548
      7   0.0689   0.3231        [35m0.4807[0m       0.9287        [94m0.4318[0m     +  9.8739
      8   0.0688   0.3223        [35m0.4595[0m       0.9287        [94m0.4144[0m     +  9.9733
      9   0.0688   0.3225        [35m0.4442[0m       0.9287        [94m0.3985[0m     +  9.5287
     10   0.0688   0.3232        [35m0.4300[0m       0.9287        [94m0.3839[0m     +  9.3705
     11   0.0689   0.3249        [35m0.4166[0m       0.9287        [94m0.3708[0m     +  9.5071
     12   0.0689   0.3268        [35m0.4058[0m       0.9287        [94m0.3592[0m     +  9.8870
     13   0.0691   0.3303        [35m0.3962[0m       0.9287        [94m0.3483[0m     +  9.4488
     14   0.0695   0.3360        [35m0.3847[0m       0.9287        [94m0.3392[0m     +  10.3515
     15   [36m0.0700[0m   0.3451        [35m0.3762[0m       0.9287        [94m0.3310[0m     +  9.9524
     16   [36m0.0708[0m   0.3572        [35m0.3696[0m       0.9287        [94m0.3236[0m     +  9.9548
     17   [36m0.0716[0m   0.3693        [35m0.3620[0m       0.9287        [94m0.3168[0m     +  9.8763
     18   [36m0.0729[0m   0.3858        [35m0.3558[0m       0.9287        [94m0.3106[0m     +  9.7183
     19   [36m0.0740[0m   0.3991        [35m0.3486[0m       0.9287        [94m0.3050[0m     +  9.4782
     20   [36m0.0754[0m   [32m0.4148[0m        [35m0.3436[0m       0.9287        [94m0.3001[0m     +  9.6757
     21   [36m0.0765[0m   [32m0.4271[0m        [35m0.3370[0m       0.9287        [94m0.2955[0m     +  9.9565
     22   [36m0.0778[0m   [32m0.4389[0m        [35m0.3324[0m       0.9287        [94m0.2913[0m     +  10.2022
     23   [36m0.0793[0m   [32m0.4505[0m        [35m0.3275[0m       0.9287        [94m0.2876[0m     +  9.8165
     24   [36m0.0806[0m   [32m0.4604[0m        [35m0.3257[0m       0.9287        [94m0.2841[0m     +  9.8168
     25   [36m0.0826[0m   [32m0.4725[0m        [35m0.3201[0m       0.9287        [94m0.2809[0m     +  9.7961
     26   [36m0.0849[0m   [32m0.4849[0m        [35m0.3157[0m       0.9287        [94m0.2780[0m     +  9.5237
     27   [36m0.0874[0m   [32m0.4955[0m        [35m0.3128[0m       0.9287        [94m0.2754[0m     +  10.0187
     28   [36m0.0899[0m   [32m0.5069[0m        [35m0.3080[0m       0.9287        [94m0.2730[0m     +  9.9402
     29   0.0856   [32m0.5176[0m        [35m0.3070[0m       0.9287        [94m0.2708[0m     +  9.9518
     30   [36m0.0912[0m   [32m0.5246[0m        [35m0.3053[0m       0.9287        [94m0.2687[0m     +  9.7198
     31   0.0909   [32m0.5333[0m        [35m0.2997[0m       0.9287        [94m0.2669[0m     +  9.8027
     32   [36m0.0940[0m   [32m0.5415[0m        [35m0.2991[0m       0.9287        [94m0.2653[0m     +  10.1678
     33   [36m0.0994[0m   [32m0.5474[0m        [35m0.2959[0m       0.9287        [94m0.2637[0m     +  10.0126
     34   [36m0.1038[0m   [32m0.5557[0m        [35m0.2948[0m       0.9287        [94m0.2622[0m     +  9.7979
     35   [36m0.1132[0m   [32m0.5618[0m        [35m0.2933[0m       0.9287        [94m0.2609[0m     +  9.7882
     36   [36m0.1158[0m   [32m0.5671[0m        [35m0.2877[0m       0.9287        [94m0.2596[0m     +  9.6962
     37   [36m0.1208[0m   [32m0.5709[0m        0.2904       0.9287        [94m0.2585[0m     +  9.7613
     38   [36m0.1225[0m   [32m0.5745[0m        0.2886       0.9287        [94m0.2576[0m     +  9.7958
     39   [36m0.1246[0m   [32m0.5790[0m        [35m0.2852[0m       0.9287        [94m0.2567[0m     +  9.6883
     40   [36m0.1257[0m   [32m0.5822[0m        [35m0.2838[0m       0.9287        [94m0.2558[0m     +  9.6554
     41   [36m0.1273[0m   [32m0.5876[0m        [35m0.2823[0m       0.9287        [94m0.2551[0m     +  9.6544
     42   [36m0.1297[0m   [32m0.5928[0m        0.2833       0.9287        [94m0.2543[0m     +  9.7201
     43   [36m0.1306[0m   [32m0.5958[0m        [35m0.2799[0m       0.9287        [94m0.2537[0m     +  9.8133
     44   [36m0.1325[0m   [32m0.6013[0m        0.2801       0.9287        [94m0.2531[0m     +  9.6199
     45   [36m0.1349[0m   [32m0.6056[0m        [35m0.2790[0m       0.9287        [94m0.2526[0m     +  9.9743
     46   [36m0.1353[0m   [32m0.6079[0m        [35m0.2783[0m       0.9287        [94m0.2521[0m     +  10.1150
     47   [36m0.1369[0m   [32m0.6104[0m        [35m0.2740[0m       0.9287        [94m0.2516[0m     +  9.7878
     48   [36m0.1384[0m   [32m0.6135[0m        0.2747       0.9287        [94m0.2511[0m     +  9.7003
     49   [36m0.1385[0m   [32m0.6150[0m        [35m0.2722[0m       0.9287        [94m0.2507[0m     +  10.1463
     50   [36m0.1397[0m   [32m0.6165[0m        0.2765       0.9287        [94m0.2504[0m     +  9.3089
[32m[I 2023-05-04 09:53:17,331][0m Trial 425 finished with value: 0.2503787665892541 and parameters: {'lr': 3.5457502697731885e-06, 'dropout': 0.6376777843437504, 'd_model_multiplier': 4, 'num_layers': 6, 'n_heads': 4, 'dim_feedforward': 372, 'batch_size': 45, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 56}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 86
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
Warning, assumed runtime error: CUDA out of memory. Tried to allocate 1.52 GiB (GPU 0; 23.70 GiB total capacity; 18.31 GiB already allocated; 121.25 MiB free; 22.49 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[32m[I 2023-05-04 09:53:22,491][0m Trial 426 finished with value: 100.0 and parameters: {'lr': 2.005823205396406e-05, 'dropout': 0.4313018532003105, 'd_model_multiplier': 4, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 341, 'batch_size': 128, 'pos_encoding': 'learnable', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 86}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 68
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2673[0m   [32m0.7837[0m        [35m0.2581[0m       [31m0.9045[0m        [94m0.2500[0m     +  27.4325
      2   [36m0.2844[0m   [32m0.8064[0m        [35m0.2437[0m       [31m0.9117[0m        [94m0.2367[0m     +  27.6621
      3   [36m0.2956[0m   0.8015        [35m0.2384[0m       [31m0.9166[0m        [94m0.2312[0m     +  27.5706
      4   [36m0.3199[0m   [32m0.8104[0m        [35m0.2359[0m       [31m0.9190[0m        [94m0.2282[0m     +  27.6784
      5   0.3121   0.8057        [35m0.2307[0m       [31m0.9202[0m        0.2299        28.0184
      6   0.2890   0.8029        [35m0.2295[0m       0.9154        0.2358        27.7549
      7   0.2619   0.7923        0.2308       0.9117        0.2351        27.5121
      8   0.2689   0.8020        [35m0.2270[0m       0.9154        0.2329        27.7123
      9   0.3053   [32m0.8210[0m        [35m0.2237[0m       0.9202        [94m0.2261[0m     +  27.6863
     10   0.3143   0.8176        [35m0.2224[0m       0.9178        [94m0.2246[0m     +  27.5797
     11   0.3186   0.8165        [35m0.2185[0m       0.9166        0.2279        27.7549
     12   0.3005   0.8148        0.2222       0.9178        0.2286        27.8155
     13   0.3143   0.8164        [35m0.2178[0m       0.9190        [94m0.2226[0m     +  27.6334
     14   0.3101   0.8160        [35m0.2141[0m       0.9166        0.2323        27.7184
     15   [36m0.3247[0m   [32m0.8234[0m        0.2157       0.9202        0.2286        27.9119
     16   0.2665   0.8105        [35m0.2109[0m       0.9081        0.2427        27.6166
     17   0.2771   0.8158        [35m0.2095[0m       0.9129        0.2445        27.5211
     18   0.2664   0.8078        [35m0.2085[0m       0.9117        0.2613        27.8122
     19   0.2582   0.8141        [35m0.2068[0m       0.9129        0.2514        27.7717
     20   0.2586   0.8163        [35m0.2062[0m       0.9129        0.2539        27.7708
     21   0.2585   0.8133        [35m0.2037[0m       0.9117        0.2609        27.6254
     22   0.2521   0.8216        [35m0.1999[0m       0.9178        0.2441        27.5706
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 10:04:00,657][0m Trial 427 finished with value: 0.22262495775519 and parameters: {'lr': 0.0002079272006000459, 'dropout': 0.3366733258046219, 'd_model_multiplier': 4, 'num_layers': 6, 'n_heads': 32, 'dim_feedforward': 362, 'batch_size': 27, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 68}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 127
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1545[0m   [32m0.6962[0m        [35m0.3148[0m       [31m0.9274[0m        [94m0.2488[0m     +  30.7608
      2   [36m0.1751[0m   [32m0.7182[0m        [35m0.2394[0m       0.9214        0.2549        30.6593
      3   [36m0.1873[0m   [32m0.7254[0m        [35m0.2365[0m       0.9154        0.2516        30.5341
      4   0.1803   [32m0.7292[0m        [35m0.2351[0m       0.9178        0.2506        30.6635
      5   [36m0.1928[0m   [32m0.7408[0m        [35m0.2325[0m       0.9178        [94m0.2475[0m     +  30.9815
      6   [36m0.2020[0m   [32m0.7472[0m        [35m0.2274[0m       0.9190        0.2504        30.8161
      7   [36m0.2169[0m   0.7331        [35m0.2239[0m       0.9202        0.2489        30.7108
      8   0.1784   0.7238        [35m0.2223[0m       0.9250        0.2518        30.8436
      9   [36m0.2262[0m   0.7068        [35m0.2191[0m       0.9250        0.2496        30.8565
     10   [36m0.2302[0m   0.7166        0.2194       0.9238        [94m0.2452[0m     +  30.8971
     11   0.1940   0.7072        [35m0.2157[0m       0.9178        0.2644        30.5647
     12   0.1789   0.7253        [35m0.2126[0m       0.9214        0.2544        30.8133
     13   0.2129   0.7162        [35m0.2077[0m       0.9105        0.2632        30.6243
     14   0.1970   0.7066        [35m0.2039[0m       0.9081        0.2678        30.7009
     15   0.1935   0.7227        [35m0.1997[0m       0.9129        0.2702        30.6613
     16   0.2103   0.7352        0.2027       0.9190        0.2648        30.8191
     17   0.2149   0.7263        [35m0.1909[0m       0.9190        0.2616        31.1580
     18   0.1915   0.7168        0.1983       0.9033        0.2814        31.1118
     19   0.1665   0.6997        0.1921       0.9008        0.2884        31.0920
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 10:14:18,419][0m Trial 428 finished with value: 0.24520914831357538 and parameters: {'lr': 0.0001120470927284956, 'dropout': 0.2792925470605569, 'd_model_multiplier': 16, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 388, 'batch_size': 52, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.001, 'top_n_features': 127}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 50
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2499[0m   [32m0.7428[0m        [35m0.3340[0m       [31m0.9214[0m        [94m0.2408[0m     +  20.4236
      2   [36m0.3158[0m   [32m0.7572[0m        [35m0.2406[0m       [31m0.9274[0m        [94m0.2370[0m     +  20.3085
      3   [36m0.3461[0m   [32m0.7584[0m        [35m0.2346[0m       0.9262        0.2381        20.5570
      4   [36m0.3529[0m   0.7570        [35m0.2313[0m       0.9250        0.2397        20.5382
      5   0.3519   0.7546        [35m0.2302[0m       0.9262        0.2406        20.3953
      6   [36m0.3530[0m   0.7526        [35m0.2296[0m       0.9250        0.2408        20.3277
      7   0.3524   0.7521        [35m0.2277[0m       0.9238        0.2417        20.4408
      8   0.3468   0.7525        [35m0.2260[0m       0.9202        0.2441        20.4017
      9   0.3497   0.7529        [35m0.2252[0m       0.9202        0.2429        20.2328
     10   0.3504   0.7529        0.2256       0.9214        0.2430        20.4556
     11   [36m0.3531[0m   0.7545        [35m0.2247[0m       0.9214        0.2443        20.3355
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 10:18:23,671][0m Trial 429 finished with value: 0.23703233883271338 and parameters: {'lr': 1.0476936351704473e-05, 'dropout': 0.2582922950626735, 'd_model_multiplier': 4, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 329, 'batch_size': 20, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 50}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 63
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2244[0m   [32m0.7158[0m        [35m0.3741[0m       [31m0.9335[0m        [94m0.2207[0m     +  54.1043
      2   [36m0.3124[0m   [32m0.7454[0m        [35m0.2413[0m       0.9274        [94m0.2169[0m     +  53.9296
      3   0.3057   [32m0.7551[0m        [35m0.2363[0m       0.9311        [94m0.2157[0m     +  54.2237
      4   [36m0.3144[0m   [32m0.7614[0m        [35m0.2334[0m       0.9274        [94m0.2133[0m     +  54.5807
      5   [36m0.3270[0m   [32m0.7641[0m        [35m0.2323[0m       0.9311        [94m0.2128[0m     +  54.0938
      6   0.3239   [32m0.7713[0m        [35m0.2308[0m       0.9323        [94m0.2109[0m     +  55.1826
      7   [36m0.3281[0m   [32m0.7774[0m        [35m0.2290[0m       0.9335        [94m0.2091[0m     +  54.3527
      8   [36m0.3336[0m   [32m0.7818[0m        [35m0.2260[0m       0.9335        [94m0.2080[0m     +  54.3395
      9   [36m0.3451[0m   [32m0.7869[0m        [35m0.2259[0m       0.9335        [94m0.2053[0m     +  54.2480
     10   [36m0.3555[0m   [32m0.7918[0m        [35m0.2247[0m       0.9335        0.2064        54.2432
     11   0.3426   [32m0.7944[0m        [35m0.2236[0m       [31m0.9371[0m        0.2088        54.3322
     12   [36m0.3679[0m   [32m0.7977[0m        [35m0.2216[0m       0.9347        0.2062        54.2913
     13   0.3622   [32m0.8032[0m        [35m0.2201[0m       0.9359        [94m0.2022[0m     +  54.4036
     14   0.3493   [32m0.8063[0m        [35m0.2178[0m       0.9347        0.2044        54.1130
     15   0.3517   [32m0.8147[0m        [35m0.2161[0m       [31m0.9383[0m        [94m0.1977[0m     +  54.2569
     16   0.3514   0.8137        [35m0.2156[0m       0.9371        0.1990        54.5675
     17   0.3470   [32m0.8155[0m        [35m0.2138[0m       0.9323        0.2017        54.5379
     18   0.3428   [32m0.8181[0m        [35m0.2135[0m       0.9335        0.2003        54.1524
     19   0.3525   [32m0.8200[0m        [35m0.2107[0m       0.9335        0.2018        54.1415
     20   0.3385   0.8190        [35m0.2083[0m       0.9323        0.2021        54.3274
     21   0.3273   0.8182        [35m0.2064[0m       0.9335        0.2054        54.2404
     22   0.3388   [32m0.8201[0m        [35m0.2063[0m       0.9359        0.2003        54.3519
     23   0.3428   0.8180        [35m0.2040[0m       0.9347        0.2033        54.3739
     24   0.3471   [32m0.8209[0m        [35m0.2026[0m       0.9347        0.2053        54.2698
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 10:41:11,524][0m Trial 430 finished with value: 0.19771506832833735 and parameters: {'lr': 2.9464918152635055e-05, 'dropout': 0.4505212528424989, 'd_model_multiplier': 32, 'num_layers': 6, 'n_heads': 32, 'dim_feedforward': 335, 'batch_size': 69, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 63}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 62
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
Warning, assumed runtime error: CUDA out of memory. Tried to allocate 848.00 MiB (GPU 0; 23.70 GiB total capacity; 20.59 GiB already allocated; 565.25 MiB free; 22.05 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[32m[I 2023-05-04 10:41:15,607][0m Trial 431 finished with value: 100.0 and parameters: {'lr': 2.6311713185285052e-05, 'dropout': 0.45119750475204884, 'd_model_multiplier': 64, 'num_layers': 6, 'n_heads': 32, 'dim_feedforward': 337, 'batch_size': 70, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 62}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 66
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2469[0m   [32m0.7175[0m        [35m0.3495[0m       [31m0.9105[0m        [94m0.2763[0m     +  53.5132
      2   [36m0.3114[0m   [32m0.7560[0m        [35m0.2341[0m       [31m0.9117[0m        [94m0.2655[0m     +  54.0552
      3   [36m0.3387[0m   [32m0.7673[0m        [35m0.2311[0m       0.9105        [94m0.2635[0m     +  53.7399
      4   [36m0.3463[0m   [32m0.7717[0m        [35m0.2283[0m       [31m0.9141[0m        [94m0.2616[0m     +  54.2072
      5   [36m0.3481[0m   [32m0.7746[0m        [35m0.2268[0m       0.9129        [94m0.2602[0m     +  54.0495
      6   [36m0.3603[0m   [32m0.7746[0m        [35m0.2261[0m       0.9141        0.2615        53.9128
      7   [36m0.3693[0m   [32m0.7783[0m        [35m0.2234[0m       [31m0.9154[0m        0.2631        54.3508
      8   [36m0.3809[0m   [32m0.7836[0m        0.2247       0.9141        [94m0.2582[0m     +  54.1312
      9   0.3694   [32m0.7882[0m        [35m0.2231[0m       [31m0.9166[0m        0.2614        54.1125
     10   0.3735   [32m0.7926[0m        [35m0.2204[0m       0.9129        0.2628        54.1886
     11   [36m0.3850[0m   0.7912        [35m0.2195[0m       0.9141        0.2601        54.0257
     12   0.3767   0.7835        [35m0.2187[0m       [31m0.9178[0m        0.2637        54.1582
     13   [36m0.4003[0m   0.7802        [35m0.2163[0m       0.9154        0.2669        54.6022
     14   0.3903   [32m0.7941[0m        [35m0.2148[0m       0.9129        0.2629        54.1145
     15   0.3755   0.7933        [35m0.2121[0m       0.9117        0.2638        54.1190
     16   0.3736   0.7877        0.2135       0.9154        0.2667        54.2501
     17   0.3611   0.7868        [35m0.2091[0m       0.9129        0.2690        54.0291
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 10:57:34,830][0m Trial 432 finished with value: 0.25818670350954637 and parameters: {'lr': 3.371308048016912e-05, 'dropout': 0.4427522440973415, 'd_model_multiplier': 32, 'num_layers': 6, 'n_heads': 32, 'dim_feedforward': 321, 'batch_size': 66, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 66}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 57
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0461[0m   [32m0.2707[0m        [35m0.5939[0m       [31m0.9299[0m        [94m0.4842[0m     +  10.8596
      2   [36m0.0483[0m   [32m0.3080[0m        [35m0.4364[0m       0.9299        [94m0.3450[0m     +  10.7699
      3   [36m0.0741[0m   [32m0.5149[0m        [35m0.3308[0m       0.9299        [94m0.2746[0m     +  10.9449
      4   [36m0.1388[0m   [32m0.6345[0m        [35m0.2808[0m       0.9299        [94m0.2487[0m     +  10.8933
      5   [36m0.1726[0m   [32m0.6835[0m        [35m0.2636[0m       0.9287        [94m0.2369[0m     +  10.8281
      6   [36m0.2104[0m   [32m0.7125[0m        [35m0.2547[0m       0.9262        [94m0.2298[0m     +  11.1723
      7   [36m0.2343[0m   [32m0.7299[0m        [35m0.2456[0m       0.9299        [94m0.2266[0m     +  10.8181
      8   [36m0.2402[0m   [32m0.7349[0m        [35m0.2421[0m       0.9287        [94m0.2266[0m     +  10.8581
      9   [36m0.2430[0m   [32m0.7400[0m        [35m0.2372[0m       0.9250        0.2272        10.9707
     10   [36m0.2502[0m   [32m0.7480[0m        [35m0.2357[0m       0.9262        [94m0.2265[0m     +  11.0695
     11   [36m0.2542[0m   [32m0.7531[0m        [35m0.2339[0m       0.9262        [94m0.2259[0m     +  11.0077
     12   [36m0.2717[0m   [32m0.7581[0m        0.2343       0.9250        [94m0.2243[0m     +  10.9504
     13   0.2703   [32m0.7607[0m        [35m0.2328[0m       0.9250        [94m0.2235[0m     +  10.8495
     14   [36m0.2737[0m   [32m0.7631[0m        0.2331       0.9274        [94m0.2233[0m     +  10.8534
     15   [36m0.2868[0m   [32m0.7677[0m        [35m0.2321[0m       0.9238        [94m0.2231[0m     +  10.9418
     16   [36m0.3030[0m   [32m0.7683[0m        [35m0.2306[0m       0.9250        [94m0.2221[0m     +  11.0212
     17   [36m0.3101[0m   [32m0.7699[0m        [35m0.2293[0m       0.9250        [94m0.2218[0m     +  11.2870
     18   [36m0.3117[0m   [32m0.7718[0m        0.2299       0.9238        0.2220        11.1893
     19   [36m0.3120[0m   [32m0.7731[0m        [35m0.2287[0m       0.9226        0.2224        10.9628
     20   [36m0.3143[0m   [32m0.7742[0m        0.2288       0.9238        [94m0.2210[0m     +  10.9697
     21   [36m0.3163[0m   [32m0.7754[0m        0.2294       0.9250        [94m0.2202[0m     +  11.1624
     22   [36m0.3181[0m   [32m0.7764[0m        [35m0.2283[0m       0.9274        [94m0.2196[0m     +  10.9208
     23   0.3178   [32m0.7780[0m        [35m0.2280[0m       0.9274        [94m0.2187[0m     +  10.8193
     24   [36m0.3204[0m   [32m0.7787[0m        [35m0.2261[0m       0.9274        0.2190        10.8499
     25   0.3204   [32m0.7796[0m        0.2288       0.9262        0.2188        10.9011
     26   [36m0.3221[0m   [32m0.7797[0m        0.2272       0.9262        [94m0.2184[0m     +  10.7961
     27   [36m0.3259[0m   [32m0.7814[0m        0.2262       0.9262        [94m0.2181[0m     +  11.0174
     28   0.3233   [32m0.7821[0m        [35m0.2261[0m       0.9287        [94m0.2171[0m     +  11.0754
     29   0.3257   [32m0.7832[0m        0.2269       0.9287        0.2175        11.0195
     30   [36m0.3323[0m   [32m0.7836[0m        [35m0.2243[0m       0.9287        [94m0.2167[0m     +  11.0199
     31   0.3319   [32m0.7838[0m        0.2259       [31m0.9323[0m        [94m0.2158[0m     +  10.9527
     32   0.3293   [32m0.7844[0m        0.2250       0.9311        0.2170        10.9940
     33   0.3319   [32m0.7857[0m        [35m0.2235[0m       0.9287        0.2160        10.9615
     34   [36m0.3330[0m   [32m0.7866[0m        0.2239       0.9311        [94m0.2155[0m     +  11.0864
     35   [36m0.3347[0m   0.7859        [35m0.2225[0m       [31m0.9335[0m        [94m0.2153[0m     +  10.7791
     36   [36m0.3350[0m   0.7852        0.2242       0.9299        0.2167        11.1055
     37   [36m0.3372[0m   0.7864        [35m0.2221[0m       0.9335        [94m0.2150[0m     +  10.8285
     38   [36m0.3426[0m   [32m0.7868[0m        0.2226       0.9311        [94m0.2150[0m     +  10.8835
     39   [36m0.3498[0m   [32m0.7884[0m        [35m0.2216[0m       0.9311        [94m0.2144[0m     +  10.8763
     40   [36m0.3604[0m   0.7880        [35m0.2213[0m       0.9311        [94m0.2133[0m     +  11.0896
     41   0.3541   [32m0.7890[0m        0.2223       0.9299        0.2143        10.9703
     42   0.3584   [32m0.7894[0m        0.2236       0.9323        [94m0.2131[0m     +  10.9314
     43   [36m0.3671[0m   [32m0.7910[0m        [35m0.2208[0m       0.9323        [94m0.2125[0m     +  10.8832
     44   0.3620   0.7899        0.2214       0.9323        0.2130        10.9885
     45   0.3616   0.7884        0.2216       0.9311        0.2136        10.7959
     46   0.3640   0.7903        [35m0.2207[0m       0.9323        0.2134        11.0286
     47   0.3643   0.7908        [35m0.2199[0m       0.9335        0.2133        10.9406
     48   [36m0.3698[0m   0.7905        [35m0.2192[0m       [31m0.9347[0m        0.2134        11.1220
     49   0.3669   0.7909        [35m0.2170[0m       0.9311        0.2137        11.1908
     50   0.3694   [32m0.7912[0m        0.2196       0.9347        0.2129        11.0218
[32m[I 2023-05-04 11:06:47,033][0m Trial 433 finished with value: 0.21251246689885242 and parameters: {'lr': 1.5411646875766804e-05, 'dropout': 0.4243253430054782, 'd_model_multiplier': 32, 'num_layers': 6, 'n_heads': 4, 'dim_feedforward': 350, 'batch_size': 72, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 57}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 73
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2799[0m   [32m0.7671[0m        [35m0.3225[0m       [31m0.9226[0m        [94m0.2432[0m     +  53.8423
      2   0.2796   [32m0.7901[0m        [35m0.2383[0m       0.9214        [94m0.2392[0m     +  54.0150
      3   [36m0.2924[0m   [32m0.7965[0m        [35m0.2323[0m       0.9226        [94m0.2343[0m     +  54.0971
      4   [36m0.3002[0m   0.7947        [35m0.2298[0m       0.9202        0.2365        54.4212
      5   0.2833   0.7906        [35m0.2289[0m       0.9202        0.2416        54.2316
      6   0.2882   0.7831        [35m0.2278[0m       0.9214        0.2412        54.1204
      7   0.2863   0.7901        [35m0.2246[0m       0.9202        0.2398        54.0601
      8   [36m0.3005[0m   0.7933        0.2246       0.9202        0.2375        54.1729
      9   0.2807   [32m0.7998[0m        [35m0.2200[0m       0.9178        0.2369        54.1421
     10   0.2942   [32m0.8040[0m        [35m0.2178[0m       0.9190        [94m0.2341[0m     +  54.1513
     11   0.2927   [32m0.8053[0m        [35m0.2148[0m       0.9202        0.2376        54.5542
     12   0.2813   0.7968        [35m0.2142[0m       0.9202        0.2422        54.7513
     13   0.2785   [32m0.8137[0m        [35m0.2099[0m       0.9178        0.2395        54.3323
     14   0.2859   0.7922        0.2101       0.9190        0.2440        54.0660
     15   0.2552   0.7880        [35m0.2084[0m       0.9190        0.2463        54.0655
     16   0.2560   0.7721        [35m0.2049[0m       0.9202        0.2563        54.5377
     17   0.2705   0.7709        [35m0.2023[0m       0.9166        0.2520        53.9873
     18   0.2669   0.7541        0.2029       0.9166        0.2548        54.1782
     19   0.2432   0.7428        [35m0.1978[0m       0.9202        0.2604        54.2213
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 11:24:55,503][0m Trial 434 finished with value: 0.23406272841912515 and parameters: {'lr': 4.946191281135119e-05, 'dropout': 0.3998130278612864, 'd_model_multiplier': 32, 'num_layers': 6, 'n_heads': 32, 'dim_feedforward': 332, 'batch_size': 62, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 73}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 94
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
Warning, assumed runtime error: CUDA out of memory. Tried to allocate 994.00 MiB (GPU 0; 23.70 GiB total capacity; 20.46 GiB already allocated; 785.25 MiB free; 21.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[32m[I 2023-05-04 11:25:00,101][0m Trial 435 finished with value: 100.0 and parameters: {'lr': 7.783619839293959e-05, 'dropout': 0.43452164251512043, 'd_model_multiplier': 32, 'num_layers': 7, 'n_heads': 32, 'dim_feedforward': 342, 'batch_size': 82, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 94}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 53
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1863[0m   [32m0.6655[0m        [35m0.3770[0m       [31m0.9178[0m        [94m0.2646[0m     +  45.6991
      2   [36m0.2762[0m   [32m0.7352[0m        [35m0.2415[0m       [31m0.9202[0m        [94m0.2497[0m     +  46.2299
      3   [36m0.2991[0m   [32m0.7508[0m        [35m0.2331[0m       [31m0.9214[0m        [94m0.2459[0m     +  46.0646
      4   [36m0.3162[0m   [32m0.7602[0m        [35m0.2300[0m       0.9202        [94m0.2455[0m     +  46.5435
      5   [36m0.3360[0m   [32m0.7638[0m        [35m0.2285[0m       0.9190        [94m0.2448[0m     +  46.4829
      6   [36m0.3407[0m   [32m0.7653[0m        [35m0.2269[0m       0.9129        0.2486        46.5560
      7   [36m0.3611[0m   [32m0.7670[0m        [35m0.2268[0m       0.9178        0.2449        46.4538
      8   0.3599   [32m0.7708[0m        [35m0.2244[0m       0.9117        0.2499        45.9128
      9   0.3515   [32m0.7729[0m        [35m0.2223[0m       0.9141        0.2482        46.1478
     10   0.3492   [32m0.7770[0m        [35m0.2209[0m       0.9154        0.2488        46.4743
     11   0.3420   [32m0.7777[0m        [35m0.2204[0m       0.9166        0.2518        46.3166
     12   0.3322   [32m0.7795[0m        [35m0.2180[0m       0.9154        0.2545        46.3760
     13   0.3270   [32m0.7799[0m        [35m0.2180[0m       0.9178        0.2502        46.2912
     14   0.3081   0.7766        [35m0.2150[0m       0.9154        0.2549        46.1775
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 11:36:38,234][0m Trial 436 finished with value: 0.2448158120373861 and parameters: {'lr': 2.1819114414988103e-05, 'dropout': 0.37644222950332173, 'd_model_multiplier': 32, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 359, 'batch_size': 76, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 53}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 61
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1595[0m   [32m0.6866[0m        [35m0.3790[0m       [31m0.9274[0m        [94m0.2569[0m     +  54.0530
      2   [36m0.2551[0m   [32m0.7448[0m        [35m0.2527[0m       0.9274        [94m0.2511[0m     +  54.2286
      3   [36m0.2782[0m   [32m0.7460[0m        [35m0.2476[0m       [31m0.9287[0m        0.2586        54.3644
      4   [36m0.3040[0m   [32m0.7533[0m        [35m0.2405[0m       [31m0.9323[0m        0.2637        54.2859
      5   [36m0.3117[0m   [32m0.7677[0m        [35m0.2402[0m       0.9274        0.2605        54.0653
      6   [36m0.3230[0m   [32m0.7787[0m        [35m0.2392[0m       0.9287        0.2601        54.0061
      7   0.3050   0.7769        [35m0.2380[0m       0.9287        0.2581        54.5928
      8   0.3093   0.7775        [35m0.2371[0m       0.9274        0.2680        54.2901
      9   0.3063   [32m0.7808[0m        [35m0.2356[0m       0.9238        0.2581        54.3903
     10   0.3084   0.7751        [35m0.2345[0m       0.9311        0.2652        54.3877
     11   0.3011   0.7781        0.2372       0.9287        0.2609        54.3823
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 11:47:31,994][0m Trial 437 finished with value: 0.25114575586086896 and parameters: {'lr': 3.737425606939871e-05, 'dropout': 0.6810717595590248, 'd_model_multiplier': 32, 'num_layers': 6, 'n_heads': 32, 'dim_feedforward': 320, 'batch_size': 57, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 61}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 78
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0483[0m   [32m0.2556[0m        [35m0.5610[0m       [31m0.9250[0m        [94m0.4159[0m     +  62.0743
      2   [36m0.0683[0m   [32m0.4818[0m        [35m0.3691[0m       0.9250        [94m0.2853[0m     +  62.0383
      3   [36m0.2427[0m   [32m0.7057[0m        [35m0.2865[0m       0.9250        [94m0.2434[0m     +  62.6332
      4   [36m0.2681[0m   [32m0.7453[0m        [35m0.2607[0m       0.9250        [94m0.2315[0m     +  62.3599
      5   [36m0.3091[0m   [32m0.7667[0m        [35m0.2506[0m       0.9250        [94m0.2258[0m     +  62.7320
      6   [36m0.3492[0m   [32m0.7817[0m        [35m0.2442[0m       [31m0.9262[0m        [94m0.2212[0m     +  62.5452
      7   [36m0.3639[0m   [32m0.7887[0m        [35m0.2408[0m       0.9262        [94m0.2190[0m     +  62.5937
      8   [36m0.3900[0m   [32m0.7916[0m        [35m0.2387[0m       [31m0.9311[0m        [94m0.2179[0m     +  62.4655
      9   0.3886   [32m0.7951[0m        [35m0.2365[0m       [31m0.9323[0m        [94m0.2178[0m     +  62.5397
     10   [36m0.3940[0m   [32m0.7980[0m        [35m0.2346[0m       0.9323        [94m0.2172[0m     +  62.5221
     11   0.3921   [32m0.8005[0m        [35m0.2336[0m       0.9323        [94m0.2165[0m     +  62.4443
     12   0.3870   [32m0.8021[0m        [35m0.2323[0m       0.9323        [94m0.2158[0m     +  63.0156
     13   0.3880   [32m0.8039[0m        [35m0.2307[0m       0.9323        [94m0.2154[0m     +  62.4405
     14   0.3899   [32m0.8060[0m        0.2309       [31m0.9335[0m        [94m0.2147[0m     +  62.4417
     15   0.3829   [32m0.8074[0m        [35m0.2291[0m       0.9323        0.2151        62.3690
     16   0.3801   [32m0.8097[0m        [35m0.2280[0m       0.9323        0.2147        62.5422
     17   0.3821   [32m0.8115[0m        0.2284       0.9311        [94m0.2142[0m     +  62.5770
     18   0.3894   [32m0.8134[0m        [35m0.2273[0m       0.9274        [94m0.2131[0m     +  62.6908
     19   0.3879   [32m0.8142[0m        0.2273       0.9287        [94m0.2130[0m     +  63.0205
     20   0.3782   [32m0.8159[0m        [35m0.2267[0m       0.9287        0.2131        62.5656
     21   0.3764   [32m0.8168[0m        [35m0.2262[0m       0.9287        0.2131        62.5295
     22   0.3805   0.8164        [35m0.2248[0m       0.9274        [94m0.2128[0m     +  62.5438
     23   0.3773   [32m0.8177[0m        0.2255       0.9250        [94m0.2124[0m     +  62.4564
     24   0.3792   [32m0.8185[0m        [35m0.2230[0m       0.9274        0.2130        62.6986
     25   0.3785   [32m0.8185[0m        0.2241       0.9250        0.2129        62.6447
     26   0.3713   [32m0.8188[0m        0.2242       0.9262        0.2134        62.4337
     27   0.3656   [32m0.8205[0m        [35m0.2220[0m       0.9262        [94m0.2123[0m     +  62.5399
     28   0.3627   0.8203        0.2229       0.9274        0.2134        62.4102
     29   0.3574   0.8200        [35m0.2219[0m       0.9299        0.2135        62.3803
     30   0.3659   [32m0.8219[0m        [35m0.2214[0m       0.9287        0.2131        62.1931
     31   0.3578   0.8209        [35m0.2198[0m       0.9299        0.2141        62.5265
     32   0.3645   0.8216        0.2208       0.9299        0.2135        62.2042
     33   0.3674   [32m0.8229[0m        0.2210       0.9299        0.2124        62.5535
     34   0.3601   [32m0.8233[0m        0.2207       0.9323        0.2125        62.6935
     35   0.3614   [32m0.8235[0m        0.2199       0.9299        0.2126        62.4720
     36   0.3625   [32m0.8237[0m        [35m0.2178[0m       0.9299        [94m0.2121[0m     +  62.5199
     37   0.3670   [32m0.8250[0m        0.2192       0.9299        0.2125        62.4336
     38   0.3579   0.8238        0.2191       0.9299        0.2126        62.5806
     39   0.3601   0.8240        [35m0.2171[0m       0.9299        0.2127        62.5285
     40   0.3569   0.8249        [35m0.2165[0m       0.9311        0.2125        62.2972
     41   0.3581   [32m0.8252[0m        0.2173       0.9287        0.2128        62.6032
     42   0.3532   [32m0.8257[0m        0.2175       0.9311        0.2122        62.4835
     43   0.3505   0.8252        0.2166       0.9287        0.2136        62.7156
     44   0.3516   [32m0.8261[0m        [35m0.2160[0m       0.9299        0.2129        63.3784
     45   0.3484   0.8252        [35m0.2158[0m       0.9274        0.2136        62.2988
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 12:35:50,053][0m Trial 438 finished with value: 0.21214225434190273 and parameters: {'lr': 2.3713912447141494e-06, 'dropout': 0.4576666746285412, 'd_model_multiplier': 32, 'num_layers': 7, 'n_heads': 32, 'dim_feedforward': 335, 'batch_size': 68, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 78}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 68
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0494[0m   [32m0.3503[0m        [35m0.5299[0m       [31m0.9347[0m        [94m0.3083[0m     +  19.7231
      2   [36m0.1366[0m   [32m0.6457[0m        [35m0.2999[0m       0.9347        [94m0.2334[0m     +  20.2018
      3   [36m0.2001[0m   [32m0.6949[0m        [35m0.2635[0m       0.9347        [94m0.2203[0m     +  20.4496
      4   [36m0.2429[0m   [32m0.7417[0m        [35m0.2494[0m       0.9335        [94m0.2129[0m     +  20.2500
      5   [36m0.2718[0m   [32m0.7738[0m        [35m0.2423[0m       0.9347        [94m0.2096[0m     +  20.0204
      6   [36m0.2882[0m   [32m0.7852[0m        [35m0.2378[0m       0.9335        [94m0.2079[0m     +  20.0159
      7   [36m0.2970[0m   [32m0.7901[0m        0.2380       0.9323        [94m0.2067[0m     +  20.3170
      8   [36m0.3015[0m   [32m0.7936[0m        [35m0.2351[0m       [31m0.9359[0m        [94m0.2054[0m     +  20.2604
      9   [36m0.3163[0m   [32m0.7979[0m        0.2354       0.9335        [94m0.2029[0m     +  20.1547
     10   [36m0.3228[0m   [32m0.8009[0m        [35m0.2345[0m       0.9335        [94m0.2011[0m     +  20.4041
     11   [36m0.3304[0m   [32m0.8027[0m        [35m0.2345[0m       0.9359        [94m0.1998[0m     +  20.4231
     12   0.3244   [32m0.8042[0m        [35m0.2338[0m       0.9347        [94m0.1993[0m     +  20.1568
     13   [36m0.3316[0m   [32m0.8075[0m        [35m0.2332[0m       0.9359        [94m0.1982[0m     +  20.2972
     14   [36m0.3356[0m   [32m0.8100[0m        [35m0.2325[0m       0.9359        [94m0.1978[0m     +  20.0618
     15   [36m0.3398[0m   [32m0.8131[0m        [35m0.2302[0m       0.9347        [94m0.1973[0m     +  20.3425
     16   0.3326   [32m0.8139[0m        0.2317       0.9347        [94m0.1967[0m     +  20.3917
     17   [36m0.3425[0m   [32m0.8145[0m        0.2303       0.9347        [94m0.1956[0m     +  20.3503
     18   [36m0.3430[0m   [32m0.8163[0m        [35m0.2297[0m       0.9347        0.1958        20.3234
     19   0.3417   0.8151        [35m0.2287[0m       0.9347        [94m0.1952[0m     +  20.4221
     20   [36m0.3460[0m   0.8149        0.2304       0.9347        [94m0.1951[0m     +  20.4128
     21   0.3444   [32m0.8173[0m        0.2296       0.9347        0.1951        20.1997
     22   0.3428   0.8169        [35m0.2279[0m       0.9335        0.1958        20.0389
     23   0.3400   [32m0.8175[0m        [35m0.2276[0m       0.9347        [94m0.1949[0m     +  20.3382
     24   0.3338   [32m0.8196[0m        [35m0.2258[0m       0.9347        0.1950        19.9553
     25   0.3433   [32m0.8221[0m        0.2286       0.9335        [94m0.1931[0m     +  20.0606
     26   0.3339   0.8209        0.2279       0.9335        0.1943        20.3119
     27   [36m0.3561[0m   [32m0.8241[0m        [35m0.2257[0m       0.9347        [94m0.1920[0m     +  20.3062
     28   0.3424   [32m0.8241[0m        0.2275       0.9347        0.1935        20.0752
     29   0.3513   [32m0.8251[0m        0.2258       0.9359        0.1924        20.3375
     30   0.3495   0.8238        [35m0.2250[0m       0.9359        0.1923        20.1506
     31   0.3470   [32m0.8252[0m        [35m0.2228[0m       0.9359        0.1925        20.7090
     32   0.3483   [32m0.8265[0m        0.2239       [31m0.9371[0m        [94m0.1917[0m     +  20.7968
     33   [36m0.3610[0m   0.8255        0.2239       0.9359        [94m0.1913[0m     +  20.3497
     34   0.3551   [32m0.8269[0m        0.2255       0.9371        0.1923        20.2427
     35   0.3542   [32m0.8278[0m        [35m0.2206[0m       0.9359        0.1925        20.7071
     36   0.3555   0.8258        0.2232       0.9371        0.1932        20.2368
     37   0.3540   0.8229        0.2211       0.9371        0.1926        20.1634
     38   0.3417   0.8245        [35m0.2198[0m       0.9371        0.1945        20.1507
     39   0.3188   0.8215        0.2202       0.9371        0.1963        20.3638
     40   0.3410   0.8184        [35m0.2191[0m       [31m0.9395[0m        0.1932        20.1855
     41   0.3365   0.8214        [35m0.2187[0m       0.9359        0.1941        20.2708
     42   [36m0.3612[0m   0.8194        0.2199       0.9359        0.1920        20.2480
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 12:50:23,833][0m Trial 439 finished with value: 0.1912960457326135 and parameters: {'lr': 5.962406085716384e-05, 'dropout': 0.5574665958812296, 'd_model_multiplier': 4, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 348, 'batch_size': 63, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 68}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 70
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.0604[0m   [32m0.3328[0m        [35m0.6391[0m       [31m0.9287[0m        [94m0.5806[0m     +  9.4147
      2   [36m0.0614[0m   0.3298        [35m0.5548[0m       0.9287        [94m0.4791[0m     +  9.2436
      3   0.0574   0.3279        [35m0.4669[0m       0.9287        [94m0.3944[0m     +  10.0588
      4   0.0572   [32m0.3375[0m        [35m0.3994[0m       0.9287        [94m0.3411[0m     +  9.7501
      5   0.0591   [32m0.3777[0m        [35m0.3528[0m       0.9287        [94m0.3088[0m     +  9.7383
      6   [36m0.0650[0m   [32m0.4355[0m        [35m0.3228[0m       0.9287        [94m0.2875[0m     +  9.6294
      7   [36m0.0873[0m   [32m0.4810[0m        [35m0.3025[0m       0.9287        [94m0.2744[0m     +  9.6867
      8   [36m0.1078[0m   [32m0.5156[0m        [35m0.2890[0m       0.9287        [94m0.2666[0m     +  9.5868
      9   [36m0.1290[0m   [32m0.5368[0m        [35m0.2797[0m       0.9287        [94m0.2616[0m     +  9.5838
     10   [36m0.1326[0m   [32m0.5531[0m        [35m0.2730[0m       0.9287        [94m0.2586[0m     +  9.7577
     11   [36m0.1391[0m   [32m0.5697[0m        [35m0.2698[0m       0.9287        [94m0.2557[0m     +  9.7130
     12   [36m0.1551[0m   [32m0.5878[0m        [35m0.2675[0m       0.9287        [94m0.2534[0m     +  10.0515
     13   [36m0.1729[0m   [32m0.6072[0m        [35m0.2604[0m       0.9287        [94m0.2506[0m     +  9.7903
     14   [36m0.1803[0m   [32m0.6324[0m        [35m0.2600[0m       0.9287        [94m0.2468[0m     +  9.5475
     15   [36m0.1946[0m   [32m0.6522[0m        [35m0.2530[0m       [31m0.9299[0m        [94m0.2441[0m     +  9.8343
     16   [36m0.2030[0m   [32m0.6646[0m        [35m0.2486[0m       0.9274        [94m0.2436[0m     +  9.8773
     17   [36m0.2197[0m   0.6621        [35m0.2462[0m       0.9262        [94m0.2436[0m     +  9.8560
     18   [36m0.2368[0m   [32m0.6828[0m        [35m0.2445[0m       0.9238        [94m0.2428[0m     +  10.1141
     19   [36m0.2455[0m   0.6816        [35m0.2439[0m       0.9238        [94m0.2418[0m     +  9.7011
     20   [36m0.2514[0m   [32m0.6921[0m        [35m0.2424[0m       0.9262        [94m0.2407[0m     +  9.5744
     21   [36m0.2545[0m   [32m0.6924[0m        [35m0.2412[0m       0.9287        [94m0.2397[0m     +  9.6985
     22   [36m0.2599[0m   [32m0.6953[0m        0.2422       0.9287        [94m0.2389[0m     +  9.8351
     23   [36m0.2618[0m   [32m0.7026[0m        [35m0.2400[0m       0.9287        [94m0.2373[0m     +  9.9397
     24   [36m0.2685[0m   [32m0.7058[0m        [35m0.2396[0m       0.9287        [94m0.2366[0m     +  10.1935
     25   [36m0.2693[0m   0.7043        [35m0.2388[0m       [31m0.9311[0m        0.2371        9.8725
     26   0.2681   [32m0.7087[0m        [35m0.2360[0m       0.9299        [94m0.2361[0m     +  10.1121
     27   0.2684   [32m0.7093[0m        [35m0.2357[0m       0.9299        0.2362        9.8256
     28   0.2688   [32m0.7095[0m        0.2368       0.9287        0.2371        10.0804
     29   0.2659   0.7091        [35m0.2351[0m       0.9287        0.2367        10.2731
     30   0.2654   [32m0.7138[0m        0.2372       0.9287        [94m0.2358[0m     +  10.3097
     31   0.2651   0.7124        [35m0.2346[0m       0.9287        0.2367        9.5796
     32   0.2690   [32m0.7139[0m        0.2355       0.9287        0.2363        9.5028
     33   0.2673   [32m0.7174[0m        0.2347       0.9287        0.2363        9.4828
     34   0.2684   [32m0.7182[0m        [35m0.2329[0m       0.9287        0.2364        9.7448
     35   0.2692   0.7180        0.2354       0.9287        0.2366        9.9716
     36   0.2686   0.7178        0.2359       0.9274        0.2368        10.0170
     37   0.2689   [32m0.7194[0m        0.2343       0.9274        0.2361        9.8895
     38   0.2660   [32m0.7208[0m        [35m0.2322[0m       0.9287        0.2366        9.8272
     39   [36m0.2704[0m   0.7198        0.2325       0.9274        0.2360        9.8609
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 12:56:58,124][0m Trial 440 finished with value: 0.2357677437247936 and parameters: {'lr': 6.133586047789549e-05, 'dropout': 0.5562370290713359, 'd_model_multiplier': 4, 'num_layers': 5, 'n_heads': 4, 'dim_feedforward': 354, 'batch_size': 73, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 70}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 58
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0612[0m   [32m0.3061[0m        [35m0.5092[0m       [31m0.9141[0m        [94m0.3665[0m     +  27.0220
      2   [36m0.2372[0m   [32m0.6680[0m        [35m0.3135[0m       0.9141        [94m0.2737[0m     +  27.4280
      3   [36m0.2977[0m   [32m0.7275[0m        [35m0.2653[0m       0.9141        [94m0.2588[0m     +  27.8957
      4   [36m0.3034[0m   [32m0.7561[0m        [35m0.2519[0m       0.9141        [94m0.2535[0m     +  27.6839
      5   [36m0.3202[0m   [32m0.7843[0m        [35m0.2458[0m       [31m0.9154[0m        [94m0.2448[0m     +  26.9986
      6   [36m0.3374[0m   [32m0.7971[0m        [35m0.2396[0m       0.9093        [94m0.2423[0m     +  27.9063
      7   0.3313   [32m0.8034[0m        [35m0.2352[0m       0.9105        0.2439        27.9120
      8   [36m0.3405[0m   [32m0.8076[0m        [35m0.2322[0m       0.9057        0.2443        27.7073
      9   0.3297   [32m0.8103[0m        [35m0.2321[0m       0.9045        0.2460        27.5441
     10   0.3335   [32m0.8113[0m        [35m0.2320[0m       0.9045        0.2447        27.4302
     11   0.3311   [32m0.8120[0m        [35m0.2289[0m       0.9057        0.2484        27.3421
     12   0.3289   [32m0.8137[0m        0.2302       0.9069        0.2462        27.7551
     13   0.3256   0.8134        [35m0.2287[0m       0.9093        0.2476        27.3092
     14   0.3318   [32m0.8145[0m        0.2299       0.9069        0.2458        27.4881
     15   0.3281   0.8143        0.2292       0.9081        0.2463        27.3499
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 13:04:19,440][0m Trial 441 finished with value: 0.24225193404445047 and parameters: {'lr': 4.062679624327049e-05, 'dropout': 0.5399498779931504, 'd_model_multiplier': 4, 'num_layers': 6, 'n_heads': 32, 'dim_feedforward': 346, 'batch_size': 65, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.001, 'top_n_features': 58}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 66
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0576[0m   [32m0.3094[0m        [35m0.5164[0m       [31m0.9166[0m        [94m0.3867[0m     +  23.6078
      2   [36m0.1147[0m   [32m0.5678[0m        [35m0.3297[0m       0.9166        [94m0.2905[0m     +  24.9422
      3   [36m0.2088[0m   [32m0.6637[0m        [35m0.2698[0m       0.9166        [94m0.2709[0m     +  23.9683
      4   [36m0.2400[0m   [32m0.6967[0m        [35m0.2533[0m       0.9166        [94m0.2652[0m     +  23.8639
      5   [36m0.2508[0m   [32m0.7292[0m        [35m0.2453[0m       0.9141        [94m0.2592[0m     +  23.8456
      6   [36m0.2640[0m   [32m0.7560[0m        [35m0.2404[0m       [31m0.9178[0m        [94m0.2544[0m     +  23.7627
      7   [36m0.2786[0m   [32m0.7635[0m        [35m0.2356[0m       0.9178        [94m0.2528[0m     +  24.0086
      8   [36m0.2940[0m   [32m0.7639[0m        [35m0.2332[0m       0.9166        [94m0.2527[0m     +  23.7710
      9   [36m0.3036[0m   [32m0.7669[0m        [35m0.2313[0m       0.9141        [94m0.2516[0m     +  23.5933
     10   [36m0.3087[0m   [32m0.7679[0m        [35m0.2287[0m       0.9117        [94m0.2510[0m     +  24.0978
     11   [36m0.3118[0m   [32m0.7701[0m        [35m0.2286[0m       0.9081        [94m0.2496[0m     +  23.5965
     12   [36m0.3159[0m   [32m0.7721[0m        [35m0.2267[0m       0.9081        [94m0.2488[0m     +  24.2987
     13   [36m0.3181[0m   [32m0.7723[0m        0.2275       0.9093        [94m0.2479[0m     +  23.6769
     14   0.3174   [32m0.7754[0m        [35m0.2266[0m       0.9069        [94m0.2474[0m     +  23.8878
     15   [36m0.3204[0m   [32m0.7771[0m        [35m0.2240[0m       0.9081        [94m0.2469[0m     +  23.9332
     16   [36m0.3224[0m   [32m0.7783[0m        [35m0.2239[0m       0.9093        [94m0.2464[0m     +  23.9947
     17   [36m0.3256[0m   [32m0.7788[0m        0.2256       0.9093        [94m0.2457[0m     +  23.7990
     18   [36m0.3331[0m   [32m0.7789[0m        0.2258       0.9093        [94m0.2451[0m     +  24.0969
     19   [36m0.3349[0m   [32m0.7822[0m        [35m0.2231[0m       0.9069        [94m0.2436[0m     +  23.5615
     20   [36m0.3357[0m   0.7810        [35m0.2230[0m       0.9069        0.2451        23.6469
     21   0.3330   0.7810        [35m0.2207[0m       0.9081        0.2461        24.0141
     22   [36m0.3396[0m   [32m0.7846[0m        0.2227       0.9081        0.2449        23.9243
     23   [36m0.3428[0m   [32m0.7863[0m        0.2215       0.9117        0.2438        23.7380
     24   0.3427   [32m0.7867[0m        [35m0.2204[0m       0.9117        0.2443        24.1884
     25   0.3400   0.7862        [35m0.2192[0m       0.9129        0.2448        23.9345
     26   [36m0.3445[0m   [32m0.7896[0m        [35m0.2191[0m       0.9141        [94m0.2430[0m     +  24.0092
     27   0.3432   [32m0.7898[0m        [35m0.2190[0m       0.9129        0.2433        24.0483
     28   [36m0.3486[0m   [32m0.7904[0m        [35m0.2177[0m       0.9141        [94m0.2423[0m     +  23.8179
     29   0.3449   0.7885        [35m0.2161[0m       0.9129        0.2445        23.7345
     30   0.3447   [32m0.7912[0m        0.2173       0.9129        0.2427        24.0586
     31   0.3476   [32m0.7917[0m        [35m0.2151[0m       0.9105        0.2428        23.6243
     32   0.3445   0.7914        0.2165       0.9117        0.2445        23.9850
     33   0.3411   [32m0.7920[0m        0.2162       0.9129        0.2425        23.8085
     34   [36m0.3496[0m   0.7907        0.2156       0.9141        [94m0.2419[0m     +  23.8660
     35   0.3449   0.7894        [35m0.2136[0m       0.9129        0.2431        23.9308
     36   0.3427   0.7879        [35m0.2134[0m       0.9129        0.2453        23.7640
     37   0.3476   0.7906        [35m0.2127[0m       0.9129        0.2439        23.9536
     38   0.3441   0.7908        [35m0.2113[0m       0.9141        0.2439        24.6922
     39   0.3487   0.7881        0.2118       0.9166        0.2442        23.7226
     40   0.3346   0.7842        0.2127       [31m0.9190[0m        0.2463        23.8470
     41   0.3461   0.7866        [35m0.2101[0m       0.9166        0.2451        23.8261
     42   0.3431   0.7853        0.2118       0.9178        0.2463        24.0310
     43   0.3396   0.7848        [35m0.2090[0m       0.9190        0.2455        23.6991
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 13:21:54,120][0m Trial 442 finished with value: 0.24192791324032262 and parameters: {'lr': 2.6142420549080384e-05, 'dropout': 0.40629838063516993, 'd_model_multiplier': 4, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 369, 'batch_size': 61, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 66}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 54
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0635[0m   [32m0.3681[0m        [35m0.5452[0m       [31m0.9190[0m        [94m0.3608[0m     +  20.1098
      2   [36m0.2372[0m   [32m0.6806[0m        [35m0.3095[0m       0.9190        [94m0.2631[0m     +  20.3044
      3   [36m0.3222[0m   [32m0.7363[0m        [35m0.2597[0m       [31m0.9202[0m        [94m0.2486[0m     +  20.4477
      4   [36m0.3562[0m   [32m0.7497[0m        [35m0.2458[0m       0.9190        [94m0.2407[0m     +  20.3763
      5   [36m0.3712[0m   [32m0.7543[0m        [35m0.2387[0m       [31m0.9226[0m        [94m0.2374[0m     +  20.6166
      6   0.3657   [32m0.7556[0m        [35m0.2353[0m       [31m0.9250[0m        0.2375        20.5879
      7   [36m0.3749[0m   [32m0.7591[0m        [35m0.2350[0m       0.9226        0.2379        20.4110
      8   [36m0.3776[0m   [32m0.7619[0m        [35m0.2318[0m       0.9214        0.2379        20.1901
      9   0.3734   [32m0.7633[0m        [35m0.2312[0m       0.9202        0.2383        20.3498
     10   0.3743   [32m0.7636[0m        [35m0.2307[0m       0.9202        0.2411        20.2120
     11   0.3678   [32m0.7647[0m        [35m0.2298[0m       0.9214        0.2413        20.4413
     12   0.3690   [32m0.7655[0m        [35m0.2287[0m       0.9202        0.2403        20.4260
     13   0.3698   0.7653        [35m0.2277[0m       0.9214        0.2412        20.3970
     14   0.3656   0.7650        [35m0.2267[0m       0.9214        0.2416        20.4465
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 13:27:00,653][0m Trial 443 finished with value: 0.23737364293874536 and parameters: {'lr': 4.605403724500836e-05, 'dropout': 0.4438111270405422, 'd_model_multiplier': 4, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 354, 'batch_size': 68, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 54}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 75
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
Warning, assumed runtime error: CUDA out of memory. Tried to allocate 1.29 GiB (GPU 0; 23.70 GiB total capacity; 21.91 GiB already allocated; 473.25 MiB free; 22.14 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[32m[I 2023-05-04 13:27:05,739][0m Trial 444 finished with value: 100.0 and parameters: {'lr': 8.592523887356206e-05, 'dropout': 0.5676466303844685, 'd_model_multiplier': 4, 'num_layers': 7, 'n_heads': 32, 'dim_feedforward': 365, 'batch_size': 109, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 75}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 61
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
Warning, assumed runtime error: CUDA out of memory. Tried to allocate 922.00 MiB (GPU 0; 23.70 GiB total capacity; 18.50 GiB already allocated; 509.25 MiB free; 22.11 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[32m[I 2023-05-04 13:27:09,260][0m Trial 445 finished with value: 100.0 and parameters: {'lr': 5.5006530920667423e-05, 'dropout': 0.5535407729209102, 'd_model_multiplier': 4, 'num_layers': 9, 'n_heads': 32, 'dim_feedforward': 345, 'batch_size': 76, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.001, 'top_n_features': 61}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 50
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0543[0m   [32m0.3081[0m        [35m0.5431[0m       [31m0.9226[0m        [94m0.3706[0m     +  26.7982
      2   [36m0.1890[0m   [32m0.6621[0m        [35m0.3174[0m       0.9226        [94m0.2600[0m     +  27.3496
      3   [36m0.2692[0m   [32m0.7174[0m        [35m0.2637[0m       0.9226        [94m0.2432[0m     +  27.0630
      4   [36m0.3146[0m   [32m0.7499[0m        [35m0.2497[0m       0.9226        [94m0.2359[0m     +  27.0611
      5   [36m0.3279[0m   [32m0.7725[0m        [35m0.2414[0m       [31m0.9274[0m        [94m0.2316[0m     +  27.5051
      6   0.3193   [32m0.7817[0m        [35m0.2372[0m       0.9202        [94m0.2308[0m     +  27.1863
      7   0.3187   [32m0.7842[0m        [35m0.2349[0m       0.9178        [94m0.2307[0m     +  27.5869
      8   0.3099   [32m0.7870[0m        [35m0.2323[0m       0.9190        [94m0.2306[0m     +  27.6951
      9   0.3045   [32m0.7882[0m        [35m0.2300[0m       0.9190        0.2312        27.7025
     10   0.2973   [32m0.7901[0m        [35m0.2300[0m       0.9190        0.2311        27.6023
     11   0.2931   [32m0.7901[0m        [35m0.2287[0m       0.9190        0.2317        27.3051
     12   0.2913   [32m0.7915[0m        0.2292       0.9190        0.2319        27.4258
     13   0.2953   [32m0.7943[0m        [35m0.2283[0m       0.9178        0.2313        27.6672
     14   0.2960   0.7928        [35m0.2247[0m       0.9190        0.2322        27.5308
     15   0.2974   0.7909        0.2249       0.9202        0.2326        27.5688
     16   0.3002   0.7935        0.2253       0.9190        0.2320        27.4994
     17   0.3070   [32m0.7956[0m        [35m0.2244[0m       0.9214        [94m0.2302[0m     +  27.6278
     18   0.3143   0.7947        [35m0.2235[0m       0.9214        [94m0.2294[0m     +  27.3995
     19   0.3080   0.7933        [35m0.2221[0m       0.9202        0.2318        27.1596
     20   0.3126   0.7948        [35m0.2220[0m       0.9190        0.2311        27.2584
     21   0.3167   0.7918        0.2225       0.9262        0.2317        27.8753
     22   0.3263   0.7935        [35m0.2210[0m       0.9238        0.2309        27.3568
     23   0.3135   0.7939        0.2219       0.9214        0.2320        27.4180
     24   0.3176   [32m0.7973[0m        [35m0.2205[0m       0.9226        [94m0.2288[0m     +  27.8656
     25   0.3238   0.7963        [35m0.2198[0m       0.9202        0.2324        27.5331
     26   [36m0.3280[0m   [32m0.7998[0m        [35m0.2191[0m       0.9202        0.2309        27.4164
     27   0.3228   0.7973        [35m0.2170[0m       0.9202        0.2322        28.2380
     28   [36m0.3356[0m   0.7980        [35m0.2167[0m       0.9202        0.2330        27.5665
     29   [36m0.3370[0m   0.7990        0.2173       0.9214        0.2332        27.5151
     30   0.3344   [32m0.8001[0m        [35m0.2165[0m       0.9214        0.2343        27.2621
     31   0.3350   0.7980        0.2172       0.9154        0.2323        27.4775
     32   0.3285   0.7981        [35m0.2163[0m       0.9202        0.2321        27.5981
     33   0.3263   0.7969        [35m0.2148[0m       0.9202        0.2346        27.3177
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 13:42:44,651][0m Trial 446 finished with value: 0.2288224682698371 and parameters: {'lr': 3.159881097626248e-05, 'dropout': 0.41569780599896455, 'd_model_multiplier': 4, 'num_layers': 6, 'n_heads': 32, 'dim_feedforward': 325, 'batch_size': 61, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 50}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 70
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.0545[0m   [32m0.3798[0m        [35m0.7264[0m       [31m0.1802[0m        [94m0.7261[0m     +  8.6369
      2   0.0485   0.2951        [35m0.6855[0m       [31m0.7703[0m        [94m0.6662[0m     +  9.7668
      3   0.0469   0.2860        [35m0.6271[0m       [31m0.9335[0m        [94m0.5962[0m     +  9.4149
      4   0.0455   0.2753        [35m0.5671[0m       0.9335        [94m0.5287[0m     +  9.9475
      5   0.0462   0.2760        [35m0.5137[0m       0.9335        [94m0.4682[0m     +  9.9786
      6   0.0459   0.2777        [35m0.4665[0m       0.9335        [94m0.4146[0m     +  10.0398
      7   0.0483   0.2825        [35m0.4269[0m       0.9335        [94m0.3715[0m     +  10.7189
      8   0.0495   0.3051        [35m0.3911[0m       0.9335        [94m0.3375[0m     +  9.7554
      9   0.0519   0.3447        [35m0.3649[0m       0.9335        [94m0.3115[0m     +  9.7530
     10   [36m0.0577[0m   [32m0.4082[0m        [35m0.3442[0m       0.9335        [94m0.2916[0m     +  9.9494
     11   [36m0.0668[0m   [32m0.4766[0m        [35m0.3272[0m       0.9335        [94m0.2767[0m     +  9.6252
     12   [36m0.0905[0m   [32m0.5333[0m        [35m0.3156[0m       0.9335        [94m0.2654[0m     +  9.6964
     13   [36m0.1230[0m   [32m0.5659[0m        [35m0.3053[0m       0.9335        [94m0.2568[0m     +  9.4661
     14   [36m0.1428[0m   [32m0.5937[0m        [35m0.2990[0m       0.9335        [94m0.2503[0m     +  9.5767
     15   [36m0.1516[0m   [32m0.6142[0m        [35m0.2923[0m       0.9335        [94m0.2451[0m     +  9.4211
     16   [36m0.1535[0m   [32m0.6324[0m        [35m0.2886[0m       0.9335        [94m0.2407[0m     +  9.7369
     17   [36m0.1578[0m   [32m0.6481[0m        [35m0.2833[0m       0.9335        [94m0.2371[0m     +  10.0101
     18   [36m0.1604[0m   [32m0.6591[0m        [35m0.2801[0m       0.9335        [94m0.2342[0m     +  9.7483
     19   [36m0.1641[0m   [32m0.6681[0m        [35m0.2742[0m       0.9335        [94m0.2318[0m     +  9.9799
     20   [36m0.1678[0m   [32m0.6759[0m        [35m0.2731[0m       0.9335        [94m0.2298[0m     +  10.1664
     21   [36m0.1716[0m   [32m0.6825[0m        [35m0.2700[0m       0.9335        [94m0.2284[0m     +  10.0501
     22   [36m0.1729[0m   [32m0.6868[0m        [35m0.2685[0m       0.9335        [94m0.2272[0m     +  10.0698
     23   0.1715   [32m0.6902[0m        [35m0.2658[0m       0.9335        [94m0.2263[0m     +  10.2991
     24   [36m0.1739[0m   [32m0.6919[0m        0.2679       0.9335        [94m0.2256[0m     +  9.8197
     25   [36m0.1752[0m   [32m0.6952[0m        [35m0.2628[0m       0.9335        [94m0.2250[0m     +  9.8645
     26   [36m0.1758[0m   [32m0.6965[0m        0.2644       0.9335        [94m0.2245[0m     +  9.8883
     27   [36m0.1774[0m   [32m0.6984[0m        [35m0.2625[0m       0.9335        [94m0.2239[0m     +  9.4973
     28   [36m0.1788[0m   0.6984        0.2628       0.9335        [94m0.2235[0m     +  9.8679
     29   0.1780   [32m0.6988[0m        0.2626       0.9335        [94m0.2232[0m     +  9.8918
     30   0.1772   [32m0.6995[0m        [35m0.2604[0m       0.9335        [94m0.2229[0m     +  9.9074
     31   0.1727   0.6988        0.2605       0.9335        [94m0.2226[0m     +  9.8787
     32   0.1757   [32m0.7012[0m        0.2605       0.9335        [94m0.2224[0m     +  10.1206
     33   0.1731   [32m0.7021[0m        [35m0.2589[0m       0.9335        [94m0.2222[0m     +  10.2259
     34   0.1752   [32m0.7053[0m        [35m0.2585[0m       0.9335        [94m0.2219[0m     +  10.6487
     35   0.1779   [32m0.7072[0m        [35m0.2575[0m       0.9335        [94m0.2216[0m     +  10.2323
     36   [36m0.1849[0m   [32m0.7113[0m        [35m0.2571[0m       0.9335        [94m0.2212[0m     +  10.6110
     37   [36m0.1889[0m   [32m0.7138[0m        0.2583       0.9335        [94m0.2208[0m     +  10.7307
     38   [36m0.1924[0m   [32m0.7179[0m        0.2579       0.9335        [94m0.2202[0m     +  10.3468
     39   [36m0.1989[0m   [32m0.7206[0m        [35m0.2557[0m       0.9335        [94m0.2199[0m     +  10.1113
     40   [36m0.2018[0m   [32m0.7226[0m        [35m0.2547[0m       0.9335        [94m0.2196[0m     +  10.2140
     41   [36m0.2023[0m   [32m0.7232[0m        0.2548       0.9335        [94m0.2194[0m     +  10.0867
     42   [36m0.2046[0m   [32m0.7270[0m        0.2550       0.9335        [94m0.2189[0m     +  10.1089
     43   [36m0.2064[0m   0.7268        [35m0.2521[0m       0.9335        [94m0.2189[0m     +  10.0717
     44   [36m0.2278[0m   [32m0.7293[0m        [35m0.2497[0m       0.9323        [94m0.2188[0m     +  9.8617
     45   [36m0.2377[0m   [32m0.7335[0m        0.2524       0.9335        [94m0.2185[0m     +  9.6855
     46   [36m0.2479[0m   [32m0.7411[0m        [35m0.2488[0m       0.9323        [94m0.2180[0m     +  9.7322
     47   [36m0.2489[0m   0.7393        [35m0.2481[0m       0.9323        0.2181        10.5590
     48   [36m0.2600[0m   [32m0.7451[0m        [35m0.2463[0m       0.9299        [94m0.2173[0m     +  10.6471
     49   [36m0.2626[0m   [32m0.7475[0m        [35m0.2453[0m       0.9299        [94m0.2166[0m     +  9.5212
     50   [36m0.2669[0m   0.7471        0.2457       0.9299        [94m0.2163[0m     +  9.4287
[32m[I 2023-05-04 13:51:05,259][0m Trial 447 finished with value: 0.21629390632226936 and parameters: {'lr': 2.039518069998004e-05, 'dropout': 0.535420232460989, 'd_model_multiplier': 4, 'num_layers': 5, 'n_heads': 4, 'dim_feedforward': 310, 'batch_size': 72, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 70}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 65
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2383[0m   [32m0.7269[0m        [35m0.3039[0m       [31m0.9335[0m        [94m0.2912[0m     +  20.2081
      2   [36m0.2458[0m   [32m0.7388[0m        [35m0.2815[0m       0.9335        [94m0.2415[0m     +  20.4646
      3   0.1997   [32m0.7471[0m        [35m0.2734[0m       0.9335        [94m0.2192[0m     +  20.3849
      4   0.2302   [32m0.7700[0m        [35m0.2711[0m       0.9335        [94m0.2144[0m     +  20.7858
      5   0.2202   0.7680        [35m0.2675[0m       0.9335        0.2275        20.8039
      6   0.2307   [32m0.8230[0m        [35m0.2662[0m       0.9335        0.2200        21.0911
      7   0.2351   0.8019        [35m0.2640[0m       0.9335        0.2332        20.6926
      8   0.2435   [32m0.8256[0m        [35m0.2632[0m       0.9335        0.2243        20.8255
      9   0.2441   0.8252        0.2645       0.9335        0.2338        20.6584
     10   [36m0.2477[0m   0.8242        [35m0.2622[0m       0.9335        0.2470        20.3423
     11   [36m0.2887[0m   [32m0.8337[0m        [35m0.2618[0m       0.9335        0.2453        20.7361
     12   0.2402   0.8014        0.2649       0.9335        0.2517        20.7453
     13   0.2484   0.8292        0.2669       0.9335        0.2362        21.0959
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 13:55:55,442][0m Trial 448 finished with value: 0.2144207387189623 and parameters: {'lr': 0.039127799567798105, 'dropout': 0.43336042183011086, 'd_model_multiplier': 4, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 377, 'batch_size': 84, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 65}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 81
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1888[0m   [32m0.6482[0m        [35m0.3772[0m       [31m0.9250[0m        [94m0.2536[0m     +  23.6585
      2   [36m0.2865[0m   [32m0.7327[0m        [35m0.2458[0m       [31m0.9299[0m        [94m0.2347[0m     +  23.7014
      3   [36m0.3038[0m   [32m0.7464[0m        [35m0.2357[0m       0.9299        [94m0.2315[0m     +  23.8163
      4   [36m0.3087[0m   [32m0.7513[0m        [35m0.2305[0m       0.9262        [94m0.2313[0m     +  23.8839
      5   [36m0.3195[0m   [32m0.7518[0m        [35m0.2265[0m       0.9250        0.2330        23.9155
      6   0.3166   0.7437        [35m0.2222[0m       0.9250        0.2393        24.3808
      7   0.3044   0.7230        [35m0.2194[0m       0.9287        0.2406        23.9177
      8   0.3078   0.7342        [35m0.2143[0m       0.9274        0.2408        23.9366
      9   0.3020   0.7193        [35m0.2110[0m       0.9287        0.2465        24.0198
     10   0.3145   0.7276        [35m0.2053[0m       0.9262        0.2469        24.0448
     11   0.3096   0.7263        [35m0.2017[0m       0.9299        0.2508        23.9577
     12   0.3115   0.7335        [35m0.1978[0m       0.9299        0.2536        23.8147
     13   0.3142   0.7372        [35m0.1928[0m       0.9250        0.2595        23.9405
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 14:01:31,135][0m Trial 449 finished with value: 0.23132154842818375 and parameters: {'lr': 0.00012891309440203616, 'dropout': 0.10814598275579052, 'd_model_multiplier': 4, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 358, 'batch_size': 65, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.001, 'top_n_features': 81}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 57
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
Warning, assumed runtime error: CUDA out of memory. Tried to allocate 2.69 GiB (GPU 0; 23.70 GiB total capacity; 19.84 GiB already allocated; 2.31 GiB free; 20.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[32m[I 2023-05-04 14:01:38,422][0m Trial 450 finished with value: 100.0 and parameters: {'lr': 0.00018000485220051555, 'dropout': 0.5218397660033426, 'd_model_multiplier': 4, 'num_layers': 6, 'n_heads': 32, 'dim_feedforward': 343, 'batch_size': 227, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 57}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 61
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0622[0m   [32m0.3315[0m        [35m0.5530[0m       [31m0.9190[0m        [94m0.3909[0m     +  20.2207
      2   [36m0.1508[0m   [32m0.6057[0m        [35m0.3295[0m       0.9190        [94m0.2768[0m     +  20.3311
      3   [36m0.2308[0m   [32m0.6944[0m        [35m0.2651[0m       0.9190        [94m0.2592[0m     +  19.7037
      4   [36m0.2603[0m   [32m0.7323[0m        [35m0.2492[0m       0.9190        [94m0.2517[0m     +  20.0521
      5   [36m0.2791[0m   [32m0.7544[0m        [35m0.2437[0m       0.9166        [94m0.2468[0m     +  19.9637
      6   [36m0.2869[0m   [32m0.7648[0m        [35m0.2365[0m       0.9178        [94m0.2457[0m     +  20.4136
      7   [36m0.2926[0m   [32m0.7714[0m        [35m0.2347[0m       0.9166        [94m0.2446[0m     +  20.1989
      8   [36m0.2955[0m   [32m0.7747[0m        [35m0.2332[0m       0.9190        [94m0.2439[0m     +  20.2192
      9   [36m0.2958[0m   [32m0.7770[0m        [35m0.2312[0m       0.9166        0.2443        20.5000
     10   [36m0.2975[0m   [32m0.7795[0m        [35m0.2308[0m       0.9178        [94m0.2438[0m     +  20.4208
     11   [36m0.3001[0m   [32m0.7832[0m        [35m0.2296[0m       0.9166        [94m0.2430[0m     +  20.5862
     12   0.2964   [32m0.7846[0m        [35m0.2284[0m       0.9166        0.2436        20.5235
     13   [36m0.3020[0m   [32m0.7869[0m        [35m0.2280[0m       0.9166        [94m0.2428[0m     +  20.2635
     14   0.2994   [32m0.7891[0m        [35m0.2270[0m       0.9178        [94m0.2421[0m     +  20.2756
     15   [36m0.3075[0m   0.7886        [35m0.2268[0m       0.9166        [94m0.2414[0m     +  20.3745
     16   [36m0.3077[0m   [32m0.7903[0m        [35m0.2256[0m       0.9166        [94m0.2413[0m     +  20.2367
     17   [36m0.3079[0m   [32m0.7915[0m        [35m0.2245[0m       0.9166        [94m0.2394[0m     +  20.3108
     18   0.3076   [32m0.7936[0m        [35m0.2230[0m       0.9129        0.2406        20.4956
     19   0.3078   [32m0.7952[0m        0.2232       0.9141        [94m0.2389[0m     +  20.3979
     20   [36m0.3150[0m   0.7950        0.2239       0.9141        0.2394        20.0756
     21   [36m0.3151[0m   0.7952        [35m0.2216[0m       0.9154        [94m0.2387[0m     +  20.1796
     22   0.3111   0.7951        [35m0.2211[0m       0.9141        0.2393        20.2897
     23   0.3129   [32m0.7953[0m        [35m0.2188[0m       0.9129        0.2411        20.4766
     24   0.3141   [32m0.7964[0m        [35m0.2176[0m       0.9129        0.2405        20.2801
     25   0.3124   [32m0.7971[0m        0.2185       0.9117        0.2425        21.3069
     26   0.3069   [32m0.7997[0m        [35m0.2169[0m       0.9141        0.2400        20.5435
     27   0.2971   [32m0.8010[0m        [35m0.2155[0m       0.9166        [94m0.2382[0m     +  20.5398
     28   0.2985   0.8008        0.2171       0.9129        [94m0.2378[0m     +  20.4518
     29   0.2889   0.7993        0.2170       0.9129        0.2389        20.2947
     30   0.2931   0.7985        0.2160       0.9141        0.2396        20.6516
     31   0.2905   0.8004        0.2157       0.9154        0.2388        20.9022
     32   0.2892   0.7992        [35m0.2132[0m       0.9178        0.2387        19.9904
     33   0.2950   [32m0.8022[0m        0.2147       0.9141        0.2408        20.3187
     34   0.2912   0.8006        0.2132       0.9166        0.2408        20.1951
     35   0.2911   0.7980        [35m0.2127[0m       0.9190        0.2417        20.2823
     36   0.2907   0.7972        0.2146       0.9178        0.2436        20.2539
     37   0.2890   0.7976        [35m0.2122[0m       0.9178        0.2439        20.4768
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 14:14:33,645][0m Trial 451 finished with value: 0.23778662957620447 and parameters: {'lr': 3.208408223851665e-05, 'dropout': 0.45967489389115507, 'd_model_multiplier': 4, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 447, 'batch_size': 57, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 61}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 196
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
Warning, assumed runtime error: CUDA out of memory. Tried to allocate 1.66 GiB (GPU 0; 23.70 GiB total capacity; 17.68 GiB already allocated; 291.25 MiB free; 22.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[32m[I 2023-05-04 14:14:37,785][0m Trial 452 finished with value: 100.0 and parameters: {'lr': 1.6262758900763565e-05, 'dropout': 0.27204080831261396, 'd_model_multiplier': 4, 'num_layers': 6, 'n_heads': 64, 'dim_feedforward': 327, 'batch_size': 70, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 196}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 101
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
Warning, assumed runtime error: CUDA out of memory. Tried to allocate 1.40 GiB (GPU 0; 23.70 GiB total capacity; 13.72 GiB already allocated; 299.25 MiB free; 22.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[32m[I 2023-05-04 14:14:43,112][0m Trial 453 finished with value: 100.0 and parameters: {'lr': 0.008500720109820408, 'dropout': 0.39252620868093, 'd_model_multiplier': 4, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 338, 'batch_size': 118, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.001, 'top_n_features': 101}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 189
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.3037[0m   [32m0.7378[0m        [35m0.3386[0m       [31m0.9166[0m        [94m0.2867[0m     +  38.1899
      2   [36m0.3242[0m   [32m0.7522[0m        [35m0.2443[0m       [31m0.9178[0m        [94m0.2698[0m     +  37.9060
      3   0.3128   0.7500        [35m0.2394[0m       0.9166        [94m0.2698[0m     +  38.5970
      4   0.3105   0.7448        [35m0.2376[0m       0.9178        0.2748        38.5477
      5   0.3060   0.7450        [35m0.2351[0m       0.9178        [94m0.2628[0m     +  38.9039
      6   [36m0.3242[0m   0.7476        [35m0.2333[0m       [31m0.9190[0m        0.2763        38.6088
      7   [36m0.3386[0m   0.7522        [35m0.2304[0m       0.9190        [94m0.2572[0m     +  38.4189
      8   0.3201   0.7510        [35m0.2242[0m       [31m0.9202[0m        0.2587        38.5609
      9   0.2949   0.7353        0.2249       0.9166        0.2668        38.6294
     10   0.2740   0.7391        [35m0.2214[0m       0.9154        0.2705        38.5110
     11   0.2852   [32m0.7597[0m        0.2242       0.9141        0.2607        38.8507
     12   0.2634   0.7527        0.2217       0.9141        0.2724        38.8697
     13   0.2519   0.7493        [35m0.2209[0m       0.9166        0.2876        38.8660
     14   0.2452   0.7446        [35m0.2160[0m       0.9141        0.2720        39.6850
     15   0.2585   0.7512        [35m0.2120[0m       0.9129        0.2663        39.1893
     16   0.2693   0.7537        0.2122       0.9141        0.2750        38.5525
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 14:25:44,021][0m Trial 454 finished with value: 0.25720177008854866 and parameters: {'lr': 8.34731052824902e-05, 'dropout': 0.44779212652409933, 'd_model_multiplier': 32, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 353, 'batch_size': 80, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 189}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 71
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1612[0m   [32m0.6179[0m        [35m0.7457[0m       [31m0.1245[0m        [94m0.7589[0m     +  23.9320
      2   0.1533   0.6055        [35m0.7404[0m       [31m0.1378[0m        [94m0.7542[0m     +  23.7826
      3   0.1475   0.5898        [35m0.7374[0m       [31m0.1644[0m        [94m0.7481[0m     +  24.2765
      4   0.1404   0.5728        [35m0.7298[0m       [31m0.1959[0m        [94m0.7411[0m     +  24.1297
      5   0.1297   0.5528        [35m0.7242[0m       [31m0.2249[0m        [94m0.7333[0m     +  23.8575
      6   0.1139   0.5307        [35m0.7173[0m       [31m0.2684[0m        [94m0.7249[0m     +  23.6630
      7   0.0959   0.5063        [35m0.7075[0m       [31m0.3265[0m        [94m0.7159[0m     +  24.1661
      8   0.0890   0.4820        [35m0.6999[0m       [31m0.3930[0m        [94m0.7066[0m     +  23.8758
      9   0.0792   0.4588        [35m0.6903[0m       [31m0.4607[0m        [94m0.6969[0m     +  24.0032
     10   0.0760   0.4377        [35m0.6815[0m       [31m0.5224[0m        [94m0.6869[0m     +  23.8113
     11   0.0738   0.4176        [35m0.6724[0m       [31m0.5586[0m        [94m0.6767[0m     +  23.7244
     12   0.0564   0.4008        [35m0.6626[0m       [31m0.6034[0m        [94m0.6663[0m     +  23.8354
     13   0.0545   0.3855        [35m0.6541[0m       [31m0.6723[0m        [94m0.6558[0m     +  23.4446
     14   0.0532   0.3728        [35m0.6441[0m       [31m0.7291[0m        [94m0.6453[0m     +  23.8656
     15   0.0523   0.3624        [35m0.6337[0m       [31m0.7811[0m        [94m0.6348[0m     +  23.8137
     16   0.0516   0.3527        [35m0.6241[0m       [31m0.8186[0m        [94m0.6242[0m     +  23.8251
     17   0.0510   0.3449        [35m0.6149[0m       [31m0.8464[0m        [94m0.6136[0m     +  23.9249
     18   0.0508   0.3400        [35m0.6044[0m       [31m0.8767[0m        [94m0.6032[0m     +  23.7227
     19   0.0505   0.3349        [35m0.5965[0m       [31m0.8984[0m        [94m0.5928[0m     +  23.6646
     20   0.0502   0.3309        [35m0.5875[0m       [31m0.9190[0m        [94m0.5825[0m     +  23.9075
     21   0.0500   0.3267        [35m0.5774[0m       [31m0.9274[0m        [94m0.5723[0m     +  23.9558
     22   0.0498   0.3235        [35m0.5685[0m       0.9274        [94m0.5623[0m     +  23.8609
     23   0.0497   0.3214        [35m0.5601[0m       [31m0.9287[0m        [94m0.5524[0m     +  23.8298
     24   0.0497   0.3204        [35m0.5504[0m       0.9287        [94m0.5427[0m     +  23.9619
     25   0.0496   0.3193        [35m0.5422[0m       0.9287        [94m0.5331[0m     +  23.8870
     26   0.0496   0.3180        [35m0.5344[0m       0.9287        [94m0.5238[0m     +  24.0295
     27   0.0495   0.3168        [35m0.5251[0m       0.9287        [94m0.5146[0m     +  23.9512
     28   0.0496   0.3169        [35m0.5171[0m       0.9287        [94m0.5057[0m     +  24.1094
     29   0.0496   0.3167        [35m0.5099[0m       0.9287        [94m0.4970[0m     +  24.0215
     30   0.0497   0.3168        [35m0.5027[0m       0.9287        [94m0.4885[0m     +  23.6865
     31   0.0497   0.3169        [35m0.4947[0m       0.9287        [94m0.4802[0m     +  23.6760
     32   0.0498   0.3169        [35m0.4868[0m       0.9287        [94m0.4722[0m     +  24.2961
     33   0.0499   0.3180        [35m0.4789[0m       0.9287        [94m0.4644[0m     +  23.7196
     34   0.0500   0.3184        [35m0.4736[0m       0.9287        [94m0.4568[0m     +  23.7401
     35   0.0502   0.3198        [35m0.4660[0m       0.9287        [94m0.4495[0m     +  23.8839
     36   0.0503   0.3209        [35m0.4606[0m       0.9287        [94m0.4424[0m     +  23.7342
     37   0.0504   0.3221        [35m0.4546[0m       0.9287        [94m0.4355[0m     +  24.0491
     38   0.0505   0.3231        [35m0.4489[0m       0.9287        [94m0.4289[0m     +  23.9587
     39   0.0507   0.3254        [35m0.4426[0m       0.9287        [94m0.4226[0m     +  23.9514
     40   0.0509   0.3268        [35m0.4376[0m       0.9287        [94m0.4164[0m     +  23.7112
     41   0.0510   0.3287        [35m0.4318[0m       0.9287        [94m0.4105[0m     +  23.8769
     42   0.0511   0.3306        [35m0.4265[0m       0.9287        [94m0.4047[0m     +  23.8504
     43   0.0513   0.3327        [35m0.4219[0m       0.9287        [94m0.3992[0m     +  24.1012
     44   0.0514   0.3347        [35m0.4159[0m       0.9287        [94m0.3939[0m     +  24.3485
     45   0.0515   0.3369        [35m0.4117[0m       0.9287        [94m0.3888[0m     +  24.2324
     46   0.0517   0.3392        [35m0.4070[0m       0.9287        [94m0.3838[0m     +  23.9732
     47   0.0519   0.3415        [35m0.4017[0m       0.9287        [94m0.3790[0m     +  23.6620
     48   0.0521   0.3444        [35m0.3981[0m       0.9287        [94m0.3744[0m     +  23.8772
     49   0.0523   0.3474        [35m0.3934[0m       0.9287        [94m0.3700[0m     +  23.8943
     50   0.0526   0.3506        [35m0.3888[0m       0.9287        [94m0.3657[0m     +  23.6191
[32m[I 2023-05-04 14:45:44,354][0m Trial 455 finished with value: 0.36574314295020327 and parameters: {'lr': 1.3376969799048533e-07, 'dropout': 0.37437667769892463, 'd_model_multiplier': 4, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 312, 'batch_size': 62, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 71}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 53
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0728[0m   [32m0.3458[0m        [35m0.6982[0m       [31m0.8549[0m        [94m0.6675[0m     +  10.4502
      2   0.0677   0.2722        [35m0.6528[0m       [31m0.9129[0m        [94m0.6025[0m     +  10.4382
      3   0.0666   0.2657        [35m0.5958[0m       0.9129        [94m0.5424[0m     +  10.3514
      4   0.0715   0.2703        [35m0.5418[0m       0.9129        [94m0.4937[0m     +  10.6271
      5   0.0709   0.2759        [35m0.4952[0m       0.9129        [94m0.4555[0m     +  10.9302
      6   0.0690   0.2782        [35m0.4596[0m       0.9129        [94m0.4269[0m     +  11.1168
      7   0.0695   0.2817        [35m0.4264[0m       0.9129        [94m0.4032[0m     +  10.5196
      8   0.0695   0.2899        [35m0.3993[0m       0.9129        [94m0.3836[0m     +  10.4622
      9   0.0706   0.2998        [35m0.3765[0m       0.9129        [94m0.3674[0m     +  10.5815
     10   0.0719   0.3192        [35m0.3592[0m       0.9129        [94m0.3521[0m     +  10.5480
     11   [36m0.0740[0m   [32m0.3470[0m        [35m0.3433[0m       0.9129        [94m0.3391[0m     +  10.6149
     12   [36m0.0776[0m   [32m0.3818[0m        [35m0.3310[0m       0.9129        [94m0.3282[0m     +  10.5150
     13   [36m0.0810[0m   [32m0.4156[0m        [35m0.3191[0m       0.9129        [94m0.3194[0m     +  10.7708
     14   [36m0.0878[0m   [32m0.4506[0m        [35m0.3088[0m       0.9129        [94m0.3121[0m     +  10.7227
     15   [36m0.1003[0m   [32m0.4857[0m        [35m0.3000[0m       0.9129        [94m0.3060[0m     +  10.8554
     16   [36m0.1194[0m   [32m0.5171[0m        [35m0.2949[0m       0.9129        [94m0.3008[0m     +  10.1646
     17   [36m0.1836[0m   [32m0.5476[0m        [35m0.2900[0m       0.9129        [94m0.2961[0m     +  10.6424
     18   [36m0.1938[0m   [32m0.5687[0m        [35m0.2869[0m       0.9129        [94m0.2925[0m     +  10.8455
     19   [36m0.1999[0m   [32m0.5873[0m        [35m0.2817[0m       0.9129        [94m0.2895[0m     +  10.9555
     20   [36m0.2109[0m   [32m0.6053[0m        [35m0.2786[0m       0.9129        [94m0.2866[0m     +  10.2105
     21   [36m0.2185[0m   [32m0.6205[0m        [35m0.2737[0m       0.9129        [94m0.2846[0m     +  10.3485
     22   [36m0.2267[0m   [32m0.6352[0m        [35m0.2731[0m       0.9129        [94m0.2826[0m     +  10.7490
     23   [36m0.2295[0m   [32m0.6437[0m        [35m0.2702[0m       0.9129        [94m0.2811[0m     +  11.1244
     24   [36m0.2374[0m   [32m0.6540[0m        [35m0.2673[0m       0.9129        [94m0.2795[0m     +  10.7311
     25   0.2362   [32m0.6600[0m        [35m0.2662[0m       0.9129        [94m0.2784[0m     +  11.1913
     26   [36m0.2494[0m   [32m0.6687[0m        [35m0.2639[0m       0.9129        [94m0.2774[0m     +  10.8576
     27   [36m0.2520[0m   [32m0.6737[0m        0.2654       0.9129        [94m0.2767[0m     +  10.7704
     28   [36m0.2600[0m   [32m0.6793[0m        [35m0.2638[0m       0.9129        [94m0.2759[0m     +  11.0580
     29   [36m0.2620[0m   [32m0.6836[0m        [35m0.2611[0m       0.9129        [94m0.2754[0m     +  10.5998
     30   [36m0.2628[0m   [32m0.6888[0m        0.2611       0.9129        [94m0.2745[0m     +  10.6504
     31   [36m0.2653[0m   [32m0.6936[0m        [35m0.2610[0m       0.9129        [94m0.2738[0m     +  11.0023
     32   [36m0.2667[0m   [32m0.6980[0m        [35m0.2601[0m       0.9129        [94m0.2734[0m     +  10.3441
     33   [36m0.2668[0m   [32m0.7027[0m        [35m0.2593[0m       0.9129        [94m0.2729[0m     +  10.5565
     34   0.2664   [32m0.7046[0m        [35m0.2581[0m       0.9129        [94m0.2727[0m     +  10.4925
     35   0.2662   [32m0.7073[0m        [35m0.2575[0m       0.9129        [94m0.2725[0m     +  10.8962
     36   0.2666   [32m0.7096[0m        0.2592       0.9129        [94m0.2722[0m     +  10.3516
     37   [36m0.2679[0m   [32m0.7129[0m        0.2580       0.9129        [94m0.2717[0m     +  10.9663
     38   [36m0.2679[0m   [32m0.7154[0m        [35m0.2544[0m       0.9129        [94m0.2714[0m     +  10.7542
     39   0.2665   [32m0.7184[0m        0.2575       0.9129        [94m0.2712[0m     +  10.8163
     40   0.2662   [32m0.7223[0m        0.2558       0.9129        [94m0.2707[0m     +  10.8296
     41   0.2647   [32m0.7244[0m        0.2570       0.9129        [94m0.2701[0m     +  10.4975
     42   [36m0.2688[0m   [32m0.7265[0m        0.2559       0.9129        [94m0.2697[0m     +  10.9221
     43   0.2649   [32m0.7283[0m        0.2555       0.9129        [94m0.2697[0m     +  11.0153
     44   0.2643   [32m0.7310[0m        0.2551       0.9129        [94m0.2692[0m     +  10.9234
     45   0.2662   [32m0.7334[0m        0.2544       0.9129        [94m0.2688[0m     +  10.9743
     46   0.2680   [32m0.7362[0m        [35m0.2535[0m       0.9129        [94m0.2684[0m     +  10.3340
     47   0.2675   [32m0.7393[0m        0.2550       0.9129        [94m0.2676[0m     +  10.7243
     48   [36m0.2701[0m   [32m0.7417[0m        [35m0.2531[0m       0.9129        [94m0.2674[0m     +  10.7389
     49   [36m0.2716[0m   [32m0.7442[0m        0.2554       0.9129        [94m0.2668[0m     +  10.6326
     50   [36m0.2746[0m   [32m0.7468[0m        0.2533       0.9129        [94m0.2660[0m     +  10.6063
[32m[I 2023-05-04 14:54:44,081][0m Trial 456 finished with value: 0.26601815866948325 and parameters: {'lr': 7.31388747201416e-06, 'dropout': 0.5648922879374294, 'd_model_multiplier': 4, 'num_layers': 7, 'n_heads': 4, 'dim_feedforward': 331, 'batch_size': 34, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 53}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 67
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
Warning, assumed runtime error: CUDA out of memory. Tried to allocate 1.49 GiB (GPU 0; 23.70 GiB total capacity; 14.77 GiB already allocated; 297.25 MiB free; 22.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[32m[I 2023-05-04 14:54:49,300][0m Trial 457 finished with value: 100.0 and parameters: {'lr': 1.2883384347812732e-06, 'dropout': 0.47017882587215687, 'd_model_multiplier': 4, 'num_layers': 6, 'n_heads': 32, 'dim_feedforward': 438, 'batch_size': 126, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.001, 'top_n_features': 67}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 58
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0664[0m   [32m0.3280[0m        [35m0.6156[0m       [31m0.9226[0m        [94m0.4614[0m     +  27.2240
      2   [36m0.1189[0m   [32m0.6224[0m        [35m0.3777[0m       0.9226        [94m0.2877[0m     +  27.3825
      3   [36m0.3162[0m   [32m0.7785[0m        [35m0.2804[0m       0.9226        [94m0.2396[0m     +  27.6663
      4   [36m0.3351[0m   [32m0.8039[0m        [35m0.2529[0m       0.9214        [94m0.2241[0m     +  27.3655
      5   0.3285   [32m0.8085[0m        [35m0.2424[0m       0.9178        [94m0.2201[0m     +  27.3477
      6   0.3300   [32m0.8126[0m        [35m0.2369[0m       0.9166        [94m0.2185[0m     +  27.4923
      7   0.3338   [32m0.8153[0m        [35m0.2343[0m       0.9202        [94m0.2177[0m     +  27.8663
      8   0.3323   [32m0.8177[0m        [35m0.2322[0m       0.9214        [94m0.2172[0m     +  27.7525
      9   [36m0.3389[0m   [32m0.8202[0m        [35m0.2316[0m       0.9202        [94m0.2170[0m     +  27.6867
     10   [36m0.3401[0m   [32m0.8210[0m        [35m0.2293[0m       0.9190        [94m0.2170[0m     +  27.7640
     11   [36m0.3420[0m   [32m0.8226[0m        0.2296       0.9190        [94m0.2165[0m     +  27.4598
     12   [36m0.3434[0m   [32m0.8241[0m        [35m0.2285[0m       0.9202        [94m0.2164[0m     +  27.5021
     13   0.3425   [32m0.8250[0m        [35m0.2282[0m       0.9214        [94m0.2160[0m     +  27.7876
     14   0.3397   0.8249        [35m0.2263[0m       [31m0.9238[0m        0.2168        27.7119
     15   0.3385   [32m0.8252[0m        0.2270       0.9238        0.2168        27.4783
     16   0.3378   [32m0.8256[0m        [35m0.2256[0m       0.9238        0.2166        27.7079
     17   0.3401   [32m0.8271[0m        [35m0.2253[0m       [31m0.9250[0m        0.2162        27.7429
     18   0.3396   0.8268        [35m0.2240[0m       0.9238        0.2164        27.4570
     19   0.3387   0.8270        [35m0.2240[0m       [31m0.9262[0m        0.2166        27.4759
     20   0.3389   [32m0.8273[0m        [35m0.2228[0m       0.9262        0.2169        27.8410
     21   0.3390   [32m0.8278[0m        [35m0.2221[0m       0.9250        0.2167        28.0046
     22   0.3429   [32m0.8286[0m        0.2222       0.9262        0.2167        27.4673
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 15:05:26,004][0m Trial 458 finished with value: 0.21597131874137518 and parameters: {'lr': 1.3806427475396543e-05, 'dropout': 0.24535575006717888, 'd_model_multiplier': 4, 'num_layers': 6, 'n_heads': 32, 'dim_feedforward': 366, 'batch_size': 54, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 58}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 63
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
Warning, assumed runtime error: CUDA out of memory. Tried to allocate 1.59 GiB (GPU 0; 23.70 GiB total capacity; 15.33 GiB already allocated; 293.25 MiB free; 22.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[32m[I 2023-05-04 15:05:29,615][0m Trial 459 finished with value: 100.0 and parameters: {'lr': 6.099086634313483e-05, 'dropout': 0.47742660951990246, 'd_model_multiplier': 4, 'num_layers': 5, 'n_heads': 64, 'dim_feedforward': 348, 'batch_size': 67, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 63}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 86
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0976[0m   [32m0.5663[0m        [35m0.4744[0m       [31m0.9214[0m        [94m0.2863[0m     +  57.6649
      2   [36m0.3638[0m   [32m0.7582[0m        [35m0.2684[0m       [31m0.9250[0m        [94m0.2319[0m     +  57.6008
      3   0.3499   [32m0.7606[0m        [35m0.2444[0m       [31m0.9274[0m        [94m0.2308[0m     +  57.6642
      4   0.3554   [32m0.7691[0m        [35m0.2377[0m       0.9274        0.2315        58.0876
      5   0.3604   [32m0.7719[0m        [35m0.2343[0m       0.9274        0.2320        58.0802
      6   0.3528   [32m0.7757[0m        [35m0.2315[0m       [31m0.9299[0m        0.2310        57.7721
      7   0.3497   0.7751        [35m0.2307[0m       0.9250        0.2318        57.6688
      8   0.3538   [32m0.7777[0m        [35m0.2304[0m       0.9250        0.2313        57.7523
      9   0.3597   [32m0.7842[0m        [35m0.2267[0m       0.9287        0.2335        57.6969
     10   0.3543   0.7792        0.2270       0.9274        0.2336        58.1323
     11   0.3439   0.7747        [35m0.2249[0m       0.9226        0.2359        57.7005
     12   0.3486   0.7712        0.2250       0.9238        0.2368        57.6584
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 15:18:02,450][0m Trial 460 finished with value: 0.23078827742834046 and parameters: {'lr': 2.1552088730571562e-05, 'dropout': 0.4127989717582712, 'd_model_multiplier': 8, 'num_layers': 13, 'n_heads': 32, 'dim_feedforward': 382, 'batch_size': 49, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 86}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 49
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.3403[0m   [32m0.8030[0m        [35m0.2690[0m       [31m0.9262[0m        [94m0.2155[0m     +  25.6146
      2   [36m0.3533[0m   [32m0.8058[0m        [35m0.2345[0m       0.9214        0.2215        25.6375
      3   [36m0.3600[0m   [32m0.8135[0m        [35m0.2308[0m       0.9190        0.2200        25.4425
      4   [36m0.3654[0m   0.8123        [35m0.2301[0m       0.9214        0.2236        25.0113
      5   0.3548   0.8119        [35m0.2298[0m       0.9202        0.2244        25.4567
      6   0.3520   0.8132        [35m0.2270[0m       0.9166        0.2275        25.6138
      7   0.3542   [32m0.8147[0m        [35m0.2258[0m       0.9190        0.2301        25.3244
      8   0.3495   0.8144        [35m0.2249[0m       0.9202        0.2251        25.3836
      9   0.3440   [32m0.8181[0m        [35m0.2223[0m       0.9238        0.2274        25.6041
     10   0.3334   0.8153        [35m0.2218[0m       0.9214        0.2339        25.6683
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 15:22:43,216][0m Trial 461 finished with value: 0.2155147243511864 and parameters: {'lr': 3.1582145689008604e-05, 'dropout': 0.42258726425454, 'd_model_multiplier': 16, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 320, 'batch_size': 60, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.001, 'top_n_features': 49}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 76
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1242[0m   [32m0.6116[0m        [35m0.4428[0m       [31m0.9335[0m        [94m0.2458[0m     +  23.5448
      2   [36m0.2159[0m   [32m0.7391[0m        [35m0.2648[0m       0.9335        [94m0.2208[0m     +  23.7496
      3   [36m0.2424[0m   [32m0.7798[0m        [35m0.2492[0m       0.9323        [94m0.2126[0m     +  23.7078
      4   [36m0.2553[0m   [32m0.7973[0m        [35m0.2408[0m       0.9311        [94m0.2089[0m     +  24.4010
      5   0.2510   [32m0.8063[0m        [35m0.2365[0m       0.9311        [94m0.2078[0m     +  23.5073
      6   0.2478   [32m0.8097[0m        [35m0.2338[0m       0.9311        0.2083        23.4421
      7   0.2515   [32m0.8097[0m        [35m0.2335[0m       0.9335        0.2081        23.8504
      8   0.2491   [32m0.8106[0m        [35m0.2331[0m       0.9323        0.2089        23.8693
      9   0.2513   0.8105        [35m0.2312[0m       0.9335        0.2081        23.8061
     10   0.2548   0.8098        [35m0.2296[0m       0.9323        0.2093        23.8947
     11   0.2495   0.8076        [35m0.2269[0m       0.9323        0.2086        24.0087
     12   [36m0.2607[0m   [32m0.8126[0m        0.2282       0.9299        0.2083        24.0411
     13   0.2593   0.8105        [35m0.2259[0m       0.9323        [94m0.2075[0m     +  23.8124
     14   0.2597   0.8124        [35m0.2243[0m       0.9311        0.2078        23.8378
     15   0.2584   0.8043        0.2248       0.9311        0.2081        23.7888
     16   [36m0.2611[0m   0.8033        [35m0.2222[0m       0.9323        0.2095        23.6682
     17   [36m0.2614[0m   0.8025        [35m0.2214[0m       0.9335        0.2083        23.7005
     18   [36m0.2677[0m   0.8064        0.2226       [31m0.9347[0m        [94m0.2064[0m     +  23.7373
     19   0.2563   0.8038        [35m0.2213[0m       0.9323        0.2094        24.0655
     20   0.2526   0.8093        [35m0.2169[0m       0.9335        0.2104        23.7637
     21   0.2591   0.8122        0.2180       0.9299        0.2099        23.5763
     22   0.2586   0.8118        [35m0.2151[0m       0.9311        0.2123        23.7888
     23   0.2568   0.8113        0.2162       0.9311        0.2115        24.4160
     24   0.2536   [32m0.8133[0m        [35m0.2145[0m       0.9311        0.2109        23.8595
     25   0.2463   0.8098        0.2159       0.9287        0.2157        24.0182
     26   0.2514   0.8129        0.2150       0.9274        0.2143        23.9653
     27   0.2412   0.8021        [35m0.2119[0m       0.9287        0.2134        23.9045
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 15:33:51,796][0m Trial 462 finished with value: 0.20638819185717364 and parameters: {'lr': 5.1520147171106866e-05, 'dropout': 0.43873858552209183, 'd_model_multiplier': 4, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 339, 'batch_size': 41, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 76}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 57
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1550[0m   [32m0.6112[0m        [35m0.7476[0m       [31m0.1064[0m        [94m0.7573[0m     +  23.8520
      2   0.1193   0.5380        [35m0.7306[0m       [31m0.1632[0m        [94m0.7352[0m     +  24.0548
      3   0.0923   0.4528        [35m0.7104[0m       [31m0.2914[0m        [94m0.7077[0m     +  24.1892
      4   0.0760   0.3878        [35m0.6852[0m       [31m0.5816[0m        [94m0.6774[0m     +  24.3384
      5   0.0708   0.3599        [35m0.6585[0m       [31m0.8440[0m        [94m0.6456[0m     +  24.3868
      6   0.0675   0.3485        [35m0.6294[0m       [31m0.9081[0m        [94m0.6134[0m     +  24.4874
      7   0.0640   0.3419        [35m0.6016[0m       [31m0.9141[0m        [94m0.5817[0m     +  24.3431
      8   0.0625   0.3376        [35m0.5715[0m       [31m0.9154[0m        [94m0.5509[0m     +  24.4457
      9   0.0619   0.3357        [35m0.5427[0m       0.9154        [94m0.5216[0m     +  24.3304
     10   0.0614   0.3342        [35m0.5160[0m       0.9154        [94m0.4939[0m     +  24.6279
     11   0.0614   0.3340        [35m0.4898[0m       0.9154        [94m0.4681[0m     +  24.4310
     12   0.0615   0.3356        [35m0.4663[0m       0.9154        [94m0.4443[0m     +  24.6402
     13   0.0617   0.3383        [35m0.4446[0m       0.9154        [94m0.4226[0m     +  24.2618
     14   0.0621   0.3437        [35m0.4243[0m       0.9154        [94m0.4030[0m     +  24.5837
     15   0.0626   0.3508        [35m0.4029[0m       0.9154        [94m0.3855[0m     +  24.2612
     16   0.0636   0.3622        [35m0.3853[0m       0.9154        [94m0.3700[0m     +  24.4861
     17   0.0647   0.3757        [35m0.3690[0m       0.9154        [94m0.3563[0m     +  24.6593
     18   0.0664   0.3954        [35m0.3558[0m       0.9154        [94m0.3444[0m     +  24.2887
     19   0.0687   0.4186        [35m0.3437[0m       0.9154        [94m0.3344[0m     +  24.7623
     20   0.0715   0.4446        [35m0.3333[0m       0.9154        [94m0.3259[0m     +  24.2500
     21   0.0744   0.4680        [35m0.3248[0m       0.9154        [94m0.3188[0m     +  24.5179
     22   0.0778   0.4911        [35m0.3158[0m       0.9154        [94m0.3128[0m     +  24.5302
     23   0.0820   0.5142        [35m0.3094[0m       0.9154        [94m0.3076[0m     +  24.5242
     24   0.0867   0.5353        [35m0.3034[0m       0.9154        [94m0.3031[0m     +  24.6198
     25   0.0922   0.5540        [35m0.2994[0m       0.9154        [94m0.2991[0m     +  24.6267
     26   0.0999   0.5712        [35m0.2939[0m       0.9154        [94m0.2955[0m     +  24.3944
     27   0.1074   0.5840        [35m0.2881[0m       0.9154        [94m0.2924[0m     +  24.2865
     28   0.1174   0.5968        [35m0.2850[0m       0.9154        [94m0.2897[0m     +  24.4058
     29   0.1280   0.6070        [35m0.2821[0m       0.9141        [94m0.2873[0m     +  24.7212
     30   0.1369   [32m0.6156[0m        [35m0.2795[0m       0.9141        [94m0.2851[0m     +  24.2709
     31   0.1467   [32m0.6238[0m        [35m0.2751[0m       0.9141        [94m0.2831[0m     +  24.3877
     32   0.1545   [32m0.6320[0m        [35m0.2735[0m       0.9141        [94m0.2813[0m     +  24.3976
     33   [36m0.1635[0m   [32m0.6391[0m        [35m0.2707[0m       0.9141        [94m0.2797[0m     +  24.2841
     34   [36m0.1721[0m   [32m0.6459[0m        [35m0.2686[0m       0.9141        [94m0.2781[0m     +  24.1409
     35   [36m0.1781[0m   [32m0.6520[0m        [35m0.2668[0m       0.9141        [94m0.2767[0m     +  24.3340
     36   [36m0.1824[0m   [32m0.6567[0m        [35m0.2664[0m       0.9141        [94m0.2754[0m     +  24.3611
     37   [36m0.1908[0m   [32m0.6618[0m        [35m0.2639[0m       0.9141        [94m0.2743[0m     +  24.4908
     38   [36m0.1976[0m   [32m0.6667[0m        [35m0.2630[0m       0.9141        [94m0.2732[0m     +  24.7617
     39   [36m0.2034[0m   [32m0.6712[0m        [35m0.2625[0m       0.9141        [94m0.2721[0m     +  24.5794
     40   [36m0.2069[0m   [32m0.6736[0m        [35m0.2595[0m       0.9141        [94m0.2712[0m     +  24.5627
     41   [36m0.2113[0m   [32m0.6775[0m        [35m0.2581[0m       0.9141        [94m0.2703[0m     +  24.2073
     42   [36m0.2185[0m   [32m0.6814[0m        [35m0.2574[0m       0.9141        [94m0.2695[0m     +  24.3784
     43   [36m0.2223[0m   [32m0.6844[0m        [35m0.2558[0m       0.9141        [94m0.2687[0m     +  23.9175
     44   [36m0.2269[0m   [32m0.6880[0m        [35m0.2545[0m       0.9141        [94m0.2680[0m     +  24.7167
     45   [36m0.2297[0m   [32m0.6908[0m        [35m0.2540[0m       0.9141        [94m0.2673[0m     +  24.6477
     46   [36m0.2336[0m   [32m0.6935[0m        [35m0.2523[0m       0.9129        [94m0.2666[0m     +  24.3847
     47   [36m0.2363[0m   [32m0.6948[0m        [35m0.2518[0m       0.9129        [94m0.2659[0m     +  24.0448
     48   [36m0.2406[0m   [32m0.6976[0m        [35m0.2517[0m       0.9129        [94m0.2652[0m     +  24.5887
     49   [36m0.2438[0m   [32m0.7002[0m        [35m0.2502[0m       0.9129        [94m0.2646[0m     +  24.6539
     50   [36m0.2450[0m   [32m0.7021[0m        [35m0.2492[0m       0.9141        [94m0.2641[0m     +  24.4162
[32m[I 2023-05-04 15:54:17,543][0m Trial 463 finished with value: 0.2641144246043144 and parameters: {'lr': 1.0129684920324714e-06, 'dropout': 0.45226688997802816, 'd_model_multiplier': 4, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 277, 'batch_size': 89, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 57}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 63
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0692[0m   [32m0.3380[0m        [35m0.5378[0m       [31m0.9190[0m        [94m0.4253[0m     +  10.2449
      2   [36m0.0756[0m   [32m0.4004[0m        [35m0.3913[0m       0.9190        [94m0.3388[0m     +  9.5960
      3   [36m0.1612[0m   [32m0.4925[0m        [35m0.3287[0m       0.9190        [94m0.3023[0m     +  9.5173
      4   [36m0.1649[0m   [32m0.5836[0m        [35m0.2960[0m       0.9190        [94m0.2808[0m     +  10.5627
      5   [36m0.1703[0m   [32m0.6561[0m        [35m0.2762[0m       0.9190        [94m0.2683[0m     +  10.1481
      6   [36m0.1881[0m   [32m0.6930[0m        [35m0.2648[0m       0.9190        [94m0.2628[0m     +  10.3485
      7   [36m0.1967[0m   [32m0.7149[0m        [35m0.2604[0m       0.9190        [94m0.2601[0m     +  10.5573
      8   [36m0.1968[0m   [32m0.7310[0m        [35m0.2555[0m       0.9190        [94m0.2586[0m     +  10.1174
      9   [36m0.2057[0m   [32m0.7450[0m        [35m0.2544[0m       0.9190        [94m0.2568[0m     +  9.8639
     10   [36m0.2181[0m   [32m0.7533[0m        [35m0.2521[0m       0.9190        [94m0.2558[0m     +  9.8025
     11   [36m0.2294[0m   0.7492        [35m0.2504[0m       0.9190        0.2565        9.7106
     12   [36m0.2447[0m   0.7488        [35m0.2452[0m       0.9190        [94m0.2555[0m     +  9.8052
     13   [36m0.2468[0m   0.7416        [35m0.2421[0m       0.9190        0.2577        10.2930
     14   [36m0.2483[0m   0.7527        [35m0.2421[0m       0.9166        0.2557        9.5985
     15   [36m0.2489[0m   0.7522        [35m0.2402[0m       0.9166        0.2572        9.4181
     16   [36m0.2544[0m   [32m0.7586[0m        [35m0.2378[0m       0.9178        0.2559        9.7229
     17   0.2535   0.7580        0.2384       0.9166        0.2561        10.1168
     18   [36m0.2577[0m   [32m0.7625[0m        [35m0.2365[0m       0.9166        0.2559        10.4164
     19   0.2576   [32m0.7662[0m        0.2375       0.9141        [94m0.2553[0m     +  10.1368
     20   [36m0.2580[0m   [32m0.7668[0m        [35m0.2362[0m       0.9141        0.2557        9.8621
     21   [36m0.2618[0m   [32m0.7696[0m        0.2374       0.9129        [94m0.2552[0m     +  9.8177
     22   [36m0.2630[0m   [32m0.7736[0m        0.2362       0.9129        [94m0.2543[0m     +  9.6715
     23   0.2623   [32m0.7740[0m        0.2363       0.9129        0.2549        9.5447
     24   0.2627   [32m0.7744[0m        [35m0.2352[0m       0.9141        0.2548        9.5056
     25   [36m0.2669[0m   [32m0.7771[0m        [35m0.2339[0m       0.9141        0.2551        9.6033
     26   [36m0.2689[0m   [32m0.7796[0m        [35m0.2319[0m       0.9141        [94m0.2537[0m     +  9.6569
     27   0.2667   0.7795        0.2342       0.9154        0.2539        10.2099
     28   0.2660   0.7780        0.2326       0.9154        0.2554        9.8422
     29   0.2689   [32m0.7804[0m        [35m0.2311[0m       0.9141        0.2559        10.1637
     30   [36m0.2692[0m   [32m0.7828[0m        [35m0.2310[0m       0.9154        0.2546        9.8944
     31   0.2673   0.7826        0.2327       0.9154        0.2551        10.3295
     32   0.2676   0.7827        [35m0.2310[0m       0.9166        0.2541        10.3484
     33   0.2688   [32m0.7834[0m        [35m0.2305[0m       0.9166        0.2539        9.8912
     34   0.2668   0.7826        0.2307       0.9178        0.2542        9.9834
     35   0.2690   [32m0.7841[0m        0.2315       0.9178        [94m0.2533[0m     +  10.3530
     36   0.2686   0.7839        [35m0.2296[0m       0.9166        0.2541        9.6086
     37   [36m0.2716[0m   [32m0.7847[0m        0.2301       0.9166        [94m0.2528[0m     +  9.5090
     38   [36m0.2720[0m   [32m0.7847[0m        0.2300       0.9166        0.2531        9.8305
     39   0.2706   0.7843        0.2298       0.9178        [94m0.2524[0m     +  9.5405
     40   0.2677   0.7836        0.2302       0.9178        0.2539        10.7807
     41   0.2709   0.7843        [35m0.2292[0m       0.9166        0.2550        10.4696
     42   0.2701   0.7843        0.2293       0.9166        0.2554        10.1779
     43   0.2693   0.7838        [35m0.2287[0m       0.9178        0.2537        9.9724
     44   0.2708   0.7844        [35m0.2268[0m       0.9166        0.2528        9.6747
     45   0.2696   0.7840        0.2296       0.9166        0.2535        10.2648
     46   [36m0.2726[0m   [32m0.7854[0m        0.2272       0.9178        0.2555        10.1421
     47   0.2718   0.7841        0.2272       0.9178        0.2533        9.4183
     48   0.2694   0.7827        [35m0.2263[0m       0.9166        0.2544        9.9032
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 16:02:26,513][0m Trial 464 finished with value: 0.2524195722650011 and parameters: {'lr': 4.0312894056660306e-05, 'dropout': 0.5097844279329043, 'd_model_multiplier': 4, 'num_layers': 4, 'n_heads': 4, 'dim_feedforward': 372, 'batch_size': 75, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 63}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 72
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0650[0m   [32m0.3913[0m        [35m0.5940[0m       [31m0.9299[0m        [94m0.3660[0m     +  29.5795
      2   [36m0.1150[0m   [32m0.6316[0m        [35m0.3300[0m       0.9299        [94m0.2494[0m     +  29.4044
      3   [36m0.1856[0m   [32m0.7152[0m        [35m0.2635[0m       0.9287        [94m0.2334[0m     +  29.9049
      4   [36m0.2155[0m   [32m0.7404[0m        [35m0.2469[0m       0.9262        [94m0.2292[0m     +  29.9079
      5   [36m0.2324[0m   [32m0.7482[0m        [35m0.2392[0m       0.9238        0.2307        29.6825
      6   [36m0.2360[0m   [32m0.7517[0m        [35m0.2360[0m       0.9250        0.2325        29.8209
      7   [36m0.2497[0m   [32m0.7518[0m        [35m0.2351[0m       0.9238        0.2333        29.8473
      8   [36m0.2553[0m   [32m0.7558[0m        [35m0.2334[0m       0.9238        0.2332        29.7550
      9   [36m0.2564[0m   [32m0.7575[0m        [35m0.2314[0m       0.9262        0.2322        30.0664
     10   [36m0.2587[0m   [32m0.7601[0m        [35m0.2298[0m       0.9262        0.2324        29.8916
     11   [36m0.2712[0m   [32m0.7614[0m        0.2306       0.9226        0.2337        29.7126
     12   [36m0.2739[0m   [32m0.7646[0m        [35m0.2292[0m       0.9226        0.2333        29.6422
     13   0.2737   0.7639        [35m0.2265[0m       0.9226        0.2352        29.7019
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 16:09:24,506][0m Trial 465 finished with value: 0.22922993605413633 and parameters: {'lr': 2.5033615233095113e-05, 'dropout': 0.5482530836504792, 'd_model_multiplier': 8, 'num_layers': 6, 'n_heads': 32, 'dim_feedforward': 360, 'batch_size': 71, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.001, 'top_n_features': 72}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 67
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2944[0m   [32m0.7791[0m        [35m0.2702[0m       [31m0.9226[0m        [94m0.2324[0m     +  69.9217
      2   [36m0.3102[0m   [32m0.7904[0m        [35m0.2358[0m       0.9226        [94m0.2292[0m     +  70.3651
      3   [36m0.3119[0m   [32m0.8023[0m        [35m0.2332[0m       0.9214        [94m0.2268[0m     +  70.2771
      4   [36m0.3146[0m   [32m0.8089[0m        [35m0.2291[0m       0.9190        [94m0.2265[0m     +  70.1885
      5   0.3125   [32m0.8145[0m        [35m0.2274[0m       0.9190        0.2278        70.1986
      6   [36m0.3193[0m   [32m0.8158[0m        [35m0.2260[0m       0.9202        [94m0.2265[0m     +  70.5033
      7   0.3176   [32m0.8190[0m        [35m0.2252[0m       0.9190        0.2282        70.4231
      8   0.3005   [32m0.8205[0m        [35m0.2217[0m       0.9154        0.2344        70.3363
      9   0.2999   0.8195        [35m0.2194[0m       0.9166        0.2324        70.3208
     10   0.3055   [32m0.8230[0m        [35m0.2183[0m       0.9214        0.2275        70.4273
     11   0.3022   0.8219        [35m0.2157[0m       0.9141        0.2316        70.3940
     12   0.2965   [32m0.8249[0m        [35m0.2141[0m       0.9166        0.2350        70.3137
     13   0.2927   [32m0.8278[0m        [35m0.2113[0m       0.9178        0.2331        70.5580
     14   0.3014   [32m0.8281[0m        [35m0.2094[0m       0.9178        0.2323        70.4072
     15   0.2969   0.8265        [35m0.2059[0m       0.9178        0.2335        70.3731
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 16:28:16,150][0m Trial 466 finished with value: 0.22646890701376973 and parameters: {'lr': 1.113984687410722e-05, 'dropout': 0.3845522601519797, 'd_model_multiplier': 32, 'num_layers': 8, 'n_heads': 32, 'dim_feedforward': 327, 'batch_size': 64, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 67}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 53
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1116[0m   [32m0.4092[0m        [35m0.6921[0m       [31m0.7606[0m        [94m0.6757[0m     +  34.1528
      2   0.0914   0.3644        [35m0.6527[0m       [31m0.9045[0m        [94m0.6270[0m     +  34.3261
      3   0.0925   0.3474        [35m0.6046[0m       0.9045        [94m0.5735[0m     +  34.5715
      4   0.0892   0.3430        [35m0.5536[0m       0.9045        [94m0.5223[0m     +  34.7201
      5   0.0877   0.3420        [35m0.5034[0m       0.9045        [94m0.4770[0m     +  34.6570
      6   0.0876   0.3448        [35m0.4591[0m       0.9045        [94m0.4388[0m     +  34.4203
      7   0.0881   0.3508        [35m0.4212[0m       0.9045        [94m0.4078[0m     +  34.6594
      8   0.0903   0.3612        [35m0.3896[0m       0.9045        [94m0.3831[0m     +  34.5262
      9   0.0919   0.3806        [35m0.3613[0m       0.9045        [94m0.3637[0m     +  34.2282
     10   0.0955   0.4087        [35m0.3420[0m       0.9045        [94m0.3488[0m     +  34.3341
     11   0.1027   [32m0.4457[0m        [35m0.3243[0m       0.9045        [94m0.3371[0m     +  34.1381
     12   [36m0.1132[0m   [32m0.4842[0m        [35m0.3110[0m       0.9045        [94m0.3282[0m     +  34.3626
     13   [36m0.1486[0m   [32m0.5215[0m        [35m0.2989[0m       0.9045        [94m0.3213[0m     +  34.3831
     14   [36m0.1663[0m   [32m0.5573[0m        [35m0.2889[0m       0.9045        [94m0.3159[0m     +  34.2693
     15   [36m0.1817[0m   [32m0.5881[0m        [35m0.2827[0m       0.9045        [94m0.3117[0m     +  34.3750
     16   [36m0.1948[0m   [32m0.6125[0m        [35m0.2759[0m       0.9045        [94m0.3085[0m     +  34.5651
     17   [36m0.1990[0m   [32m0.6345[0m        [35m0.2703[0m       0.9045        [94m0.3059[0m     +  34.3938
     18   [36m0.2049[0m   [32m0.6513[0m        [35m0.2678[0m       0.9045        [94m0.3037[0m     +  34.4834
     19   [36m0.2136[0m   [32m0.6655[0m        [35m0.2639[0m       0.9045        [94m0.3021[0m     +  34.3667
     20   [36m0.2204[0m   [32m0.6784[0m        [35m0.2609[0m       0.9045        [94m0.3007[0m     +  34.1598
     21   [36m0.2278[0m   [32m0.6903[0m        [35m0.2589[0m       0.9045        [94m0.2993[0m     +  34.3697
     22   [36m0.2297[0m   [32m0.6996[0m        [35m0.2555[0m       0.9045        [94m0.2982[0m     +  34.4201
     23   [36m0.2358[0m   [32m0.7074[0m        [35m0.2546[0m       0.9045        [94m0.2972[0m     +  34.3881
     24   [36m0.2435[0m   [32m0.7156[0m        [35m0.2537[0m       0.9045        [94m0.2961[0m     +  34.3994
     25   [36m0.2478[0m   [32m0.7222[0m        [35m0.2514[0m       0.9045        [94m0.2956[0m     +  34.5688
     26   [36m0.2526[0m   [32m0.7270[0m        [35m0.2498[0m       0.9045        [94m0.2948[0m     +  34.2485
     27   [36m0.2575[0m   [32m0.7312[0m        [35m0.2492[0m       0.9045        [94m0.2940[0m     +  34.2141
     28   [36m0.2608[0m   [32m0.7347[0m        [35m0.2489[0m       0.9045        [94m0.2933[0m     +  34.4761
     29   [36m0.2663[0m   [32m0.7398[0m        [35m0.2459[0m       0.9045        [94m0.2926[0m     +  34.5583
     30   [36m0.2675[0m   [32m0.7430[0m        0.2464       0.9045        [94m0.2918[0m     +  34.3436
     31   [36m0.2688[0m   [32m0.7468[0m        0.2463       [31m0.9057[0m        [94m0.2910[0m     +  34.1508
     32   [36m0.2738[0m   [32m0.7515[0m        [35m0.2450[0m       0.9057        [94m0.2902[0m     +  34.3491
     33   [36m0.2757[0m   [32m0.7538[0m        [35m0.2448[0m       0.9057        [94m0.2896[0m     +  34.1586
     34   [36m0.2785[0m   [32m0.7568[0m        [35m0.2424[0m       0.9057        [94m0.2890[0m     +  34.5762
     35   [36m0.2828[0m   [32m0.7598[0m        0.2431       0.9045        [94m0.2883[0m     +  34.2817
     36   [36m0.2851[0m   [32m0.7620[0m        0.2429       0.9045        [94m0.2876[0m     +  34.2475
     37   [36m0.2853[0m   [32m0.7636[0m        [35m0.2406[0m       0.9045        [94m0.2869[0m     +  34.2954
     38   [36m0.2860[0m   [32m0.7660[0m        [35m0.2406[0m       0.9045        [94m0.2861[0m     +  34.1980
     39   [36m0.2873[0m   [32m0.7677[0m        [35m0.2401[0m       0.9045        [94m0.2856[0m     +  34.5566
     40   [36m0.2898[0m   [32m0.7703[0m        [35m0.2392[0m       0.9045        [94m0.2847[0m     +  34.5248
     41   [36m0.2932[0m   [32m0.7730[0m        0.2400       0.9045        [94m0.2838[0m     +  34.2998
     42   [36m0.2952[0m   [32m0.7748[0m        [35m0.2392[0m       0.9057        [94m0.2834[0m     +  34.0895
     43   [36m0.2980[0m   [32m0.7766[0m        [35m0.2378[0m       0.9057        [94m0.2824[0m     +  34.2486
     44   [36m0.3001[0m   [32m0.7790[0m        0.2390       0.9057        [94m0.2819[0m     +  34.2706
     45   [36m0.3031[0m   [32m0.7806[0m        [35m0.2366[0m       0.9045        [94m0.2813[0m     +  34.5316
     46   [36m0.3059[0m   [32m0.7827[0m        0.2372       0.9045        [94m0.2806[0m     +  34.5338
     47   [36m0.3082[0m   [32m0.7839[0m        [35m0.2364[0m       0.9045        [94m0.2802[0m     +  34.1874
     48   [36m0.3105[0m   [32m0.7857[0m        0.2365       0.9045        [94m0.2794[0m     +  34.3046
     49   [36m0.3125[0m   [32m0.7869[0m        [35m0.2356[0m       0.9045        [94m0.2787[0m     +  34.1138
     50   [36m0.3147[0m   [32m0.7892[0m        [35m0.2351[0m       0.9045        [94m0.2778[0m     +  34.2072
[32m[I 2023-05-04 16:57:01,587][0m Trial 467 finished with value: 0.2778454970798152 and parameters: {'lr': 7.987172806841552e-07, 'dropout': 0.4897928002114429, 'd_model_multiplier': 4, 'num_layers': 4, 'n_heads': 64, 'dim_feedforward': 342, 'batch_size': 55, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 53}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 48
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0634[0m   [32m0.3926[0m        [35m0.3381[0m       [31m0.9311[0m        [94m0.3026[0m     +  27.0642
      2   [36m0.0846[0m   [32m0.5377[0m        [35m0.2771[0m       0.9311        [94m0.2640[0m     +  26.9350
      3   [36m0.1259[0m   [32m0.6153[0m        [35m0.2647[0m       0.9311        [94m0.2470[0m     +  27.1683
      4   [36m0.1564[0m   0.6121        0.2743       0.9311        [94m0.2444[0m     +  27.3667
      5   0.1499   [32m0.6641[0m        0.2706       0.9178        0.2600        27.1854
      6   [36m0.1834[0m   0.6575        0.2683       0.9311        [94m0.2428[0m     +  27.5835
      7   0.1639   [32m0.6656[0m        [35m0.2627[0m       0.9311        [94m0.2392[0m     +  27.3605
      8   0.1675   [32m0.6743[0m        0.2682       0.9311        0.2423        27.3741
      9   0.0689   0.5000        0.2764       0.9311        0.2543        27.7081
     10   0.0689   0.5000        0.2783       0.9311        0.2541        27.1752
     11   0.0689   0.5000        0.3007       0.9311        0.2539        27.4100
     12   0.0689   0.5000        0.2781       0.9311        0.2540        27.5446
     13   0.0689   0.5000        0.2780       0.9311        0.2539        27.4834
     14   0.0689   0.5000        0.2779       0.9311        0.2539        27.4873
     15   0.0689   0.5000        0.2864       0.9311        0.2538        27.3376
     16   0.0689   0.5000        0.2780       0.9311        0.2539        27.3162
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 17:04:47,298][0m Trial 468 finished with value: 0.23916485125453468 and parameters: {'lr': 0.08823108384852245, 'dropout': 0.29187271144703386, 'd_model_multiplier': 4, 'num_layers': 6, 'n_heads': 32, 'dim_feedforward': 319, 'batch_size': 46, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 48}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 60
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1941[0m   [32m0.6923[0m        [35m0.2824[0m       [31m0.8936[0m        [94m0.2771[0m     +  12.6707
      2   [36m0.1967[0m   0.6802        [35m0.2626[0m       [31m0.8984[0m        [94m0.2708[0m     +  12.8431
      3   0.1893   0.6786        [35m0.2622[0m       [31m0.8996[0m        [94m0.2699[0m     +  12.7180
      4   0.1961   0.6795        [35m0.2606[0m       [31m0.9154[0m        [94m0.2605[0m     +  12.7084
      5   [36m0.2064[0m   0.6816        [35m0.2562[0m       0.9057        0.2630        13.1785
      6   [36m0.2148[0m   0.6758        0.2588       [31m0.9178[0m        [94m0.2587[0m     +  12.9373
      7   0.2026   0.6780        0.2581       [31m0.9190[0m        0.2603        12.9835
      8   [36m0.2163[0m   0.6705        0.2578       [31m0.9202[0m        [94m0.2574[0m     +  12.7583
      9   0.2030   0.6735        0.2575       0.9166        0.2609        12.7695
     10   0.2068   0.6780        [35m0.2538[0m       0.9190        0.2575        13.0506
     11   0.2083   0.6815        0.2542       0.9190        [94m0.2557[0m     +  12.9356
     12   0.2148   0.6900        0.2568       0.9190        [94m0.2557[0m     +  13.0386
     13   0.2013   0.6850        0.2549       0.9190        [94m0.2555[0m     +  13.1657
     14   0.1894   0.6645        0.2571       0.9190        0.2568        13.1199
     15   0.1915   0.6695        [35m0.2524[0m       0.9190        0.2575        12.8968
     16   0.2014   0.6732        [35m0.2521[0m       0.9190        0.2579        13.1545
     17   0.2086   0.6801        0.2529       0.9190        0.2615        13.2814
     18   0.2047   0.6844        0.2533       0.9190        0.2567        13.3149
     19   0.2072   0.6801        [35m0.2509[0m       0.9190        [94m0.2551[0m     +  13.2859
     20   0.2042   0.6837        0.2511       0.9190        0.2555        13.0123
     21   0.2148   0.6855        0.2511       0.9190        0.2570        12.8574
     22   [36m0.2174[0m   0.6846        0.2521       0.9190        0.2556        13.0803
     23   0.2173   0.6751        0.2522       0.9190        [94m0.2548[0m     +  12.9111
     24   [36m0.2243[0m   0.6876        0.2512       0.9190        [94m0.2542[0m     +  13.1924
     25   0.2087   0.6820        [35m0.2492[0m       0.9190        0.2550        12.9717
     26   0.2036   0.6750        0.2501       0.9190        0.2553        12.9621
     27   0.2155   0.6806        0.2500       0.9190        0.2545        13.1451
     28   0.2158   0.6746        [35m0.2488[0m       0.9190        0.2544        13.1630
     29   0.2120   0.6732        [35m0.2482[0m       0.9190        0.2550        12.9765
     30   0.2215   0.6757        0.2498       0.9190        0.2547        12.8181
     31   [36m0.2243[0m   0.6740        0.2498       0.9190        0.2548        12.7085
     32   0.2073   0.6769        [35m0.2482[0m       0.9190        0.2548        12.5505
     33   0.2015   0.6794        0.2484       0.9190        0.2550        13.3192
     34   0.2081   0.6808        0.2490       0.9190        [94m0.2541[0m     +  13.0147
     35   0.2050   0.6820        0.2482       0.9190        0.2549        12.8908
     36   0.2089   0.6822        [35m0.2477[0m       0.9190        0.2549        12.8556
     37   0.2150   0.6824        0.2484       0.9190        [94m0.2540[0m     +  12.9096
     38   0.2161   0.6869        0.2480       0.9190        [94m0.2537[0m     +  12.8087
     39   0.2102   0.6877        [35m0.2476[0m       0.9190        0.2541        12.7796
     40   0.2034   0.6866        0.2480       0.9190        0.2545        13.3141
     41   0.2073   0.6826        0.2478       0.9190        0.2542        13.1844
     42   0.2064   0.6917        [35m0.2468[0m       0.9190        0.2544        12.9580
     43   0.1999   0.6869        0.2481       0.9190        0.2552        12.8544
     44   0.2002   0.6825        0.2481       0.9190        0.2549        13.2865
     45   0.2042   0.6908        0.2478       0.9190        0.2545        13.0915
     46   0.2050   0.6905        0.2484       0.9190        0.2551        12.8829
     47   0.2083   0.6857        0.2476       0.9190        0.2554        13.2415
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 17:15:12,338][0m Trial 469 finished with value: 0.2536532978922525 and parameters: {'lr': 0.005301349052793872, 'dropout': 0.4024065765330892, 'd_model_multiplier': 4, 'num_layers': 7, 'n_heads': 8, 'dim_feedforward': 350, 'batch_size': 59, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.001, 'top_n_features': 60}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 91
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2042[0m   [32m0.6503[0m        [35m0.4103[0m       [31m0.9190[0m        [94m0.2657[0m     +  25.5508
      2   [36m0.3004[0m   [32m0.7708[0m        [35m0.2505[0m       [31m0.9202[0m        [94m0.2412[0m     +  25.6249
      3   0.2900   [32m0.7815[0m        [35m0.2373[0m       0.9141        0.2488        25.8140
      4   0.2866   [32m0.7927[0m        [35m0.2313[0m       0.9129        0.2482        26.0240
      5   0.2908   [32m0.7949[0m        [35m0.2302[0m       0.9105        0.2506        25.5442
      6   0.2938   [32m0.7983[0m        [35m0.2280[0m       0.9105        0.2514        25.7191
      7   0.2814   0.7976        [35m0.2274[0m       0.9117        0.2562        25.7797
      8   0.2730   0.7871        [35m0.2264[0m       0.9141        0.2565        25.9943
      9   0.2764   0.7838        [35m0.2252[0m       0.9105        0.2631        26.0285
     10   0.2758   0.7890        [35m0.2215[0m       0.9093        0.2613        25.8496
     11   0.2817   0.7858        [35m0.2210[0m       0.9141        0.2634        26.2233
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 17:20:23,141][0m Trial 470 finished with value: 0.24120934827404125 and parameters: {'lr': 7.737917161615645e-05, 'dropout': 0.43037969430871204, 'd_model_multiplier': 8, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 333, 'batch_size': 67, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 91}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 106
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
Warning, assumed runtime error: CUDA out of memory. Tried to allocate 1.79 GiB (GPU 0; 23.70 GiB total capacity; 18.54 GiB already allocated; 51.25 MiB free; 22.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[32m[I 2023-05-04 17:20:29,306][0m Trial 471 finished with value: 100.0 and parameters: {'lr': 5.2728241020330314e-08, 'dropout': 0.36658501627411466, 'd_model_multiplier': 4, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 337, 'batch_size': 151, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 106}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 68
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
Warning, assumed runtime error: CUDA out of memory. Tried to allocate 1.85 GiB (GPU 0; 23.70 GiB total capacity; 19.76 GiB already allocated; 785.25 MiB free; 21.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[32m[I 2023-05-04 17:20:33,168][0m Trial 472 finished with value: 100.0 and parameters: {'lr': 4.241232046413705e-05, 'dropout': 0.46666292307400054, 'd_model_multiplier': 4, 'num_layers': 5, 'n_heads': 64, 'dim_feedforward': 435, 'batch_size': 78, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 68}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 55
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0705[0m   [32m0.3554[0m        [35m0.6188[0m       [31m0.9178[0m        [94m0.4630[0m     +  15.8806
      2   [36m0.0832[0m   [32m0.4776[0m        [35m0.3958[0m       0.9178        [94m0.3161[0m     +  16.0813
      3   [36m0.1898[0m   [32m0.6265[0m        [35m0.3004[0m       0.9178        [94m0.2751[0m     +  16.2288
      4   [36m0.2456[0m   [32m0.6736[0m        [35m0.2674[0m       0.9178        [94m0.2626[0m     +  16.1599
      5   [36m0.2787[0m   [32m0.7043[0m        [35m0.2545[0m       0.9178        [94m0.2575[0m     +  15.9441
      6   [36m0.2801[0m   [32m0.7229[0m        [35m0.2482[0m       [31m0.9190[0m        [94m0.2547[0m     +  16.3294
      7   [36m0.2906[0m   [32m0.7413[0m        [35m0.2440[0m       0.9178        [94m0.2523[0m     +  16.2413
      8   [36m0.2934[0m   [32m0.7553[0m        [35m0.2397[0m       0.9178        [94m0.2503[0m     +  16.0015
      9   [36m0.3014[0m   [32m0.7629[0m        [35m0.2385[0m       [31m0.9202[0m        [94m0.2491[0m     +  16.1996
     10   [36m0.3071[0m   [32m0.7687[0m        [35m0.2358[0m       [31m0.9238[0m        [94m0.2481[0m     +  16.1880
     11   [36m0.3164[0m   [32m0.7732[0m        0.2361       0.9238        [94m0.2474[0m     +  16.2923
     12   0.3144   [32m0.7769[0m        [35m0.2343[0m       [31m0.9250[0m        [94m0.2468[0m     +  16.2203
     13   [36m0.3258[0m   [32m0.7799[0m        [35m0.2336[0m       0.9250        [94m0.2462[0m     +  16.0296
     14   [36m0.3336[0m   [32m0.7832[0m        [35m0.2316[0m       0.9250        [94m0.2457[0m     +  16.4038
     15   [36m0.3347[0m   [32m0.7849[0m        0.2318       0.9238        [94m0.2451[0m     +  15.9341
     16   [36m0.3405[0m   [32m0.7862[0m        [35m0.2304[0m       0.9214        [94m0.2447[0m     +  16.2690
     17   [36m0.3427[0m   [32m0.7889[0m        [35m0.2292[0m       0.9202        [94m0.2443[0m     +  16.2547
     18   [36m0.3434[0m   [32m0.7908[0m        0.2309       0.9202        [94m0.2435[0m     +  16.1917
     19   [36m0.3496[0m   [32m0.7924[0m        0.2298       0.9202        [94m0.2431[0m     +  16.0484
     20   [36m0.3516[0m   [32m0.7938[0m        [35m0.2282[0m       0.9178        [94m0.2430[0m     +  16.2730
     21   [36m0.3536[0m   [32m0.7954[0m        [35m0.2275[0m       0.9178        [94m0.2423[0m     +  16.1697
     22   [36m0.3601[0m   [32m0.7971[0m        0.2279       0.9190        [94m0.2420[0m     +  16.1173
     23   [36m0.3636[0m   [32m0.7982[0m        [35m0.2267[0m       0.9202        0.2422        16.2672
     24   [36m0.3656[0m   [32m0.7991[0m        0.2270       0.9202        [94m0.2417[0m     +  16.2789
     25   [36m0.3685[0m   [32m0.8006[0m        0.2278       0.9214        [94m0.2414[0m     +  16.1806
     26   [36m0.3716[0m   [32m0.8022[0m        [35m0.2265[0m       0.9214        [94m0.2411[0m     +  16.2632
     27   0.3714   [32m0.8037[0m        [35m0.2251[0m       0.9214        [94m0.2406[0m     +  16.3223
     28   0.3713   [32m0.8039[0m        [35m0.2245[0m       0.9214        0.2407        16.0407
     29   [36m0.3717[0m   [32m0.8043[0m        0.2257       0.9214        0.2408        16.1641
     30   [36m0.3746[0m   [32m0.8051[0m        0.2246       0.9202        0.2411        15.9935
     31   [36m0.3749[0m   [32m0.8058[0m        0.2257       0.9190        [94m0.2400[0m     +  16.1869
     32   [36m0.3799[0m   [32m0.8063[0m        0.2247       0.9202        [94m0.2396[0m     +  16.0277
     33   0.3797   [32m0.8064[0m        [35m0.2245[0m       0.9190        [94m0.2394[0m     +  16.2352
     34   [36m0.3859[0m   [32m0.8073[0m        0.2246       0.9190        [94m0.2389[0m     +  16.2601
     35   0.3858   [32m0.8078[0m        [35m0.2233[0m       0.9178        [94m0.2384[0m     +  16.1036
     36   0.3771   [32m0.8083[0m        0.2247       0.9178        0.2384        16.2682
     37   0.3788   [32m0.8090[0m        0.2236       0.9178        [94m0.2383[0m     +  16.1651
     38   0.3774   [32m0.8096[0m        [35m0.2233[0m       0.9178        [94m0.2383[0m     +  16.0030
     39   0.3772   [32m0.8100[0m        [35m0.2232[0m       0.9166        0.2386        16.2588
     40   0.3782   [32m0.8101[0m        [35m0.2230[0m       0.9166        [94m0.2382[0m     +  16.4219
     41   0.3798   [32m0.8106[0m        [35m0.2224[0m       0.9166        0.2383        16.2792
     42   0.3815   [32m0.8111[0m        [35m0.2221[0m       0.9166        [94m0.2379[0m     +  16.1450
     43   0.3730   [32m0.8115[0m        [35m0.2219[0m       0.9166        0.2383        16.1322
     44   0.3728   [32m0.8119[0m        0.2219       0.9166        0.2385        16.1052
     45   0.3722   [32m0.8128[0m        0.2225       0.9166        0.2380        16.1617
     46   0.3762   [32m0.8133[0m        0.2226       0.9166        [94m0.2371[0m     +  16.2658
     47   0.3802   0.8131        [35m0.2207[0m       0.9166        0.2374        15.9646
     48   0.3729   0.8131        [35m0.2203[0m       0.9166        0.2375        16.1137
     49   0.3727   0.8128        0.2224       0.9166        0.2377        16.1611
     50   0.3741   [32m0.8135[0m        0.2211       0.9166        0.2374        16.0921
[32m[I 2023-05-04 17:34:05,157][0m Trial 473 finished with value: 0.23710792394582422 and parameters: {'lr': 8.647048128004342e-06, 'dropout': 0.42327543708582893, 'd_model_multiplier': 4, 'num_layers': 3, 'n_heads': 32, 'dim_feedforward': 196, 'batch_size': 31, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.001, 'top_n_features': 55}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 80
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.0553[0m   [32m0.3240[0m        [35m0.6760[0m       [31m0.7340[0m        [94m0.6686[0m     +  9.6760
      2   0.0549   0.3231        [35m0.6704[0m       [31m0.7823[0m        [94m0.6610[0m     +  9.5026
      3   0.0550   0.3233        [35m0.6636[0m       [31m0.8392[0m        [94m0.6512[0m     +  9.6050
      4   0.0550   [32m0.3247[0m        [35m0.6541[0m       [31m0.8646[0m        [94m0.6397[0m     +  10.5154
      5   0.0549   [32m0.3257[0m        [35m0.6425[0m       [31m0.8960[0m        [94m0.6270[0m     +  10.3082
      6   [36m0.0553[0m   [32m0.3293[0m        [35m0.6301[0m       [31m0.9117[0m        [94m0.6131[0m     +  10.0343
      7   [36m0.0555[0m   [32m0.3310[0m        [35m0.6185[0m       [31m0.9190[0m        [94m0.5985[0m     +  9.8226
      8   [36m0.0555[0m   [32m0.3316[0m        [35m0.6048[0m       [31m0.9226[0m        [94m0.5837[0m     +  10.8440
      9   0.0554   [32m0.3320[0m        [35m0.5913[0m       0.9226        [94m0.5687[0m     +  10.5342
     10   0.0552   0.3311        [35m0.5779[0m       0.9226        [94m0.5538[0m     +  10.6115
     11   0.0548   0.3286        [35m0.5633[0m       0.9226        [94m0.5392[0m     +  10.5843
     12   0.0546   0.3273        [35m0.5491[0m       0.9226        [94m0.5249[0m     +  9.9955
     13   0.0545   0.3264        [35m0.5358[0m       0.9226        [94m0.5111[0m     +  10.4096
     14   0.0543   0.3245        [35m0.5220[0m       0.9226        [94m0.4980[0m     +  10.2163
     15   0.0541   0.3228        [35m0.5093[0m       0.9226        [94m0.4853[0m     +  10.1429
     16   0.0541   0.3219        [35m0.4971[0m       0.9226        [94m0.4734[0m     +  10.4742
     17   0.0541   0.3212        [35m0.4865[0m       0.9226        [94m0.4621[0m     +  10.3463
     18   0.0541   0.3203        [35m0.4759[0m       0.9226        [94m0.4515[0m     +  10.2682
     19   0.0540   0.3190        [35m0.4657[0m       0.9226        [94m0.4417[0m     +  10.4752
     20   0.0540   0.3191        [35m0.4559[0m       0.9226        [94m0.4326[0m     +  10.5732
     21   0.0540   0.3187        [35m0.4460[0m       0.9226        [94m0.4241[0m     +  10.3823
     22   0.0540   0.3187        [35m0.4383[0m       0.9226        [94m0.4162[0m     +  10.4974
     23   0.0541   0.3195        [35m0.4295[0m       0.9226        [94m0.4088[0m     +  10.4960
     24   0.0542   0.3210        [35m0.4225[0m       0.9226        [94m0.4020[0m     +  9.5809
     25   0.0544   0.3227        [35m0.4160[0m       0.9226        [94m0.3957[0m     +  9.3367
     26   0.0547   0.3259        [35m0.4098[0m       0.9226        [94m0.3897[0m     +  9.7513
     27   0.0549   0.3290        [35m0.4041[0m       0.9226        [94m0.3841[0m     +  9.9647
     28   0.0552   [32m0.3338[0m        [35m0.3982[0m       0.9226        [94m0.3788[0m     +  9.7536
     29   [36m0.0556[0m   [32m0.3386[0m        [35m0.3921[0m       0.9226        [94m0.3738[0m     +  9.9353
     30   [36m0.0561[0m   [32m0.3449[0m        [35m0.3880[0m       0.9226        [94m0.3692[0m     +  10.2299
     31   [36m0.0566[0m   [32m0.3511[0m        [35m0.3824[0m       0.9226        [94m0.3648[0m     +  9.9571
     32   [36m0.0571[0m   [32m0.3576[0m        [35m0.3778[0m       0.9226        [94m0.3606[0m     +  9.9790
     33   [36m0.0575[0m   [32m0.3628[0m        [35m0.3731[0m       0.9226        [94m0.3567[0m     +  10.3359
     34   [36m0.0578[0m   [32m0.3670[0m        [35m0.3700[0m       0.9226        [94m0.3530[0m     +  10.1124
     35   [36m0.0582[0m   [32m0.3709[0m        [35m0.3662[0m       0.9226        [94m0.3494[0m     +  9.9385
     36   [36m0.0586[0m   [32m0.3757[0m        [35m0.3621[0m       0.9226        [94m0.3461[0m     +  10.3513
     37   [36m0.0592[0m   [32m0.3814[0m        [35m0.3582[0m       0.9226        [94m0.3429[0m     +  9.6051
     38   [36m0.0597[0m   [32m0.3857[0m        [35m0.3543[0m       0.9226        [94m0.3399[0m     +  9.5889
     39   [36m0.0605[0m   [32m0.3931[0m        [35m0.3511[0m       0.9226        [94m0.3369[0m     +  9.6421
     40   [36m0.0612[0m   [32m0.4010[0m        [35m0.3478[0m       0.9226        [94m0.3341[0m     +  9.8114
     41   [36m0.0617[0m   [32m0.4065[0m        [35m0.3457[0m       0.9226        [94m0.3314[0m     +  9.6605
     42   [36m0.0625[0m   [32m0.4140[0m        [35m0.3424[0m       0.9226        [94m0.3289[0m     +  9.8749
     43   [36m0.0633[0m   [32m0.4222[0m        [35m0.3395[0m       0.9226        [94m0.3264[0m     +  9.9027
     44   [36m0.0640[0m   [32m0.4298[0m        [35m0.3370[0m       0.9226        [94m0.3240[0m     +  9.7120
     45   [36m0.0648[0m   [32m0.4373[0m        [35m0.3347[0m       0.9226        [94m0.3218[0m     +  9.9929
     46   [36m0.0657[0m   [32m0.4452[0m        [35m0.3322[0m       0.9226        [94m0.3196[0m     +  10.1132
     47   [36m0.0664[0m   [32m0.4511[0m        [35m0.3299[0m       0.9226        [94m0.3175[0m     +  10.2294
     48   [36m0.0669[0m   [32m0.4562[0m        [35m0.3279[0m       0.9226        [94m0.3154[0m     +  9.9414
     49   [36m0.0675[0m   [32m0.4603[0m        [35m0.3252[0m       0.9226        [94m0.3135[0m     +  9.8914
     50   [36m0.0682[0m   [32m0.4645[0m        [35m0.3228[0m       0.9226        [94m0.3116[0m     +  10.4207
[32m[I 2023-05-04 17:42:33,198][0m Trial 474 finished with value: 0.3116361559734667 and parameters: {'lr': 5.766008703424425e-07, 'dropout': 0.2587060065470019, 'd_model_multiplier': 8, 'num_layers': 6, 'n_heads': 4, 'dim_feedforward': 328, 'batch_size': 50, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 80}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 63
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2498[0m   [32m0.7359[0m        [35m0.3459[0m       [31m0.9250[0m        [94m0.2390[0m     +  19.7579
      2   [36m0.3135[0m   [32m0.7757[0m        [35m0.2466[0m       0.9214        [94m0.2237[0m     +  20.0376
      3   [36m0.3210[0m   [32m0.7837[0m        [35m0.2358[0m       0.9214        [94m0.2215[0m     +  20.0154
      4   0.3187   [32m0.7856[0m        [35m0.2335[0m       0.9250        [94m0.2211[0m     +  20.2568
      5   0.3201   0.7826        [35m0.2316[0m       [31m0.9262[0m        0.2216        20.2607
      6   [36m0.3232[0m   0.7836        [35m0.2312[0m       [31m0.9299[0m        0.2213        20.0781
      7   [36m0.3270[0m   0.7848        [35m0.2290[0m       0.9262        0.2214        20.2370
      8   [36m0.3289[0m   0.7831        [35m0.2288[0m       0.9250        0.2218        20.5467
      9   [36m0.3328[0m   0.7828        [35m0.2274[0m       0.9238        0.2223        20.3543
     10   [36m0.3332[0m   0.7834        [35m0.2268[0m       0.9238        0.2223        20.4098
     11   [36m0.3343[0m   0.7832        [35m0.2255[0m       0.9250        0.2218        20.1211
     12   [36m0.3362[0m   0.7845        [35m0.2249[0m       0.9250        0.2218        20.1626
     13   [36m0.3391[0m   0.7837        [35m0.2239[0m       0.9250        0.2217        20.0689
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 17:47:16,445][0m Trial 475 finished with value: 0.22105688032352938 and parameters: {'lr': 1.516199763541174e-05, 'dropout': 0.27973380972102085, 'd_model_multiplier': 4, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 358, 'batch_size': 39, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 63}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 59
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2234[0m   [32m0.6823[0m        [35m0.3916[0m       [31m0.8972[0m        [94m0.3068[0m     +  23.2940
      2   [36m0.3358[0m   [32m0.7792[0m        [35m0.2476[0m       [31m0.9008[0m        [94m0.2841[0m     +  24.0992
      3   [36m0.3517[0m   [32m0.7867[0m        [35m0.2324[0m       [31m0.9033[0m        0.2864        23.3573
      4   [36m0.3767[0m   [32m0.7914[0m        [35m0.2297[0m       [31m0.9069[0m        0.2862        24.2986
      5   [36m0.3822[0m   [32m0.7937[0m        [35m0.2273[0m       [31m0.9081[0m        [94m0.2832[0m     +  24.1037
      6   [36m0.3822[0m   0.7934        [35m0.2257[0m       0.9069        0.2859        23.7300
      7   [36m0.3941[0m   [32m0.7956[0m        [35m0.2242[0m       [31m0.9105[0m        0.2861        23.5747
      8   [36m0.4003[0m   [32m0.7971[0m        0.2243       0.9081        0.2849        23.8402
      9   [36m0.4146[0m   [32m0.7990[0m        [35m0.2234[0m       0.9093        [94m0.2793[0m     +  24.0432
     10   0.4130   0.7984        [35m0.2204[0m       [31m0.9129[0m        0.2797        23.6739
     11   0.4045   0.7984        [35m0.2202[0m       0.9093        0.2831        23.9047
     12   [36m0.4184[0m   [32m0.7992[0m        0.2209       0.9093        0.2816        23.9398
     13   [36m0.4262[0m   [32m0.8014[0m        [35m0.2180[0m       0.9105        0.2829        23.7018
     14   0.4065   0.7997        0.2186       0.9093        0.2869        24.1209
     15   0.4019   0.8013        0.2186       0.9069        0.2821        23.7765
     16   0.4043   [32m0.8033[0m        [35m0.2156[0m       0.9081        0.2807        23.6719
     17   0.4075   0.8011        0.2181       0.9117        [94m0.2767[0m     +  24.0482
     18   0.4018   [32m0.8039[0m        0.2167       0.9093        0.2771        23.7299
     19   0.4034   [32m0.8040[0m        [35m0.2146[0m       0.9033        [94m0.2759[0m     +  23.8400
     20   0.3913   [32m0.8044[0m        [35m0.2141[0m       0.9033        0.2785        23.9708
     21   0.3853   [32m0.8089[0m        [35m0.2115[0m       0.9021        [94m0.2745[0m     +  24.0387
     22   0.4035   0.8069        0.2118       0.9057        0.2756        23.9922
     23   0.3991   0.8073        [35m0.2106[0m       0.9057        0.2747        23.9209
     24   0.3995   [32m0.8098[0m        [35m0.2102[0m       0.9069        0.2755        23.9358
     25   0.3908   0.8089        [35m0.2072[0m       0.9045        0.2780        23.7332
     26   0.3794   0.8091        0.2093       0.9033        0.2759        23.7597
     27   0.3604   0.8011        0.2074       0.9021        0.2814        23.6599
     28   0.3667   0.8079        [35m0.2049[0m       0.8996        0.2799        24.1680
     29   0.3607   0.8041        [35m0.2037[0m       0.9045        0.2866        23.6919
     30   0.3401   0.8017        0.2046       0.8996        0.2904        23.8060
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 17:59:36,939][0m Trial 476 finished with value: 0.2745070803230302 and parameters: {'lr': 0.00011650017100465306, 'dropout': 0.44554417551039105, 'd_model_multiplier': 4, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 310, 'batch_size': 57, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 59}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 73
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0757[0m   [32m0.3835[0m        [35m0.5859[0m       [31m0.9214[0m        [94m0.3603[0m     +  62.4277
      2   [36m0.1744[0m   [32m0.6727[0m        [35m0.3296[0m       0.9214        [94m0.2599[0m     +  62.7994
      3   [36m0.2445[0m   [32m0.7368[0m        [35m0.2649[0m       0.9202        [94m0.2461[0m     +  62.5274
      4   [36m0.2766[0m   [32m0.7581[0m        [35m0.2492[0m       0.9202        [94m0.2426[0m     +  62.7521
      5   [36m0.2835[0m   [32m0.7715[0m        [35m0.2423[0m       [31m0.9226[0m        [94m0.2410[0m     +  62.9533
      6   [36m0.2905[0m   [32m0.7793[0m        [35m0.2373[0m       0.9202        0.2414        63.1983
      7   [36m0.2921[0m   [32m0.7835[0m        [35m0.2355[0m       0.9154        0.2437        63.1506
      8   [36m0.2946[0m   [32m0.7859[0m        [35m0.2347[0m       0.9190        0.2455        62.5783
      9   [36m0.2950[0m   [32m0.7895[0m        [35m0.2305[0m       0.9178        0.2457        62.8644
     10   0.2927   [32m0.7911[0m        0.2314       0.9190        0.2463        62.9995
     11   0.2949   [32m0.7930[0m        [35m0.2292[0m       0.9202        0.2467        62.8924
     12   [36m0.2957[0m   [32m0.7952[0m        [35m0.2281[0m       0.9190        0.2471        62.6677
     13   [36m0.2973[0m   [32m0.7970[0m        0.2285       0.9190        0.2485        62.8254
     14   0.2942   [32m0.7973[0m        [35m0.2281[0m       0.9178        0.2487        62.5043
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 18:15:24,600][0m Trial 477 finished with value: 0.24098752242622237 and parameters: {'lr': 5.287115264939855e-06, 'dropout': 0.5568417697718698, 'd_model_multiplier': 32, 'num_layers': 7, 'n_heads': 32, 'dim_feedforward': 348, 'batch_size': 72, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.001, 'top_n_features': 73}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 65
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0658[0m   [32m0.3465[0m        [35m0.6098[0m       [31m0.9226[0m        [94m0.4500[0m     +  27.5902
      2   [36m0.0988[0m   [32m0.5105[0m        [35m0.3661[0m       0.9226        [94m0.2865[0m     +  27.7206
      3   [36m0.1732[0m   [32m0.6362[0m        [35m0.2774[0m       0.9226        [94m0.2624[0m     +  27.6262
      4   [36m0.1816[0m   [32m0.6921[0m        [35m0.2553[0m       0.9226        [94m0.2559[0m     +  27.4059
      5   [36m0.2022[0m   [32m0.7252[0m        [35m0.2452[0m       0.9214        [94m0.2516[0m     +  28.1460
      6   [36m0.2122[0m   [32m0.7483[0m        [35m0.2390[0m       0.9226        [94m0.2490[0m     +  27.7823
      7   0.2118   [32m0.7586[0m        [35m0.2352[0m       0.9190        [94m0.2483[0m     +  27.6202
      8   [36m0.2140[0m   [32m0.7626[0m        [35m0.2334[0m       0.9202        0.2485        28.0298
      9   0.2113   [32m0.7666[0m        [35m0.2306[0m       0.9190        [94m0.2480[0m     +  27.6951
     10   0.2125   [32m0.7699[0m        0.2310       0.9190        [94m0.2473[0m     +  27.6512
     11   [36m0.2160[0m   [32m0.7714[0m        [35m0.2279[0m       0.9214        0.2474        27.7449
     12   0.2152   [32m0.7728[0m        [35m0.2265[0m       0.9202        0.2481        27.8046
     13   0.2077   [32m0.7732[0m        0.2274       0.9202        0.2483        27.8998
     14   0.2154   [32m0.7773[0m        0.2269       0.9214        0.2477        28.2308
     15   0.2116   0.7767        0.2273       0.9202        0.2479        27.7563
     16   0.2146   [32m0.7783[0m        [35m0.2253[0m       0.9202        0.2480        27.9407
     17   0.2125   [32m0.7786[0m        0.2257       0.9214        0.2474        27.8719
     18   0.2118   [32m0.7793[0m        [35m0.2234[0m       0.9214        0.2485        27.7327
     19   0.2104   0.7784        [35m0.2227[0m       0.9202        0.2482        27.7505
     20   0.2098   [32m0.7795[0m        0.2233       0.9214        [94m0.2472[0m     +  27.7833
     21   0.2095   [32m0.7798[0m        0.2228       0.9202        0.2478        27.9405
     22   0.2089   0.7788        [35m0.2223[0m       0.9202        0.2473        27.8557
     23   0.2073   0.7785        [35m0.2216[0m       0.9202        [94m0.2470[0m     +  27.8103
     24   0.2063   0.7786        [35m0.2204[0m       0.9202        0.2473        27.9701
     25   0.2076   0.7781        0.2214       0.9202        0.2486        28.0338
     26   0.2069   0.7795        0.2206       0.9202        [94m0.2468[0m     +  28.4464
     27   0.2068   [32m0.7801[0m        [35m0.2195[0m       0.9178        0.2480        27.7688
     28   0.2037   0.7783        0.2200       0.9178        [94m0.2466[0m     +  27.5528
     29   0.2060   0.7775        [35m0.2170[0m       0.9154        0.2468        27.6818
     30   0.2044   0.7768        0.2198       0.9166        [94m0.2464[0m     +  27.8195
     31   0.2014   0.7768        [35m0.2165[0m       0.9166        [94m0.2463[0m     +  27.5553
     32   0.2021   0.7762        0.2180       0.9154        [94m0.2461[0m     +  27.8448
     33   0.2029   0.7772        0.2183       0.9141        [94m0.2457[0m     +  27.9326
     34   0.2006   0.7770        [35m0.2152[0m       0.9129        0.2467        27.8866
     35   0.2011   0.7771        0.2170       0.9129        [94m0.2456[0m     +  27.6731
     36   0.2144   0.7756        0.2165       0.9141        0.2457        27.7579
     37   0.2030   0.7745        0.2154       0.9154        [94m0.2454[0m     +  27.8757
     38   [36m0.2161[0m   0.7740        [35m0.2143[0m       0.9154        [94m0.2448[0m     +  28.1433
     39   [36m0.2171[0m   0.7740        0.2144       0.9141        [94m0.2445[0m     +  27.6203
     40   [36m0.2340[0m   0.7723        0.2157       0.9141        [94m0.2439[0m     +  27.6105
     41   0.2281   0.7719        [35m0.2124[0m       0.9141        0.2450        27.8191
     42   0.2214   0.7709        0.2130       0.9154        0.2454        27.9790
     43   0.2249   0.7719        0.2140       0.9154        0.2450        27.6102
     44   0.2238   0.7721        [35m0.2117[0m       0.9141        0.2450        27.9953
     45   0.2254   0.7708        0.2119       0.9154        0.2447        27.7805
     46   0.2240   0.7709        [35m0.2110[0m       0.9154        0.2452        27.5516
     47   0.2294   0.7715        0.2114       0.9141        0.2456        28.0301
     48   0.2261   0.7690        0.2117       0.9129        0.2468        27.5834
     49   0.2283   0.7672        [35m0.2100[0m       0.9141        0.2464        27.6147
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 18:38:38,264][0m Trial 478 finished with value: 0.24392313316482442 and parameters: {'lr': 2.3271453417374723e-05, 'dropout': 0.4078085970798573, 'd_model_multiplier': 4, 'num_layers': 6, 'n_heads': 32, 'dim_feedforward': 390, 'batch_size': 65, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 65}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 52
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
Warning, assumed runtime error: CUDA out of memory. Tried to allocate 1.44 GiB (GPU 0; 23.70 GiB total capacity; 16.19 GiB already allocated; 779.25 MiB free; 21.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[32m[I 2023-05-04 18:38:41,893][0m Trial 479 finished with value: 100.0 and parameters: {'lr': 0.0001851678862505566, 'dropout': 0.5002424027345571, 'd_model_multiplier': 8, 'num_layers': 5, 'n_heads': 64, 'dim_feedforward': 430, 'batch_size': 61, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 52}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 95
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1448[0m   [32m0.6291[0m        [35m0.4317[0m       [31m0.9311[0m        [94m0.2502[0m     +  34.4880
      2   [36m0.2035[0m   [32m0.7003[0m        [35m0.2630[0m       0.9311        [94m0.2273[0m     +  34.5615
      3   [36m0.2937[0m   [32m0.7312[0m        [35m0.2458[0m       [31m0.9323[0m        [94m0.2175[0m     +  35.0292
      4   [36m0.3163[0m   [32m0.7342[0m        [35m0.2365[0m       [31m0.9347[0m        [94m0.2162[0m     +  34.9189
      5   0.3074   [32m0.7397[0m        [35m0.2326[0m       [31m0.9359[0m        0.2170        34.9548
      6   0.2878   0.7358        [35m0.2321[0m       0.9287        0.2176        34.9022
      7   0.2912   0.7361        [35m0.2293[0m       0.9323        0.2182        35.1932
      8   0.2905   0.7356        [35m0.2291[0m       0.9311        0.2190        35.2564
      9   0.2806   0.7309        [35m0.2283[0m       0.9299        0.2200        34.9243
     10   0.2824   0.7283        [35m0.2260[0m       0.9311        0.2193        35.1032
     11   0.2779   0.7280        [35m0.2250[0m       0.9299        0.2208        35.1383
     12   0.2733   0.7231        [35m0.2233[0m       0.9323        0.2213        35.1198
     13   0.2942   0.7289        0.2233       0.9323        0.2192        35.0168
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 18:46:52,604][0m Trial 480 finished with value: 0.21618800476968217 and parameters: {'lr': 6.488907078559896e-05, 'dropout': 0.35612245074698334, 'd_model_multiplier': 4, 'num_layers': 8, 'n_heads': 32, 'dim_feedforward': 374, 'batch_size': 51, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 95}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 69
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2650[0m   [32m0.7145[0m        [35m0.3403[0m       [31m0.9359[0m        [94m0.2202[0m     +  29.3021
      2   [36m0.2915[0m   [32m0.7586[0m        [35m0.2460[0m       [31m0.9371[0m        [94m0.2165[0m     +  29.4872
      3   [36m0.3032[0m   [32m0.7827[0m        [35m0.2422[0m       0.9359        [94m0.2134[0m     +  29.5137
      4   [36m0.3109[0m   [32m0.7907[0m        [35m0.2402[0m       0.9371        [94m0.2133[0m     +  29.3845
      5   0.3053   [32m0.7987[0m        [35m0.2384[0m       0.9371        [94m0.2093[0m     +  29.3112
      6   0.3034   [32m0.8011[0m        [35m0.2348[0m       0.9371        0.2099        29.3865
      7   0.3032   0.8003        [35m0.2330[0m       0.9347        0.2103        29.4703
      8   0.2961   0.8010        0.2334       0.9359        [94m0.2081[0m     +  29.4346
      9   0.3056   [32m0.8064[0m        [35m0.2321[0m       0.9335        [94m0.2055[0m     +  29.4633
     10   0.2923   0.8046        [35m0.2297[0m       0.9347        0.2088        29.4372
     11   0.2956   0.8039        [35m0.2283[0m       0.9335        0.2091        29.3237
     12   0.2946   0.8039        [35m0.2264[0m       0.9347        0.2077        29.4010
     13   0.2942   0.8061        [35m0.2246[0m       0.9347        0.2093        29.3257
     14   0.2912   0.8051        [35m0.2241[0m       0.9347        0.2068        29.3119
     15   0.2940   0.8030        [35m0.2225[0m       0.9347        0.2091        29.2574
     16   0.2949   0.8064        [35m0.2215[0m       0.9347        0.2060        29.1447
     17   0.2928   0.8020        [35m0.2193[0m       0.9347        0.2082        29.1860
     18   0.2842   0.8003        [35m0.2189[0m       0.9335        0.2100        29.1540
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 18:56:11,480][0m Trial 481 finished with value: 0.20553121713081324 and parameters: {'lr': 3.41160018292123e-05, 'dropout': 0.39197258786742406, 'd_model_multiplier': 4, 'num_layers': 6, 'n_heads': 32, 'dim_feedforward': 323, 'batch_size': 13, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 69}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 57
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1361[0m   [32m0.6223[0m        [35m0.3914[0m       [31m0.9250[0m        [94m0.2617[0m     +  19.9772
      2   [36m0.1920[0m   [32m0.7145[0m        [35m0.2616[0m       0.9250        [94m0.2441[0m     +  20.1437
      3   [36m0.2352[0m   [32m0.7397[0m        [35m0.2479[0m       0.9250        [94m0.2387[0m     +  20.6113
      4   [36m0.2497[0m   [32m0.7500[0m        [35m0.2431[0m       0.9250        [94m0.2363[0m     +  20.4211
      5   [36m0.2614[0m   [32m0.7521[0m        [35m0.2372[0m       0.9238        0.2371        20.6326
      6   [36m0.2689[0m   [32m0.7522[0m        [35m0.2337[0m       0.9250        0.2377        20.5581
      7   [36m0.2831[0m   [32m0.7554[0m        [35m0.2323[0m       [31m0.9287[0m        0.2364        20.7730
      8   0.2823   [32m0.7568[0m        0.2330       [31m0.9299[0m        [94m0.2356[0m     +  20.6268
      9   [36m0.2856[0m   [32m0.7596[0m        [35m0.2302[0m       0.9274        [94m0.2352[0m     +  20.2154
     10   [36m0.2858[0m   [32m0.7608[0m        0.2305       0.9274        [94m0.2342[0m     +  20.3956
     11   [36m0.2868[0m   [32m0.7625[0m        [35m0.2300[0m       0.9287        [94m0.2340[0m     +  20.2512
     12   [36m0.2891[0m   [32m0.7641[0m        [35m0.2295[0m       0.9287        [94m0.2335[0m     +  20.0518
     13   [36m0.2918[0m   [32m0.7650[0m        [35m0.2275[0m       0.9287        0.2335        20.4240
     14   [36m0.2949[0m   [32m0.7664[0m        [35m0.2270[0m       0.9287        0.2338        20.3484
     15   [36m0.2954[0m   [32m0.7681[0m        0.2279       0.9287        [94m0.2331[0m     +  19.9403
     16   [36m0.2984[0m   [32m0.7691[0m        0.2275       0.9287        0.2332        20.2101
     17   [36m0.3050[0m   [32m0.7705[0m        [35m0.2268[0m       0.9287        [94m0.2323[0m     +  20.7485
     18   0.3050   [32m0.7709[0m        0.2271       0.9287        0.2330        20.4937
     19   [36m0.3068[0m   [32m0.7728[0m        [35m0.2246[0m       0.9274        0.2325        20.4125
     20   [36m0.3116[0m   [32m0.7734[0m        0.2250       0.9262        0.2332        20.6516
     21   [36m0.3117[0m   [32m0.7735[0m        0.2251       0.9262        0.2325        20.6129
     22   0.3040   [32m0.7747[0m        [35m0.2229[0m       0.9262        0.2330        20.4358
     23   0.3058   [32m0.7760[0m        0.2254       0.9262        0.2324        20.5378
     24   [36m0.3131[0m   [32m0.7771[0m        0.2254       0.9250        [94m0.2315[0m     +  20.5593
     25   [36m0.3157[0m   [32m0.7772[0m        0.2235       0.9274        0.2317        20.6301
     26   [36m0.3176[0m   0.7771        0.2237       0.9262        0.2327        20.2129
     27   0.3174   0.7770        [35m0.2226[0m       0.9238        0.2333        20.2912
     28   [36m0.3177[0m   [32m0.7780[0m        0.2231       0.9274        0.2320        20.3263
     29   0.3161   [32m0.7782[0m        [35m0.2207[0m       0.9250        0.2332        20.5799
     30   [36m0.3182[0m   [32m0.7787[0m        0.2231       0.9238        0.2322        20.5127
     31   [36m0.3195[0m   0.7784        [35m0.2199[0m       0.9214        0.2342        20.3704
     32   [36m0.3207[0m   [32m0.7791[0m        0.2209       0.9226        0.2326        20.1668
     33   [36m0.3213[0m   [32m0.7791[0m        0.2209       0.9226        0.2331        20.5856
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 19:07:47,370][0m Trial 482 finished with value: 0.2314631808751872 and parameters: {'lr': 1.968951975198616e-05, 'dropout': 0.45920995610944854, 'd_model_multiplier': 4, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 286, 'batch_size': 68, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.001, 'top_n_features': 57}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 62
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.2349[0m   [32m0.7411[0m        [35m0.2769[0m       [31m0.9335[0m        [94m0.2258[0m     +  9.0888
      2   [36m0.3407[0m   [32m0.7853[0m        [35m0.2564[0m       [31m0.9347[0m        0.2287        8.8056
      3   0.2232   0.7648        [35m0.2518[0m       0.9190        0.2361        9.4163
      4   0.3136   0.7780        [35m0.2485[0m       0.9323        0.2311        8.9830
      5   0.1655   0.6751        [35m0.2483[0m       0.9323        0.2512        9.4264
      6   0.2119   0.6845        0.2619       0.9323        0.2426        9.4882
      7   0.3272   0.7508        0.2550       0.9347        0.2340        9.1126
      8   0.3243   0.7144        0.2530       [31m0.9383[0m        [94m0.2225[0m     +  9.1527
      9   0.2003   0.6791        0.2503       0.9323        0.2406        9.0746
     10   0.2203   0.6880        0.2567       0.9323        0.2327        9.3228
     11   0.2099   0.6731        0.2569       0.9323        0.2369        9.3419
     12   0.2284   0.6731        0.2577       0.9323        0.2342        9.4816
     13   0.2304   0.6783        0.2581       0.9323        0.2369        9.3219
     14   0.2125   0.6752        0.2556       0.9323        0.2358        9.2827
     15   0.1797   0.6722        0.2575       0.9323        0.2411        9.5486
     16   0.2283   0.6720        0.2563       0.9323        0.2451        9.3669
     17   0.2308   0.6800        0.2556       0.9323        0.2424        9.4141
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 19:10:34,756][0m Trial 483 finished with value: 0.22253391319635846 and parameters: {'lr': 0.0031929823809267125, 'dropout': 0.5273310411881164, 'd_model_multiplier': 16, 'num_layers': 3, 'n_heads': 4, 'dim_feedforward': 172, 'batch_size': 24, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 62}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 48
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0824[0m   [32m0.3508[0m        [35m0.6100[0m       [31m0.9202[0m        [94m0.4865[0m     +  33.0439
      2   [36m0.0899[0m   [32m0.3912[0m        [35m0.4093[0m       0.9202        [94m0.3270[0m     +  33.3778
      3   [36m0.1434[0m   [32m0.5478[0m        [35m0.3030[0m       0.9202        [94m0.2803[0m     +  33.3613
      4   [36m0.1671[0m   [32m0.6191[0m        [35m0.2682[0m       0.9202        [94m0.2686[0m     +  33.5125
      5   [36m0.1742[0m   [32m0.6572[0m        [35m0.2533[0m       0.9190        [94m0.2652[0m     +  33.5343
      6   [36m0.1791[0m   [32m0.6839[0m        [35m0.2450[0m       0.9166        [94m0.2646[0m     +  33.6929
      7   [36m0.1840[0m   [32m0.7022[0m        [35m0.2393[0m       0.9105        0.2658        33.4464
      8   [36m0.1880[0m   [32m0.7083[0m        [35m0.2347[0m       0.9069        0.2675        33.4243
      9   [36m0.1924[0m   [32m0.7129[0m        [35m0.2331[0m       0.9069        0.2679        33.4501
     10   [36m0.1966[0m   [32m0.7165[0m        [35m0.2314[0m       0.9117        0.2677        33.6329
     11   [36m0.2059[0m   [32m0.7218[0m        [35m0.2299[0m       0.9117        0.2670        33.4920
     12   [36m0.2089[0m   [32m0.7253[0m        [35m0.2295[0m       0.9129        0.2654        33.4028
     13   [36m0.2119[0m   [32m0.7289[0m        [35m0.2277[0m       0.9141        [94m0.2645[0m     +  33.3542
     14   [36m0.2169[0m   [32m0.7323[0m        0.2285       0.9129        [94m0.2634[0m     +  33.3730
     15   [36m0.2172[0m   [32m0.7344[0m        [35m0.2269[0m       0.9141        0.2636        33.5132
     16   [36m0.2215[0m   [32m0.7366[0m        [35m0.2265[0m       0.9129        [94m0.2626[0m     +  33.3929
     17   0.2210   [32m0.7394[0m        [35m0.2259[0m       0.9129        [94m0.2619[0m     +  33.5118
     18   [36m0.2251[0m   [32m0.7417[0m        0.2265       0.9105        [94m0.2619[0m     +  33.3632
     19   [36m0.2281[0m   [32m0.7447[0m        [35m0.2253[0m       0.9093        [94m0.2610[0m     +  33.5774
     20   [36m0.2289[0m   [32m0.7462[0m        0.2266       0.9105        [94m0.2605[0m     +  33.5591
     21   0.2284   [32m0.7474[0m        [35m0.2251[0m       0.9105        [94m0.2601[0m     +  33.3519
     22   0.2274   [32m0.7486[0m        [35m0.2243[0m       0.9093        0.2603        33.6103
     23   0.2287   [32m0.7498[0m        0.2246       0.9105        [94m0.2598[0m     +  33.4026
     24   [36m0.2299[0m   [32m0.7506[0m        [35m0.2239[0m       0.9093        [94m0.2589[0m     +  33.4274
     25   0.2282   [32m0.7517[0m        [35m0.2227[0m       0.9081        0.2601        33.4081
     26   0.2271   [32m0.7521[0m        0.2232       0.9057        0.2607        33.4847
     27   0.2289   [32m0.7545[0m        0.2230       0.9057        0.2597        33.5727
     28   0.2298   [32m0.7548[0m        0.2232       0.9069        0.2597        33.5872
     29   [36m0.2299[0m   [32m0.7554[0m        [35m0.2221[0m       0.9069        0.2593        33.5536
     30   [36m0.2306[0m   0.7553        [35m0.2216[0m       0.9069        0.2599        33.5841
     31   [36m0.2332[0m   [32m0.7560[0m        0.2222       0.9069        0.2591        33.4693
     32   0.2317   0.7558        [35m0.2211[0m       0.9069        0.2597        33.3912
     33   [36m0.2340[0m   [32m0.7574[0m        0.2228       0.9069        0.2600        33.6269
     34   [36m0.2346[0m   [32m0.7588[0m        [35m0.2205[0m       0.9057        [94m0.2586[0m     +  33.6545
     35   [36m0.2352[0m   [32m0.7594[0m        0.2221       0.9057        0.2588        33.4881
     36   [36m0.2425[0m   [32m0.7596[0m        [35m0.2204[0m       0.9045        0.2591        33.5084
     37   [36m0.2427[0m   [32m0.7604[0m        [35m0.2203[0m       0.9069        [94m0.2582[0m     +  33.4151
     38   0.2422   0.7600        [35m0.2200[0m       0.9069        0.2597        33.6682
     39   [36m0.2436[0m   [32m0.7611[0m        [35m0.2200[0m       0.9069        0.2588        33.5487
     40   0.2433   [32m0.7618[0m        [35m0.2194[0m       0.9069        0.2590        33.5957
     41   0.2433   [32m0.7622[0m        [35m0.2180[0m       0.9093        0.2595        33.4901
     42   0.2354   0.7619        0.2191       0.9069        0.2600        33.7244
     43   0.2351   [32m0.7625[0m        0.2182       0.9081        0.2610        33.4988
     44   [36m0.2450[0m   [32m0.7637[0m        0.2186       0.9081        0.2598        33.7104
     45   [36m0.2462[0m   [32m0.7645[0m        [35m0.2173[0m       0.9081        0.2590        33.4065
     46   0.2454   0.7639        [35m0.2165[0m       0.9069        0.2604        33.4386
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 19:36:52,661][0m Trial 484 finished with value: 0.25820248454196565 and parameters: {'lr': 4.503585466871234e-06, 'dropout': 0.41979423681567396, 'd_model_multiplier': 8, 'num_layers': 7, 'n_heads': 32, 'dim_feedforward': 364, 'batch_size': 44, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 48}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 76
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2121[0m   [32m0.6596[0m        [35m0.2938[0m       [31m0.9154[0m        [94m0.3788[0m     +  14.5467
      2   [36m0.2240[0m   [32m0.6710[0m        [35m0.2672[0m       0.9141        [94m0.3229[0m     +  14.6231
      3   0.2192   [32m0.6738[0m        0.2675       0.8815        0.3297        14.8502
      4   0.1991   0.6719        [35m0.2643[0m       0.9154        [94m0.2780[0m     +  14.8204
      5   0.1863   0.6665        0.2659       0.9154        0.2922        14.6943
      6   0.2153   0.6650        0.2647       0.9141        [94m0.2711[0m     +  14.8894
      7   0.2008   0.6678        [35m0.2643[0m       0.9154        0.2726        14.8541
      8   0.2081   0.6674        0.2654       0.9154        [94m0.2704[0m     +  14.6658
      9   0.1973   0.6654        [35m0.2613[0m       0.9154        0.2778        14.7103
     10   0.2203   0.6732        [35m0.2613[0m       0.9154        0.2826        14.9071
     11   0.1988   0.6604        [35m0.2594[0m       0.9154        0.2752        14.6936
     12   0.1928   0.6672        0.2602       0.9154        0.2806        14.8825
     13   0.2227   0.6723        0.2596       0.9154        0.2753        14.7217
     14   [36m0.2260[0m   [32m0.6787[0m        [35m0.2590[0m       0.9154        0.2737        14.8120
     15   0.1970   0.6683        0.2643       0.9154        0.2768        14.6742
     16   0.1956   0.6626        0.2608       0.9154        0.2811        14.7944
     17   0.0846   0.5000        0.2642       0.9154        0.2914        14.8215
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 19:41:19,156][0m Trial 485 finished with value: 0.2703958765356976 and parameters: {'lr': 0.028873545837393322, 'dropout': 0.6469690742823837, 'd_model_multiplier': 4, 'num_layers': 5, 'n_heads': 16, 'dim_feedforward': 261, 'batch_size': 35, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 76}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 55
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0584[0m   [32m0.3284[0m        [35m0.6314[0m       [31m0.9202[0m        [94m0.5579[0m     +  20.1838
      2   0.0564   0.3127        [35m0.5260[0m       0.9202        [94m0.4663[0m     +  20.1184
      3   0.0565   0.3230        [35m0.4504[0m       0.9202        [94m0.4045[0m     +  20.2963
      4   [36m0.0599[0m   [32m0.3676[0m        [35m0.3981[0m       0.9202        [94m0.3617[0m     +  20.3608
      5   [36m0.0654[0m   [32m0.4268[0m        [35m0.3591[0m       0.9202        [94m0.3320[0m     +  20.4732
      6   [36m0.0732[0m   [32m0.4875[0m        [35m0.3326[0m       0.9202        [94m0.3119[0m     +  20.7755
      7   [36m0.0922[0m   [32m0.5359[0m        [35m0.3134[0m       0.9202        [94m0.2983[0m     +  20.4641
      8   [36m0.1033[0m   [32m0.5656[0m        [35m0.3009[0m       0.9202        [94m0.2888[0m     +  20.5544
      9   [36m0.1190[0m   [32m0.5948[0m        [35m0.2914[0m       0.9202        [94m0.2818[0m     +  20.3678
     10   [36m0.1407[0m   [32m0.6201[0m        [35m0.2836[0m       0.9202        [94m0.2764[0m     +  20.5279
     11   [36m0.1579[0m   [32m0.6374[0m        [35m0.2787[0m       0.9202        [94m0.2723[0m     +  20.5613
     12   [36m0.1713[0m   [32m0.6494[0m        [35m0.2728[0m       0.9202        [94m0.2690[0m     +  20.7198
     13   [36m0.1774[0m   [32m0.6592[0m        [35m0.2700[0m       0.9202        [94m0.2661[0m     +  20.6205
     14   [36m0.1824[0m   [32m0.6658[0m        [35m0.2664[0m       0.9202        [94m0.2638[0m     +  20.7961
     15   [36m0.1865[0m   [32m0.6728[0m        [35m0.2632[0m       0.9202        [94m0.2618[0m     +  20.4256
     16   [36m0.1943[0m   [32m0.6784[0m        [35m0.2604[0m       0.9202        [94m0.2600[0m     +  20.5568
     17   [36m0.1989[0m   [32m0.6831[0m        [35m0.2594[0m       0.9202        [94m0.2585[0m     +  20.6612
     18   [36m0.2026[0m   [32m0.6878[0m        [35m0.2568[0m       0.9202        [94m0.2570[0m     +  20.6336
     19   [36m0.2037[0m   [32m0.6921[0m        [35m0.2558[0m       0.9202        [94m0.2558[0m     +  20.3652
     20   [36m0.2096[0m   [32m0.6967[0m        [35m0.2535[0m       0.9202        [94m0.2547[0m     +  20.6256
     21   [36m0.2143[0m   [32m0.7017[0m        [35m0.2530[0m       0.9202        [94m0.2536[0m     +  20.4199
     22   [36m0.2189[0m   [32m0.7059[0m        [35m0.2518[0m       0.9202        [94m0.2526[0m     +  20.5732
     23   [36m0.2218[0m   [32m0.7105[0m        [35m0.2506[0m       0.9190        [94m0.2516[0m     +  20.4884
     24   [36m0.2253[0m   [32m0.7147[0m        [35m0.2498[0m       0.9190        [94m0.2508[0m     +  20.3611
     25   [36m0.2284[0m   [32m0.7185[0m        [35m0.2478[0m       0.9190        [94m0.2501[0m     +  20.5395
     26   [36m0.2341[0m   [32m0.7232[0m        [35m0.2465[0m       0.9190        [94m0.2493[0m     +  20.5169
     27   [36m0.2369[0m   [32m0.7263[0m        [35m0.2459[0m       0.9190        [94m0.2486[0m     +  20.6037
     28   [36m0.2383[0m   [32m0.7288[0m        [35m0.2450[0m       0.9190        [94m0.2478[0m     +  20.6583
     29   0.2368   [32m0.7320[0m        [35m0.2437[0m       0.9190        [94m0.2471[0m     +  20.5332
     30   [36m0.2385[0m   [32m0.7347[0m        [35m0.2426[0m       0.9178        [94m0.2466[0m     +  20.5809
     31   [36m0.2417[0m   [32m0.7384[0m        [35m0.2425[0m       0.9154        [94m0.2460[0m     +  20.2933
     32   [36m0.2445[0m   [32m0.7414[0m        [35m0.2418[0m       0.9166        [94m0.2456[0m     +  20.3838
     33   [36m0.2467[0m   [32m0.7450[0m        [35m0.2415[0m       0.9190        [94m0.2452[0m     +  20.3485
     34   [36m0.2491[0m   [32m0.7478[0m        [35m0.2402[0m       0.9166        [94m0.2448[0m     +  20.5223
     35   [36m0.2495[0m   [32m0.7507[0m        [35m0.2390[0m       0.9166        [94m0.2446[0m     +  20.7671
     36   [36m0.2500[0m   [32m0.7536[0m        [35m0.2380[0m       0.9166        [94m0.2443[0m     +  20.7967
     37   [36m0.2519[0m   [32m0.7559[0m        0.2388       0.9166        [94m0.2442[0m     +  20.3075
     38   [36m0.2538[0m   [32m0.7584[0m        [35m0.2380[0m       0.9166        [94m0.2440[0m     +  20.9216
     39   [36m0.2542[0m   [32m0.7598[0m        [35m0.2379[0m       0.9166        [94m0.2439[0m     +  20.5759
     40   [36m0.2547[0m   [32m0.7619[0m        0.2380       0.9154        [94m0.2437[0m     +  20.5008
     41   [36m0.2555[0m   [32m0.7635[0m        [35m0.2365[0m       0.9154        [94m0.2437[0m     +  20.6890
     42   0.2543   [32m0.7648[0m        0.2368       0.9141        0.2437        20.7881
     43   0.2551   [32m0.7666[0m        [35m0.2353[0m       0.9166        0.2438        20.4111
     44   [36m0.2567[0m   [32m0.7675[0m        0.2356       0.9166        0.2439        20.7106
     45   0.2564   [32m0.7687[0m        [35m0.2349[0m       0.9178        0.2438        20.7607
     46   [36m0.2572[0m   [32m0.7698[0m        0.2353       0.9166        0.2438        20.6617
     47   [36m0.2576[0m   [32m0.7706[0m        [35m0.2329[0m       0.9166        0.2439        20.6816
     48   0.2574   [32m0.7714[0m        0.2334       0.9154        0.2439        20.6029
     49   [36m0.2576[0m   [32m0.7718[0m        0.2330       0.9154        0.2440        20.8059
     50   [36m0.2581[0m   [32m0.7725[0m        0.2339       0.9154        0.2440        20.3057
[32m[I 2023-05-04 19:58:30,434][0m Trial 486 finished with value: 0.24367160643783128 and parameters: {'lr': 1.8528136813438114e-06, 'dropout': 0.4365268719918299, 'd_model_multiplier': 4, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 340, 'batch_size': 75, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.001, 'top_n_features': 55}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 112
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0682[0m   [32m0.4057[0m        [35m0.4596[0m       [31m0.9250[0m        [94m0.3022[0m     +  39.5651
      2   [36m0.1868[0m   [32m0.6884[0m        [35m0.2838[0m       0.9250        [94m0.2473[0m     +  39.9560
      3   [36m0.2251[0m   [32m0.7247[0m        [35m0.2559[0m       0.9250        [94m0.2409[0m     +  40.6573
      4   [36m0.2651[0m   [32m0.7555[0m        [35m0.2497[0m       0.9250        [94m0.2352[0m     +  39.8321
      5   [36m0.3100[0m   [32m0.7778[0m        [35m0.2445[0m       [31m0.9262[0m        [94m0.2311[0m     +  39.7823
      6   [36m0.3222[0m   [32m0.7859[0m        [35m0.2390[0m       [31m0.9299[0m        [94m0.2281[0m     +  39.7500
      7   0.3188   0.7853        [35m0.2380[0m       0.9274        0.2290        40.0905
      8   0.3207   [32m0.7908[0m        [35m0.2359[0m       0.9299        [94m0.2275[0m     +  39.9169
      9   [36m0.3242[0m   0.7898        [35m0.2337[0m       0.9287        0.2281        40.0801
     10   [36m0.3297[0m   [32m0.7943[0m        [35m0.2326[0m       0.9262        0.2278        39.9344
     11   0.3254   [32m0.7977[0m        [35m0.2311[0m       0.9299        0.2280        39.9890
     12   0.3179   0.7921        [35m0.2301[0m       0.9262        0.2281        39.9950
     13   0.3221   0.7926        0.2306       0.9250        0.2277        40.0146
     14   0.3153   0.7887        [35m0.2289[0m       0.9287        0.2306        39.7518
     15   0.3137   0.7869        [35m0.2277[0m       0.9287        0.2322        40.0563
     16   0.3129   0.7900        [35m0.2270[0m       0.9274        0.2307        39.9675
     17   0.3255   0.7918        [35m0.2252[0m       0.9274        0.2295        39.8339
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 20:10:30,365][0m Trial 487 finished with value: 0.22751419190339917 and parameters: {'lr': 5.137767337789533e-05, 'dropout': 0.4771877554816742, 'd_model_multiplier': 2, 'num_layers': 5, 'n_heads': 64, 'dim_feedforward': 315, 'batch_size': 56, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 112}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 66
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2387[0m   [32m0.7187[0m        [35m0.2782[0m       [31m0.9105[0m        [94m0.3144[0m     +  26.9671
      2   0.2216   [32m0.7190[0m        [35m0.2704[0m       [31m0.9129[0m        [94m0.3032[0m     +  27.6094
      3   0.1953   0.6774        [35m0.2654[0m       0.9105        [94m0.2904[0m     +  27.4156
      4   [36m0.2496[0m   0.7176        [35m0.2572[0m       [31m0.9154[0m        [94m0.2878[0m     +  27.4449
      5   0.2273   0.6730        [35m0.2555[0m       0.9105        0.2985        27.2256
      6   0.2073   0.6761        0.2561       0.9105        0.2976        27.7382
      7   0.1998   0.6741        [35m0.2547[0m       0.9117        0.2997        27.5815
      8   0.1988   0.6734        0.2548       0.9129        0.3080        27.6343
      9   0.2099   0.6802        [35m0.2531[0m       0.9129        0.2940        27.6069
     10   0.2085   0.6781        0.2546       0.9129        0.2945        27.5266
     11   0.2086   0.6778        [35m0.2522[0m       0.9129        [94m0.2849[0m     +  27.4728
     12   0.2055   0.6750        [35m0.2503[0m       0.9129        [94m0.2845[0m     +  27.5265
     13   0.2213   0.6786        0.2518       0.9129        [94m0.2752[0m     +  27.5316
     14   0.2114   0.6792        0.2520       0.9129        [94m0.2745[0m     +  27.2630
     15   0.2164   0.6885        0.2530       0.9129        0.2778        27.4453
     16   0.2130   0.6829        0.2516       0.9129        0.2752        27.6095
     17   0.2232   0.6803        0.2514       0.9129        [94m0.2723[0m     +  27.4575
     18   0.2147   0.6770        0.2525       0.9129        0.2800        27.8747
     19   0.2004   0.6739        [35m0.2497[0m       0.9129        0.2746        27.9212
     20   0.1979   0.6719        0.2508       0.9129        0.2839        27.5526
     21   0.2174   0.6757        0.2529       0.9129        0.2796        27.9571
     22   0.1994   0.6748        [35m0.2494[0m       0.9129        0.2810        27.4781
     23   0.2193   0.6783        0.2496       0.9129        0.2776        27.5038
     24   0.2071   0.6786        0.2514       0.9129        0.2775        27.4490
     25   0.2045   0.6766        0.2511       0.9129        0.2754        27.6892
     26   0.2048   0.6784        0.2509       0.9129        0.2740        27.5009
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 20:22:55,088][0m Trial 488 finished with value: 0.2723433031929159 and parameters: {'lr': 0.004163953150037487, 'dropout': 0.6670370566427266, 'd_model_multiplier': 4, 'num_layers': 6, 'n_heads': 32, 'dim_feedforward': 331, 'batch_size': 64, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 66}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 59
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0596[0m   [32m0.3434[0m        [35m0.4677[0m       [31m0.9166[0m        [94m0.3246[0m     +  21.6423
      2   [36m0.2045[0m   [32m0.6372[0m        [35m0.2891[0m       0.9166        [94m0.2779[0m     +  21.7215
      3   [36m0.2528[0m   [32m0.7011[0m        [35m0.2563[0m       0.9166        [94m0.2673[0m     +  22.0683
      4   [36m0.2834[0m   [32m0.7294[0m        [35m0.2475[0m       0.9154        [94m0.2600[0m     +  22.0780
      5   [36m0.3164[0m   [32m0.7432[0m        [35m0.2416[0m       0.9166        [94m0.2534[0m     +  22.0964
      6   [36m0.3277[0m   [32m0.7493[0m        [35m0.2384[0m       [31m0.9178[0m        [94m0.2522[0m     +  22.1355
      7   [36m0.3329[0m   [32m0.7521[0m        [35m0.2356[0m       0.9178        [94m0.2522[0m     +  22.0652
      8   [36m0.3371[0m   [32m0.7560[0m        [35m0.2331[0m       [31m0.9202[0m        0.2523        22.4982
      9   [36m0.3426[0m   [32m0.7583[0m        0.2336       0.9202        [94m0.2520[0m     +  22.0362
     10   [36m0.3445[0m   [32m0.7595[0m        [35m0.2313[0m       0.9190        0.2536        21.9462
     11   [36m0.3542[0m   [32m0.7620[0m        [35m0.2309[0m       0.9178        0.2536        22.0056
     12   0.3518   [32m0.7628[0m        [35m0.2288[0m       0.9178        0.2553        21.8566
     13   0.3523   [32m0.7646[0m        0.2297       [31m0.9214[0m        0.2534        21.8383
     14   0.3524   [32m0.7662[0m        0.2295       0.9202        0.2527        21.8646
     15   [36m0.3572[0m   [32m0.7679[0m        0.2298       0.9190        0.2533        21.8091
     16   [36m0.3577[0m   [32m0.7690[0m        0.2289       0.9166        0.2531        21.8348
     17   [36m0.3589[0m   [32m0.7694[0m        [35m0.2272[0m       0.9166        0.2522        21.8758
     18   0.3578   0.7688        [35m0.2253[0m       0.9166        0.2556        22.0038
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 20:29:53,712][0m Trial 489 finished with value: 0.25203626297477366 and parameters: {'lr': 3.226603529759864e-05, 'dropout': 0.5680499903513823, 'd_model_multiplier': 8, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 452, 'batch_size': 71, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 59}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 70
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2834[0m   [32m0.7312[0m        [35m0.3228[0m       [31m0.9069[0m        [94m0.3388[0m     +  29.9631
      2   [36m0.3112[0m   0.7236        [35m0.2688[0m       [31m0.9081[0m        0.4068        30.4244
      3   0.2470   0.6843        [35m0.2613[0m       0.9069        0.4527        30.3037
      4   0.3080   [32m0.7320[0m        [35m0.2584[0m       [31m0.9105[0m        0.4049        30.4262
      5   [36m0.3333[0m   [32m0.7517[0m        [35m0.2489[0m       0.9105        0.3621        30.1101
      6   0.2887   0.7352        [35m0.2468[0m       0.9081        0.4686        30.6206
      7   0.3210   [32m0.7677[0m        0.2491       0.9081        0.3487        30.8365
      8   [36m0.3492[0m   [32m0.7802[0m        [35m0.2371[0m       0.9081        [94m0.3167[0m     +  30.5534
      9   0.3265   0.7673        0.2442       0.9069        0.3628        30.4852
     10   0.3169   0.7599        0.2372       0.9105        0.4091        30.5899
     11   0.3098   0.7704        0.2383       0.9057        0.3489        30.5420
     12   0.3121   [32m0.7823[0m        0.2383       0.9069        0.3270        30.5555
     13   0.3263   0.7744        [35m0.2303[0m       0.9081        0.3651        30.5123
     14   0.3074   0.7806        0.2334       0.9045        0.3480        30.3728
     15   0.3076   0.7737        0.2330       0.9057        0.3442        30.0220
     16   0.3023   0.7682        0.2313       0.9069        0.3537        30.5692
     17   0.3210   0.7736        [35m0.2264[0m       0.9069        0.3420        30.4483
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 20:39:02,977][0m Trial 490 finished with value: 0.31666344729676715 and parameters: {'lr': 0.0002652435396219593, 'dropout': 0.4511177903792686, 'd_model_multiplier': 32, 'num_layers': 3, 'n_heads': 32, 'dim_feedforward': 443, 'batch_size': 82, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.001, 'top_n_features': 70}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 61
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2410[0m   [32m0.7002[0m        [35m0.3608[0m       [31m0.9154[0m        [94m0.2654[0m     +  27.0595
      2   [36m0.3216[0m   [32m0.7785[0m        [35m0.2396[0m       [31m0.9202[0m        [94m0.2456[0m     +  27.0950
      3   [36m0.3278[0m   [32m0.7846[0m        [35m0.2292[0m       [31m0.9262[0m        [94m0.2447[0m     +  27.0580
      4   [36m0.3406[0m   [32m0.7888[0m        [35m0.2286[0m       0.9226        0.2450        27.3140
      5   0.3262   [32m0.7908[0m        [35m0.2259[0m       0.9178        0.2461        27.3474
      6   0.3352   [32m0.7966[0m        [35m0.2246[0m       0.9178        0.2471        27.4203
      7   0.3134   [32m0.7969[0m        [35m0.2231[0m       0.9154        0.2481        27.8044
      8   0.3304   [32m0.7993[0m        [35m0.2224[0m       0.9141        0.2487        27.4412
      9   0.3127   0.7963        [35m0.2196[0m       0.9141        0.2524        27.3546
     10   0.3154   0.7984        [35m0.2187[0m       0.9093        0.2562        27.3055
     11   0.3087   0.7975        [35m0.2179[0m       0.9141        0.2548        27.3291
     12   0.3211   [32m0.8025[0m        [35m0.2151[0m       0.9117        0.2541        27.4960
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 20:44:59,026][0m Trial 491 finished with value: 0.24473300215843366 and parameters: {'lr': 0.00014090815378506213, 'dropout': 0.30710615325570306, 'd_model_multiplier': 4, 'num_layers': 6, 'n_heads': 32, 'dim_feedforward': 300, 'batch_size': 60, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 61}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 85
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.1534[0m   [32m0.6515[0m        [35m0.7196[0m       [31m0.5018[0m        [94m0.6958[0m     +  9.2073
      2   [36m0.1564[0m   0.6457        [35m0.7025[0m       [31m0.5308[0m        [94m0.6944[0m     +  9.0349
      3   [36m0.1582[0m   [32m0.6567[0m        [35m0.6878[0m       0.5030        0.6945        9.2944
      4   [36m0.1714[0m   [32m0.6572[0m        [35m0.6648[0m       0.5042        [94m0.6942[0m     +  9.5777
      5   0.1556   [32m0.6623[0m        [35m0.6433[0m       0.5091        [94m0.6934[0m     +  9.5771
      6   0.1595   0.6520        [35m0.6216[0m       [31m0.5780[0m        [94m0.6908[0m     +  9.7111
      7   0.1291   0.6369        [35m0.5976[0m       [31m0.6143[0m        [94m0.6875[0m     +  9.3719
      8   0.1140   0.6049        [35m0.5773[0m       [31m0.7037[0m        [94m0.6841[0m     +  9.5971
      9   0.1140   0.6022        [35m0.5549[0m       0.6917        0.6847        9.6314
     10   0.0980   0.5545        [35m0.5303[0m       [31m0.7654[0m        [94m0.6800[0m     +  9.5374
     11   0.0922   0.5232        [35m0.5056[0m       [31m0.8561[0m        [94m0.6714[0m     +  9.5487
     12   0.0931   0.5252        [35m0.4873[0m       0.8307        0.6744        9.4422
     13   0.0752   0.4452        [35m0.4661[0m       [31m0.8888[0m        [94m0.6606[0m     +  9.4592
     14   0.0800   0.4705        [35m0.4514[0m       [31m0.8924[0m        0.6637        9.3286
     15   0.0716   0.4439        [35m0.4333[0m       [31m0.9141[0m        [94m0.6372[0m     +  9.6432
     16   0.0711   0.4314        [35m0.4200[0m       [31m0.9154[0m        [94m0.6364[0m     +  9.5949
     17   0.0700   0.4330        [35m0.4016[0m       0.9129        [94m0.6305[0m     +  9.6501
     18   0.0680   0.4183        [35m0.3899[0m       [31m0.9166[0m        [94m0.6302[0m     +  9.4717
     19   0.0721   0.4470        [35m0.3837[0m       [31m0.9178[0m        [94m0.6078[0m     +  9.8544
     20   0.0670   0.4140        [35m0.3718[0m       [31m0.9202[0m        0.6123        9.5935
     21   0.0696   0.4436        [35m0.3660[0m       0.9202        [94m0.5661[0m     +  9.6256
     22   0.0695   0.4381        [35m0.3533[0m       0.9202        [94m0.5622[0m     +  9.5250
     23   0.0729   0.4668        [35m0.3451[0m       0.9202        [94m0.5539[0m     +  9.6818
     24   0.0732   0.4703        [35m0.3446[0m       0.9202        [94m0.5470[0m     +  9.5763
     25   0.0749   0.4805        [35m0.3372[0m       0.9202        [94m0.5435[0m     +  9.5373
     26   0.0787   0.5043        [35m0.3300[0m       0.9202        [94m0.5325[0m     +  9.7950
     27   0.0793   0.5048        [35m0.3234[0m       0.9202        [94m0.5208[0m     +  9.3752
     28   0.0806   0.5087        0.3237       0.9202        [94m0.5079[0m     +  9.6224
     29   0.0824   0.5193        [35m0.3203[0m       0.9202        0.5295        9.4432
     30   0.0893   0.5465        [35m0.3161[0m       0.9202        [94m0.4858[0m     +  9.4452
     31   0.0919   0.5553        [35m0.3113[0m       0.9202        [94m0.4694[0m     +  9.3035
     32   0.0950   0.5619        0.3121       0.9202        [94m0.4660[0m     +  9.2978
     33   0.0991   0.5730        [35m0.3099[0m       0.9202        0.4729        9.6586
     34   0.1072   0.5869        [35m0.3034[0m       0.9202        0.4785        9.4504
     35   0.1129   0.5986        [35m0.3016[0m       0.9202        [94m0.4612[0m     +  9.6222
     36   0.1175   0.6069        0.3052       0.9202        [94m0.4596[0m     +  9.6857
     37   0.1223   0.6204        [35m0.3003[0m       0.9202        [94m0.4586[0m     +  9.5357
     38   0.1277   0.6149        [35m0.2997[0m       0.9202        0.4591        9.6947
     39   0.1350   0.6298        [35m0.2972[0m       0.9202        [94m0.4362[0m     +  9.3748
     40   0.1388   0.6321        0.2999       0.9202        0.4388        9.5460
     41   0.1409   0.6395        [35m0.2899[0m       0.9202        [94m0.4130[0m     +  9.5650
     42   0.1499   0.6462        0.2944       0.9190        0.4315        9.5849
     43   0.1535   0.6518        0.2923       0.9202        [94m0.4051[0m     +  9.5841
     44   0.1549   0.6498        0.2921       0.9178        0.4381        9.2021
     45   0.1579   0.6579        0.2934       0.9190        0.4118        9.4285
     46   0.1608   [32m0.6636[0m        0.2920       0.9190        0.4083        9.6351
     47   0.1639   [32m0.6644[0m        [35m0.2880[0m       0.9178        0.4118        9.7269
     48   0.1659   [32m0.6648[0m        0.2892       0.9190        [94m0.3963[0m     +  9.4285
     49   0.1713   [32m0.6703[0m        [35m0.2872[0m       0.9178        0.4037        9.6759
     50   [36m0.1768[0m   [32m0.6758[0m        0.2890       0.9178        [94m0.3848[0m     +  9.4744
[32m[I 2023-05-04 20:52:58,233][0m Trial 492 finished with value: 0.38482100131866287 and parameters: {'lr': 6.395798201420775e-06, 'dropout': 0.5158627955203665, 'd_model_multiplier': 4, 'num_layers': 5, 'n_heads': 4, 'dim_feedforward': 352, 'batch_size': 53, 'pos_encoding': 'learnable', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 85}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 51
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2784[0m   [32m0.7788[0m        [35m0.2896[0m       [31m0.9093[0m        [94m0.2480[0m     +  19.9396
      2   [36m0.2964[0m   [32m0.7900[0m        [35m0.2398[0m       [31m0.9166[0m        0.2505        20.0330
      3   0.2931   0.7873        [35m0.2350[0m       0.9166        0.2501        20.0490
      4   0.2892   0.7842        [35m0.2344[0m       0.9154        0.2529        20.1042
      5   0.2915   0.7860        [35m0.2324[0m       0.9166        0.2525        20.3492
      6   0.2909   0.7846        [35m0.2302[0m       0.9129        0.2527        20.1221
      7   0.2924   0.7847        [35m0.2300[0m       0.9141        0.2519        20.3501
      8   [36m0.3025[0m   0.7859        [35m0.2286[0m       0.9154        0.2504        20.1373
      9   0.2979   0.7869        0.2295       0.9166        0.2524        20.2663
     10   [36m0.3104[0m   0.7885        [35m0.2267[0m       0.9154        0.2553        20.2006
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 20:56:40,356][0m Trial 493 finished with value: 0.24795732284111152 and parameters: {'lr': 8.861860904202104e-05, 'dropout': 0.5380472707875124, 'd_model_multiplier': 4, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 342, 'batch_size': 68, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 51}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 65
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2586[0m   [32m0.7204[0m        [35m0.3865[0m       [31m0.9202[0m        [94m0.2407[0m     +  46.1067
      2   [36m0.2954[0m   [32m0.7625[0m        [35m0.2423[0m       [31m0.9214[0m        [94m0.2338[0m     +  46.3346
      3   0.2843   [32m0.7677[0m        [35m0.2346[0m       [31m0.9226[0m        0.2378        46.3905
      4   0.2812   [32m0.7720[0m        [35m0.2323[0m       [31m0.9238[0m        0.2398        46.3121
      5   0.2810   [32m0.7742[0m        [35m0.2294[0m       0.9214        0.2427        46.4284
      6   0.2762   0.7741        [35m0.2275[0m       0.9214        0.2454        46.5096
      7   0.2773   [32m0.7749[0m        [35m0.2271[0m       0.9214        0.2461        46.3672
      8   0.2778   0.7743        [35m0.2242[0m       0.9202        0.2476        46.2892
      9   0.2816   0.7744        [35m0.2222[0m       0.9214        0.2501        46.4632
     10   0.2827   0.7737        [35m0.2207[0m       0.9214        0.2506        46.5219
     11   0.2781   [32m0.7756[0m        [35m0.2199[0m       0.9190        0.2504        46.4734
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 21:05:57,831][0m Trial 494 finished with value: 0.23379956447083544 and parameters: {'lr': 1.4017837738244384e-05, 'dropout': 0.3456076085881248, 'd_model_multiplier': 8, 'num_layers': 5, 'n_heads': 64, 'dim_feedforward': 323, 'batch_size': 27, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.001, 'top_n_features': 65}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 45
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2459[0m   [32m0.7211[0m        [35m0.2771[0m       [31m0.9069[0m        [94m0.3060[0m     +  56.6147
      2   0.2388   0.7102        [35m0.2630[0m       0.9069        0.3120        56.9608
      3   0.2392   0.7110        [35m0.2611[0m       0.9057        0.3142        56.8826
      4   0.2364   0.7087        [35m0.2582[0m       0.9045        0.3223        57.0247
      5   0.2379   0.7125        [35m0.2541[0m       0.9069        0.3159        57.1081
      6   0.2427   0.7118        0.2543       0.9069        0.3186        57.0016
      7   0.2340   0.7125        0.2557       0.9069        0.3247        56.9580
      8   0.2246   0.7100        [35m0.2537[0m       [31m0.9081[0m        0.3278        56.9086
      9   0.2267   0.7104        [35m0.2518[0m       0.9069        0.3153        57.0061
     10   0.2300   0.7122        [35m0.2505[0m       0.9069        0.3198        56.8318
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 21:16:24,811][0m Trial 495 finished with value: 0.3060079520926366 and parameters: {'lr': 0.0026472947019409525, 'dropout': 0.6135190295000892, 'd_model_multiplier': 4, 'num_layers': 14, 'n_heads': 32, 'dim_feedforward': 334, 'batch_size': 46, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 45}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 78
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1330[0m   [32m0.6103[0m        [35m0.8423[0m       [31m0.8634[0m        [94m0.3426[0m     +  27.4276
      2   [36m0.1489[0m   [32m0.6407[0m        [35m0.2763[0m       [31m0.9214[0m        [94m0.2704[0m     +  27.9645
      3   0.1463   [32m0.6516[0m        [35m0.2559[0m       0.9214        [94m0.2666[0m     +  27.7276
      4   [36m0.1518[0m   [32m0.6522[0m        [35m0.2557[0m       0.9214        [94m0.2642[0m     +  27.8364
      5   0.1460   0.6472        [35m0.2551[0m       0.9214        0.2643        27.9071
      6   0.1511   [32m0.6559[0m        [35m0.2533[0m       0.9214        [94m0.2622[0m     +  27.8720
      7   0.1483   0.6553        [35m0.2529[0m       0.9214        0.2631        27.9399
      8   0.1467   0.6521        [35m0.2517[0m       0.9214        0.2630        27.6204
      9   0.1437   0.6491        0.2532       0.9214        0.2639        27.7126
     10   0.1449   0.6493        0.2551       0.9214        0.2633        27.7759
     11   0.1484   0.6529        0.2531       0.9214        0.2630        27.9334
     12   [36m0.1535[0m   0.6551        [35m0.2503[0m       0.9214        0.2636        28.0025
     13   0.1463   0.6518        0.2510       0.9214        0.2638        27.7008
     14   0.1451   0.6529        0.2512       0.9214        0.2636        28.1975
     15   0.1494   [32m0.6593[0m        0.2515       0.9214        0.2639        28.1940
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 21:23:51,100][0m Trial 496 finished with value: 0.26224948581141455 and parameters: {'lr': 0.007802251748834195, 'dropout': 0.23609379488921695, 'd_model_multiplier': 4, 'num_layers': 6, 'n_heads': 32, 'dim_feedforward': 367, 'batch_size': 77, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 78}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 99
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0708[0m   [32m0.3450[0m        [35m0.5711[0m       [31m0.9166[0m        [94m0.5523[0m     +  32.0253
      2   [36m0.0907[0m   [32m0.4367[0m        [35m0.4041[0m       0.9166        [94m0.4294[0m     +  31.8275
      3   [36m0.2006[0m   [32m0.6361[0m        [35m0.3151[0m       0.9166        [94m0.3628[0m     +  32.2401
      4   [36m0.2618[0m   [32m0.7054[0m        [35m0.2818[0m       [31m0.9190[0m        [94m0.3316[0m     +  32.1925
      5   [36m0.2737[0m   [32m0.7290[0m        [35m0.2690[0m       0.9178        [94m0.3178[0m     +  32.2104
      6   [36m0.2794[0m   [32m0.7411[0m        [35m0.2658[0m       [31m0.9202[0m        [94m0.3039[0m     +  32.2941
      7   [36m0.2825[0m   [32m0.7458[0m        [35m0.2593[0m       [31m0.9214[0m        [94m0.2976[0m     +  32.2133
      8   [36m0.2878[0m   [32m0.7485[0m        0.2619       0.9214        [94m0.2945[0m     +  32.2966
      9   [36m0.2878[0m   [32m0.7526[0m        [35m0.2570[0m       0.9202        [94m0.2924[0m     +  32.0886
     10   [36m0.2936[0m   [32m0.7551[0m        [35m0.2568[0m       0.9214        0.2928        32.2456
     11   [36m0.3017[0m   0.7530        [35m0.2512[0m       0.9214        [94m0.2818[0m     +  32.2286
     12   [36m0.3068[0m   [32m0.7578[0m        0.2525       0.9202        0.2849        32.2508
     13   0.3050   0.7528        0.2528       0.9214        [94m0.2805[0m     +  32.3442
     14   0.3001   0.7545        [35m0.2509[0m       0.9190        [94m0.2780[0m     +  32.0366
     15   0.2986   0.7523        [35m0.2508[0m       0.9190        [94m0.2732[0m     +  32.1553
     16   0.2987   0.7538        [35m0.2486[0m       0.9178        [94m0.2731[0m     +  32.3442
     17   0.2912   0.7547        [35m0.2471[0m       0.9202        [94m0.2677[0m     +  32.2599
     18   0.2892   0.7511        0.2471       0.9202        [94m0.2658[0m     +  31.8839
     19   0.2910   0.7531        [35m0.2457[0m       0.9202        [94m0.2622[0m     +  32.0994
     20   0.2827   0.7451        [35m0.2439[0m       0.9202        0.2647        32.2655
     21   0.2895   0.7490        [35m0.2418[0m       0.9202        0.2623        32.2883
     22   0.2913   0.7549        [35m0.2414[0m       0.9190        [94m0.2585[0m     +  32.3831
     23   0.2905   0.7527        0.2421       0.9190        0.2604        32.2621
     24   0.2850   0.7531        0.2417       0.9190        0.2599        32.1969
     25   0.2865   0.7519        [35m0.2398[0m       0.9178        0.2600        32.1888
     26   0.2877   0.7534        [35m0.2368[0m       0.9190        [94m0.2551[0m     +  32.4211
     27   0.2968   [32m0.7580[0m        0.2383       0.9190        [94m0.2548[0m     +  32.2764
     28   0.2909   0.7507        [35m0.2340[0m       0.9178        0.2561        32.6759
     29   0.2872   0.7549        0.2388       0.9190        [94m0.2530[0m     +  32.2730
     30   0.2876   0.7519        0.2366       0.9190        0.2538        32.4528
     31   0.2840   0.7484        0.2375       0.9202        0.2537        32.1562
     32   0.2841   0.7498        0.2348       0.9202        0.2537        32.2106
     33   0.2836   0.7482        0.2351       0.9202        [94m0.2524[0m     +  32.1064
     34   0.2849   0.7489        0.2351       0.9190        0.2531        32.0677
     35   0.2811   0.7492        0.2349       0.9190        0.2535        32.3299
     36   0.2829   0.7518        [35m0.2307[0m       0.9202        [94m0.2520[0m     +  32.1795
     37   0.2788   0.7475        [35m0.2302[0m       0.9190        [94m0.2519[0m     +  32.0285
     38   0.2877   0.7528        [35m0.2281[0m       0.9214        [94m0.2506[0m     +  32.2847
     39   0.2779   0.7486        [35m0.2270[0m       0.9202        0.2524        32.1449
     40   0.2773   0.7493        0.2299       0.9202        0.2522        32.1636
     41   0.2728   0.7440        0.2310       0.9202        0.2537        32.2131
     42   0.2747   0.7444        [35m0.2260[0m       0.9214        0.2542        32.2293
     43   0.2741   0.7461        0.2266       0.9202        0.2536        32.1492
     44   0.2774   0.7526        0.2274       0.9202        0.2523        32.1326
     45   0.2805   0.7555        [35m0.2246[0m       0.9214        0.2517        31.8356
     46   0.2759   0.7540        [35m0.2231[0m       0.9202        0.2518        32.2895
     47   0.2707   0.7482        0.2238       0.9202        0.2549        32.2229
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 21:49:40,663][0m Trial 497 finished with value: 0.25062639403191844 and parameters: {'lr': 2.59204628011979e-05, 'dropout': 0.4317016895563515, 'd_model_multiplier': 4, 'num_layers': 7, 'n_heads': 32, 'dim_feedforward': 357, 'batch_size': 63, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 99}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 72
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0659[0m   [32m0.3686[0m        [35m0.5902[0m       [31m0.9262[0m        [94m0.4565[0m     +  10.3553
      2   [36m0.0797[0m   [32m0.4799[0m        [35m0.3918[0m       0.9262        [94m0.3048[0m     +  10.5275
      3   [36m0.1913[0m   [32m0.6276[0m        [35m0.2994[0m       0.9262        [94m0.2601[0m     +  10.7532
      4   [36m0.1999[0m   [32m0.6642[0m        [35m0.2676[0m       0.9262        [94m0.2486[0m     +  10.5778
      5   [36m0.2166[0m   [32m0.6986[0m        [35m0.2546[0m       0.9262        [94m0.2421[0m     +  10.5410
      6   [36m0.2194[0m   [32m0.7219[0m        [35m0.2465[0m       0.9262        [94m0.2384[0m     +  10.7118
      7   [36m0.2203[0m   [32m0.7334[0m        [35m0.2410[0m       0.9238        [94m0.2370[0m     +  10.4583
      8   [36m0.2295[0m   [32m0.7399[0m        [35m0.2375[0m       0.9214        [94m0.2362[0m     +  10.5517
      9   0.2105   0.7396        [35m0.2355[0m       0.9202        0.2369        10.8147
     10   0.2166   [32m0.7444[0m        [35m0.2341[0m       0.9190        0.2371        10.8015
     11   0.2124   [32m0.7463[0m        [35m0.2324[0m       0.9178        0.2380        10.5869
     12   0.2149   [32m0.7483[0m        [35m0.2309[0m       0.9141        0.2387        10.7995
     13   0.2139   0.7479        [35m0.2292[0m       0.9154        0.2404        10.5968
     14   0.2140   0.7479        0.2305       0.9141        0.2400        10.5792
     15   0.2127   [32m0.7493[0m        [35m0.2288[0m       0.9141        0.2411        10.4964
     16   0.2119   [32m0.7500[0m        [35m0.2265[0m       0.9141        0.2419        10.7821
     17   0.2126   [32m0.7513[0m        [35m0.2262[0m       0.9141        0.2424        10.6561
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 21:52:52,639][0m Trial 498 finished with value: 0.23616621532492077 and parameters: {'lr': 4.987050561784107e-05, 'dropout': 0.2879379202582376, 'd_model_multiplier': 4, 'num_layers': 5, 'n_heads': 8, 'dim_feedforward': 345, 'batch_size': 54, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.001, 'top_n_features': 72}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 55
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2413[0m   [32m0.7836[0m        [35m0.3699[0m       [31m0.9166[0m        [94m0.2440[0m     +  13.2872
      2   [36m0.2970[0m   [32m0.7996[0m        [35m0.2390[0m       0.9154        [94m0.2339[0m     +  13.5060
      3   [36m0.3129[0m   [32m0.8030[0m        [35m0.2336[0m       0.9166        [94m0.2329[0m     +  13.4701
      4   [36m0.3169[0m   [32m0.8046[0m        [35m0.2300[0m       [31m0.9178[0m        0.2354        13.4475
      5   0.3073   0.8011        [35m0.2280[0m       [31m0.9190[0m        0.2390        13.5586
      6   0.2997   [32m0.8054[0m        [35m0.2244[0m       0.9166        0.2421        13.5185
      7   0.2935   0.7999        [35m0.2231[0m       0.9154        0.2458        13.4162
      8   0.2919   0.7908        [35m0.2211[0m       0.9141        0.2482        13.5482
      9   0.2802   0.7725        [35m0.2176[0m       0.9141        0.2523        13.7601
     10   0.2884   0.7803        0.2182       0.9141        0.2512        13.4229
     11   0.2770   0.7693        [35m0.2145[0m       0.9141        0.2557        13.4601
     12   0.2763   0.7703        [35m0.2128[0m       0.9141        0.2558        13.5559
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 21:55:48,555][0m Trial 499 finished with value: 0.23288851724137993 and parameters: {'lr': 9.890012232299579e-05, 'dropout': 0.26732204857915015, 'd_model_multiplier': 8, 'num_layers': 4, 'n_heads': 16, 'dim_feedforward': 378, 'batch_size': 37, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 55}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 62
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2760[0m   [32m0.7640[0m        [35m0.2940[0m       [31m0.9033[0m        [94m0.2650[0m     +  26.9770
      2   [36m0.2925[0m   [32m0.7679[0m        [35m0.2367[0m       0.9021        [94m0.2635[0m     +  27.1520
      3   [36m0.3124[0m   [32m0.7830[0m        [35m0.2323[0m       [31m0.9105[0m        [94m0.2609[0m     +  27.2795
      4   [36m0.3163[0m   [32m0.7904[0m        [35m0.2311[0m       0.9105        [94m0.2595[0m     +  27.2641
      5   0.3141   [32m0.7909[0m        [35m0.2295[0m       0.9105        0.2623        27.1046
      6   [36m0.3307[0m   [32m0.7988[0m        [35m0.2287[0m       0.9105        [94m0.2571[0m     +  27.2017
      7   0.3187   0.7986        [35m0.2281[0m       0.9093        0.2614        27.3971
      8   [36m0.3353[0m   [32m0.8036[0m        [35m0.2276[0m       [31m0.9141[0m        0.2589        27.3841
      9   0.3329   [32m0.8084[0m        [35m0.2258[0m       0.9105        [94m0.2558[0m     +  27.4541
     10   [36m0.3406[0m   [32m0.8110[0m        [35m0.2249[0m       0.9141        [94m0.2535[0m     +  27.2400
     11   [36m0.3428[0m   [32m0.8141[0m        [35m0.2228[0m       0.9141        0.2543        27.2836
     12   0.3423   [32m0.8179[0m        0.2237       0.9117        [94m0.2525[0m     +  27.3194
     13   [36m0.3444[0m   [32m0.8201[0m        [35m0.2218[0m       0.9129        0.2533        27.3777
     14   [36m0.3472[0m   [32m0.8224[0m        [35m0.2208[0m       0.9129        0.2527        27.1833
     15   0.3464   0.8196        0.2215       0.9117        [94m0.2525[0m     +  27.3556
     16   [36m0.3502[0m   [32m0.8244[0m        [35m0.2195[0m       0.9141        [94m0.2522[0m     +  27.3517
     17   [36m0.3571[0m   0.8238        [35m0.2182[0m       0.9117        [94m0.2511[0m     +  27.3491
     18   [36m0.3581[0m   0.8233        [35m0.2182[0m       [31m0.9154[0m        [94m0.2508[0m     +  27.2234
     19   [36m0.3629[0m   [32m0.8268[0m        0.2200       0.9117        [94m0.2502[0m     +  27.4093
     20   0.3580   0.8220        0.2195       0.9129        0.2519        27.2226
     21   0.3602   0.8232        [35m0.2172[0m       0.9129        0.2509        27.1068
     22   0.3547   0.8234        [35m0.2159[0m       0.9141        0.2503        27.1272
     23   0.3513   0.8198        [35m0.2131[0m       0.9129        0.2547        26.9298
     24   0.3576   [32m0.8283[0m        [35m0.2130[0m       0.9141        0.2514        27.2413
     25   0.3525   0.8239        0.2140       0.9129        0.2528        26.9530
     26   0.3557   0.8262        0.2139       0.9129        0.2522        27.0861
     27   0.3539   0.8224        0.2146       0.9141        0.2536        27.2112
     28   0.3404   0.8190        [35m0.2120[0m       0.9129        0.2571        27.2383
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 22:08:59,962][0m Trial 500 finished with value: 0.2502186149989391 and parameters: {'lr': 3.5975376604777725e-05, 'dropout': 0.40742732962006545, 'd_model_multiplier': 2, 'num_layers': 6, 'n_heads': 32, 'dim_feedforward': 316, 'batch_size': 18, 'pos_encoding': 'learnable', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 62}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 183
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0601[0m   [32m0.4747[0m        [35m0.5775[0m       [31m0.9371[0m        [94m0.4740[0m     +  19.8901
      2   0.0548   0.4373        [35m0.4425[0m       0.9371        [94m0.3318[0m     +  20.0567
      3   [36m0.0703[0m   [32m0.5502[0m        [35m0.3484[0m       0.9371        [94m0.2638[0m     +  20.5794
      4   [36m0.1278[0m   [32m0.6153[0m        [35m0.3013[0m       0.9371        [94m0.2385[0m     +  20.1094
      5   [36m0.1451[0m   [32m0.6446[0m        [35m0.2786[0m       0.9371        [94m0.2292[0m     +  20.6428
      6   [36m0.1534[0m   [32m0.6575[0m        [35m0.2676[0m       0.9371        [94m0.2255[0m     +  20.2301
      7   [36m0.1623[0m   [32m0.6654[0m        [35m0.2607[0m       0.9371        [94m0.2238[0m     +  20.4638
      8   [36m0.1656[0m   [32m0.6699[0m        [35m0.2568[0m       0.9371        [94m0.2229[0m     +  20.2551
      9   [36m0.1669[0m   [32m0.6739[0m        [35m0.2548[0m       0.9371        [94m0.2222[0m     +  20.4659
     10   [36m0.1762[0m   [32m0.6784[0m        [35m0.2521[0m       0.9371        [94m0.2216[0m     +  20.2709
     11   [36m0.1815[0m   [32m0.6852[0m        [35m0.2502[0m       0.9371        [94m0.2208[0m     +  20.3478
     12   [36m0.1873[0m   [32m0.6910[0m        [35m0.2486[0m       0.9359        [94m0.2204[0m     +  20.2878
     13   [36m0.1993[0m   [32m0.6991[0m        [35m0.2471[0m       0.9359        [94m0.2194[0m     +  20.3635
     14   0.1990   [32m0.7049[0m        [35m0.2455[0m       0.9371        [94m0.2184[0m     +  20.4229
     15   [36m0.2044[0m   [32m0.7125[0m        [35m0.2437[0m       0.9371        [94m0.2184[0m     +  20.3738
     16   [36m0.2107[0m   [32m0.7179[0m        [35m0.2427[0m       0.9359        [94m0.2178[0m     +  20.2788
     17   [36m0.2164[0m   [32m0.7220[0m        [35m0.2413[0m       0.9299        0.2180        20.3011
     18   0.2130   [32m0.7253[0m        [35m0.2406[0m       0.9311        0.2180        20.0470
     19   0.2158   [32m0.7275[0m        [35m0.2392[0m       0.9299        [94m0.2177[0m     +  19.8873
     20   0.2149   [32m0.7281[0m        [35m0.2388[0m       0.9299        0.2179        20.3754
     21   [36m0.2195[0m   [32m0.7306[0m        0.2391       0.9299        0.2185        20.2556
     22   [36m0.2288[0m   [32m0.7325[0m        [35m0.2384[0m       0.9299        0.2185        20.3611
     23   [36m0.2307[0m   [32m0.7328[0m        0.2387       0.9311        0.2182        20.3151
     24   0.2264   [32m0.7337[0m        [35m0.2376[0m       0.9323        [94m0.2177[0m     +  20.2493
     25   [36m0.2319[0m   [32m0.7371[0m        [35m0.2360[0m       0.9311        [94m0.2175[0m     +  20.1543
     26   [36m0.2334[0m   [32m0.7377[0m        0.2374       0.9311        0.2179        20.3567
     27   [36m0.2360[0m   [32m0.7391[0m        [35m0.2351[0m       0.9335        [94m0.2168[0m     +  20.2875
     28   [36m0.2404[0m   [32m0.7408[0m        [35m0.2346[0m       0.9335        0.2177        20.0403
     29   [36m0.2414[0m   [32m0.7412[0m        [35m0.2341[0m       0.9347        0.2192        20.2799
     30   0.2399   [32m0.7425[0m        0.2347       0.9347        0.2173        20.2274
     31   0.2395   [32m0.7439[0m        [35m0.2324[0m       0.9323        0.2192        20.7784
     32   [36m0.2424[0m   [32m0.7443[0m        0.2333       0.9299        0.2198        20.5530
     33   0.2416   [32m0.7457[0m        [35m0.2323[0m       0.9274        0.2211        20.5345
     34   [36m0.2432[0m   0.7454        0.2335       0.9274        0.2205        20.4978
     35   [36m0.2450[0m   0.7454        0.2329       0.9299        0.2182        20.2551
     36   [36m0.2476[0m   [32m0.7474[0m        0.2326       0.9287        0.2196        20.3246
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 22:21:33,197][0m Trial 501 finished with value: 0.21684558655359437 and parameters: {'lr': 1.0729459312744097e-05, 'dropout': 0.46387891391531133, 'd_model_multiplier': 4, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 333, 'batch_size': 59, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 183}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 59
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
Warning, assumed runtime error: CUDA out of memory. Tried to allocate 1.68 GiB (GPU 0; 23.70 GiB total capacity; 19.05 GiB already allocated; 795.25 MiB free; 21.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[32m[I 2023-05-04 22:21:37,695][0m Trial 502 finished with value: 100.0 and parameters: {'lr': 2.475658984923004e-06, 'dropout': 0.16727086636019955, 'd_model_multiplier': 32, 'num_layers': 5, 'n_heads': 64, 'dim_feedforward': 428, 'batch_size': 71, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.001, 'top_n_features': 59}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 68
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.3652[0m   [32m0.8050[0m        [35m0.2658[0m       [31m0.9274[0m        [94m0.2291[0m     +  8.3324
      2   0.2422   0.7632        [35m0.2656[0m       0.9190        0.2561        8.7026
      3   0.2420   0.7579        [35m0.2607[0m       0.9166        0.2447        9.0133
      4   0.2305   0.7553        0.2611       0.9190        0.2584        9.0915
      5   0.2371   0.7558        [35m0.2604[0m       0.9190        0.2648        9.2237
      6   0.2442   0.7517        0.2620       0.9190        0.2694        9.1808
      7   0.2394   0.7603        0.2617       0.9190        0.2562        9.0593
      8   0.2642   0.7624        [35m0.2597[0m       0.9154        0.2530        9.3177
      9   0.2470   0.7568        0.2610       0.9190        0.2586        9.4337
     10   0.2553   0.7630        [35m0.2589[0m       0.9190        0.2503        9.1898
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 22:23:17,753][0m Trial 503 finished with value: 0.22906589244538525 and parameters: {'lr': 0.020815793196103983, 'dropout': 0.486129175291392, 'd_model_multiplier': 4, 'num_layers': 3, 'n_heads': 4, 'dim_feedforward': 326, 'batch_size': 49, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 68}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 51
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0596[0m   [32m0.3115[0m        [35m0.5531[0m       [31m0.9141[0m        [94m0.4367[0m     +  27.4675
      2   [36m0.0613[0m   [32m0.3386[0m        [35m0.3937[0m       0.9141        [94m0.3509[0m     +  27.5272
      3   [36m0.0695[0m   [32m0.4221[0m        [35m0.3318[0m       0.9141        [94m0.3193[0m     +  27.6551
      4   [36m0.0828[0m   [32m0.5023[0m        [35m0.3011[0m       0.9141        [94m0.3037[0m     +  27.5807
      5   [36m0.1211[0m   [32m0.5569[0m        [35m0.2847[0m       0.9141        [94m0.2943[0m     +  27.6541
      6   [36m0.1533[0m   [32m0.5915[0m        [35m0.2733[0m       0.9141        [94m0.2886[0m     +  27.5205
      7   [36m0.1771[0m   [32m0.6156[0m        [35m0.2667[0m       0.9141        [94m0.2846[0m     +  27.5692
      8   [36m0.1942[0m   [32m0.6307[0m        [35m0.2628[0m       0.9141        [94m0.2817[0m     +  27.4636
      9   [36m0.2052[0m   [32m0.6434[0m        [35m0.2588[0m       0.9141        [94m0.2795[0m     +  27.6450
     10   [36m0.2243[0m   [32m0.6537[0m        [35m0.2557[0m       0.9141        [94m0.2776[0m     +  27.6021
     11   [36m0.2327[0m   [32m0.6639[0m        [35m0.2531[0m       0.9141        [94m0.2758[0m     +  27.5924
     12   [36m0.2490[0m   [32m0.6741[0m        [35m0.2516[0m       0.9141        [94m0.2739[0m     +  27.6120
     13   [36m0.2597[0m   [32m0.6864[0m        [35m0.2503[0m       0.9141        [94m0.2718[0m     +  27.5440
     14   [36m0.2718[0m   [32m0.6978[0m        [35m0.2477[0m       0.9141        [94m0.2697[0m     +  27.5667
     15   [36m0.2825[0m   [32m0.7104[0m        [35m0.2464[0m       0.9141        [94m0.2671[0m     +  27.7367
     16   [36m0.2886[0m   [32m0.7228[0m        [35m0.2441[0m       0.9141        [94m0.2641[0m     +  27.6149
     17   [36m0.2960[0m   [32m0.7333[0m        [35m0.2432[0m       0.9141        [94m0.2609[0m     +  27.5553
     18   [36m0.3040[0m   [32m0.7430[0m        [35m0.2421[0m       0.9141        [94m0.2583[0m     +  27.6114
     19   0.3037   [32m0.7504[0m        [35m0.2385[0m       [31m0.9154[0m        [94m0.2557[0m     +  27.6713
     20   [36m0.3043[0m   [32m0.7580[0m        [35m0.2376[0m       0.9154        [94m0.2537[0m     +  27.5528
     21   [36m0.3085[0m   [32m0.7643[0m        [35m0.2354[0m       0.9154        [94m0.2519[0m     +  27.6831
     22   [36m0.3086[0m   [32m0.7664[0m        0.2359       0.9129        [94m0.2511[0m     +  27.6915
     23   0.3042   [32m0.7686[0m        [35m0.2346[0m       0.9117        [94m0.2502[0m     +  27.4407
     24   0.3067   [32m0.7713[0m        [35m0.2344[0m       0.9117        [94m0.2499[0m     +  27.8414
     25   [36m0.3175[0m   [32m0.7735[0m        0.2352       0.9093        [94m0.2498[0m     +  27.5963
     26   [36m0.3179[0m   [32m0.7751[0m        [35m0.2339[0m       0.9081        [94m0.2493[0m     +  27.7876
     27   0.3179   [32m0.7760[0m        [35m0.2321[0m       0.9081        0.2495        27.6993
     28   [36m0.3188[0m   [32m0.7774[0m        0.2327       0.9081        0.2494        27.6643
     29   [36m0.3190[0m   [32m0.7783[0m        [35m0.2315[0m       0.9081        [94m0.2491[0m     +  27.6673
     30   0.3080   [32m0.7791[0m        [35m0.2314[0m       0.9081        0.2493        27.6576
     31   0.3059   [32m0.7797[0m        0.2328       0.9081        0.2492        27.6480
     32   0.3081   [32m0.7805[0m        0.2315       0.9081        [94m0.2491[0m     +  27.5614
     33   0.3055   [32m0.7814[0m        [35m0.2301[0m       0.9081        [94m0.2489[0m     +  27.5400
     34   0.3045   [32m0.7819[0m        0.2309       0.9081        0.2490        27.4661
     35   0.3051   [32m0.7823[0m        [35m0.2293[0m       0.9081        0.2490        27.4763
     36   0.3057   [32m0.7832[0m        0.2302       0.9105        0.2489        27.7901
     37   0.3061   [32m0.7840[0m        0.2312       0.9105        [94m0.2488[0m     +  27.6432
     38   0.3065   [32m0.7853[0m        [35m0.2292[0m       0.9105        [94m0.2485[0m     +  27.6217
     39   0.3076   [32m0.7860[0m        [35m0.2288[0m       0.9105        [94m0.2484[0m     +  27.5444
     40   0.3084   [32m0.7870[0m        0.2297       0.9105        [94m0.2481[0m     +  27.5673
     41   0.3066   0.7868        [35m0.2285[0m       0.9105        0.2482        27.6845
     42   0.3025   [32m0.7874[0m        [35m0.2279[0m       0.9117        0.2482        27.7160
     43   0.3029   [32m0.7876[0m        0.2303       0.9117        0.2482        27.6699
     44   0.3028   [32m0.7880[0m        0.2288       0.9105        [94m0.2480[0m     +  27.5116
     45   0.3036   [32m0.7891[0m        0.2285       0.9105        [94m0.2476[0m     +  27.6403
     46   0.3040   [32m0.7897[0m        [35m0.2271[0m       0.9105        0.2477        27.5675
     47   0.3029   [32m0.7902[0m        0.2280       0.9105        0.2477        27.7030
     48   0.3030   [32m0.7905[0m        0.2287       0.9105        0.2476        27.7693
     49   0.3012   [32m0.7907[0m        0.2284       0.9105        0.2477        27.6788
     50   0.3009   [32m0.7910[0m        0.2273       0.9105        0.2478        27.8357
[32m[I 2023-05-04 22:46:23,760][0m Trial 504 finished with value: 0.24760272219650978 and parameters: {'lr': 1.63959810369693e-06, 'dropout': 0.42165213929224954, 'd_model_multiplier': 4, 'num_layers': 6, 'n_heads': 32, 'dim_feedforward': 348, 'batch_size': 31, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 51}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 90
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2754[0m   [32m0.7435[0m        [35m0.3166[0m       [31m0.9250[0m        [94m0.2465[0m     +  41.5645
      2   0.2528   [32m0.7942[0m        [35m0.2735[0m       [31m0.9262[0m        [94m0.2371[0m     +  41.2272
      3   0.2583   [32m0.7956[0m        [35m0.2653[0m       0.9262        0.2392        41.9482
      4   0.2414   [32m0.8003[0m        0.2678       0.9262        [94m0.2327[0m     +  41.8801
      5   0.2579   0.7952        0.2692       0.9250        0.2368        41.8626
      6   0.2349   0.7991        [35m0.2622[0m       0.9250        0.2340        42.0047
      7   0.2480   0.7944        [35m0.2607[0m       0.9250        0.2439        41.5346
      8   0.2403   0.7909        [35m0.2574[0m       0.9250        0.2501        41.6396
      9   0.2470   0.7974        0.2587       0.9250        0.2525        41.7180
     10   0.2386   0.7912        [35m0.2557[0m       0.9250        0.2364        41.5265
     11   0.2356   0.7906        0.2565       0.9250        0.2374        41.5962
     12   0.2262   0.7899        [35m0.2552[0m       0.9250        0.2382        41.6962
     13   0.2333   0.7917        0.2556       0.9250        0.2452        41.7825
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 22:56:07,913][0m Trial 505 finished with value: 0.23272360873914255 and parameters: {'lr': 0.012749471605500282, 'dropout': 0.4453132868965452, 'd_model_multiplier': 8, 'num_layers': 9, 'n_heads': 32, 'dim_feedforward': 384, 'batch_size': 66, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 90}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 64
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0569[0m   [32m0.2998[0m        [35m0.6150[0m       [31m0.9166[0m        [94m0.5296[0m     +  24.0450
      2   [36m0.0573[0m   [32m0.3022[0m        [35m0.4721[0m       0.9166        [94m0.4026[0m     +  24.4220
      3   [36m0.0626[0m   [32m0.3705[0m        [35m0.3840[0m       0.9166        [94m0.3453[0m     +  24.2792
      4   [36m0.0725[0m   [32m0.4596[0m        [35m0.3336[0m       0.9166        [94m0.3127[0m     +  24.3046
      5   [36m0.1239[0m   [32m0.5402[0m        [35m0.3040[0m       0.9166        [94m0.2940[0m     +  24.2190
      6   [36m0.1713[0m   [32m0.5884[0m        [35m0.2858[0m       0.9166        [94m0.2831[0m     +  24.1870
      7   [36m0.1911[0m   [32m0.6175[0m        [35m0.2737[0m       0.9166        [94m0.2771[0m     +  24.3666
      8   [36m0.2017[0m   [32m0.6347[0m        [35m0.2658[0m       0.9166        [94m0.2739[0m     +  24.3236
      9   [36m0.2125[0m   [32m0.6460[0m        [35m0.2618[0m       0.9166        [94m0.2722[0m     +  24.2645
     10   [36m0.2191[0m   [32m0.6566[0m        [35m0.2584[0m       0.9166        [94m0.2710[0m     +  24.4560
     11   [36m0.2255[0m   [32m0.6621[0m        0.2587       0.9166        [94m0.2703[0m     +  24.4143
     12   [36m0.2319[0m   [32m0.6681[0m        [35m0.2543[0m       0.9166        [94m0.2699[0m     +  24.2112
     13   [36m0.2329[0m   [32m0.6731[0m        0.2551       0.9166        [94m0.2697[0m     +  24.3247
     14   [36m0.2351[0m   [32m0.6778[0m        [35m0.2538[0m       0.9166        [94m0.2690[0m     +  24.3215
     15   0.2348   [32m0.6853[0m        [35m0.2518[0m       0.9166        [94m0.2683[0m     +  24.2379
     16   [36m0.2364[0m   [32m0.6935[0m        0.2539       0.9166        [94m0.2673[0m     +  24.1552
     17   [36m0.2438[0m   [32m0.7031[0m        [35m0.2510[0m       0.9166        [94m0.2659[0m     +  24.1704
     18   [36m0.2536[0m   [32m0.7221[0m        [35m0.2488[0m       0.9166        [94m0.2614[0m     +  24.2002
     19   [36m0.2766[0m   [32m0.7443[0m        [35m0.2477[0m       0.9166        [94m0.2532[0m     +  24.4537
     20   [36m0.2915[0m   [32m0.7481[0m        [35m0.2417[0m       0.9166        [94m0.2492[0m     +  24.2038
     21   [36m0.2967[0m   [32m0.7492[0m        [35m0.2384[0m       0.9141        [94m0.2482[0m     +  24.4136
     22   [36m0.3166[0m   [32m0.7521[0m        [35m0.2371[0m       0.9166        [94m0.2477[0m     +  24.1739
     23   0.3130   [32m0.7540[0m        0.2381       [31m0.9178[0m        [94m0.2473[0m     +  24.2353
     24   0.3138   [32m0.7569[0m        0.2387       [31m0.9190[0m        [94m0.2466[0m     +  24.3493
     25   [36m0.3169[0m   [32m0.7571[0m        [35m0.2348[0m       0.9190        0.2470        24.3691
     26   0.3120   [32m0.7612[0m        0.2350       0.9178        [94m0.2461[0m     +  24.1588
     27   0.3121   [32m0.7628[0m        0.2358       0.9178        0.2461        24.2879
     28   0.3124   [32m0.7650[0m        [35m0.2335[0m       0.9190        [94m0.2457[0m     +  24.2926
     29   [36m0.3177[0m   [32m0.7655[0m        [35m0.2320[0m       0.9190        0.2460        24.4921
     30   0.3176   [32m0.7662[0m        0.2338       [31m0.9214[0m        0.2458        24.2805
     31   [36m0.3181[0m   [32m0.7676[0m        0.2340       [31m0.9226[0m        0.2459        24.2441
     32   [36m0.3183[0m   [32m0.7702[0m        [35m0.2319[0m       0.9214        [94m0.2445[0m     +  24.2660
     33   [36m0.3210[0m   [32m0.7714[0m        0.2331       [31m0.9250[0m        0.2446        24.3053
     34   [36m0.3212[0m   [32m0.7718[0m        0.2326       0.9250        0.2448        24.3276
     35   [36m0.3213[0m   [32m0.7721[0m        0.2331       0.9250        0.2450        24.2046
     36   [36m0.3231[0m   [32m0.7740[0m        0.2330       0.9250        [94m0.2442[0m     +  24.2130
     37   [36m0.3240[0m   [32m0.7750[0m        0.2322       0.9238        [94m0.2435[0m     +  24.3056
     38   [36m0.3266[0m   [32m0.7764[0m        0.2330       0.9238        [94m0.2434[0m     +  24.2857
     39   [36m0.3277[0m   [32m0.7764[0m        0.2329       0.9238        0.2438        24.3059
     40   [36m0.3285[0m   [32m0.7777[0m        0.2321       0.9226        [94m0.2431[0m     +  24.4681
     41   0.3276   0.7775        [35m0.2315[0m       0.9226        0.2438        24.1596
     42   0.3283   [32m0.7796[0m        0.2315       0.9226        [94m0.2430[0m     +  24.2333
     43   0.3282   0.7793        [35m0.2305[0m       0.9226        0.2433        24.1454
     44   [36m0.3296[0m   [32m0.7800[0m        0.2306       0.9238        0.2434        24.1994
     45   [36m0.3324[0m   [32m0.7808[0m        [35m0.2300[0m       0.9238        0.2433        24.2879
     46   0.3318   [32m0.7821[0m        0.2309       0.9250        [94m0.2428[0m     +  24.2702
     47   [36m0.3356[0m   [32m0.7830[0m        [35m0.2291[0m       0.9238        [94m0.2427[0m     +  24.1419
     48   0.3352   [32m0.7838[0m        0.2308       0.9238        [94m0.2425[0m     +  24.2456
     49   0.3350   [32m0.7847[0m        0.2304       0.9238        [94m0.2421[0m     +  24.1228
     50   0.3346   0.7843        0.2297       0.9238        0.2429        24.1874
[32m[I 2023-05-04 23:16:24,856][0m Trial 506 finished with value: 0.24214823586368503 and parameters: {'lr': 1.7938938228581815e-05, 'dropout': 0.5430414139363801, 'd_model_multiplier': 1, 'num_layers': 7, 'n_heads': 32, 'dim_feedforward': 368, 'batch_size': 42, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.001, 'top_n_features': 64}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 56
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0654[0m   [32m0.3729[0m        [35m0.5266[0m       [31m0.9262[0m        [94m0.5992[0m     +  20.5868
      2   [36m0.0923[0m   [32m0.5068[0m        [35m0.3919[0m       0.9262        [94m0.5294[0m     +  20.5812
      3   [36m0.1756[0m   [32m0.6348[0m        [35m0.3316[0m       0.9226        [94m0.4947[0m     +  20.5941
      4   [36m0.2159[0m   [32m0.6875[0m        [35m0.3032[0m       0.9202        [94m0.4886[0m     +  20.9490
      5   [36m0.2507[0m   [32m0.7112[0m        [35m0.2870[0m       0.9141        [94m0.4594[0m     +  20.6079
      6   [36m0.2560[0m   [32m0.7279[0m        [35m0.2817[0m       0.9190        [94m0.4464[0m     +  20.8304
      7   [36m0.2694[0m   [32m0.7341[0m        [35m0.2752[0m       0.9166        [94m0.4134[0m     +  20.6202
      8   [36m0.2861[0m   [32m0.7376[0m        [35m0.2713[0m       0.9190        0.4188        20.5831
      9   [36m0.3059[0m   [32m0.7423[0m        [35m0.2659[0m       0.9202        [94m0.4003[0m     +  20.7092
     10   [36m0.3178[0m   [32m0.7433[0m        0.2661       0.9226        0.4109        21.0072
     11   [36m0.3200[0m   0.7401        [35m0.2653[0m       0.9226        0.4042        20.8214
     12   [36m0.3226[0m   [32m0.7471[0m        [35m0.2609[0m       0.9226        0.4063        20.6517
     13   [36m0.3379[0m   [32m0.7505[0m        0.2628       0.9250        [94m0.3868[0m     +  21.0719
     14   [36m0.3383[0m   0.7433        [35m0.2603[0m       0.9250        0.3958        20.9605
     15   [36m0.3400[0m   0.7471        [35m0.2597[0m       0.9238        [94m0.3810[0m     +  20.9016
     16   0.3358   0.7493        0.2609       0.9250        0.4035        20.7650
     17   [36m0.3540[0m   [32m0.7506[0m        [35m0.2588[0m       0.9250        [94m0.3806[0m     +  20.4097
     18   [36m0.3556[0m   0.7480        [35m0.2563[0m       0.9250        [94m0.3673[0m     +  20.5331
     19   [36m0.3633[0m   0.7501        0.2577       [31m0.9299[0m        0.3770        20.6668
     20   0.3570   [32m0.7521[0m        0.2583       0.9262        0.3771        20.5336
     21   [36m0.3665[0m   [32m0.7538[0m        [35m0.2548[0m       0.9274        [94m0.3634[0m     +  20.6637
     22   0.3664   0.7534        0.2557       [31m0.9311[0m        [94m0.3476[0m     +  20.6160
     23   [36m0.3782[0m   0.7467        0.2561       0.9311        0.3585        20.7290
     24   [36m0.3808[0m   0.7490        0.2561       0.9311        [94m0.3414[0m     +  20.7133
     25   0.3754   0.7538        [35m0.2546[0m       0.9274        0.3659        20.6978
     26   0.3770   0.7502        [35m0.2531[0m       [31m0.9323[0m        0.3451        20.6925
     27   0.3748   0.7513        0.2533       0.9299        0.3559        20.8056
     28   0.3729   [32m0.7575[0m        [35m0.2511[0m       0.9311        [94m0.3365[0m     +  20.9696
     29   0.3756   0.7530        0.2541       [31m0.9335[0m        0.3422        20.7346
     30   0.3767   0.7517        [35m0.2510[0m       0.9323        0.3413        20.6109
     31   0.3754   0.7529        0.2513       0.9323        0.3481        20.6882
     32   0.3791   0.7497        0.2518       [31m0.9347[0m        [94m0.3207[0m     +  20.5271
     33   [36m0.3869[0m   0.7533        [35m0.2505[0m       0.9347        0.3256        20.9963
     34   0.3802   0.7512        0.2515       0.9323        0.3401        20.8454
     35   0.3778   0.7479        [35m0.2500[0m       0.9347        0.3287        20.8090
     36   0.3712   0.7512        0.2501       0.9323        0.3382        20.6715
     37   0.3805   0.7526        [35m0.2494[0m       0.9347        [94m0.3187[0m     +  21.0906
     38   0.3773   0.7506        [35m0.2486[0m       [31m0.9359[0m        [94m0.3126[0m     +  20.5626
     39   0.3799   0.7529        [35m0.2482[0m       0.9335        [94m0.3098[0m     +  20.6352
     40   0.3825   0.7475        0.2484       0.9347        0.3161        20.7846
     41   0.3810   0.7507        0.2494       0.9359        0.3201        20.5615
     42   0.3848   0.7488        [35m0.2469[0m       0.9359        0.3185        20.6260
     43   0.3700   0.7487        0.2476       0.9335        0.3120        20.4928
     44   0.3706   0.7479        [35m0.2469[0m       0.9347        [94m0.3080[0m     +  20.7271
     45   [36m0.3888[0m   0.7530        [35m0.2445[0m       [31m0.9371[0m        [94m0.2973[0m     +  20.6384
     46   0.3834   0.7502        0.2481       0.9359        0.3123        20.5829
     47   0.3806   0.7546        0.2475       0.9359        0.3102        20.7287
     48   0.3824   0.7541        0.2465       0.9359        [94m0.2948[0m     +  20.5507
     49   0.3820   0.7521        0.2483       0.9359        0.3016        20.6307
     50   0.3806   0.7519        0.2471       0.9359        0.3051        20.8594
[32m[I 2023-05-04 23:33:43,186][0m Trial 507 finished with value: 0.29475177952427384 and parameters: {'lr': 3.94052927512233e-06, 'dropout': 0.37970019951573253, 'd_model_multiplier': 4, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 336, 'batch_size': 56, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 56}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 72
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
Warning, assumed runtime error: CUDA out of memory. Tried to allocate 3.34 GiB (GPU 0; 23.70 GiB total capacity; 19.63 GiB already allocated; 569.25 MiB free; 22.05 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[32m[I 2023-05-04 23:33:49,201][0m Trial 508 finished with value: 100.0 and parameters: {'lr': 6.465140712667419e-05, 'dropout': 0.3981674443462895, 'd_model_multiplier': 4, 'num_layers': 5, 'n_heads': 64, 'dim_feedforward': 399, 'batch_size': 141, 'pos_encoding': 'learnable', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 72}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 67
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
Warning, assumed runtime error: CUDA out of memory. Tried to allocate 1.46 GiB (GPU 0; 23.70 GiB total capacity; 21.16 GiB already allocated; 571.25 MiB free; 22.05 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[32m[I 2023-05-04 23:33:54,868][0m Trial 509 finished with value: 100.0 and parameters: {'lr': 3.7343506578039414e-07, 'dropout': 0.33362483132633497, 'd_model_multiplier': 4, 'num_layers': 6, 'n_heads': 32, 'dim_feedforward': 359, 'batch_size': 123, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 67}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 60
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
Warning, assumed runtime error: CUDA out of memory. Tried to allocate 1.55 GiB (GPU 0; 23.70 GiB total capacity; 21.17 GiB already allocated; 561.25 MiB free; 22.06 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[32m[I 2023-05-04 23:34:00,366][0m Trial 510 finished with value: 100.0 and parameters: {'lr': 0.0010627044824838804, 'dropout': 0.5768392450305837, 'd_model_multiplier': 16, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 322, 'batch_size': 131, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.001, 'top_n_features': 60}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 82
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.3044[0m   [32m0.7841[0m        [35m0.2794[0m       [31m0.9238[0m        [94m0.2508[0m     +  17.0863
      2   0.2942   [32m0.7860[0m        [35m0.2424[0m       0.9190        0.2527        17.1996
      3   0.3031   0.7745        [35m0.2422[0m       0.9202        0.2557        17.4610
      4   [36m0.3166[0m   [32m0.7866[0m        [35m0.2397[0m       0.9226        0.2636        17.2822
      5   [36m0.3250[0m   0.7745        [35m0.2357[0m       [31m0.9250[0m        0.2725        17.2690
      6   0.3169   0.7748        [35m0.2341[0m       0.9238        0.2911        17.4654
      7   0.3229   [32m0.7925[0m        [35m0.2325[0m       0.9154        0.2850        17.2704
      8   0.3114   0.7902        [35m0.2267[0m       0.9141        0.2835        17.4699
      9   [36m0.3390[0m   [32m0.8042[0m        [35m0.2252[0m       0.9202        0.2680        17.4040
     10   0.3052   [32m0.8110[0m        [35m0.2211[0m       0.9154        0.2689        17.2465
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 23:37:11,264][0m Trial 511 finished with value: 0.2507589951071983 and parameters: {'lr': 0.0002236318023442764, 'dropout': 0.3624340876522146, 'd_model_multiplier': 8, 'num_layers': 3, 'n_heads': 32, 'dim_feedforward': 344, 'batch_size': 24, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 82}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 118
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.1082[0m   [32m0.5804[0m        [35m0.6970[0m       [31m0.8307[0m        [94m0.6748[0m     +  9.4069
      2   0.0992   0.5740        [35m0.6844[0m       [31m0.8815[0m        [94m0.6680[0m     +  9.6484
      3   0.0933   0.5516        [35m0.6755[0m       [31m0.9154[0m        [94m0.6576[0m     +  9.4812
      4   0.0862   0.5361        [35m0.6604[0m       [31m0.9287[0m        [94m0.6475[0m     +  9.6210
      5   0.0850   0.5354        [35m0.6403[0m       [31m0.9299[0m        [94m0.6363[0m     +  10.0120
      6   0.0838   0.5215        [35m0.6191[0m       0.9299        [94m0.6292[0m     +  10.0883
      7   0.0901   0.5244        [35m0.5986[0m       0.9299        [94m0.6077[0m     +  9.9650
      8   0.0963   0.5154        [35m0.5634[0m       0.9299        [94m0.5761[0m     +  9.9895
      9   0.0998   0.5147        [35m0.5337[0m       0.9299        [94m0.5628[0m     +  10.1188
     10   [36m0.1083[0m   0.5116        [35m0.4958[0m       0.9299        [94m0.5311[0m     +  9.8014
     11   [36m0.1138[0m   0.5172        [35m0.4620[0m       0.9299        [94m0.5152[0m     +  9.8957
     12   0.1138   0.5219        [35m0.4328[0m       0.9299        [94m0.4781[0m     +  9.6643
     13   0.1137   0.5229        [35m0.4113[0m       0.9299        [94m0.4396[0m     +  9.9357
     14   0.1087   0.5281        [35m0.3971[0m       0.9299        [94m0.4288[0m     +  9.7126
     15   0.1103   0.5362        [35m0.3799[0m       0.9299        [94m0.4041[0m     +  9.7112
     16   0.1125   0.5425        [35m0.3643[0m       0.9299        [94m0.3908[0m     +  9.7606
     17   [36m0.1144[0m   0.5515        [35m0.3505[0m       0.9299        [94m0.3780[0m     +  9.8030
     18   [36m0.1145[0m   0.5595        0.3512       0.9299        [94m0.3635[0m     +  9.7174
     19   [36m0.1167[0m   0.5663        [35m0.3395[0m       0.9299        [94m0.3413[0m     +  9.9426
     20   [36m0.1197[0m   0.5740        [35m0.3306[0m       0.9299        [94m0.3411[0m     +  9.6880
     21   [36m0.1247[0m   [32m0.5827[0m        [35m0.3301[0m       0.9299        [94m0.3282[0m     +  10.0322
     22   [36m0.1247[0m   [32m0.5895[0m        [35m0.3237[0m       0.9299        0.3314        10.1046
     23   [36m0.1269[0m   [32m0.5956[0m        [35m0.3182[0m       0.9299        [94m0.3123[0m     +  9.9150
     24   [36m0.1309[0m   [32m0.6009[0m        [35m0.3167[0m       0.9299        [94m0.3097[0m     +  9.8214
     25   [36m0.1321[0m   [32m0.6067[0m        [35m0.3099[0m       0.9299        [94m0.3039[0m     +  9.6099
     26   [36m0.1360[0m   [32m0.6116[0m        0.3129       0.9299        [94m0.3017[0m     +  9.7958
     27   [36m0.1395[0m   [32m0.6178[0m        [35m0.2986[0m       0.9299        [94m0.2891[0m     +  10.1144
     28   [36m0.1434[0m   [32m0.6226[0m        0.3065       0.9299        0.2939        9.9083
     29   [36m0.1484[0m   [32m0.6283[0m        0.3002       0.9299        [94m0.2861[0m     +  9.6913
     30   [36m0.1524[0m   [32m0.6329[0m        0.2989       0.9299        [94m0.2841[0m     +  9.9959
     31   [36m0.1563[0m   [32m0.6367[0m        0.3002       0.9299        [94m0.2775[0m     +  9.9303
     32   [36m0.1616[0m   [32m0.6393[0m        [35m0.2940[0m       0.9299        [94m0.2758[0m     +  10.0047
     33   [36m0.1660[0m   [32m0.6428[0m        [35m0.2934[0m       0.9299        [94m0.2747[0m     +  9.9281
     34   0.1596   [32m0.6440[0m        [35m0.2925[0m       0.9299        0.2792        9.8166
     35   0.1613   [32m0.6493[0m        [35m0.2895[0m       0.9299        [94m0.2667[0m     +  9.5123
     36   0.1650   [32m0.6517[0m        0.2897       0.9299        0.2669        9.8892
     37   [36m0.1677[0m   [32m0.6541[0m        [35m0.2820[0m       0.9299        [94m0.2610[0m     +  9.8045
     38   [36m0.1697[0m   [32m0.6565[0m        0.2884       0.9299        [94m0.2602[0m     +  9.5945
     39   [36m0.1715[0m   [32m0.6576[0m        0.2832       0.9299        [94m0.2600[0m     +  9.6100
     40   [36m0.1736[0m   [32m0.6591[0m        [35m0.2760[0m       0.9299        [94m0.2579[0m     +  10.6816
     41   [36m0.1762[0m   [32m0.6610[0m        0.2834       0.9299        [94m0.2532[0m     +  9.8949
     42   [36m0.1784[0m   [32m0.6627[0m        0.2813       0.9299        0.2554        9.9856
     43   [36m0.1797[0m   [32m0.6642[0m        0.2819       0.9299        0.2569        10.0023
     44   0.1774   [32m0.6653[0m        [35m0.2746[0m       0.9299        0.2560        9.9360
     45   0.1786   [32m0.6667[0m        [35m0.2736[0m       0.9299        0.2545        9.9052
     46   [36m0.1810[0m   0.6664        0.2763       0.9299        0.2575        9.7016
     47   [36m0.1811[0m   [32m0.6685[0m        0.2815       0.9299        0.2593        9.8282
     48   0.1806   [32m0.6686[0m        0.2750       0.9299        [94m0.2516[0m     +  9.7844
     49   [36m0.1827[0m   [32m0.6687[0m        [35m0.2727[0m       0.9299        [94m0.2497[0m     +  9.8286
     50   [36m0.1838[0m   [32m0.6703[0m        [35m0.2720[0m       0.9299        [94m0.2485[0m     +  9.6711
[32m[I 2023-05-04 23:45:25,866][0m Trial 512 finished with value: 0.24852512962440482 and parameters: {'lr': 2.6373919938318424e-05, 'dropout': 0.560788942787924, 'd_model_multiplier': 1, 'num_layers': 4, 'n_heads': 4, 'dim_feedforward': 307, 'batch_size': 79, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0, 'top_n_features': 118}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 45
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0549[0m   [32m0.3474[0m        [35m0.5407[0m       [31m0.9250[0m        [94m0.4177[0m     +  14.7360
      2   [36m0.0891[0m   [32m0.5284[0m        [35m0.3564[0m       0.9250        [94m0.2904[0m     +  14.6677
      3   [36m0.1684[0m   [32m0.6285[0m        [35m0.2794[0m       0.9250        [94m0.2612[0m     +  14.7655
      4   [36m0.1943[0m   [32m0.6745[0m        [35m0.2525[0m       0.9226        [94m0.2501[0m     +  15.1436
      5   [36m0.2059[0m   [32m0.6834[0m        [35m0.2397[0m       0.9202        [94m0.2476[0m     +  15.0108
      6   [36m0.2137[0m   [32m0.6887[0m        [35m0.2369[0m       0.9214        [94m0.2464[0m     +  14.9140
      7   [36m0.2228[0m   [32m0.6933[0m        [35m0.2334[0m       0.9226        [94m0.2455[0m     +  15.0283
      8   [36m0.2302[0m   [32m0.6971[0m        [35m0.2326[0m       0.9226        [94m0.2444[0m     +  14.7927
      9   [36m0.2376[0m   [32m0.7030[0m        [35m0.2319[0m       0.9226        [94m0.2437[0m     +  15.0664
     10   [36m0.2392[0m   [32m0.7057[0m        [35m0.2308[0m       0.9226        [94m0.2429[0m     +  15.0140
     11   [36m0.2450[0m   [32m0.7096[0m        [35m0.2294[0m       0.9226        [94m0.2425[0m     +  15.1566
     12   [36m0.2499[0m   [32m0.7132[0m        [35m0.2287[0m       0.9226        [94m0.2418[0m     +  15.0078
     13   [36m0.2534[0m   [32m0.7144[0m        [35m0.2284[0m       0.9214        [94m0.2416[0m     +  14.7876
     14   [36m0.2564[0m   [32m0.7173[0m        [35m0.2277[0m       0.9250        [94m0.2412[0m     +  15.2455
     15   [36m0.2601[0m   [32m0.7195[0m        [35m0.2263[0m       0.9250        [94m0.2407[0m     +  15.0298
     16   [36m0.2655[0m   [32m0.7214[0m        [35m0.2259[0m       0.9238        [94m0.2403[0m     +  15.1392
     17   0.2640   [32m0.7234[0m        0.2260       0.9250        [94m0.2399[0m     +  14.9767
     18   [36m0.2689[0m   [32m0.7264[0m        [35m0.2242[0m       0.9250        [94m0.2396[0m     +  15.1866
     19   [36m0.2724[0m   [32m0.7302[0m        [35m0.2226[0m       0.9238        [94m0.2388[0m     +  15.1060
     20   0.2713   0.7298        [35m0.2225[0m       0.9238        0.2396        15.7279
     21   [36m0.2735[0m   [32m0.7339[0m        0.2241       0.9238        0.2402        15.0397
     22   [36m0.2755[0m   [32m0.7355[0m        [35m0.2218[0m       0.9238        0.2409        14.8928
     23   [36m0.2785[0m   [32m0.7391[0m        [35m0.2211[0m       0.9250        0.2398        15.1751
     24   [36m0.2819[0m   [32m0.7399[0m        [35m0.2199[0m       [31m0.9262[0m        0.2396        15.0303
     25   [36m0.2824[0m   [32m0.7431[0m        0.2208       0.9238        [94m0.2380[0m     +  15.3411
     26   0.2790   [32m0.7446[0m        [35m0.2193[0m       0.9238        0.2391        14.9731
     27   0.2811   [32m0.7456[0m        [35m0.2184[0m       0.9250        0.2391        15.1587
     28   [36m0.2832[0m   [32m0.7457[0m        [35m0.2175[0m       0.9250        0.2396        15.0685
     29   [36m0.2861[0m   [32m0.7501[0m        [35m0.2160[0m       [31m0.9274[0m        0.2397        15.3218
     30   [36m0.2872[0m   [32m0.7516[0m        0.2161       0.9274        0.2396        15.1963
     31   0.2852   [32m0.7531[0m        [35m0.2153[0m       0.9262        0.2412        14.9003
     32   0.2872   [32m0.7548[0m        [35m0.2152[0m       0.9238        0.2410        15.1271
     33   0.2777   [32m0.7552[0m        0.2157       0.9214        0.2420        15.3335
     34   0.2792   0.7543        [35m0.2142[0m       0.9226        0.2421        15.3460
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 23:54:15,267][0m Trial 513 finished with value: 0.2380409766857973 and parameters: {'lr': 3.916348521771369e-05, 'dropout': 0.300183998943618, 'd_model_multiplier': 4, 'num_layers': 5, 'n_heads': 16, 'dim_feedforward': 329, 'batch_size': 62, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 45}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 53
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2706[0m   [32m0.7496[0m        [35m0.3041[0m       [31m0.9238[0m        [94m0.2470[0m     +  27.3334
      2   [36m0.2816[0m   0.7451        [35m0.2634[0m       [31m0.9262[0m        [94m0.2455[0m     +  27.6169
      3   0.2639   0.7352        [35m0.2621[0m       0.9069        0.2545        27.5528
      4   0.2289   0.7361        [35m0.2586[0m       0.9238        0.2500        27.5727
      5   0.2347   0.7336        [35m0.2550[0m       0.8972        0.2581        27.7023
      6   0.2401   0.7305        [35m0.2546[0m       0.8960        0.2589        27.6852
      7   0.2583   0.7341        [35m0.2545[0m       0.9141        0.2503        27.8439
      8   0.2780   0.7350        0.2545       0.9154        0.2494        27.4688
      9   0.2502   0.7332        [35m0.2531[0m       0.9008        0.2532        27.5583
     10   [36m0.2841[0m   0.7338        0.2532       0.9033        0.2521        27.7602
     11   0.2801   0.7347        0.2559       0.9141        0.2472        27.8023
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-04 23:59:47,532][0m Trial 514 finished with value: 0.24552990989860918 and parameters: {'lr': 0.0024472587386554618, 'dropout': 0.44118449132388404, 'd_model_multiplier': 4, 'num_layers': 6, 'n_heads': 32, 'dim_feedforward': 438, 'batch_size': 69, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.01, 'top_n_features': 53}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 64
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2197[0m   [32m0.6983[0m        [35m0.2649[0m       [31m0.9238[0m        [94m0.2621[0m     +  20.6308
      2   [36m0.2207[0m   [32m0.7052[0m        [35m0.2516[0m       0.9214        0.2643        20.6440
      3   0.1796   0.6854        [35m0.2417[0m       0.9190        0.2708        21.2321
      4   0.2150   [32m0.7098[0m        [35m0.2413[0m       [31m0.9274[0m        [94m0.2522[0m     +  21.0284
      5   [36m0.2388[0m   [32m0.7259[0m        [35m0.2363[0m       0.9190        [94m0.2496[0m     +  21.1771
      6   0.2227   0.7181        [35m0.2346[0m       0.9214        0.2556        21.1580
      7   [36m0.2581[0m   0.7154        [35m0.2340[0m       [31m0.9287[0m        [94m0.2451[0m     +  21.0484
      8   0.2547   [32m0.7267[0m        0.2354       0.9287        [94m0.2396[0m     +  21.4202
      9   0.2140   0.7066        [35m0.2335[0m       0.9250        0.2518        21.3135
     10   0.2438   0.7199        [35m0.2321[0m       0.9250        0.2444        21.1668
     11   0.2028   0.7258        [35m0.2306[0m       0.9238        0.2505        21.2386
     12   0.2408   0.6846        0.2340       0.9238        0.2550        20.9478
     13   [36m0.2653[0m   0.7262        0.2327       0.9262        0.2458        21.4013
     14   0.2022   0.7231        0.2341       0.9226        0.2522        20.9436
     15   0.1863   0.6725        0.2534       0.9262        0.2554        22.0983
     16   0.1772   0.6736        0.2534       0.9262        0.2546        21.2536
     17   0.1891   0.6744        0.2528       0.9262        0.2554        21.1507
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 00:06:09,072][0m Trial 515 finished with value: 0.2396171045688937 and parameters: {'lr': 0.005553591963263223, 'dropout': 0.3140636510798146, 'd_model_multiplier': 4, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 354, 'batch_size': 103, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 64}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 75
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
Warning, assumed runtime error: CUDA out of memory. Tried to allocate 1.16 GiB (GPU 0; 23.70 GiB total capacity; 17.10 GiB already allocated; 575.25 MiB free; 22.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[32m[I 2023-05-05 00:06:12,543][0m Trial 516 finished with value: 100.0 and parameters: {'lr': 0.0012617427941270572, 'dropout': 0.2785633838625376, 'd_model_multiplier': 2, 'num_layers': 6, 'n_heads': 64, 'dim_feedforward': 338, 'batch_size': 49, 'pos_encoding': 'learnable', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.001, 'top_n_features': 75}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 206
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0734[0m   [32m0.3026[0m        [35m0.5489[0m       [31m0.9190[0m        [94m0.5276[0m     +  69.5168
      2   [36m0.2491[0m   [32m0.7092[0m        [35m0.3146[0m       0.9141        [94m0.4418[0m     +  69.6088
      3   [36m0.3070[0m   [32m0.7991[0m        [35m0.2705[0m       0.9117        [94m0.4153[0m     +  69.8697
      4   [36m0.3133[0m   [32m0.8187[0m        [35m0.2607[0m       0.9129        [94m0.4020[0m     +  69.8974
      5   [36m0.3134[0m   [32m0.8216[0m        [35m0.2528[0m       0.9154        [94m0.3918[0m     +  69.6562
      6   0.3070   0.8204        [35m0.2508[0m       0.9117        [94m0.3866[0m     +  69.8895
      7   0.3021   0.8057        [35m0.2463[0m       0.9093        [94m0.3763[0m     +  70.0088
      8   0.3075   0.8047        [35m0.2438[0m       0.9105        [94m0.3729[0m     +  70.0746
      9   0.3042   0.8044        [35m0.2404[0m       0.9081        [94m0.3715[0m     +  70.0948
     10   0.2942   0.7948        [35m0.2397[0m       0.9069        [94m0.3661[0m     +  69.7854
     11   0.2847   0.7791        [35m0.2356[0m       0.9105        [94m0.3571[0m     +  69.8753
     12   0.2901   0.7705        [35m0.2342[0m       0.9105        [94m0.3541[0m     +  69.9740
     13   0.2984   0.7791        [35m0.2324[0m       0.9081        [94m0.3393[0m     +  69.7868
     14   0.2993   0.7770        [35m0.2296[0m       0.9069        0.3457        70.2079
     15   0.2815   0.7723        [35m0.2275[0m       0.9081        [94m0.3229[0m     +  70.1646
     16   0.2760   0.7700        0.2276       0.9033        [94m0.3161[0m     +  70.3065
     17   0.2779   0.7699        [35m0.2253[0m       0.9069        0.3197        69.9429
     18   0.2813   0.7690        [35m0.2222[0m       0.9057        [94m0.3108[0m     +  69.9947
     19   0.2795   0.7544        [35m0.2209[0m       0.9069        0.3134        70.0382
     20   0.2723   0.7510        0.2213       0.9081        0.3131        70.2305
     21   0.2762   0.7569        [35m0.2170[0m       0.9081        [94m0.3072[0m     +  70.0143
     22   0.2718   0.7548        [35m0.2163[0m       0.9069        0.3152        69.8202
     23   0.2658   0.7393        [35m0.2159[0m       0.9069        0.3273        70.5135
     24   0.2685   0.7500        [35m0.2147[0m       0.9093        0.3172        70.0581
     25   0.2636   0.7428        [35m0.2111[0m       0.9093        0.3113        69.9298
     26   0.2777   0.7401        [35m0.2093[0m       0.9081        [94m0.3060[0m     +  70.0164
     27   0.2725   0.7441        [35m0.2088[0m       0.9069        0.3129        69.9387
     28   0.2529   0.7226        [35m0.2050[0m       0.9069        [94m0.3050[0m     +  69.7875
     29   0.2666   0.7402        [35m0.2022[0m       0.9045        [94m0.2988[0m     +  70.1807
     30   0.2625   0.7271        0.2059       0.9008        0.3108        70.0529
     31   0.2544   0.7200        [35m0.2011[0m       0.8984        0.3164        70.0684
     32   0.2499   0.7229        [35m0.1979[0m       0.8996        0.3288        69.8215
     33   0.2600   0.7356        0.1991       0.8996        0.3141        70.1829
     34   0.2502   0.7202        0.1984       0.9045        0.3149        69.9687
     35   0.2486   0.7210        0.1994       0.9081        0.3133        70.2610
     36   0.2300   0.6978        [35m0.1938[0m       0.9033        0.3209        70.1024
     37   0.2272   0.7016        [35m0.1924[0m       0.8984        0.3241        69.6969
     38   0.2285   0.6947        [35m0.1881[0m       0.9021        0.3292        69.7904
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 00:52:03,338][0m Trial 517 finished with value: 0.298805949117137 and parameters: {'lr': 9.65361027513458e-06, 'dropout': 0.45836752109441764, 'd_model_multiplier': 32, 'num_layers': 7, 'n_heads': 32, 'dim_feedforward': 317, 'batch_size': 58, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 206}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 59
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2069[0m   [32m0.6272[0m        [35m0.3369[0m       [31m0.9069[0m        [94m0.3019[0m     +  25.2951
      2   [36m0.2503[0m   [32m0.6858[0m        [35m0.2540[0m       0.9069        [94m0.2979[0m     +  25.0856
      3   [36m0.2772[0m   [32m0.7141[0m        [35m0.2464[0m       [31m0.9081[0m        [94m0.2942[0m     +  25.6616
      4   [36m0.2806[0m   [32m0.7247[0m        [35m0.2426[0m       [31m0.9105[0m        [94m0.2930[0m     +  25.5844
      5   [36m0.2839[0m   [32m0.7312[0m        [35m0.2391[0m       0.9069        0.2961        25.4428
      6   [36m0.2907[0m   [32m0.7325[0m        [35m0.2364[0m       0.9021        0.3038        25.7351
      7   [36m0.2920[0m   0.7317        [35m0.2330[0m       0.8912        0.3151        25.6050
      8   [36m0.2992[0m   0.7319        [35m0.2321[0m       0.8888        0.3206        25.5184
      9   [36m0.3001[0m   0.7324        [35m0.2318[0m       0.8827        0.3290        25.3208
     10   0.2952   0.7311        [35m0.2294[0m       0.8851        0.3308        25.6186
     11   0.2962   0.7318        [35m0.2286[0m       0.8839        0.3352        25.7034
     12   0.2966   [32m0.7333[0m        [35m0.2286[0m       0.8791        0.3404        25.6576
     13   0.2963   [32m0.7339[0m        [35m0.2260[0m       0.8815        0.3408        25.7614
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 00:58:01,841][0m Trial 518 finished with value: 0.293017566852731 and parameters: {'lr': 2.10208756135477e-05, 'dropout': 0.6637871445509704, 'd_model_multiplier': 8, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 269, 'batch_size': 73, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 59}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 49
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0656[0m   [32m0.4586[0m        [35m0.4483[0m       [31m0.9250[0m        [94m0.2849[0m     +  19.5790
      2   [36m0.2815[0m   [32m0.7182[0m        [35m0.2738[0m       0.9250        [94m0.2388[0m     +  20.0583
      3   [36m0.3076[0m   [32m0.7542[0m        [35m0.2553[0m       [31m0.9262[0m        [94m0.2287[0m     +  20.0225
      4   [36m0.3110[0m   [32m0.7693[0m        [35m0.2523[0m       0.9141        0.2310        20.0947
      5   0.2899   0.7412        [35m0.2434[0m       0.9057        0.2708        20.2500
      6   0.2827   0.7280        [35m0.2410[0m       0.8948        0.2947        20.1492
      7   0.3045   0.7384        [35m0.2407[0m       0.9021        0.2989        19.9587
      8   0.2959   0.7435        [35m0.2381[0m       0.8996        0.2913        20.1475
      9   0.2936   0.7468        0.2385       0.8996        0.2985        20.0216
     10   0.2868   0.7509        [35m0.2360[0m       0.8984        0.2992        19.9898
     11   0.2790   0.7569        0.2365       0.9045        0.2866        20.0305
     12   0.2849   0.7541        [35m0.2345[0m       0.9081        0.2752        19.9944
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 01:02:22,804][0m Trial 519 finished with value: 0.22868363528614896 and parameters: {'lr': 0.0001402552812537478, 'dropout': 0.6827869734520424, 'd_model_multiplier': 4, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 374, 'batch_size': 64, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 49}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 69
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
Warning, assumed runtime error: CUDA out of memory. Tried to allocate 1.01 GiB (GPU 0; 23.70 GiB total capacity; 21.31 GiB already allocated; 559.25 MiB free; 22.06 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[32m[I 2023-05-05 01:02:26,403][0m Trial 520 finished with value: 100.0 and parameters: {'lr': 6.391738211071683e-06, 'dropout': 0.6334131773253149, 'd_model_multiplier': 1, 'num_layers': 11, 'n_heads': 32, 'dim_feedforward': 328, 'batch_size': 85, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.001, 'top_n_features': 69}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 63
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1130[0m   [32m0.5325[0m        [35m0.6657[0m       [31m0.8972[0m        [94m0.6435[0m     +  10.6870
      2   0.0846   0.4471        [35m0.6437[0m       [31m0.9166[0m        [94m0.6130[0m     +  10.4623
      3   0.0683   0.3862        [35m0.6110[0m       [31m0.9190[0m        [94m0.5777[0m     +  10.7981
      4   0.0653   0.3582        [35m0.5769[0m       0.9190        [94m0.5411[0m     +  10.6932
      5   0.0649   0.3491        [35m0.5430[0m       0.9190        [94m0.5057[0m     +  10.7430
      6   0.0663   0.3461        [35m0.5059[0m       0.9190        [94m0.4717[0m     +  10.6967
      7   0.0662   0.3475        [35m0.4746[0m       0.9190        [94m0.4401[0m     +  10.7403
      8   0.0662   0.3523        [35m0.4471[0m       0.9190        [94m0.4123[0m     +  10.4899
      9   0.0667   0.3633        [35m0.4196[0m       0.9190        [94m0.3881[0m     +  10.8094
     10   0.0664   0.3756        [35m0.3983[0m       0.9190        [94m0.3681[0m     +  10.5851
     11   0.0671   0.3906        [35m0.3801[0m       0.9190        [94m0.3521[0m     +  10.8468
     12   0.0684   0.4079        [35m0.3660[0m       0.9190        [94m0.3390[0m     +  10.8470
     13   0.0702   0.4318        [35m0.3521[0m       0.9190        [94m0.3279[0m     +  10.8575
     14   0.0743   0.4569        [35m0.3395[0m       0.9190        [94m0.3181[0m     +  10.7851
     15   0.0787   0.4850        [35m0.3303[0m       0.9190        [94m0.3094[0m     +  10.9581
     16   0.0835   0.5134        [35m0.3215[0m       0.9190        [94m0.3019[0m     +  11.2566
     17   0.0911   [32m0.5420[0m        [35m0.3129[0m       0.9190        [94m0.2952[0m     +  10.8855
     18   0.1007   [32m0.5670[0m        [35m0.3047[0m       0.9190        [94m0.2896[0m     +  10.9858
     19   [36m0.1293[0m   [32m0.5907[0m        [35m0.2994[0m       0.9190        [94m0.2846[0m     +  11.0990
     20   [36m0.1662[0m   [32m0.6074[0m        [35m0.2943[0m       0.9190        [94m0.2804[0m     +  11.1774
     21   [36m0.1852[0m   [32m0.6203[0m        [35m0.2895[0m       0.9190        [94m0.2768[0m     +  11.0701
     22   [36m0.2009[0m   [32m0.6335[0m        [35m0.2867[0m       0.9190        [94m0.2736[0m     +  10.8960
     23   [36m0.2127[0m   [32m0.6449[0m        [35m0.2808[0m       0.9190        [94m0.2709[0m     +  11.2113
     24   [36m0.2204[0m   [32m0.6540[0m        [35m0.2785[0m       0.9190        [94m0.2685[0m     +  10.8277
     25   [36m0.2325[0m   [32m0.6651[0m        [35m0.2763[0m       0.9190        [94m0.2662[0m     +  11.1802
     26   [36m0.2389[0m   [32m0.6713[0m        [35m0.2732[0m       0.9190        [94m0.2644[0m     +  10.8757
     27   [36m0.2430[0m   [32m0.6784[0m        [35m0.2701[0m       0.9190        [94m0.2626[0m     +  10.9253
     28   [36m0.2451[0m   [32m0.6845[0m        [35m0.2682[0m       0.9190        [94m0.2609[0m     +  10.8521
     29   [36m0.2534[0m   [32m0.6925[0m        [35m0.2660[0m       0.9190        [94m0.2592[0m     +  10.6386
     30   [36m0.2545[0m   [32m0.6990[0m        0.2665       0.9190        [94m0.2577[0m     +  10.7445
     31   [36m0.2591[0m   [32m0.7034[0m        [35m0.2631[0m       0.9190        [94m0.2564[0m     +  10.8686
     32   [36m0.2644[0m   [32m0.7085[0m        [35m0.2618[0m       0.9190        [94m0.2552[0m     +  10.9415
     33   [36m0.2670[0m   [32m0.7132[0m        0.2618       0.9190        [94m0.2538[0m     +  10.6941
     34   [36m0.2692[0m   [32m0.7178[0m        [35m0.2585[0m       0.9190        [94m0.2527[0m     +  10.7120
     35   [36m0.2725[0m   [32m0.7226[0m        [35m0.2582[0m       0.9190        [94m0.2513[0m     +  10.6869
     36   [36m0.2792[0m   [32m0.7299[0m        [35m0.2569[0m       0.9190        [94m0.2499[0m     +  10.6687
     37   [36m0.2825[0m   [32m0.7343[0m        [35m0.2563[0m       0.9190        [94m0.2486[0m     +  10.6187
     38   [36m0.2916[0m   [32m0.7395[0m        [35m0.2538[0m       0.9190        [94m0.2472[0m     +  10.7777
     39   [36m0.3024[0m   [32m0.7448[0m        0.2542       0.9190        [94m0.2457[0m     +  10.5254
     40   [36m0.3030[0m   [32m0.7484[0m        [35m0.2511[0m       0.9190        [94m0.2445[0m     +  10.7893
     41   [36m0.3129[0m   [32m0.7543[0m        [35m0.2502[0m       0.9178        [94m0.2432[0m     +  11.0409
     42   [36m0.3174[0m   [32m0.7577[0m        [35m0.2487[0m       0.9178        [94m0.2421[0m     +  10.6818
     43   [36m0.3177[0m   [32m0.7602[0m        [35m0.2481[0m       0.9178        [94m0.2413[0m     +  10.6760
     44   [36m0.3216[0m   [32m0.7628[0m        [35m0.2481[0m       0.9178        [94m0.2402[0m     +  10.7151
     45   [36m0.3274[0m   [32m0.7644[0m        0.2483       0.9178        [94m0.2395[0m     +  10.7720
     46   [36m0.3287[0m   [32m0.7662[0m        [35m0.2470[0m       0.9178        [94m0.2387[0m     +  10.6140
     47   [36m0.3297[0m   [32m0.7678[0m        [35m0.2467[0m       0.9178        [94m0.2384[0m     +  10.7234
     48   [36m0.3318[0m   [32m0.7702[0m        [35m0.2448[0m       0.9190        [94m0.2376[0m     +  10.8890
     49   0.3317   [32m0.7719[0m        0.2458       0.9190        [94m0.2370[0m     +  11.0403
     50   [36m0.3351[0m   [32m0.7742[0m        [35m0.2428[0m       0.9190        [94m0.2365[0m     +  10.7972
[32m[I 2023-05-05 01:11:31,072][0m Trial 521 finished with value: 0.23646133375931824 and parameters: {'lr': 3.1298397162183098e-06, 'dropout': 0.41259927480800057, 'd_model_multiplier': 4, 'num_layers': 5, 'n_heads': 8, 'dim_feedforward': 387, 'batch_size': 54, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 63}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 55
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.0714[0m   [32m0.4098[0m        [35m0.6542[0m       [31m0.7896[0m        [94m0.6747[0m     +  8.5053
      2   0.0558   0.3113        [35m0.5832[0m       [31m0.9141[0m        [94m0.6531[0m     +  9.2403
      3   0.0558   0.3108        [35m0.4964[0m       [31m0.9202[0m        [94m0.6115[0m     +  9.4546
      4   0.0578   0.3439        [35m0.4153[0m       [31m0.9214[0m        [94m0.5558[0m     +  9.0506
      5   0.0653   [32m0.4154[0m        [35m0.3583[0m       0.9214        [94m0.5049[0m     +  9.3036
      6   [36m0.1084[0m   [32m0.5290[0m        [35m0.3232[0m       0.9214        [94m0.4664[0m     +  9.1898
      7   [36m0.1793[0m   [32m0.5883[0m        [35m0.3004[0m       0.9214        [94m0.4547[0m     +  9.3410
      8   [36m0.2040[0m   [32m0.6462[0m        [35m0.2862[0m       0.9214        [94m0.4272[0m     +  9.6553
      9   [36m0.2083[0m   [32m0.6692[0m        [35m0.2762[0m       0.9214        [94m0.3975[0m     +  9.6297
     10   0.2057   [32m0.6853[0m        [35m0.2699[0m       0.9214        [94m0.3791[0m     +  9.5613
     11   [36m0.2163[0m   [32m0.7081[0m        0.2714       0.9214        0.4040        9.5319
     12   0.2154   [32m0.7100[0m        [35m0.2651[0m       0.9214        [94m0.3604[0m     +  9.4220
     13   [36m0.2204[0m   [32m0.7291[0m        [35m0.2618[0m       0.9214        [94m0.3141[0m     +  9.8287
     14   0.2109   [32m0.7340[0m        [35m0.2613[0m       0.9214        0.3180        9.5377
     15   0.2202   [32m0.7378[0m        [35m0.2563[0m       0.9214        [94m0.2880[0m     +  9.6145
     16   [36m0.2233[0m   [32m0.7410[0m        0.2587       0.9214        0.3139        9.3638
     17   [36m0.2297[0m   [32m0.7525[0m        [35m0.2525[0m       0.9214        0.3017        9.2847
     18   [36m0.2329[0m   0.7487        0.2559       0.9214        0.3055        9.5485
     19   0.2272   [32m0.7573[0m        0.2564       0.9214        0.2913        9.3608
     20   0.2199   0.7488        0.2569       0.9214        0.3056        9.2622
     21   [36m0.2366[0m   [32m0.7608[0m        [35m0.2520[0m       0.9214        [94m0.2839[0m     +  9.5611
     22   [36m0.2407[0m   0.7596        [35m0.2518[0m       0.9214        [94m0.2808[0m     +  9.7413
     23   [36m0.2413[0m   [32m0.7642[0m        [35m0.2512[0m       0.9202        [94m0.2672[0m     +  9.6550
     24   [36m0.2481[0m   [32m0.7669[0m        [35m0.2502[0m       0.9190        0.2712        9.6530
     25   0.2393   0.7622        [35m0.2483[0m       0.9214        0.2673        9.9546
     26   0.2470   0.7663        [35m0.2472[0m       [31m0.9226[0m        0.2711        9.4819
     27   0.2434   [32m0.7685[0m        0.2477       0.9226        0.2684        9.3555
     28   0.2449   [32m0.7688[0m        [35m0.2452[0m       0.9226        [94m0.2548[0m     +  9.4256
     29   [36m0.2511[0m   [32m0.7705[0m        [35m0.2431[0m       0.9214        0.2594        9.2238
     30   0.2448   0.7675        0.2466       0.9226        0.2599        9.1921
     31   [36m0.2522[0m   [32m0.7730[0m        0.2435       0.9190        0.2601        9.1024
     32   0.2481   0.7706        [35m0.2419[0m       0.9214        0.2565        9.0623
     33   0.2482   0.7719        0.2428       0.9214        [94m0.2495[0m     +  9.2978
     34   0.2498   [32m0.7746[0m        0.2435       0.9214        0.2523        9.6477
     35   0.2414   0.7702        [35m0.2417[0m       0.9226        0.2505        9.3637
     36   0.2480   0.7746        0.2419       0.9202        0.2511        9.5731
     37   0.2498   0.7738        [35m0.2409[0m       0.9226        0.2503        9.8109
     38   [36m0.2582[0m   [32m0.7805[0m        0.2450       0.9190        0.2568        9.5598
     39   0.2563   0.7775        [35m0.2403[0m       0.9214        [94m0.2463[0m     +  10.0343
     40   0.2505   0.7754        0.2412       0.9226        0.2479        9.5245
     41   0.2504   0.7754        [35m0.2373[0m       0.9226        0.2470        9.6981
     42   0.2521   0.7780        0.2373       0.9226        [94m0.2424[0m     +  9.6132
     43   0.2524   0.7785        0.2394       0.9226        0.2461        9.5165
     44   0.2503   0.7771        0.2389       0.9226        0.2449        9.4391
     45   0.2568   0.7797        0.2374       0.9226        0.2457        9.5202
     46   [36m0.2595[0m   [32m0.7834[0m        0.2380       0.9226        0.2430        9.4509
     47   [36m0.2643[0m   [32m0.7865[0m        [35m0.2367[0m       0.9190        0.2428        9.5449
     48   0.2614   0.7854        0.2376       0.9178        [94m0.2415[0m     +  9.3747
     49   [36m0.2658[0m   [32m0.7877[0m        [35m0.2346[0m       0.9202        [94m0.2390[0m     +  9.3013
     50   0.2653   0.7862        0.2353       0.9226        0.2409        9.2205
[32m[I 2023-05-05 01:19:25,034][0m Trial 522 finished with value: 0.23902177098944802 and parameters: {'lr': 5.623417353081057e-05, 'dropout': 0.43053194444258236, 'd_model_multiplier': 4, 'num_layers': 3, 'n_heads': 4, 'dim_feedforward': 362, 'batch_size': 93, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 55}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 57
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1174[0m   [32m0.5676[0m        [35m0.3108[0m       [31m0.9214[0m        [94m0.2863[0m     +  25.6248
      2   [36m0.1785[0m   [32m0.6914[0m        [35m0.2820[0m       [31m0.9238[0m        0.3140        25.7756
      3   0.1697   [32m0.7008[0m        [35m0.2663[0m       0.9238        0.2940        25.8737
      4   [36m0.1905[0m   [32m0.7146[0m        [35m0.2630[0m       0.9238        [94m0.2569[0m     +  25.8099
      5   0.1791   0.7129        [35m0.2592[0m       0.9202        [94m0.2534[0m     +  25.8842
      6   0.1693   [32m0.7165[0m        0.2596       0.9238        0.2556        25.8681
      7   0.1814   0.7126        [35m0.2581[0m       0.9226        0.3033        25.8185
      8   0.1675   0.7148        [35m0.2549[0m       0.9238        0.3087        25.8507
      9   0.1745   0.7141        0.2582       0.9238        0.3308        25.7925
     10   0.1589   0.7015        [35m0.2548[0m       0.9238        0.2957        25.8165
     11   0.1725   0.7136        [35m0.2546[0m       0.9238        0.2842        25.7100
     12   0.1674   0.7079        [35m0.2537[0m       0.9238        0.2910        25.8437
     13   0.1703   0.7139        [35m0.2537[0m       0.9238        0.2741        25.7981
     14   0.1699   0.7147        0.2547       0.9238        0.2931        25.9429
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 01:25:53,153][0m Trial 523 finished with value: 0.253411483163737 and parameters: {'lr': 0.0032598088187686864, 'dropout': 0.5230173422844056, 'd_model_multiplier': 8, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 348, 'batch_size': 19, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 57}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 103
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2366[0m   [32m0.7715[0m        [35m0.2793[0m       [31m0.9093[0m        [94m0.2640[0m     +  51.1260
      2   [36m0.2651[0m   [32m0.7826[0m        [35m0.2601[0m       0.9093        0.2662        51.3441
      3   [36m0.2863[0m   0.7825        [35m0.2551[0m       [31m0.9129[0m        0.2730        51.1402
      4   [36m0.2914[0m   [32m0.7960[0m        [35m0.2510[0m       0.9129        0.2687        51.3338
      5   0.2832   0.7932        [35m0.2494[0m       0.9105        0.2757        51.3374
      6   0.2853   [32m0.7999[0m        [35m0.2482[0m       [31m0.9141[0m        0.2748        51.4025
      7   0.2845   0.7811        [35m0.2464[0m       0.9141        0.2840        51.2659
      8   0.2807   0.7875        [35m0.2453[0m       0.9129        0.2832        51.4000
      9   0.2861   0.7866        0.2456       [31m0.9154[0m        0.2797        51.2195
     10   [36m0.2970[0m   0.7976        [35m0.2434[0m       0.9154        0.2802        51.1072
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 01:35:18,063][0m Trial 524 finished with value: 0.26402778583415476 and parameters: {'lr': 4.2397211832439665e-05, 'dropout': 0.4805699733652126, 'd_model_multiplier': 4, 'num_layers': 6, 'n_heads': 64, 'dim_feedforward': 340, 'batch_size': 10, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.001, 'top_n_features': 103}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 74
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1285[0m   [32m0.6512[0m        [35m0.3553[0m       [31m0.9262[0m        [94m0.2672[0m     +  35.2561
      2   0.1137   0.6057        [35m0.2782[0m       0.9262        [94m0.2664[0m     +  35.1410
      3   [36m0.1658[0m   [32m0.6809[0m        [35m0.2656[0m       0.9262        [94m0.2579[0m     +  35.2518
      4   0.1611   0.6737        [35m0.2560[0m       0.9262        [94m0.2509[0m     +  35.1356
      5   0.0738   0.5000        0.2726       0.9262        0.2664        34.9753
      6   0.0738   0.5000        0.2750       0.9262        0.2662        35.2900
      7   0.0738   0.5000        0.2766       0.9262        0.2644        35.0557
      8   0.0738   0.5000        0.2760       0.9262        0.2673        35.3148
      9   [36m0.1778[0m   0.5973        0.2756       0.9262        0.2664        35.0679
     10   0.0738   0.5000        0.2773       0.9262        0.2677        35.0669
     11   0.0738   0.5000        0.2774       0.9262        0.2656        35.0840
     12   0.0738   0.5000        0.2772       0.9262        0.2680        35.2600
     13   0.0738   0.5000        0.2771       0.9262        0.2679        35.2169
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 01:43:31,331][0m Trial 525 finished with value: 0.25093181908779305 and parameters: {'lr': 0.05637891332627354, 'dropout': 0.45159910896415434, 'd_model_multiplier': 4, 'num_layers': 8, 'n_heads': 32, 'dim_feedforward': 420, 'batch_size': 30, 'pos_encoding': 'learnable', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0, 'top_n_features': 74}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 66
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0663[0m   [32m0.4068[0m        [35m0.6105[0m       [31m0.9299[0m        [94m0.4890[0m     +  13.0721
      2   [36m0.0743[0m   [32m0.4090[0m        [35m0.4907[0m       0.9299        [94m0.3730[0m     +  13.3352
      3   0.0601   [32m0.4289[0m        [35m0.3997[0m       0.9299        [94m0.3144[0m     +  13.0398
      4   0.0631   [32m0.4670[0m        [35m0.3485[0m       0.9299        [94m0.2855[0m     +  13.3188
      5   [36m0.0780[0m   [32m0.5046[0m        [35m0.3178[0m       0.9299        [94m0.2703[0m     +  13.3044
      6   [36m0.0879[0m   [32m0.5336[0m        [35m0.2983[0m       0.9299        [94m0.2619[0m     +  13.2986
      7   [36m0.0991[0m   [32m0.5519[0m        [35m0.2853[0m       0.9299        [94m0.2577[0m     +  13.1531
      8   [36m0.1062[0m   [32m0.5638[0m        [35m0.2786[0m       0.9299        [94m0.2560[0m     +  13.2381
      9   [36m0.1084[0m   [32m0.5704[0m        [35m0.2695[0m       0.9299        [94m0.2554[0m     +  13.3958
     10   [36m0.1122[0m   [32m0.5773[0m        [35m0.2667[0m       0.9299        [94m0.2553[0m     +  13.3373
     11   [36m0.1160[0m   [32m0.5816[0m        [35m0.2631[0m       0.9299        [94m0.2552[0m     +  13.1483
     12   [36m0.1172[0m   [32m0.5852[0m        [35m0.2616[0m       0.9299        [94m0.2551[0m     +  13.2219
     13   [36m0.1201[0m   [32m0.5882[0m        [35m0.2586[0m       0.9299        0.2556        13.0968
     14   [36m0.1239[0m   [32m0.5923[0m        0.2602       0.9287        0.2564        13.2013
     15   [36m0.1257[0m   [32m0.5945[0m        [35m0.2564[0m       0.9287        0.2565        13.2904
     16   [36m0.1281[0m   [32m0.5959[0m        [35m0.2529[0m       0.9287        0.2566        13.2010
     17   [36m0.1302[0m   [32m0.5998[0m        0.2535       0.9287        0.2567        13.1290
     18   [36m0.1327[0m   [32m0.6045[0m        0.2542       0.9274        0.2567        13.3307
     19   [36m0.1353[0m   [32m0.6080[0m        [35m0.2511[0m       0.9214        0.2571        13.0690
     20   [36m0.1390[0m   [32m0.6122[0m        0.2512       0.9226        0.2575        13.4641
     21   [36m0.1421[0m   [32m0.6158[0m        [35m0.2483[0m       0.9238        0.2577        13.1629
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 01:48:23,280][0m Trial 526 finished with value: 0.25514743023945025 and parameters: {'lr': 2.8687451347399405e-05, 'dropout': 0.6987987386551427, 'd_model_multiplier': 1, 'num_layers': 3, 'n_heads': 32, 'dim_feedforward': 335, 'batch_size': 42, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 66}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 94
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1017[0m   [32m0.4487[0m        [35m0.4851[0m       [31m0.9141[0m        [94m0.4873[0m     +  35.4064
      2   [36m0.2559[0m   [32m0.7244[0m        [35m0.2925[0m       0.9081        [94m0.4133[0m     +  35.5789
      3   [36m0.2827[0m   [32m0.7555[0m        [35m0.2670[0m       0.9117        [94m0.3858[0m     +  35.7595
      4   [36m0.3006[0m   [32m0.7616[0m        [35m0.2570[0m       0.9033        [94m0.3655[0m     +  35.8114
      5   [36m0.3055[0m   0.7595        [35m0.2536[0m       0.9045        [94m0.3459[0m     +  35.7786
      6   [36m0.3085[0m   0.7582        [35m0.2510[0m       0.9081        [94m0.3315[0m     +  35.7507
      7   [36m0.3366[0m   0.7563        [35m0.2463[0m       0.9081        [94m0.3220[0m     +  35.9317
      8   [36m0.3416[0m   0.7549        [35m0.2442[0m       0.9081        [94m0.3073[0m     +  35.8178
      9   0.3366   0.7520        [35m0.2418[0m       0.9105        [94m0.2967[0m     +  36.0327
     10   0.3368   0.7506        [35m0.2408[0m       [31m0.9154[0m        [94m0.2865[0m     +  35.8426
     11   0.3395   0.7486        [35m0.2384[0m       0.9154        [94m0.2795[0m     +  35.8095
     12   0.3326   0.7451        [35m0.2342[0m       [31m0.9166[0m        [94m0.2726[0m     +  35.8648
     13   0.3380   0.7443        [35m0.2332[0m       0.9166        [94m0.2668[0m     +  35.9401
     14   0.3383   0.7463        0.2357       [31m0.9178[0m        [94m0.2668[0m     +  35.7408
     15   0.3402   0.7477        [35m0.2307[0m       [31m0.9190[0m        [94m0.2620[0m     +  36.0320
     16   [36m0.3449[0m   0.7515        [35m0.2292[0m       0.9190        [94m0.2567[0m     +  35.9364
     17   [36m0.3467[0m   0.7500        0.2292       0.9166        [94m0.2548[0m     +  36.0526
     18   0.3361   0.7507        [35m0.2259[0m       0.9166        [94m0.2539[0m     +  35.8987
     19   0.3380   0.7550        0.2278       0.9166        [94m0.2513[0m     +  35.7832
     20   0.3407   0.7518        [35m0.2255[0m       0.9166        0.2523        35.8408
     21   0.3384   0.7549        [35m0.2229[0m       0.9154        [94m0.2510[0m     +  36.0257
     22   0.3430   0.7592        [35m0.2204[0m       0.9154        [94m0.2503[0m     +  36.0247
     23   0.3351   0.7565        0.2205       0.9154        0.2519        35.8775
     24   0.3290   0.7528        [35m0.2182[0m       0.9154        0.2533        35.7949
     25   0.3452   0.7577        [35m0.2181[0m       0.9166        0.2518        35.9801
     26   0.3424   0.7585        [35m0.2155[0m       0.9154        0.2539        36.0476
     27   0.3392   0.7606        [35m0.2135[0m       0.9166        0.2546        36.0165
     28   0.3440   [32m0.7637[0m        [35m0.2130[0m       0.9154        0.2535        36.1902
     29   0.3426   [32m0.7678[0m        [35m0.2125[0m       0.9166        0.2528        35.9824
     30   0.3396   [32m0.7702[0m        [35m0.2097[0m       0.9166        0.2547        36.3062
     31   0.3387   0.7684        [35m0.2066[0m       0.9166        0.2582        36.0746
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 02:07:45,389][0m Trial 527 finished with value: 0.25034270989635404 and parameters: {'lr': 1.48978802506906e-05, 'dropout': 0.49711884341633805, 'd_model_multiplier': 64, 'num_layers': 4, 'n_heads': 16, 'dim_feedforward': 189, 'batch_size': 67, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.01, 'top_n_features': 94}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 61
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1916[0m   [32m0.6968[0m        [35m0.2870[0m       [31m0.9129[0m        [94m0.3113[0m     +  27.7166
      2   [36m0.2061[0m   [32m0.7105[0m        [35m0.2620[0m       0.9129        0.3342        27.4230
      3   0.2003   0.7066        [35m0.2571[0m       0.9129        [94m0.3019[0m     +  27.6723
      4   0.1994   0.7016        [35m0.2545[0m       0.9129        [94m0.2940[0m     +  27.7966
      5   0.2003   0.6983        [35m0.2529[0m       0.9129        0.2956        27.6396
      6   0.2015   0.6898        [35m0.2510[0m       0.9129        0.2978        27.9690
      7   0.2008   0.6947        [35m0.2500[0m       0.9117        [94m0.2922[0m     +  27.8249
      8   0.1999   0.6960        [35m0.2480[0m       0.9093        [94m0.2903[0m     +  27.8452
      9   0.2010   0.6918        0.2487       0.9117        [94m0.2889[0m     +  27.5969
     10   0.2045   0.6965        0.2492       0.9129        [94m0.2884[0m     +  27.5985
     11   0.2041   0.6982        [35m0.2480[0m       0.9117        0.2917        27.5504
     12   0.2023   0.7002        0.2489       0.9129        0.2898        27.7393
     13   0.2037   0.6986        [35m0.2470[0m       0.9117        0.2906        27.5680
     14   0.2024   0.6977        [35m0.2469[0m       0.9129        [94m0.2848[0m     +  27.8696
     15   0.2000   0.6914        [35m0.2466[0m       0.9129        0.2848        27.8108
     16   0.1989   0.6840        0.2467       0.9105        [94m0.2820[0m     +  27.5396
     17   0.2013   0.6888        0.2470       0.9105        0.2838        27.8746
     18   0.2024   0.6893        0.2467       0.9093        0.2832        27.6832
     19   0.2031   0.6956        [35m0.2460[0m       0.9129        0.2893        27.6973
     20   0.2037   0.6922        0.2465       0.9093        0.2873        27.7055
     21   0.2030   0.6893        0.2472       0.9105        0.2874        28.0550
     22   0.2023   0.6907        [35m0.2455[0m       0.9105        [94m0.2790[0m     +  27.6943
     23   0.2044   0.6939        0.2457       0.9129        0.2811        27.8764
     24   0.2017   0.6880        0.2467       0.9129        0.2800        27.5744
     25   0.2030   0.6876        0.2461       0.9093        [94m0.2783[0m     +  27.6618
     26   0.2033   0.6866        0.2455       0.9117        [94m0.2763[0m     +  27.5546
     27   0.2051   0.6920        0.2455       0.9105        0.2769        27.8440
     28   0.2060   0.6958        [35m0.2453[0m       0.9117        [94m0.2763[0m     +  27.5092
     29   0.2053   0.6847        [35m0.2446[0m       0.9129        [94m0.2755[0m     +  27.8883
     30   0.2058   0.6901        0.2466       0.9105        0.2773        27.6202
     31   0.2040   0.6870        0.2464       0.9129        [94m0.2748[0m     +  27.6664
     32   0.2022   0.6856        0.2454       0.9105        0.2754        27.8484
     33   0.2044   0.6912        [35m0.2444[0m       0.9129        0.2752        27.7430
     34   0.2052   0.6923        0.2460       0.9105        [94m0.2744[0m     +  27.7316
     35   0.2038   0.6897        0.2452       0.9093        0.2756        27.6400
     36   0.2060   0.6972        0.2458       0.9117        0.2749        27.9634
     37   0.2036   0.6943        0.2468       0.9105        [94m0.2739[0m     +  27.6825
     38   [36m0.2097[0m   0.6884        [35m0.2442[0m       0.9129        [94m0.2727[0m     +  27.7236
     39   0.2064   0.6934        0.2484       0.9129        0.2765        27.6319
     40   0.2058   0.6959        0.2460       0.9129        0.2779        27.6345
     41   0.2043   0.6912        0.2453       0.9129        0.2760        27.5646
     42   0.2033   0.6921        0.2465       0.9129        0.2759        27.8408
     43   0.2036   0.6951        0.2449       0.9105        0.2760        27.6455
     44   0.2047   0.6960        0.2454       0.9117        0.2731        27.6399
     45   0.2041   0.6954        0.2453       0.9129        [94m0.2719[0m     +  27.7518
     46   0.2033   0.6925        0.2446       0.9093        0.2740        27.6474
     47   0.2020   0.6875        0.2445       0.9117        0.2737        27.7192
     48   0.2045   0.6899        0.2451       0.9117        0.2722        27.5603
     49   0.2041   0.6885        0.2445       0.9117        0.2725        27.6376
     50   0.2009   0.6895        0.2448       0.9081        0.2733        27.6497
[32m[I 2023-05-05 02:30:53,561][0m Trial 528 finished with value: 0.271927000389422 and parameters: {'lr': 0.0016491987607830274, 'dropout': 0.38347250419888673, 'd_model_multiplier': 4, 'num_layers': 6, 'n_heads': 32, 'dim_feedforward': 428, 'batch_size': 73, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 61}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 49
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
Warning, assumed runtime error: CUDA out of memory. Tried to allocate 2.05 GiB (GPU 0; 23.70 GiB total capacity; 16.73 GiB already allocated; 369.25 MiB free; 22.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[32m[I 2023-05-05 02:30:59,952][0m Trial 529 finished with value: 100.0 and parameters: {'lr': 0.000747048706949581, 'dropout': 0.2599352613984931, 'd_model_multiplier': 2, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 322, 'batch_size': 173, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 49}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 70
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2308[0m   [32m0.6838[0m        [35m0.3852[0m       [31m0.9214[0m        [94m0.2564[0m     +  32.4282
      2   [36m0.2951[0m   [32m0.7503[0m        [35m0.2460[0m       0.9178        [94m0.2393[0m     +  32.3166
      3   [36m0.3019[0m   [32m0.7569[0m        [35m0.2371[0m       0.9202        [94m0.2389[0m     +  32.6285
      4   [36m0.3041[0m   [32m0.7607[0m        [35m0.2329[0m       0.9178        [94m0.2376[0m     +  32.7453
      5   [36m0.3127[0m   0.7600        [35m0.2305[0m       [31m0.9238[0m        0.2410        32.8284
      6   0.3083   0.7598        [35m0.2280[0m       0.9214        0.2430        32.7848
      7   0.3007   0.7594        [35m0.2265[0m       0.9166        0.2483        32.8813
      8   0.2965   [32m0.7673[0m        [35m0.2254[0m       0.9178        0.2470        33.0330
      9   0.2957   0.7554        [35m0.2252[0m       0.9166        0.2559        32.6590
     10   0.2902   0.7645        [35m0.2223[0m       0.9166        0.2481        32.9444
     11   0.2889   0.7595        [35m0.2215[0m       0.9214        0.2515        32.8040
     12   0.2882   0.7587        [35m0.2190[0m       0.9178        0.2538        32.7748
     13   0.2795   0.7661        [35m0.2184[0m       0.9166        0.2483        32.8911
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 02:38:39,355][0m Trial 530 finished with value: 0.23755438065442383 and parameters: {'lr': 7.296384823003803e-05, 'dropout': 0.46821083551315135, 'd_model_multiplier': 8, 'num_layers': 7, 'n_heads': 32, 'dim_feedforward': 207, 'batch_size': 60, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.001, 'top_n_features': 70}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 54
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
Warning, assumed runtime error: CUDA out of memory. Tried to allocate 5.89 GiB (GPU 0; 23.70 GiB total capacity; 20.47 GiB already allocated; 1.96 GiB free; 20.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[32m[I 2023-05-05 02:38:46,119][0m Trial 531 finished with value: 100.0 and parameters: {'lr': 0.0001061235930204272, 'dropout': 0.41860064543465714, 'd_model_multiplier': 4, 'num_layers': 4, 'n_heads': 64, 'dim_feedforward': 352, 'batch_size': 249, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 54}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 80
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2791[0m   [32m0.7323[0m        [35m0.3137[0m       [31m0.9129[0m        [94m0.3373[0m     +  10.4992
      2   [36m0.3286[0m   [32m0.7428[0m        [35m0.2564[0m       [31m0.9214[0m        [94m0.3057[0m     +  10.7214
      3   [36m0.3454[0m   [32m0.7456[0m        [35m0.2515[0m       0.9214        [94m0.2867[0m     +  10.8038
      4   [36m0.3651[0m   [32m0.7493[0m        [35m0.2453[0m       0.9214        [94m0.2684[0m     +  10.9547
      5   [36m0.3766[0m   [32m0.7564[0m        [35m0.2408[0m       0.9214        [94m0.2557[0m     +  10.8660
      6   0.3690   0.7551        [35m0.2380[0m       0.9214        [94m0.2525[0m     +  11.6649
      7   0.3662   [32m0.7574[0m        [35m0.2343[0m       [31m0.9226[0m        0.2525        11.0289
      8   [36m0.3794[0m   [32m0.7625[0m        0.2353       0.9214        [94m0.2470[0m     +  10.8292
      9   0.3763   [32m0.7659[0m        [35m0.2302[0m       0.9214        [94m0.2446[0m     +  11.1017
     10   0.3655   0.7631        [35m0.2285[0m       0.9226        0.2463        10.8433
     11   0.3602   0.7653        [35m0.2226[0m       0.9202        0.2474        11.0446
     12   0.3619   0.7626        [35m0.2222[0m       0.9214        0.2459        10.9999
     13   0.3720   [32m0.7693[0m        0.2224       0.9190        [94m0.2420[0m     +  11.1428
     14   0.3602   0.7677        [35m0.2201[0m       0.9202        0.2428        11.1928
     15   0.3722   [32m0.7734[0m        [35m0.2158[0m       0.9226        [94m0.2411[0m     +  10.9819
     16   0.3577   0.7705        0.2174       0.9214        0.2437        10.8084
     17   0.3528   [32m0.7741[0m        [35m0.2149[0m       0.9202        0.2424        11.5373
     18   0.3528   0.7732        [35m0.2147[0m       0.9226        0.2428        11.2938
     19   0.3478   [32m0.7746[0m        [35m0.2100[0m       0.9214        0.2431        11.4305
     20   0.3386   0.7730        [35m0.2088[0m       0.9202        0.2448        10.8293
     21   0.3448   0.7714        0.2112       0.9226        0.2450        10.9560
     22   0.3343   0.7701        [35m0.2070[0m       0.9202        0.2471        11.2838
     23   0.3417   0.7699        [35m0.2060[0m       0.9226        0.2467        11.0948
     24   0.3319   0.7694        [35m0.2051[0m       0.9226        0.2475        11.0530
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 02:43:24,102][0m Trial 532 finished with value: 0.24114386382385494 and parameters: {'lr': 4.3363640590740356e-05, 'dropout': 0.40042439091519305, 'd_model_multiplier': 32, 'num_layers': 6, 'n_heads': 4, 'dim_feedforward': 314, 'batch_size': 36, 'pos_encoding': 'learnable', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 80}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 63
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0556[0m   [32m0.3013[0m        [35m0.6314[0m       [31m0.9274[0m        [94m0.4518[0m     +  19.8015
      2   [36m0.0978[0m   [32m0.5561[0m        [35m0.3682[0m       0.9274        [94m0.2776[0m     +  19.9681
      3   [36m0.1871[0m   [32m0.6832[0m        [35m0.2750[0m       0.9250        [94m0.2445[0m     +  20.0899
      4   [36m0.2075[0m   [32m0.7118[0m        [35m0.2516[0m       0.9214        [94m0.2377[0m     +  20.0074
      5   [36m0.2129[0m   [32m0.7254[0m        [35m0.2428[0m       0.9154        0.2381        20.2407
      6   [36m0.2179[0m   [32m0.7304[0m        [35m0.2374[0m       0.9178        0.2382        20.0488
      7   [36m0.2270[0m   [32m0.7358[0m        [35m0.2355[0m       0.9190        0.2387        20.1535
      8   [36m0.2333[0m   [32m0.7395[0m        [35m0.2331[0m       0.9202        [94m0.2366[0m     +  20.1341
      9   [36m0.2376[0m   [32m0.7435[0m        [35m0.2319[0m       0.9190        [94m0.2363[0m     +  19.9454
     10   [36m0.2399[0m   [32m0.7458[0m        [35m0.2307[0m       0.9190        0.2374        20.5440
     11   [36m0.2483[0m   [32m0.7485[0m        [35m0.2305[0m       0.9202        [94m0.2362[0m     +  20.2144
     12   [36m0.2525[0m   [32m0.7511[0m        [35m0.2301[0m       0.9214        [94m0.2355[0m     +  19.9687
     13   0.2506   [32m0.7522[0m        [35m0.2275[0m       0.9226        [94m0.2352[0m     +  20.0586
     14   0.2511   [32m0.7534[0m        [35m0.2269[0m       0.9238        0.2355        20.1961
     15   0.2466   [32m0.7572[0m        0.2271       0.9226        [94m0.2350[0m     +  20.0681
     16   0.2492   [32m0.7585[0m        0.2273       0.9226        0.2351        20.0287
     17   0.2521   [32m0.7592[0m        0.2270       0.9226        0.2353        19.9777
     18   0.2481   [32m0.7600[0m        [35m0.2253[0m       0.9226        0.2363        20.1210
     19   [36m0.2542[0m   [32m0.7625[0m        0.2255       0.9202        0.2361        20.1846
     20   0.2525   [32m0.7632[0m        [35m0.2246[0m       0.9202        0.2358        20.1022
     21   0.2530   [32m0.7642[0m        [35m0.2243[0m       0.9202        0.2373        20.0949
     22   0.2535   [32m0.7667[0m        [35m0.2228[0m       0.9214        0.2351        20.1939
     23   0.2527   [32m0.7673[0m        [35m0.2223[0m       0.9214        0.2377        20.0289
     24   [36m0.2572[0m   [32m0.7708[0m        [35m0.2213[0m       0.9202        0.2352        19.9486
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 02:51:47,274][0m Trial 533 finished with value: 0.2350248522344783 and parameters: {'lr': 1.7577620935507155e-05, 'dropout': 0.35322250876783007, 'd_model_multiplier': 4, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 442, 'batch_size': 50, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 63}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 43
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1906[0m   [32m0.7291[0m        [35m0.2675[0m       [31m0.9081[0m        [94m0.2627[0m     +  23.6444
      2   [36m0.2022[0m   [32m0.7367[0m        [35m0.2382[0m       [31m0.9129[0m        [94m0.2557[0m     +  23.9401
      3   [36m0.2069[0m   [32m0.7406[0m        [35m0.2345[0m       0.9129        0.2593        23.8794
      4   [36m0.2117[0m   [32m0.7458[0m        [35m0.2343[0m       [31m0.9154[0m        [94m0.2548[0m     +  24.0427
      5   [36m0.2129[0m   [32m0.7464[0m        [35m0.2323[0m       0.9154        [94m0.2540[0m     +  24.2292
      6   [36m0.2173[0m   0.7462        [35m0.2299[0m       0.9105        [94m0.2539[0m     +  24.1400
      7   [36m0.2202[0m   0.7460        [35m0.2292[0m       0.9093        0.2585        24.2713
      8   0.2129   0.7457        [35m0.2260[0m       0.9081        0.2622        24.1626
      9   [36m0.2272[0m   [32m0.7578[0m        0.2263       0.9141        [94m0.2532[0m     +  24.3772
     10   0.2185   0.7552        [35m0.2238[0m       0.9129        0.2630        24.3780
     11   0.2191   0.7553        [35m0.2228[0m       0.9093        0.2683        24.0769
     12   0.2231   0.7538        [35m0.2211[0m       0.9141        0.2741        24.0584
     13   0.2268   [32m0.7619[0m        [35m0.2203[0m       0.9129        0.2708        24.2185
     14   0.2238   [32m0.7625[0m        0.2209       0.9081        0.2700        24.1103
     15   [36m0.2283[0m   0.7599        [35m0.2153[0m       0.9129        0.2728        24.3739
     16   0.2234   0.7558        0.2175       0.9129        0.2764        23.8823
     17   0.2120   0.7605        [35m0.2153[0m       0.9069        0.2790        23.8804
     18   0.2183   0.7586        0.2154       0.9045        0.2851        24.0623
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 02:59:26,378][0m Trial 534 finished with value: 0.25323859770684315 and parameters: {'lr': 0.0002590101776087858, 'dropout': 0.43572326941159634, 'd_model_multiplier': 4, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 392, 'batch_size': 77, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 43}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 87
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
Warning, assumed runtime error: CUDA out of memory. Tried to allocate 2.83 GiB (GPU 0; 23.70 GiB total capacity; 16.38 GiB already allocated; 2.05 GiB free; 20.55 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[32m[I 2023-05-05 02:59:32,370][0m Trial 535 finished with value: 100.0 and parameters: {'lr': 0.003992896997390689, 'dropout': 0.12426305328986376, 'd_model_multiplier': 16, 'num_layers': 6, 'n_heads': 32, 'dim_feedforward': 291, 'batch_size': 239, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.001, 'top_n_features': 87}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 68
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1412[0m   [32m0.4428[0m        [35m0.7104[0m       [31m0.2430[0m        [94m0.7177[0m     +  18.5514
      2   0.1202   0.4222        [35m0.7064[0m       [31m0.3144[0m        [94m0.7089[0m     +  19.0596
      3   0.0906   0.3979        [35m0.6960[0m       [31m0.3930[0m        [94m0.6977[0m     +  18.9449
      4   0.0792   0.3706        [35m0.6866[0m       [31m0.4788[0m        [94m0.6850[0m     +  19.2102
      5   0.0692   0.3490        [35m0.6727[0m       [31m0.5889[0m        [94m0.6712[0m     +  18.9117
      6   0.0675   0.3350        [35m0.6618[0m       [31m0.7497[0m        [94m0.6564[0m     +  19.1357
      7   0.0663   0.3225        [35m0.6487[0m       [31m0.8597[0m        [94m0.6412[0m     +  18.9796
      8   0.0662   0.3135        [35m0.6346[0m       [31m0.9057[0m        [94m0.6258[0m     +  18.9111
      9   0.0683   0.3082        [35m0.6214[0m       [31m0.9117[0m        [94m0.6102[0m     +  18.6343
     10   0.0682   0.3042        [35m0.6072[0m       0.9117        [94m0.5948[0m     +  18.8944
     11   0.0682   0.3006        [35m0.5941[0m       0.9117        [94m0.5796[0m     +  19.1551
     12   0.0683   0.2991        [35m0.5811[0m       0.9117        [94m0.5647[0m     +  19.2493
     13   0.0689   0.2989        [35m0.5651[0m       0.9117        [94m0.5501[0m     +  19.0596
     14   0.0692   0.2987        [35m0.5528[0m       0.9117        [94m0.5363[0m     +  19.1258
     15   0.0697   0.2990        [35m0.5407[0m       0.9117        [94m0.5229[0m     +  19.1232
     16   0.0701   0.2986        [35m0.5268[0m       0.9117        [94m0.5102[0m     +  18.9614
     17   0.0705   0.2990        [35m0.5161[0m       0.9117        [94m0.4981[0m     +  18.8724
     18   0.0705   0.2987        [35m0.5039[0m       0.9117        [94m0.4868[0m     +  19.2478
     19   0.0707   0.2993        [35m0.4924[0m       0.9117        [94m0.4761[0m     +  19.0231
     20   0.0711   0.3003        [35m0.4815[0m       0.9117        [94m0.4659[0m     +  18.9928
     21   0.0712   0.3017        [35m0.4698[0m       0.9117        [94m0.4564[0m     +  19.2101
     22   0.0714   0.3039        [35m0.4615[0m       0.9117        [94m0.4474[0m     +  18.9067
     23   0.0716   0.3065        [35m0.4511[0m       0.9117        [94m0.4390[0m     +  18.9038
     24   0.0719   0.3095        [35m0.4424[0m       0.9117        [94m0.4311[0m     +  18.8539
     25   0.0723   0.3124        [35m0.4352[0m       0.9117        [94m0.4236[0m     +  19.0590
     26   0.0729   0.3166        [35m0.4262[0m       0.9117        [94m0.4165[0m     +  18.9024
     27   0.0734   0.3204        [35m0.4203[0m       0.9117        [94m0.4099[0m     +  18.9994
     28   0.0740   0.3248        [35m0.4122[0m       0.9117        [94m0.4035[0m     +  18.9711
     29   0.0743   0.3289        [35m0.4065[0m       0.9117        [94m0.3976[0m     +  18.9548
     30   0.0752   0.3337        [35m0.4016[0m       0.9117        [94m0.3920[0m     +  18.7954
     31   0.0757   0.3388        [35m0.3930[0m       0.9117        [94m0.3866[0m     +  18.9582
     32   0.0764   0.3444        [35m0.3876[0m       0.9117        [94m0.3816[0m     +  18.9055
     33   0.0767   0.3507        [35m0.3839[0m       0.9117        [94m0.3769[0m     +  19.3675
     34   0.0773   0.3570        [35m0.3786[0m       0.9117        [94m0.3724[0m     +  19.0794
     35   0.0778   0.3629        [35m0.3748[0m       0.9117        [94m0.3683[0m     +  18.9613
     36   0.0782   0.3688        [35m0.3701[0m       0.9117        [94m0.3642[0m     +  19.0119
     37   0.0788   0.3755        [35m0.3654[0m       0.9117        [94m0.3605[0m     +  18.9420
     38   0.0789   0.3809        [35m0.3612[0m       0.9117        [94m0.3569[0m     +  18.7958
     39   0.0796   0.3879        [35m0.3572[0m       0.9117        [94m0.3534[0m     +  18.9453
     40   0.0870   0.3940        [35m0.3526[0m       0.9117        [94m0.3503[0m     +  18.9066
     41   0.0877   0.4002        [35m0.3492[0m       0.9117        [94m0.3472[0m     +  19.0074
     42   0.0885   0.4069        [35m0.3455[0m       0.9117        [94m0.3443[0m     +  19.2434
     43   0.0894   0.4139        [35m0.3427[0m       0.9117        [94m0.3415[0m     +  18.9510
     44   0.0902   0.4210        [35m0.3397[0m       0.9117        [94m0.3389[0m     +  19.0001
     45   0.0914   0.4289        [35m0.3373[0m       0.9117        [94m0.3363[0m     +  18.6422
     46   0.0924   0.4363        [35m0.3334[0m       0.9117        [94m0.3339[0m     +  19.3014
     47   0.0934   0.4426        [35m0.3296[0m       0.9117        [94m0.3316[0m     +  18.7743
     48   0.0947   [32m0.4497[0m        [35m0.3285[0m       0.9117        [94m0.3294[0m     +  19.1508
     49   0.0961   [32m0.4573[0m        [35m0.3248[0m       0.9117        [94m0.3272[0m     +  18.8702
     50   0.0976   [32m0.4646[0m        [35m0.3219[0m       0.9117        [94m0.3251[0m     +  19.1141
[32m[I 2023-05-05 03:15:25,352][0m Trial 536 finished with value: 0.32511412765844266 and parameters: {'lr': 1.0232557907072572e-06, 'dropout': 0.5367009299731346, 'd_model_multiplier': 1, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 366, 'batch_size': 64, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 68}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 198
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.0779[0m   [32m0.3931[0m        [35m0.7067[0m       [31m0.7267[0m        [94m0.6697[0m     +  9.8644
      2   0.0602   0.3547        [35m0.6447[0m       [31m0.9250[0m        [94m0.6215[0m     +  9.9041
      3   0.0580   0.3421        [35m0.5627[0m       [31m0.9311[0m        [94m0.5687[0m     +  9.8448
      4   0.0544   0.3400        [35m0.4886[0m       0.9311        [94m0.5181[0m     +  10.4451
      5   0.0533   0.3473        [35m0.4301[0m       0.9311        [94m0.4721[0m     +  10.1926
      6   0.0540   0.3678        [35m0.3856[0m       0.9311        [94m0.4327[0m     +  9.9013
      7   0.0567   [32m0.3963[0m        [35m0.3521[0m       0.9311        [94m0.4006[0m     +  10.2766
      8   0.0682   [32m0.4349[0m        [35m0.3260[0m       0.9311        [94m0.3738[0m     +  10.2012
      9   0.0734   [32m0.4749[0m        [35m0.3082[0m       0.9311        [94m0.3525[0m     +  10.3288
     10   [36m0.0792[0m   [32m0.5158[0m        [35m0.2943[0m       0.9311        [94m0.3359[0m     +  10.0544
     11   [36m0.0933[0m   [32m0.5525[0m        [35m0.2843[0m       0.9311        [94m0.3230[0m     +  9.9736
     12   [36m0.1042[0m   [32m0.5805[0m        [35m0.2780[0m       0.9311        [94m0.3128[0m     +  10.0497
     13   [36m0.1164[0m   [32m0.6018[0m        [35m0.2728[0m       0.9311        [94m0.3042[0m     +  10.3531
     14   [36m0.1262[0m   [32m0.6182[0m        [35m0.2701[0m       0.9311        [94m0.2970[0m     +  10.3297
     15   [36m0.1338[0m   [32m0.6329[0m        [35m0.2638[0m       0.9299        [94m0.2905[0m     +  10.1440
     16   [36m0.1436[0m   [32m0.6470[0m        [35m0.2619[0m       0.9299        [94m0.2862[0m     +  10.1501
     17   [36m0.1529[0m   [32m0.6578[0m        [35m0.2599[0m       0.9299        [94m0.2815[0m     +  10.4312
     18   [36m0.1618[0m   [32m0.6650[0m        [35m0.2584[0m       0.9299        [94m0.2773[0m     +  10.3253
     19   [36m0.1700[0m   [32m0.6723[0m        [35m0.2566[0m       0.9299        [94m0.2742[0m     +  10.2600
     20   [36m0.1808[0m   [32m0.6776[0m        [35m0.2555[0m       0.9299        [94m0.2721[0m     +  10.4139
     21   [36m0.1869[0m   [32m0.6833[0m        [35m0.2533[0m       0.9299        [94m0.2695[0m     +  10.2840
     22   [36m0.1904[0m   [32m0.6877[0m        0.2545       0.9299        [94m0.2674[0m     +  10.0982
     23   [36m0.1978[0m   [32m0.6926[0m        [35m0.2518[0m       0.9299        [94m0.2663[0m     +  9.9060
     24   [36m0.2038[0m   [32m0.6955[0m        [35m0.2500[0m       0.9299        [94m0.2646[0m     +  9.9061
     25   [36m0.2067[0m   [32m0.6979[0m        [35m0.2496[0m       0.9299        [94m0.2624[0m     +  10.1304
     26   [36m0.2082[0m   [32m0.7004[0m        0.2520       0.9299        [94m0.2612[0m     +  10.2162
     27   [36m0.2089[0m   [32m0.7033[0m        [35m0.2491[0m       0.9299        [94m0.2606[0m     +  10.0446
     28   [36m0.2102[0m   [32m0.7057[0m        0.2495       0.9287        [94m0.2594[0m     +  10.1622
     29   [36m0.2112[0m   [32m0.7063[0m        0.2514       0.9287        [94m0.2582[0m     +  10.0959
     30   0.2110   [32m0.7079[0m        [35m0.2478[0m       0.9287        [94m0.2571[0m     +  10.1356
     31   [36m0.2131[0m   [32m0.7100[0m        0.2490       0.9287        [94m0.2568[0m     +  10.2465
     32   [36m0.2140[0m   [32m0.7125[0m        0.2490       0.9287        [94m0.2558[0m     +  10.3097
     33   [36m0.2159[0m   [32m0.7140[0m        0.2481       0.9287        [94m0.2553[0m     +  9.9009
     34   [36m0.2181[0m   [32m0.7155[0m        [35m0.2470[0m       0.9287        [94m0.2544[0m     +  9.9097
     35   [36m0.2188[0m   [32m0.7171[0m        [35m0.2452[0m       0.9299        [94m0.2541[0m     +  10.1276
     36   [36m0.2194[0m   [32m0.7179[0m        0.2459       0.9299        [94m0.2540[0m     +  9.7992
     37   [36m0.2197[0m   [32m0.7190[0m        0.2464       0.9299        [94m0.2533[0m     +  10.3653
     38   [36m0.2205[0m   [32m0.7203[0m        0.2462       0.9299        [94m0.2527[0m     +  9.9814
     39   [36m0.2208[0m   [32m0.7219[0m        0.2466       0.9299        [94m0.2521[0m     +  10.0078
     40   [36m0.2218[0m   [32m0.7233[0m        [35m0.2449[0m       0.9299        0.2525        10.1910
     41   [36m0.2219[0m   [32m0.7239[0m        0.2455       0.9299        [94m0.2516[0m     +  10.0605
     42   [36m0.2228[0m   [32m0.7249[0m        0.2452       0.9299        [94m0.2515[0m     +  10.1499
     43   0.2217   [32m0.7254[0m        [35m0.2436[0m       0.9299        [94m0.2514[0m     +  9.8822
     44   0.2224   [32m0.7271[0m        0.2450       0.9299        [94m0.2507[0m     +  10.0470
     45   [36m0.2240[0m   [32m0.7279[0m        0.2464       0.9299        [94m0.2505[0m     +  10.0535
     46   0.2237   [32m0.7290[0m        0.2450       0.9299        [94m0.2501[0m     +  10.1791
     47   [36m0.2244[0m   [32m0.7298[0m        [35m0.2431[0m       0.9299        [94m0.2500[0m     +  9.9599
     48   [36m0.2250[0m   [32m0.7305[0m        [35m0.2423[0m       0.9299        [94m0.2495[0m     +  10.0629
     49   [36m0.2257[0m   [32m0.7318[0m        0.2434       0.9299        0.2496        10.3027
     50   [36m0.2264[0m   [32m0.7324[0m        [35m0.2423[0m       0.9299        [94m0.2493[0m     +  10.2220
[32m[I 2023-05-05 03:23:55,747][0m Trial 537 finished with value: 0.24931455254915116 and parameters: {'lr': 1.4193791424754464e-06, 'dropout': 0.36820512897256225, 'd_model_multiplier': 8, 'num_layers': 1, 'n_heads': 32, 'dim_feedforward': 331, 'batch_size': 54, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 198}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 57
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2461[0m   [32m0.7496[0m        [35m0.3153[0m       [31m0.9154[0m        [94m0.2660[0m     +  26.7692
      2   0.2044   0.6732        [35m0.2724[0m       0.9141        0.2763        26.7610
      3   0.2056   0.6787        [35m0.2593[0m       0.9129        0.2836        26.9069
      4   0.2061   0.6843        [35m0.2559[0m       0.9154        0.2957        26.7367
      5   0.1924   0.6820        [35m0.2549[0m       0.9154        0.2968        26.8810
      6   0.1991   0.6858        [35m0.2537[0m       0.9154        0.2933        26.9386
      7   0.1966   0.6835        [35m0.2514[0m       0.9154        0.2914        26.9446
      8   0.1983   0.6853        0.2524       0.9141        0.2931        26.9134
      9   0.1951   0.6820        0.2519       0.9141        0.2909        26.8950
     10   0.2026   0.6848        0.2523       [31m0.9166[0m        0.2872        26.9174
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 03:28:51,622][0m Trial 538 finished with value: 0.2659773157037876 and parameters: {'lr': 0.000892063139451751, 'dropout': 0.3240065847407292, 'd_model_multiplier': 4, 'num_layers': 3, 'n_heads': 64, 'dim_feedforward': 404, 'batch_size': 23, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 57}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 60
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0571[0m   [32m0.3457[0m        [35m0.6070[0m       [31m0.9287[0m        [94m0.5076[0m     +  20.0109
      2   [36m0.0823[0m   [32m0.3936[0m        [35m0.4442[0m       0.9287        [94m0.3446[0m     +  19.7710
      3   [36m0.1209[0m   [32m0.5945[0m        [35m0.3330[0m       0.9287        [94m0.2687[0m     +  19.8515
      4   [36m0.2203[0m   [32m0.6746[0m        [35m0.2842[0m       0.9287        [94m0.2435[0m     +  19.9667
      5   [36m0.2400[0m   [32m0.7047[0m        [35m0.2647[0m       0.9287        [94m0.2334[0m     +  19.8581
      6   [36m0.2488[0m   [32m0.7218[0m        [35m0.2535[0m       0.9287        [94m0.2286[0m     +  20.0376
      7   [36m0.2646[0m   [32m0.7376[0m        [35m0.2482[0m       [31m0.9299[0m        [94m0.2256[0m     +  19.8054
      8   [36m0.2755[0m   [32m0.7487[0m        [35m0.2430[0m       0.9299        [94m0.2238[0m     +  19.7172
      9   [36m0.2843[0m   [32m0.7586[0m        [35m0.2404[0m       [31m0.9311[0m        [94m0.2224[0m     +  19.8249
     10   [36m0.2968[0m   [32m0.7651[0m        [35m0.2383[0m       [31m0.9335[0m        [94m0.2212[0m     +  20.0155
     11   [36m0.3000[0m   [32m0.7705[0m        [35m0.2359[0m       0.9287        [94m0.2203[0m     +  19.7460
     12   [36m0.3053[0m   [32m0.7744[0m        [35m0.2351[0m       0.9287        [94m0.2195[0m     +  19.9085
     13   [36m0.3112[0m   [32m0.7780[0m        [35m0.2350[0m       0.9299        [94m0.2186[0m     +  19.7876
     14   [36m0.3155[0m   [32m0.7810[0m        [35m0.2338[0m       0.9311        [94m0.2179[0m     +  19.9346
     15   [36m0.3162[0m   [32m0.7829[0m        [35m0.2332[0m       0.9311        [94m0.2172[0m     +  20.0208
     16   0.3153   [32m0.7845[0m        0.2339       0.9311        [94m0.2166[0m     +  19.9340
     17   0.3133   [32m0.7862[0m        [35m0.2318[0m       0.9299        [94m0.2162[0m     +  19.8270
     18   0.3143   [32m0.7884[0m        0.2326       0.9299        [94m0.2159[0m     +  19.9637
     19   0.3152   [32m0.7896[0m        [35m0.2317[0m       0.9299        [94m0.2155[0m     +  20.5855
     20   0.3161   [32m0.7911[0m        [35m0.2310[0m       0.9287        [94m0.2150[0m     +  19.7695
     21   [36m0.3171[0m   [32m0.7922[0m        [35m0.2297[0m       0.9274        [94m0.2146[0m     +  19.8851
     22   0.3136   [32m0.7933[0m        0.2301       0.9274        [94m0.2145[0m     +  19.7840
     23   0.3149   [32m0.7952[0m        [35m0.2293[0m       0.9287        [94m0.2142[0m     +  19.7889
     24   0.3135   [32m0.7959[0m        0.2299       0.9274        [94m0.2139[0m     +  19.8963
     25   0.3140   [32m0.7972[0m        [35m0.2284[0m       0.9274        [94m0.2138[0m     +  19.9059
     26   0.3090   [32m0.7980[0m        [35m0.2282[0m       0.9250        [94m0.2137[0m     +  19.7356
     27   0.3058   [32m0.7988[0m        0.2293       0.9250        0.2137        19.6929
     28   0.3079   [32m0.8009[0m        0.2287       0.9262        [94m0.2133[0m     +  19.6648
     29   0.3072   [32m0.8015[0m        [35m0.2276[0m       0.9238        0.2133        19.7676
     30   0.3089   [32m0.8025[0m        0.2277       0.9250        [94m0.2131[0m     +  19.9009
     31   0.3101   [32m0.8036[0m        [35m0.2269[0m       0.9250        [94m0.2126[0m     +  19.7020
     32   0.2996   [32m0.8040[0m        0.2272       0.9250        0.2130        19.9165
     33   0.3005   [32m0.8052[0m        0.2272       0.9262        0.2130        19.7932
     34   0.3003   [32m0.8059[0m        [35m0.2265[0m       0.9250        0.2129        19.9082
     35   0.2993   [32m0.8061[0m        [35m0.2258[0m       0.9262        0.2132        19.9591
     36   0.3002   [32m0.8073[0m        0.2264       0.9274        0.2132        19.9165
     37   0.3023   [32m0.8082[0m        [35m0.2240[0m       0.9250        0.2133        19.8436
     38   0.3010   [32m0.8085[0m        0.2261       0.9262        0.2130        19.7750
     39   0.3014   [32m0.8093[0m        0.2251       0.9262        0.2127        19.7018
     40   0.3003   [32m0.8100[0m        0.2250       0.9250        0.2134        19.8229
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 03:42:29,041][0m Trial 539 finished with value: 0.21256487669722776 and parameters: {'lr': 7.5541980910898266e-06, 'dropout': 0.34036384744760373, 'd_model_multiplier': 4, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 346, 'batch_size': 46, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0, 'top_n_features': 60}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 52
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2617[0m   [32m0.6927[0m        [35m0.3827[0m       [31m0.9045[0m        [94m0.2911[0m     +  18.5039
      2   [36m0.3946[0m   [32m0.7604[0m        [35m0.2465[0m       [31m0.9081[0m        [94m0.2672[0m     +  19.0868
      3   0.3940   [32m0.7830[0m        [35m0.2372[0m       [31m0.9166[0m        [94m0.2601[0m     +  19.6071
      4   [36m0.4027[0m   [32m0.7907[0m        [35m0.2333[0m       0.9154        [94m0.2597[0m     +  18.8264
      5   0.3979   0.7898        [35m0.2315[0m       0.9141        0.2608        18.6393
      6   0.3745   0.7864        [35m0.2302[0m       0.9129        0.2669        18.9221
      7   0.3899   [32m0.7933[0m        0.2305       [31m0.9190[0m        0.2638        18.8320
      8   0.3721   0.7916        [35m0.2296[0m       0.9105        0.2680        19.0092
      9   0.3761   0.7925        0.2306       0.9129        0.2684        19.0282
     10   0.3723   [32m0.7947[0m        0.2297       0.9141        0.2683        19.2108
     11   0.3777   0.7934        [35m0.2253[0m       0.9105        0.2712        19.1495
     12   0.3650   0.7900        0.2290       0.9093        0.2752        19.0700
     13   0.3751   0.7923        0.2273       0.9141        0.2814        19.0481
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 03:46:55,354][0m Trial 540 finished with value: 0.2596938995489042 and parameters: {'lr': 0.00044365074183126457, 'dropout': 0.5526652998010461, 'd_model_multiplier': 4, 'num_layers': 7, 'n_heads': 16, 'dim_feedforward': 357, 'batch_size': 69, 'pos_encoding': 'learnable', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.001, 'top_n_features': 52}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 77
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0649[0m   [32m0.4398[0m        [35m0.6997[0m       [31m0.7473[0m        [94m0.6538[0m     +  27.4269
      2   0.0523   0.3392        [35m0.6064[0m       [31m0.9323[0m        [94m0.5276[0m     +  27.0138
      3   0.0527   0.3339        [35m0.5055[0m       0.9323        [94m0.4180[0m     +  27.5536
      4   0.0524   0.3496        [35m0.4218[0m       0.9323        [94m0.3421[0m     +  27.1681
      5   0.0549   0.3926        [35m0.3639[0m       0.9323        [94m0.2970[0m     +  27.6170
      6   0.0634   [32m0.4658[0m        [35m0.3278[0m       0.9323        [94m0.2716[0m     +  27.5979
      7   [36m0.0902[0m   [32m0.5300[0m        [35m0.3050[0m       0.9323        [94m0.2569[0m     +  27.1703
      8   [36m0.1105[0m   [32m0.5853[0m        [35m0.2907[0m       0.9323        [94m0.2475[0m     +  27.2061
      9   [36m0.1368[0m   [32m0.6246[0m        [35m0.2811[0m       0.9323        [94m0.2412[0m     +  27.4511
     10   [36m0.1528[0m   [32m0.6528[0m        [35m0.2752[0m       0.9323        [94m0.2368[0m     +  27.3170
     11   [36m0.1674[0m   [32m0.6744[0m        [35m0.2705[0m       0.9323        [94m0.2333[0m     +  27.3447
     12   [36m0.1751[0m   [32m0.6858[0m        [35m0.2665[0m       0.9323        [94m0.2310[0m     +  27.4288
     13   [36m0.1862[0m   [32m0.6958[0m        [35m0.2623[0m       0.9323        [94m0.2290[0m     +  27.5466
     14   [36m0.2009[0m   [32m0.7043[0m        [35m0.2617[0m       0.9323        [94m0.2273[0m     +  27.4010
     15   [36m0.2251[0m   [32m0.7107[0m        [35m0.2596[0m       0.9323        [94m0.2258[0m     +  27.4284
     16   [36m0.2379[0m   [32m0.7156[0m        [35m0.2572[0m       0.9323        [94m0.2247[0m     +  27.2777
     17   [36m0.2492[0m   [32m0.7192[0m        [35m0.2553[0m       0.9323        [94m0.2236[0m     +  27.3945
     18   [36m0.2615[0m   [32m0.7227[0m        [35m0.2537[0m       0.9323        [94m0.2225[0m     +  27.1947
     19   [36m0.2703[0m   [32m0.7263[0m        0.2545       0.9323        [94m0.2215[0m     +  27.5026
     20   [36m0.2748[0m   [32m0.7286[0m        [35m0.2517[0m       0.9323        [94m0.2203[0m     +  27.5131
     21   0.2730   [32m0.7288[0m        [35m0.2488[0m       0.9323        [94m0.2196[0m     +  27.4200
     22   [36m0.2812[0m   [32m0.7302[0m        [35m0.2478[0m       [31m0.9347[0m        [94m0.2186[0m     +  27.3978
     23   [36m0.2873[0m   [32m0.7305[0m        [35m0.2453[0m       0.9323        [94m0.2183[0m     +  27.2824
     24   0.2854   0.7295        0.2461       0.9335        [94m0.2182[0m     +  27.5615
     25   0.2867   0.7292        [35m0.2451[0m       0.9323        0.2187        27.4280
     26   [36m0.2877[0m   0.7276        [35m0.2433[0m       0.9323        0.2193        27.7595
     27   0.2803   0.7274        [35m0.2418[0m       0.9335        0.2193        27.4488
     28   0.2804   0.7260        [35m0.2413[0m       0.9335        0.2202        27.9471
     29   0.2819   0.7250        0.2423       0.9335        0.2205        27.4546
     30   0.2800   0.7243        [35m0.2392[0m       0.9335        0.2211        27.8061
     31   0.2806   0.7247        0.2417       0.9347        0.2210        27.6569
     32   0.2760   0.7243        0.2403       0.9347        0.2215        27.3740
     33   0.2754   0.7245        [35m0.2386[0m       0.9347        0.2219        27.5831
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 04:02:31,619][0m Trial 541 finished with value: 0.21819599484067587 and parameters: {'lr': 4.563666372415372e-06, 'dropout': 0.5771450778643983, 'd_model_multiplier': 4, 'num_layers': 6, 'n_heads': 32, 'dim_feedforward': 380, 'batch_size': 58, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.01, 'top_n_features': 77}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 65
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2297[0m   [32m0.6832[0m        [35m0.3271[0m       [31m0.8875[0m        [94m0.4768[0m     +  10.9335
      2   [36m0.2332[0m   0.6765        [35m0.2944[0m       [31m0.8936[0m        [94m0.3839[0m     +  10.5075
      3   [36m0.2444[0m   0.6693        [35m0.2866[0m       [31m0.9105[0m        [94m0.3437[0m     +  10.5215
      4   0.2399   [32m0.6985[0m        [35m0.2832[0m       [31m0.9214[0m        [94m0.3114[0m     +  11.0971
      5   [36m0.2630[0m   [32m0.7104[0m        [35m0.2751[0m       0.9129        [94m0.2977[0m     +  10.8920
      6   0.2443   0.7049        [35m0.2734[0m       0.9178        [94m0.2638[0m     +  10.7125
      7   0.2571   [32m0.7104[0m        [35m0.2725[0m       0.9178        [94m0.2637[0m     +  11.0353
      8   0.2468   0.7084        [35m0.2662[0m       0.9178        [94m0.2546[0m     +  10.9168
      9   0.2386   [32m0.7139[0m        [35m0.2646[0m       0.9178        [94m0.2532[0m     +  10.9649
     10   0.2448   [32m0.7244[0m        [35m0.2632[0m       0.9166        [94m0.2518[0m     +  10.7887
     11   0.2413   [32m0.7314[0m        [35m0.2597[0m       0.9178        [94m0.2498[0m     +  10.6262
     12   0.2531   [32m0.7391[0m        [35m0.2550[0m       0.9178        [94m0.2472[0m     +  10.8058
     13   0.2587   [32m0.7424[0m        0.2577       0.9178        [94m0.2465[0m     +  10.6421
     14   0.2597   0.7293        0.2552       0.9190        0.2478        10.7449
     15   0.2527   0.7300        [35m0.2511[0m       0.9214        0.2471        10.6743
     16   [36m0.2675[0m   [32m0.7441[0m        0.2521       0.9166        0.2475        10.9354
     17   0.2659   0.7306        [35m0.2457[0m       0.9214        [94m0.2459[0m     +  10.8239
     18   0.2654   0.7345        0.2484       0.9214        [94m0.2452[0m     +  11.1715
     19   0.2669   0.7328        [35m0.2373[0m       [31m0.9238[0m        [94m0.2441[0m     +  10.6431
     20   [36m0.2773[0m   [32m0.7569[0m        0.2405       0.9226        [94m0.2409[0m     +  11.0453
     21   0.2703   0.7428        0.2453       0.9238        0.2420        10.6726
     22   0.2648   0.7474        0.2431       [31m0.9250[0m        [94m0.2401[0m     +  10.9284
     23   [36m0.2818[0m   0.7467        0.2410       [31m0.9262[0m        [94m0.2387[0m     +  10.9951
     24   0.2734   0.7424        [35m0.2332[0m       0.9250        0.2416        10.9783
     25   0.2651   0.7482        [35m0.2314[0m       0.9250        0.2438        11.0096
     26   0.2604   0.7513        0.2330       0.9238        0.2422        10.8045
     27   0.2676   0.7493        [35m0.2300[0m       0.9250        0.2422        11.0251
     28   0.2739   0.7486        0.2335       0.9202        0.2410        10.7118
     29   0.2759   0.7557        0.2316       0.9226        [94m0.2384[0m     +  10.8472
     30   0.2705   0.7453        [35m0.2279[0m       0.9226        0.2436        10.7885
     31   0.2657   0.7432        0.2298       0.9190        0.2472        10.7601
     32   0.2671   0.7503        0.2293       0.9214        0.2438        10.6241
     33   0.2616   0.7422        [35m0.2278[0m       0.9166        0.2514        11.0227
     34   0.2741   0.7493        0.2307       0.9214        0.2466        10.8607
     35   [36m0.2833[0m   0.7515        0.2286       0.9129        0.2507        10.8021
     36   [36m0.2861[0m   0.7537        [35m0.2278[0m       0.9214        0.2423        10.7118
     37   0.2770   [32m0.7592[0m        0.2292       0.9202        0.2417        10.9707
     38   0.2826   [32m0.7649[0m        0.2280       0.9226        0.2417        10.7578
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 04:09:37,323][0m Trial 542 finished with value: 0.23841744792865005 and parameters: {'lr': 0.00017994274499048412, 'dropout': 0.6184543778894086, 'd_model_multiplier': 64, 'num_layers': 4, 'n_heads': 4, 'dim_feedforward': 340, 'batch_size': 65, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 65}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 72
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2700[0m   [32m0.7438[0m        [35m0.3237[0m       [31m0.9335[0m        [94m0.2128[0m     +  26.2039
      2   [36m0.2968[0m   [32m0.7712[0m        [35m0.2470[0m       0.9311        [94m0.2077[0m     +  26.4053
      3   0.2938   [32m0.7803[0m        [35m0.2425[0m       0.9311        [94m0.2072[0m     +  26.3378
      4   0.2839   [32m0.7847[0m        [35m0.2379[0m       0.9335        0.2076        26.2247
      5   0.2768   [32m0.7892[0m        [35m0.2361[0m       [31m0.9347[0m        0.2103        26.3818
      6   0.2852   [32m0.7926[0m        [35m0.2321[0m       0.9335        0.2109        26.4339
      7   0.2728   0.7885        [35m0.2290[0m       0.9287        0.2169        26.2781
      8   0.2753   0.7913        [35m0.2257[0m       0.9250        0.2224        26.3764
      9   0.2605   0.7834        [35m0.2230[0m       0.9250        0.2257        26.7978
     10   0.2706   0.7855        [35m0.2203[0m       0.9287        0.2281        26.3226
     11   0.2670   0.7854        [35m0.2183[0m       0.9262        0.2322        26.3716
     12   0.2619   0.7811        [35m0.2155[0m       0.9262        0.2392        26.4196
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 04:15:21,104][0m Trial 543 finished with value: 0.20719900210145883 and parameters: {'lr': 2.9257653626012884e-05, 'dropout': 0.2962349390751788, 'd_model_multiplier': 8, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 327, 'batch_size': 14, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 72}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 58
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2026[0m   [32m0.6889[0m        [35m0.2981[0m       [31m0.9226[0m        [94m0.2500[0m     +  21.7255
      2   [36m0.3184[0m   [32m0.8007[0m        [35m0.2510[0m       [31m0.9250[0m        [94m0.2289[0m     +  22.0232
      3   [36m0.3614[0m   [32m0.8223[0m        [35m0.2406[0m       [31m0.9274[0m        [94m0.2197[0m     +  21.9332
      4   0.3602   [32m0.8264[0m        0.2407       0.9262        [94m0.2197[0m     +  21.8657
      5   0.3612   [32m0.8322[0m        [35m0.2366[0m       0.9262        [94m0.2129[0m     +  21.9806
      6   0.3449   0.7986        [35m0.2362[0m       [31m0.9299[0m        0.2275        21.8216
      7   0.3571   0.8227        [35m0.2341[0m       0.9250        0.2188        21.8434
      8   0.3344   0.8094        [35m0.2325[0m       0.9238        0.2242        22.2697
      9   [36m0.3707[0m   0.8222        0.2331       0.9299        0.2181        21.7882
     10   0.2922   0.8047        [35m0.2316[0m       0.9250        0.2278        22.0593
     11   0.2742   0.7788        0.2337       0.9190        0.2372        22.0406
     12   0.3481   0.8203        [35m0.2305[0m       0.9214        0.2245        22.0171
     13   0.3465   0.8032        0.2327       [31m0.9311[0m        0.2265        21.9763
     14   0.3596   0.8014        0.2319       0.9299        0.2280        21.7830
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 04:20:50,689][0m Trial 544 finished with value: 0.21285660761299272 and parameters: {'lr': 0.0023473655618679533, 'dropout': 0.2729839536659405, 'd_model_multiplier': 1, 'num_layers': 6, 'n_heads': 32, 'dim_feedforward': 432, 'batch_size': 74, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 58}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 48
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2447[0m   [32m0.7485[0m        [35m0.2992[0m       [31m0.9371[0m        [94m0.2108[0m     +  19.8918
      2   [36m0.2546[0m   0.7429        [35m0.2739[0m       0.9371        [94m0.2054[0m     +  19.7672
      3   0.2288   [32m0.7512[0m        [35m0.2678[0m       0.9371        0.2073        19.9570
      4   [36m0.2650[0m   0.7486        [35m0.2632[0m       0.9371        0.2117        19.9242
      5   [36m0.2847[0m   [32m0.7607[0m        0.2638       0.9371        0.2092        19.7948
      6   0.2475   0.7579        [35m0.2615[0m       0.9371        0.2144        19.8759
      7   0.2752   0.7584        0.2624       0.9371        [94m0.2053[0m     +  20.1654
      8   0.2629   0.7562        [35m0.2607[0m       0.9371        0.2106        19.7937
      9   0.2820   0.7583        0.2627       0.9371        0.2079        19.8907
     10   0.2618   0.7560        [35m0.2601[0m       0.9371        [94m0.2033[0m     +  19.8917
     11   0.2721   0.7553        0.2609       0.9371        [94m0.2011[0m     +  20.0133
     12   [36m0.2899[0m   0.7517        0.2609       0.9371        [94m0.1993[0m     +  20.0224
     13   0.2289   0.7524        [35m0.2598[0m       0.9371        0.2008        20.0366
     14   0.2480   0.7522        0.2607       0.9371        0.2011        19.9623
     15   0.2615   0.7558        [35m0.2582[0m       0.9359        0.2033        19.9650
     16   0.2413   0.7492        0.2583       0.9371        0.2030        19.8634
     17   0.2608   0.7588        0.2604       0.9371        0.2008        20.0460
     18   0.2378   0.7489        0.2614       0.9371        0.2085        19.9613
     19   0.2538   0.7567        0.2597       0.9371        0.2170        19.9533
     20   0.2368   0.7553        [35m0.2569[0m       0.9371        0.2105        20.1128
     21   0.2498   0.7541        0.2590       0.9371        0.2165        19.8125
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 04:28:10,238][0m Trial 545 finished with value: 0.19934087172330794 and parameters: {'lr': 0.001380749531328924, 'dropout': 0.50538458884799, 'd_model_multiplier': 4, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 319, 'batch_size': 27, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.001, 'top_n_features': 48}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 46
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1819[0m   [32m0.7114[0m        [35m0.3199[0m       [31m0.9081[0m        [94m0.4105[0m     +  41.5703
      2   [36m0.1853[0m   [32m0.7131[0m        [35m0.2861[0m       0.8863        [94m0.3703[0m     +  41.5288
      3   0.1823   [32m0.7141[0m        [35m0.2706[0m       0.8839        [94m0.3484[0m     +  41.5351
      4   0.1838   [32m0.7184[0m        [35m0.2649[0m       0.8936        [94m0.3240[0m     +  41.6381
      5   0.1822   0.7135        [35m0.2602[0m       0.9081        [94m0.3114[0m     +  41.4571
      6   0.1829   0.7149        0.2612       [31m0.9105[0m        [94m0.3040[0m     +  41.6319
      7   [36m0.1867[0m   0.7159        0.2608       [31m0.9117[0m        [94m0.2774[0m     +  41.5946
      8   0.1829   0.7164        [35m0.2602[0m       0.9105        [94m0.2738[0m     +  41.5361
      9   0.1810   0.7163        [35m0.2600[0m       0.9105        0.2785        41.5616
     10   0.1835   0.7160        0.2611       0.9117        [94m0.2695[0m     +  41.5563
     11   0.1816   0.7172        [35m0.2588[0m       0.9105        [94m0.2667[0m     +  41.7401
     12   0.1816   0.7170        [35m0.2564[0m       0.9093        [94m0.2546[0m     +  41.5444
     13   0.1817   0.7153        [35m0.2552[0m       0.9105        0.2599        41.4908
     14   0.1790   0.7124        0.2559       0.8924        0.2761        41.6149
     15   0.1811   0.7177        0.2558       0.9093        [94m0.2464[0m     +  41.4098
     16   0.1818   [32m0.7196[0m        0.2558       [31m0.9166[0m        [94m0.2389[0m     +  41.5643
     17   0.1815   0.7145        0.2555       0.9105        0.2564        41.4167
     18   0.1856   0.7147        0.2557       0.9117        0.2561        41.5094
     19   0.1851   0.7139        0.2571       0.9081        0.2505        41.6635
     20   0.1818   0.7133        0.2556       0.9129        0.2628        41.6127
     21   0.1823   0.7126        0.2578       0.9093        0.2736        41.4719
     22   0.1811   0.7159        0.2567       0.9093        0.2789        41.5052
     23   0.1841   0.7140        [35m0.2541[0m       [31m0.9178[0m        0.2632        41.6413
     24   0.1853   0.7179        0.2572       0.9117        0.2633        41.6209
     25   [36m0.1876[0m   0.7161        0.2554       0.9178        0.2613        41.6201
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 04:46:13,073][0m Trial 546 finished with value: 0.23886770462254658 and parameters: {'lr': 0.0013832409674248342, 'dropout': 0.5206581455260998, 'd_model_multiplier': 4, 'num_layers': 5, 'n_heads': 64, 'dim_feedforward': 371, 'batch_size': 29, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.001, 'top_n_features': 46}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 110
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.1456[0m   [32m0.6787[0m        [35m0.2779[0m       [31m0.9250[0m        [94m0.2655[0m     +  8.7949
      2   [36m0.1493[0m   [32m0.6866[0m        [35m0.2623[0m       0.9226        [94m0.2646[0m     +  8.8353
      3   0.1485   [32m0.6911[0m        [35m0.2586[0m       0.9226        [94m0.2640[0m     +  9.5682
      4   [36m0.1908[0m   [32m0.7662[0m        [35m0.2545[0m       0.9250        [94m0.2440[0m     +  9.4435
      5   [36m0.1949[0m   0.7374        [35m0.2460[0m       0.9238        [94m0.2439[0m     +  9.6400
      6   [36m0.2200[0m   0.7414        [35m0.2406[0m       0.9250        [94m0.2437[0m     +  9.5616
      7   0.1914   0.7170        [35m0.2368[0m       0.9250        0.2483        9.4847
      8   0.1485   0.6902        0.2402       0.9250        0.2496        9.4365
      9   [36m0.2481[0m   [32m0.7925[0m        0.2393       0.9226        [94m0.2273[0m     +  9.5734
     10   0.2248   0.7835        0.2379       0.9238        0.2302        9.4205
     11   0.2448   0.7860        0.2377       0.9214        0.2290        9.5292
     12   [36m0.2589[0m   0.7921        [35m0.2349[0m       0.9214        0.2358        9.5704
     13   [36m0.2797[0m   [32m0.7959[0m        0.2359       0.9214        [94m0.2272[0m     +  9.8068
     14   0.1926   0.7568        [35m0.2348[0m       0.9093        0.2545        9.3176
     15   0.2258   0.7546        0.2371       0.9238        0.2371        9.6036
     16   0.2450   0.7535        0.2379       0.9250        0.2528        9.5417
     17   0.2118   0.7637        0.2383       0.9214        0.2367        9.5004
     18   0.1749   0.7255        0.2364       0.8960        0.2570        9.6136
     19   0.2642   0.7692        0.2361       0.9202        0.2396        9.3052
     20   0.2259   0.7092        0.2395       [31m0.9262[0m        0.2533        9.5458
     21   0.1790   0.7301        0.2357       0.8960        0.2715        9.5932
     22   0.2461   0.7606        0.2350       0.9226        0.2417        9.4407
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 04:49:51,346][0m Trial 547 finished with value: 0.22723334033082734 and parameters: {'lr': 0.0018127505436266401, 'dropout': 0.31184575807554343, 'd_model_multiplier': 4, 'num_layers': 3, 'n_heads': 8, 'dim_feedforward': 305, 'batch_size': 33, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.001, 'top_n_features': 110}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 42
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2190[0m   [32m0.7198[0m        [35m0.3095[0m       [31m0.9129[0m        [94m0.2859[0m     +  31.1111
      2   [36m0.2363[0m   [32m0.7214[0m        [35m0.2623[0m       0.9021        0.2868        31.2726
      3   0.2314   0.7194        [35m0.2582[0m       0.9105        [94m0.2813[0m     +  31.5565
      4   0.2348   0.7214        0.2586       0.9105        [94m0.2786[0m     +  31.4938
      5   0.2343   [32m0.7251[0m        [35m0.2573[0m       0.9129        [94m0.2725[0m     +  31.3889
      6   0.2261   0.7152        [35m0.2559[0m       [31m0.9141[0m        0.2737        31.5201
      7   0.2359   0.7249        [35m0.2552[0m       0.9105        [94m0.2696[0m     +  31.4243
      8   [36m0.2366[0m   [32m0.7362[0m        0.2577       0.9081        0.2763        31.3073
      9   0.2355   0.7236        [35m0.2536[0m       0.9105        0.2903        31.4809
     10   [36m0.2436[0m   0.7346        [35m0.2532[0m       0.9105        0.2996        31.5001
     11   0.2361   0.7289        0.2553       0.9141        0.2960        31.4546
     12   0.2377   0.7263        [35m0.2527[0m       0.9105        0.3108        31.3894
     13   0.2296   0.7243        0.2548       0.9129        0.3488        31.2489
     14   0.2260   0.7166        0.2543       0.9141        0.3803        31.4714
     15   [36m0.2624[0m   0.7330        0.2556       0.9129        0.3849        31.5259
     16   0.2288   0.7121        0.2559       0.9129        0.3327        31.6411
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 04:58:46,713][0m Trial 548 finished with value: 0.26959631007340396 and parameters: {'lr': 0.002313858826025731, 'dropout': 0.49397709230802156, 'd_model_multiplier': 4, 'num_layers': 7, 'n_heads': 32, 'dim_feedforward': 334, 'batch_size': 26, 'pos_encoding': 'learnable', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.001, 'top_n_features': 42}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 50
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.0693[0m   [32m0.3355[0m        [35m0.6300[0m       [31m0.9214[0m        [94m0.5669[0m     +  9.0102
      2   0.0633   0.3229        [35m0.5256[0m       0.9214        [94m0.4806[0m     +  9.1797
      3   0.0599   0.3262        [35m0.4541[0m       0.9214        [94m0.4213[0m     +  9.8369
      4   0.0599   [32m0.3432[0m        [35m0.4058[0m       0.9214        [94m0.3802[0m     +  9.3370
      5   0.0613   [32m0.3686[0m        [35m0.3714[0m       0.9214        [94m0.3511[0m     +  9.1079
      6   0.0636   [32m0.3974[0m        [35m0.3452[0m       0.9214        [94m0.3301[0m     +  9.6116
      7   0.0659   [32m0.4232[0m        [35m0.3264[0m       0.9214        [94m0.3147[0m     +  9.3117
      8   0.0689   [32m0.4525[0m        [35m0.3125[0m       0.9214        [94m0.3031[0m     +  9.9281
      9   [36m0.0734[0m   [32m0.4852[0m        [35m0.3023[0m       0.9214        [94m0.2942[0m     +  9.5712
     10   [36m0.0796[0m   [32m0.5209[0m        [35m0.2947[0m       0.9214        [94m0.2870[0m     +  9.3460
     11   [36m0.0873[0m   [32m0.5541[0m        [35m0.2877[0m       0.9214        [94m0.2812[0m     +  9.4408
     12   [36m0.0953[0m   [32m0.5802[0m        [35m0.2818[0m       0.9214        [94m0.2764[0m     +  9.7005
     13   [36m0.1167[0m   [32m0.6059[0m        [35m0.2773[0m       0.9214        [94m0.2722[0m     +  9.2182
     14   [36m0.1303[0m   [32m0.6268[0m        [35m0.2729[0m       0.9214        [94m0.2686[0m     +  9.4286
     15   [36m0.1565[0m   [32m0.6464[0m        [35m0.2708[0m       0.9214        [94m0.2654[0m     +  9.4575
     16   [36m0.1749[0m   [32m0.6619[0m        [35m0.2676[0m       0.9214        [94m0.2627[0m     +  9.4696
     17   [36m0.1849[0m   [32m0.6746[0m        [35m0.2651[0m       0.9214        [94m0.2603[0m     +  9.4505
     18   [36m0.2005[0m   [32m0.6889[0m        [35m0.2625[0m       0.9214        [94m0.2582[0m     +  9.0706
     19   [36m0.2128[0m   [32m0.6992[0m        [35m0.2604[0m       0.9214        [94m0.2564[0m     +  9.5071
     20   [36m0.2216[0m   [32m0.7063[0m        [35m0.2586[0m       0.9214        [94m0.2547[0m     +  9.2295
     21   [36m0.2265[0m   [32m0.7133[0m        [35m0.2563[0m       0.9214        [94m0.2532[0m     +  9.1334
     22   [36m0.2296[0m   [32m0.7187[0m        [35m0.2547[0m       0.9214        [94m0.2519[0m     +  9.3592
     23   [36m0.2339[0m   [32m0.7253[0m        [35m0.2544[0m       0.9214        [94m0.2508[0m     +  9.2884
     24   [36m0.2368[0m   [32m0.7302[0m        [35m0.2533[0m       0.9214        [94m0.2497[0m     +  9.3313
     25   [36m0.2392[0m   [32m0.7346[0m        [35m0.2518[0m       0.9214        [94m0.2488[0m     +  9.3140
     26   [36m0.2409[0m   [32m0.7391[0m        [35m0.2516[0m       0.9214        [94m0.2480[0m     +  9.4205
     27   0.2399   [32m0.7422[0m        [35m0.2511[0m       0.9214        [94m0.2473[0m     +  9.3999
     28   [36m0.2444[0m   [32m0.7452[0m        [35m0.2498[0m       0.9214        [94m0.2467[0m     +  9.3183
     29   [36m0.2467[0m   [32m0.7485[0m        [35m0.2490[0m       0.9214        [94m0.2462[0m     +  9.3948
     30   [36m0.2483[0m   [32m0.7514[0m        [35m0.2484[0m       0.9214        [94m0.2455[0m     +  9.3382
     31   [36m0.2491[0m   [32m0.7545[0m        [35m0.2474[0m       0.9214        [94m0.2451[0m     +  9.4156
     32   [36m0.2514[0m   [32m0.7574[0m        [35m0.2470[0m       0.9214        [94m0.2445[0m     +  9.1957
     33   0.2509   [32m0.7596[0m        [35m0.2467[0m       0.9214        [94m0.2441[0m     +  9.5339
     34   [36m0.2532[0m   [32m0.7616[0m        [35m0.2459[0m       0.9214        [94m0.2437[0m     +  9.3122
     35   0.2529   [32m0.7625[0m        [35m0.2450[0m       0.9214        [94m0.2433[0m     +  9.1585
     36   [36m0.2535[0m   [32m0.7646[0m        [35m0.2448[0m       0.9214        [94m0.2429[0m     +  9.0936
     37   [36m0.2544[0m   [32m0.7664[0m        [35m0.2445[0m       0.9214        [94m0.2426[0m     +  9.2915
     38   [36m0.2559[0m   [32m0.7672[0m        [35m0.2439[0m       0.9214        [94m0.2423[0m     +  9.5046
     39   [36m0.2565[0m   [32m0.7691[0m        [35m0.2432[0m       0.9214        [94m0.2420[0m     +  9.3090
     40   [36m0.2586[0m   [32m0.7709[0m        [35m0.2427[0m       0.9214        [94m0.2418[0m     +  9.2752
     41   [36m0.2587[0m   [32m0.7722[0m        [35m0.2425[0m       0.9214        [94m0.2416[0m     +  9.2075
     42   [36m0.2601[0m   [32m0.7737[0m        0.2428       0.9214        [94m0.2413[0m     +  9.4480
     43   [36m0.2635[0m   [32m0.7749[0m        [35m0.2419[0m       0.9214        [94m0.2412[0m     +  9.4539
     44   [36m0.2646[0m   [32m0.7758[0m        [35m0.2413[0m       0.9214        [94m0.2410[0m     +  9.3269
     45   [36m0.2658[0m   [32m0.7767[0m        [35m0.2410[0m       0.9214        [94m0.2409[0m     +  9.2400
     46   [36m0.2665[0m   [32m0.7773[0m        0.2416       0.9214        [94m0.2407[0m     +  9.0861
     47   0.2649   [32m0.7787[0m        [35m0.2404[0m       0.9214        [94m0.2405[0m     +  9.2217
     48   0.2659   [32m0.7790[0m        0.2404       0.9214        [94m0.2404[0m     +  9.3679
     49   [36m0.2666[0m   [32m0.7801[0m        0.2405       0.9202        [94m0.2402[0m     +  9.4246
     50   [36m0.2676[0m   [32m0.7810[0m        [35m0.2403[0m       0.9202        [94m0.2401[0m     +  9.4228
[32m[I 2023-05-05 05:06:40,645][0m Trial 549 finished with value: 0.2400809368197474 and parameters: {'lr': 5.471493623090236e-07, 'dropout': 0.28306509124872403, 'd_model_multiplier': 4, 'num_layers': 1, 'n_heads': 32, 'dim_feedforward': 413, 'batch_size': 17, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.001, 'top_n_features': 50}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 99
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1586[0m   [32m0.6929[0m        [35m0.3022[0m       [31m0.9383[0m        [94m0.2430[0m     +  23.8099
      2   [36m0.1626[0m   [32m0.6956[0m        [35m0.2742[0m       0.9383        0.2616        23.9564
      3   [36m0.1742[0m   0.6898        [35m0.2681[0m       0.9383        0.2636        24.0231
      4   0.1705   [32m0.6995[0m        [35m0.2677[0m       0.9383        0.2628        23.9627
      5   [36m0.1931[0m   [32m0.7048[0m        [35m0.2613[0m       [31m0.9395[0m        0.2650        24.0248
      6   0.1784   0.6967        0.2634       0.9383        0.2568        24.0313
      7   0.1795   0.6819        [35m0.2600[0m       0.9383        0.2611        24.0476
      8   0.1801   0.6886        0.2618       0.9383        0.2490        24.0874
      9   0.1846   0.6887        [35m0.2596[0m       0.9383        0.2437        23.9476
     10   0.1860   0.7000        [35m0.2577[0m       0.9383        0.2467        24.1552
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 05:11:05,182][0m Trial 550 finished with value: 0.24296800335902977 and parameters: {'lr': 0.0012127622649995948, 'dropout': 0.5350265700489737, 'd_model_multiplier': 4, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 353, 'batch_size': 22, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.001, 'top_n_features': 99}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 67
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0942[0m   [32m0.4856[0m        [35m0.6483[0m       [31m0.9190[0m        [94m0.6211[0m     +  12.3910
      2   0.0673   0.3862        [35m0.5979[0m       [31m0.9226[0m        [94m0.5684[0m     +  12.4152
      3   0.0607   0.3670        [35m0.5548[0m       0.9226        [94m0.5198[0m     +  12.4578
      4   0.0588   0.3585        [35m0.5175[0m       0.9226        [94m0.4767[0m     +  12.3006
      5   0.0589   0.3570        [35m0.4839[0m       0.9226        [94m0.4408[0m     +  12.0891
      6   0.0589   0.3548        [35m0.4562[0m       0.9226        [94m0.4114[0m     +  11.9559
      7   0.0592   0.3566        [35m0.4330[0m       0.9226        [94m0.3885[0m     +  11.9712
      8   0.0594   0.3589        [35m0.4100[0m       0.9226        [94m0.3704[0m     +  11.9533
      9   0.0598   0.3621        [35m0.3935[0m       0.9226        [94m0.3559[0m     +  12.0115
     10   0.0603   0.3658        [35m0.3778[0m       0.9226        [94m0.3440[0m     +  12.1320
     11   0.0608   0.3725        [35m0.3663[0m       0.9226        [94m0.3340[0m     +  11.8407
     12   0.0614   0.3856        [35m0.3539[0m       0.9226        [94m0.3253[0m     +  11.9638
     13   0.0630   0.4049        [35m0.3452[0m       0.9226        [94m0.3178[0m     +  11.8699
     14   0.0648   0.4227        [35m0.3354[0m       0.9226        [94m0.3112[0m     +  12.1293
     15   0.0668   0.4403        [35m0.3277[0m       0.9226        [94m0.3055[0m     +  12.0390
     16   0.0692   0.4567        [35m0.3218[0m       0.9226        [94m0.3003[0m     +  11.7889
     17   0.0723   0.4732        [35m0.3167[0m       0.9226        [94m0.2959[0m     +  12.0487
     18   0.0752   [32m0.4870[0m        [35m0.3118[0m       0.9226        [94m0.2920[0m     +  12.0643
     19   0.0791   [32m0.4989[0m        [35m0.3074[0m       0.9226        [94m0.2886[0m     +  12.0766
     20   0.0837   [32m0.5106[0m        [35m0.3025[0m       0.9226        [94m0.2855[0m     +  11.9764
     21   [36m0.1049[0m   [32m0.5242[0m        [35m0.2976[0m       0.9226        [94m0.2828[0m     +  12.0517
     22   [36m0.1096[0m   [32m0.5324[0m        [35m0.2967[0m       0.9226        [94m0.2804[0m     +  11.9689
     23   [36m0.1282[0m   [32m0.5412[0m        [35m0.2919[0m       0.9226        [94m0.2783[0m     +  12.2680
     24   [36m0.1519[0m   [32m0.5483[0m        [35m0.2890[0m       0.9226        [94m0.2764[0m     +  11.8511
     25   [36m0.1524[0m   [32m0.5509[0m        [35m0.2865[0m       0.9226        [94m0.2748[0m     +  11.9848
     26   [36m0.1693[0m   [32m0.5582[0m        [35m0.2852[0m       0.9226        [94m0.2733[0m     +  11.9859
     27   [36m0.1713[0m   [32m0.5618[0m        [35m0.2814[0m       0.9226        [94m0.2720[0m     +  11.7681
     28   [36m0.1756[0m   [32m0.5644[0m        [35m0.2806[0m       0.9226        [94m0.2708[0m     +  11.9765
     29   [36m0.1790[0m   [32m0.5670[0m        [35m0.2785[0m       0.9226        [94m0.2698[0m     +  12.0936
     30   [36m0.1829[0m   [32m0.5705[0m        [35m0.2758[0m       0.9226        [94m0.2689[0m     +  12.3998
     31   [36m0.1846[0m   [32m0.5731[0m        0.2760       0.9226        [94m0.2681[0m     +  11.9645
     32   [36m0.1899[0m   [32m0.5738[0m        [35m0.2755[0m       0.9226        [94m0.2674[0m     +  12.0374
     33   [36m0.1927[0m   [32m0.5769[0m        [35m0.2712[0m       0.9226        [94m0.2668[0m     +  11.8582
     34   [36m0.1940[0m   [32m0.5787[0m        [35m0.2708[0m       0.9226        [94m0.2662[0m     +  12.0588
     35   [36m0.1950[0m   [32m0.5799[0m        0.2716       0.9226        [94m0.2657[0m     +  12.3002
     36   [36m0.1956[0m   [32m0.5816[0m        [35m0.2696[0m       0.9226        [94m0.2652[0m     +  12.5267
     37   [36m0.1956[0m   [32m0.5837[0m        0.2696       0.9226        [94m0.2648[0m     +  12.2069
     38   0.1954   [32m0.5859[0m        [35m0.2668[0m       0.9226        [94m0.2643[0m     +  11.9372
     39   [36m0.1968[0m   [32m0.5895[0m        0.2672       0.9226        [94m0.2638[0m     +  12.0384
     40   [36m0.2031[0m   [32m0.5921[0m        [35m0.2661[0m       0.9226        [94m0.2635[0m     +  12.0295
     41   0.2004   [32m0.5946[0m        [35m0.2622[0m       0.9226        [94m0.2631[0m     +  12.0086
     42   0.2018   [32m0.5981[0m        0.2633       0.9226        [94m0.2627[0m     +  12.0968
     43   [36m0.2038[0m   [32m0.6019[0m        0.2629       0.9226        [94m0.2622[0m     +  11.8930
     44   [36m0.2051[0m   [32m0.6050[0m        [35m0.2612[0m       0.9226        [94m0.2617[0m     +  11.8568
     45   [36m0.2082[0m   [32m0.6098[0m        0.2642       0.9226        [94m0.2611[0m     +  11.8266
     46   0.2014   [32m0.6131[0m        0.2622       [31m0.9238[0m        [94m0.2608[0m     +  12.1080
     47   0.2035   [32m0.6178[0m        0.2620       0.9238        [94m0.2603[0m     +  11.7260
     48   0.2043   [32m0.6222[0m        [35m0.2592[0m       0.9238        [94m0.2596[0m     +  12.3067
     49   0.2061   [32m0.6276[0m        [35m0.2577[0m       0.9238        [94m0.2590[0m     +  11.9793
     50   0.2071   [32m0.6327[0m        0.2588       0.9238        [94m0.2584[0m     +  11.8479
[32m[I 2023-05-05 05:21:13,334][0m Trial 551 finished with value: 0.25837419212693313 and parameters: {'lr': 2.844544289299414e-06, 'dropout': 0.548651115937647, 'd_model_multiplier': 4, 'num_layers': 8, 'n_heads': 4, 'dim_feedforward': 321, 'batch_size': 27, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.001, 'top_n_features': 67}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 47
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1093[0m   [32m0.4632[0m        [35m0.5933[0m       [31m0.9093[0m        [94m0.4639[0m     +  26.9483
      2   0.1084   [32m0.4660[0m        [35m0.4307[0m       0.9093        [94m0.3647[0m     +  26.8964
      3   [36m0.1161[0m   [32m0.5205[0m        [35m0.3541[0m       0.9093        [94m0.3230[0m     +  27.1908
      4   [36m0.1268[0m   [32m0.5888[0m        [35m0.3129[0m       0.9093        [94m0.3046[0m     +  27.0948
      5   [36m0.1479[0m   [32m0.6439[0m        [35m0.2900[0m       0.9093        [94m0.2948[0m     +  27.2626
      6   [36m0.1692[0m   [32m0.6830[0m        [35m0.2761[0m       0.9093        [94m0.2891[0m     +  27.0949
      7   [36m0.1919[0m   [32m0.7081[0m        [35m0.2688[0m       0.9093        [94m0.2851[0m     +  27.2719
      8   [36m0.2034[0m   [32m0.7232[0m        [35m0.2632[0m       0.9093        [94m0.2822[0m     +  27.2546
      9   [36m0.2187[0m   [32m0.7354[0m        [35m0.2572[0m       0.9093        [94m0.2802[0m     +  27.0129
     10   [36m0.2418[0m   [32m0.7477[0m        [35m0.2518[0m       0.9093        [94m0.2776[0m     +  26.9512
     11   [36m0.2554[0m   [32m0.7574[0m        [35m0.2489[0m       0.9093        [94m0.2748[0m     +  27.0641
     12   [36m0.2655[0m   [32m0.7661[0m        [35m0.2462[0m       0.9093        [94m0.2723[0m     +  27.0877
     13   [36m0.2755[0m   [32m0.7733[0m        [35m0.2437[0m       [31m0.9105[0m        [94m0.2701[0m     +  27.1481
     14   [36m0.2946[0m   [32m0.7808[0m        [35m0.2424[0m       0.9105        [94m0.2680[0m     +  26.9954
     15   [36m0.3056[0m   [32m0.7883[0m        [35m0.2411[0m       0.9105        [94m0.2659[0m     +  27.0260
     16   [36m0.3111[0m   [32m0.7921[0m        [35m0.2391[0m       0.9093        [94m0.2641[0m     +  27.0317
     17   [36m0.3156[0m   [32m0.7955[0m        [35m0.2384[0m       [31m0.9117[0m        [94m0.2629[0m     +  27.1703
     18   [36m0.3190[0m   [32m0.7980[0m        [35m0.2373[0m       [31m0.9129[0m        [94m0.2615[0m     +  27.1558
     19   [36m0.3228[0m   [32m0.8009[0m        [35m0.2351[0m       0.9105        [94m0.2605[0m     +  27.2630
     20   [36m0.3295[0m   [32m0.8029[0m        [35m0.2349[0m       0.9105        [94m0.2600[0m     +  26.9555
     21   [36m0.3324[0m   [32m0.8048[0m        [35m0.2335[0m       0.9093        [94m0.2594[0m     +  27.0591
     22   [36m0.3354[0m   [32m0.8054[0m        0.2337       0.9081        [94m0.2590[0m     +  27.1235
     23   [36m0.3399[0m   [32m0.8064[0m        0.2336       0.9081        [94m0.2590[0m     +  27.1703
     24   0.3380   [32m0.8067[0m        [35m0.2332[0m       0.9081        [94m0.2582[0m     +  27.0671
     25   [36m0.3402[0m   [32m0.8071[0m        0.2337       0.9081        [94m0.2580[0m     +  27.0884
     26   [36m0.3416[0m   0.8070        [35m0.2316[0m       0.9081        [94m0.2577[0m     +  27.1690
     27   [36m0.3418[0m   [32m0.8074[0m        0.2330       0.9081        0.2578        27.1227
     28   [36m0.3418[0m   [32m0.8075[0m        [35m0.2302[0m       0.9081        [94m0.2574[0m     +  27.1456
     29   [36m0.3423[0m   0.8073        0.2305       0.9081        0.2574        27.3740
     30   [36m0.3427[0m   0.8071        0.2315       0.9081        [94m0.2573[0m     +  27.5132
     31   0.3425   0.8069        [35m0.2302[0m       0.9081        [94m0.2572[0m     +  27.0049
     32   [36m0.3477[0m   0.8069        [35m0.2300[0m       0.9081        [94m0.2569[0m     +  27.2186
     33   0.3461   0.8069        0.2309       0.9081        0.2570        27.1987
     34   0.3462   0.8071        0.2302       0.9081        [94m0.2569[0m     +  27.1095
     35   0.3464   0.8070        0.2318       0.9081        0.2569        27.1688
     36   0.3412   0.8067        [35m0.2291[0m       0.9069        [94m0.2569[0m     +  27.1748
     37   0.3410   0.8065        0.2296       0.9069        [94m0.2568[0m     +  27.2176
     38   0.3416   0.8062        [35m0.2289[0m       0.9069        [94m0.2566[0m     +  27.2269
     39   0.3446   0.8065        [35m0.2280[0m       0.9069        0.2567        27.0769
     40   0.3453   0.8060        0.2302       0.9069        [94m0.2563[0m     +  27.2808
     41   0.3457   0.8063        0.2300       0.9081        [94m0.2563[0m     +  27.1261
     42   0.3466   0.8060        0.2284       0.9081        [94m0.2562[0m     +  27.0719
     43   0.3465   0.8060        0.2296       0.9081        [94m0.2562[0m     +  27.1175
     44   0.3466   0.8058        0.2293       0.9081        0.2563        27.1915
     45   0.3453   0.8060        0.2292       0.9081        0.2563        27.1431
     46   0.3451   0.8060        [35m0.2279[0m       0.9081        0.2564        27.0299
     47   0.3456   0.8065        [35m0.2274[0m       0.9093        0.2563        27.1760
     48   0.3447   0.8063        0.2297       0.9093        [94m0.2562[0m     +  27.0622
     49   0.3441   0.8057        0.2292       0.9093        [94m0.2562[0m     +  27.1264
     50   0.3418   0.8054        0.2302       0.9093        0.2563        27.1899
[32m[I 2023-05-05 05:43:55,071][0m Trial 552 finished with value: 0.2561874202212731 and parameters: {'lr': 2.060242550008125e-06, 'dropout': 0.5105229319987684, 'd_model_multiplier': 4, 'num_layers': 6, 'n_heads': 32, 'dim_feedforward': 238, 'batch_size': 39, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.001, 'top_n_features': 47}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 63
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0698[0m   [32m0.4957[0m        [35m0.4093[0m       [31m0.9287[0m        [94m0.2705[0m     +  19.9246
      2   [36m0.1694[0m   [32m0.6649[0m        [35m0.2759[0m       0.9287        [94m0.2422[0m     +  19.8049
      3   [36m0.2135[0m   [32m0.7038[0m        [35m0.2599[0m       0.9287        [94m0.2343[0m     +  20.1693
      4   [36m0.2361[0m   [32m0.7245[0m        [35m0.2525[0m       0.9287        [94m0.2313[0m     +  19.8620
      5   [36m0.2617[0m   [32m0.7396[0m        [35m0.2491[0m       [31m0.9299[0m        [94m0.2291[0m     +  20.0813
      6   [36m0.2715[0m   [32m0.7487[0m        [35m0.2464[0m       0.9299        [94m0.2282[0m     +  19.9934
      7   [36m0.2720[0m   [32m0.7532[0m        [35m0.2422[0m       [31m0.9311[0m        0.2294        19.9941
      8   [36m0.2796[0m   0.7477        [35m0.2398[0m       0.9311        0.2324        19.9973
      9   0.2773   0.7456        0.2398       0.9299        0.2339        20.0291
     10   0.2735   0.7456        [35m0.2372[0m       0.9299        0.2352        20.1378
     11   0.2761   0.7489        [35m0.2371[0m       0.9299        0.2351        20.0220
     12   0.2788   0.7504        [35m0.2347[0m       0.9299        0.2349        20.0137
     13   [36m0.2803[0m   0.7500        [35m0.2345[0m       0.9287        0.2363        20.0139
     14   [36m0.2824[0m   [32m0.7543[0m        0.2360       0.9287        0.2348        19.9476
     15   [36m0.2851[0m   [32m0.7554[0m        [35m0.2341[0m       0.9299        0.2346        20.0312
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 05:49:15,769][0m Trial 553 finished with value: 0.22818843064494115 and parameters: {'lr': 1.1046312984172524e-05, 'dropout': 0.5639442818469321, 'd_model_multiplier': 4, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 343, 'batch_size': 34, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.001, 'top_n_features': 63}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 132
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1222[0m   [32m0.5998[0m        [35m0.2937[0m       [31m0.9323[0m        [94m0.2491[0m     +  39.8937
      2   [36m0.1240[0m   [32m0.6187[0m        [35m0.2411[0m       0.9323        0.2539        40.0315
      3   [36m0.1392[0m   [32m0.6356[0m        [35m0.2337[0m       0.9262        0.2570        40.3565
      4   [36m0.1429[0m   [32m0.6432[0m        [35m0.2306[0m       0.9238        0.2596        40.3529
      5   0.1418   [32m0.6523[0m        [35m0.2270[0m       0.9226        0.2613        40.2908
      6   [36m0.1445[0m   [32m0.6556[0m        [35m0.2241[0m       0.9250        0.2622        40.0996
      7   [36m0.1492[0m   [32m0.6635[0m        [35m0.2237[0m       0.9226        0.2614        40.1843
      8   0.1439   [32m0.6672[0m        [35m0.2204[0m       0.9178        0.2644        40.1135
      9   0.1439   0.6649        [35m0.2190[0m       0.9238        0.2648        40.0236
     10   0.1444   0.6630        [35m0.2163[0m       0.9202        0.2653        40.0472
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 05:56:37,620][0m Trial 554 finished with value: 0.2490809207293402 and parameters: {'lr': 2.428352771875113e-05, 'dropout': 0.2152879432534969, 'd_model_multiplier': 2, 'num_layers': 5, 'n_heads': 64, 'dim_feedforward': 448, 'batch_size': 30, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.001, 'top_n_features': 132}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 83
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2131[0m   [32m0.7273[0m        [35m0.6959[0m       [31m0.5550[0m        [94m0.6901[0m     +  27.2234
      2   0.2105   0.7236        [35m0.6923[0m       [31m0.5792[0m        [94m0.6866[0m     +  27.5417
      3   0.2070   0.7198        [35m0.6890[0m       [31m0.6046[0m        [94m0.6831[0m     +  27.4361
      4   0.2025   0.7147        [35m0.6851[0m       [31m0.6191[0m        [94m0.6796[0m     +  27.3012
      5   0.1981   0.7098        [35m0.6808[0m       [31m0.6336[0m        [94m0.6762[0m     +  27.2725
      6   0.1946   0.7052        [35m0.6795[0m       [31m0.6445[0m        [94m0.6728[0m     +  27.4717
      7   0.1896   0.7001        [35m0.6748[0m       [31m0.6578[0m        [94m0.6694[0m     +  27.1071
      8   0.1849   0.6946        [35m0.6731[0m       [31m0.6747[0m        [94m0.6660[0m     +  27.2808
      9   0.1802   0.6891        [35m0.6686[0m       [31m0.6929[0m        [94m0.6627[0m     +  27.5021
     10   0.1750   0.6829        [35m0.6651[0m       [31m0.7050[0m        [94m0.6594[0m     +  27.2256
     11   0.1692   0.6754        [35m0.6648[0m       [31m0.7207[0m        [94m0.6561[0m     +  27.6131
     12   0.1633   0.6682        [35m0.6600[0m       [31m0.7328[0m        [94m0.6528[0m     +  27.5155
     13   0.1579   0.6611        [35m0.6575[0m       [31m0.7533[0m        [94m0.6496[0m     +  27.5850
     14   0.1535   0.6551        [35m0.6551[0m       [31m0.7582[0m        [94m0.6463[0m     +  27.2923
     15   0.1492   0.6488        [35m0.6531[0m       [31m0.7703[0m        [94m0.6431[0m     +  27.4008
     16   0.1437   0.6420        [35m0.6500[0m       [31m0.7763[0m        [94m0.6400[0m     +  27.7261
     17   0.1358   0.6348        [35m0.6469[0m       [31m0.7811[0m        [94m0.6368[0m     +  27.4023
     18   0.1307   0.6289        [35m0.6428[0m       [31m0.8017[0m        [94m0.6337[0m     +  27.4320
     19   0.1249   0.6218        [35m0.6389[0m       [31m0.8102[0m        [94m0.6306[0m     +  27.2107
     20   0.1197   0.6149        [35m0.6373[0m       [31m0.8235[0m        [94m0.6275[0m     +  27.3183
     21   0.1158   0.6084        [35m0.6348[0m       [31m0.8331[0m        [94m0.6244[0m     +  27.2902
     22   0.1116   0.6017        [35m0.6321[0m       [31m0.8476[0m        [94m0.6214[0m     +  27.3260
     23   0.1082   0.5941        [35m0.6289[0m       [31m0.8513[0m        [94m0.6184[0m     +  27.4654
     24   0.1057   0.5872        [35m0.6261[0m       [31m0.8609[0m        [94m0.6154[0m     +  27.1733
     25   0.1034   0.5802        [35m0.6239[0m       [31m0.8682[0m        [94m0.6124[0m     +  27.3807
     26   0.1013   0.5735        [35m0.6214[0m       [31m0.8742[0m        [94m0.6095[0m     +  27.3856
     27   0.0998   0.5661        [35m0.6192[0m       [31m0.8815[0m        [94m0.6066[0m     +  27.2805
     28   0.0986   0.5598        [35m0.6166[0m       [31m0.8851[0m        [94m0.6037[0m     +  27.4643
     29   0.0972   0.5521        [35m0.6138[0m       [31m0.8924[0m        [94m0.6008[0m     +  27.5385
     30   0.0965   0.5458        [35m0.6103[0m       [31m0.8948[0m        [94m0.5980[0m     +  27.4546
     31   0.0954   0.5398        [35m0.6088[0m       [31m0.8972[0m        [94m0.5951[0m     +  27.6159
     32   0.0954   0.5341        [35m0.6055[0m       [31m0.9021[0m        [94m0.5923[0m     +  27.4197
     33   0.1021   0.5286        [35m0.6030[0m       [31m0.9045[0m        [94m0.5896[0m     +  27.4050
     34   0.1014   0.5237        [35m0.6016[0m       [31m0.9093[0m        [94m0.5868[0m     +  27.4950
     35   0.1007   0.5182        [35m0.5995[0m       [31m0.9141[0m        [94m0.5841[0m     +  27.6342
     36   0.0923   0.5130        [35m0.5975[0m       [31m0.9166[0m        [94m0.5814[0m     +  27.2551
     37   0.0870   0.5075        [35m0.5931[0m       [31m0.9178[0m        [94m0.5787[0m     +  27.4788
     38   0.0856   0.5023        [35m0.5894[0m       [31m0.9190[0m        [94m0.5760[0m     +  27.4525
     39   0.0838   0.4972        0.5899       [31m0.9202[0m        [94m0.5734[0m     +  27.4679
     40   0.0844   0.4923        [35m0.5861[0m       [31m0.9214[0m        [94m0.5708[0m     +  27.4753
     41   0.0834   0.4872        [35m0.5854[0m       0.9214        [94m0.5682[0m     +  27.5097
     42   0.0853   0.4822        [35m0.5805[0m       0.9214        [94m0.5656[0m     +  27.2380
     43   0.0843   0.4776        [35m0.5793[0m       0.9214        [94m0.5630[0m     +  27.5089
     44   0.0833   0.4724        [35m0.5775[0m       0.9214        [94m0.5605[0m     +  27.2457
     45   0.0904   0.4686        [35m0.5737[0m       0.9214        [94m0.5580[0m     +  27.5993
     46   0.0897   0.4643        [35m0.5721[0m       0.9214        [94m0.5555[0m     +  27.5073
     47   0.0894   0.4606        [35m0.5704[0m       0.9214        [94m0.5530[0m     +  27.2777
     48   0.0888   0.4561        [35m0.5681[0m       0.9214        [94m0.5506[0m     +  27.2119
     49   0.0882   0.4518        [35m0.5660[0m       0.9214        [94m0.5481[0m     +  27.3869
     50   0.0878   0.4482        [35m0.5635[0m       0.9214        [94m0.5457[0m     +  27.4615
[32m[I 2023-05-05 06:19:34,279][0m Trial 555 finished with value: 0.5457392741431636 and parameters: {'lr': 2.2331047463691933e-08, 'dropout': 0.47556423566833156, 'd_model_multiplier': 4, 'num_layers': 6, 'n_heads': 32, 'dim_feedforward': 329, 'batch_size': 45, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.001, 'top_n_features': 83}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 71
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0946[0m   [32m0.5721[0m        [35m0.4632[0m       [31m0.9250[0m        [94m0.2769[0m     +  12.6516
      2   [36m0.2299[0m   [32m0.7245[0m        [35m0.2632[0m       0.9238        [94m0.2383[0m     +  12.8508
      3   [36m0.2426[0m   [32m0.7433[0m        [35m0.2416[0m       0.9214        [94m0.2339[0m     +  12.7385
      4   [36m0.2481[0m   0.7429        [35m0.2348[0m       0.9226        [94m0.2337[0m     +  12.6646
      5   [36m0.2615[0m   [32m0.7460[0m        [35m0.2304[0m       0.9238        0.2337        12.7071
      6   [36m0.2740[0m   [32m0.7515[0m        [35m0.2286[0m       0.9238        [94m0.2334[0m     +  12.8356
      7   0.2703   [32m0.7561[0m        [35m0.2264[0m       [31m0.9262[0m        [94m0.2333[0m     +  12.7654
      8   0.2711   [32m0.7587[0m        [35m0.2249[0m       0.9262        0.2351        12.9453
      9   [36m0.2756[0m   [32m0.7620[0m        [35m0.2231[0m       [31m0.9274[0m        0.2342        13.0197
     10   0.2700   [32m0.7623[0m        [35m0.2208[0m       0.9274        0.2349        12.6047
     11   0.2678   [32m0.7641[0m        [35m0.2182[0m       0.9250        0.2356        12.6919
     12   0.2617   0.7635        [35m0.2177[0m       0.9250        0.2371        12.8377
     13   0.2610   [32m0.7651[0m        [35m0.2145[0m       0.9250        0.2385        12.7655
     14   0.2488   0.7637        [35m0.2124[0m       0.9238        0.2397        12.8358
     15   0.2522   [32m0.7655[0m        [35m0.2118[0m       0.9250        0.2388        12.8233
     16   0.2498   [32m0.7655[0m        [35m0.2100[0m       0.9214        0.2401        12.8304
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 06:23:12,463][0m Trial 556 finished with value: 0.23327785980816765 and parameters: {'lr': 7.038242655052855e-05, 'dropout': 0.24175942501453535, 'd_model_multiplier': 4, 'num_layers': 4, 'n_heads': 16, 'dim_feedforward': 230, 'batch_size': 38, 'pos_encoding': 'learnable', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.001, 'top_n_features': 71}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 54
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2116[0m   [32m0.7445[0m        [35m0.9289[0m       [31m0.9166[0m        [94m0.2392[0m     +  47.3694
      2   [36m0.2355[0m   0.7334        [35m0.2781[0m       0.8815        0.2879        47.5619
      3   0.2258   [32m0.7492[0m        [35m0.2684[0m       [31m0.9262[0m        0.2549        47.6596
      4   0.2188   0.7434        [35m0.2637[0m       0.9262        0.2916        47.5166
      5   0.2068   0.7444        [35m0.2600[0m       [31m0.9274[0m        0.3032        47.5566
      6   0.2241   0.7439        0.2617       0.9008        0.2953        47.5940
      7   0.2137   [32m0.7506[0m        [35m0.2583[0m       [31m0.9287[0m        0.2833        47.6288
      8   0.1953   0.7489        [35m0.2577[0m       0.9287        0.2797        47.5827
      9   0.1897   0.7395        [35m0.2562[0m       0.9274        0.3476        47.5376
     10   0.2039   0.7498        0.2566       0.9287        0.2823        47.2336
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 06:31:56,522][0m Trial 557 finished with value: 0.23923437155900296 and parameters: {'lr': 0.0031431571346132014, 'dropout': 0.25503139699736355, 'd_model_multiplier': 32, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 362, 'batch_size': 22, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.001, 'top_n_features': 54}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 75
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1272[0m   [32m0.5665[0m        [35m0.4811[0m       [31m0.9141[0m        [94m0.3017[0m     +  16.6967
      2   [36m0.2605[0m   [32m0.7491[0m        [35m0.2624[0m       0.9141        [94m0.2615[0m     +  16.2719
      3   [36m0.2828[0m   [32m0.7774[0m        [35m0.2422[0m       [31m0.9154[0m        [94m0.2520[0m     +  16.5574
      4   [36m0.3007[0m   [32m0.7908[0m        [35m0.2365[0m       0.9129        [94m0.2480[0m     +  16.7298
      5   [36m0.3113[0m   [32m0.7953[0m        [35m0.2346[0m       0.9117        [94m0.2463[0m     +  17.2006
      6   [36m0.3135[0m   [32m0.7969[0m        [35m0.2302[0m       0.9129        0.2463        16.8344
      7   [36m0.3281[0m   [32m0.7976[0m        [35m0.2281[0m       0.9117        [94m0.2460[0m     +  16.7658
      8   [36m0.3342[0m   [32m0.7995[0m        [35m0.2268[0m       0.9117        [94m0.2459[0m     +  16.5749
      9   0.3323   0.7973        [35m0.2250[0m       0.9093        0.2481        16.2995
     10   [36m0.3372[0m   [32m0.8002[0m        [35m0.2234[0m       0.9105        0.2486        16.5455
     11   0.3353   [32m0.8003[0m        [35m0.2214[0m       0.9105        0.2483        16.8519
     12   [36m0.3396[0m   0.8002        [35m0.2196[0m       0.9105        0.2480        16.4634
     13   0.3210   [32m0.8010[0m        [35m0.2178[0m       0.9093        0.2500        16.5466
     14   0.3278   [32m0.8010[0m        [35m0.2172[0m       0.9081        0.2504        16.5268
     15   0.3329   [32m0.8020[0m        [35m0.2149[0m       0.9093        0.2524        16.9506
     16   0.3381   [32m0.8042[0m        [35m0.2125[0m       0.9081        0.2516        16.6116
     17   0.3331   0.8024        [35m0.2117[0m       0.9093        0.2539        16.6760
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 06:36:57,129][0m Trial 558 finished with value: 0.24586119319896743 and parameters: {'lr': 5.2235169871576485e-05, 'dropout': 0.29174520419375716, 'd_model_multiplier': 4, 'num_layers': 3, 'n_heads': 32, 'dim_feedforward': 420, 'batch_size': 51, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0, 'top_n_features': 75}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 59
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1575[0m   [32m0.6864[0m        [35m0.2916[0m       [31m0.9117[0m        [94m0.3124[0m     +  19.7033
      2   [36m0.1593[0m   0.6807        [35m0.2674[0m       [31m0.9129[0m        [94m0.3122[0m     +  20.0967
      3   0.1564   [32m0.6877[0m        [35m0.2639[0m       0.9008        [94m0.3024[0m     +  19.9338
      4   [36m0.1646[0m   0.6845        [35m0.2593[0m       [31m0.9154[0m        [94m0.2901[0m     +  19.9474
      5   0.1592   0.6866        [35m0.2586[0m       0.9105        [94m0.2875[0m     +  20.2143
      6   0.1567   0.6844        [35m0.2569[0m       0.8960        0.3009        20.2252
      7   0.1540   [32m0.6888[0m        [35m0.2558[0m       0.9154        0.3016        20.2418
      8   0.1604   0.6802        [35m0.2555[0m       0.9154        0.3264        20.4492
      9   [36m0.1904[0m   0.6874        [35m0.2550[0m       0.9154        0.2985        20.5270
     10   0.1528   0.6538        [35m0.2515[0m       0.9154        0.3083        20.1733
     11   0.1587   0.6544        0.2517       0.9154        0.3132        20.3832
     12   0.1602   0.6529        [35m0.2507[0m       0.9154        0.2963        20.0911
     13   0.1602   0.6782        [35m0.2494[0m       0.9154        [94m0.2805[0m     +  20.7298
     14   0.1673   0.6700        0.2502       0.9154        [94m0.2769[0m     +  20.1406
     15   0.1647   0.6788        0.2505       0.9154        0.2782        20.2025
     16   0.1604   0.6636        [35m0.2480[0m       0.9154        0.2819        20.2870
     17   0.1620   0.6731        0.2516       0.9154        [94m0.2768[0m     +  20.2895
     18   0.1611   0.6770        0.2495       0.9154        0.2789        20.1029
     19   0.1619   0.6808        [35m0.2477[0m       0.9154        0.2788        20.1730
     20   0.1607   0.6869        0.2506       0.9154        [94m0.2765[0m     +  20.1010
     21   0.1631   0.6792        0.2480       0.9154        0.2786        19.9755
     22   0.1631   0.6859        0.2480       0.9129        0.2813        20.1472
     23   0.1582   0.6819        [35m0.2473[0m       0.9154        0.2793        20.2112
     24   0.1649   0.6836        [35m0.2468[0m       0.9154        0.2766        20.0859
     25   0.1624   0.6863        [35m0.2454[0m       0.9154        0.2773        20.2366
     26   0.1612   0.6791        0.2459       0.9154        0.2766        20.2367
     27   0.1628   0.6788        0.2455       0.9154        0.2796        20.1865
     28   0.1634   0.6794        [35m0.2448[0m       0.9154        0.2786        20.1205
     29   0.1628   0.6752        0.2469       0.9154        0.2770        20.2152
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 06:47:04,038][0m Trial 559 finished with value: 0.2764896899461746 and parameters: {'lr': 0.007119560905178371, 'dropout': 0.42581607268041183, 'd_model_multiplier': 4, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 343, 'batch_size': 28, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.01, 'top_n_features': 59}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 66
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1498[0m   [32m0.6899[0m        [35m0.2815[0m       [31m0.9371[0m        [94m0.2443[0m     +  21.9290
      2   [36m0.1514[0m   [32m0.7012[0m        [35m0.2694[0m       0.9335        [94m0.2380[0m     +  21.9681
      3   0.1504   [32m0.7052[0m        [35m0.2641[0m       0.9299        0.2406        21.9909
      4   0.1500   [32m0.7064[0m        [35m0.2612[0m       0.9287        0.2415        22.1381
      5   [36m0.1606[0m   0.7060        [35m0.2603[0m       0.9371        0.2398        22.3493
      6   0.1517   0.7062        0.2603       0.9287        0.2481        22.1620
      7   0.1456   0.7034        [35m0.2599[0m       0.9347        0.2452        21.9664
      8   0.1453   0.7028        [35m0.2590[0m       0.9347        0.2498        22.2891
      9   0.1407   0.7046        0.2628       0.9371        0.2453        22.1021
     10   0.1449   [32m0.7064[0m        0.2594       0.9335        0.2455        22.4082
     11   0.1436   0.7046        [35m0.2589[0m       0.9323        0.2494        22.2229
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 06:51:30,258][0m Trial 560 finished with value: 0.23795399722049718 and parameters: {'lr': 0.0016525437322629914, 'dropout': 0.5116963296205657, 'd_model_multiplier': 1, 'num_layers': 6, 'n_heads': 32, 'dim_feedforward': 337, 'batch_size': 20, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.001, 'top_n_features': 66}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 122
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
Warning, assumed runtime error: CUDA out of memory. Tried to allocate 1.92 GiB (GPU 0; 23.70 GiB total capacity; 20.48 GiB already allocated; 139.25 MiB free; 22.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[32m[I 2023-05-05 06:51:34,940][0m Trial 561 finished with value: 100.0 and parameters: {'lr': 3.398106412382825e-05, 'dropout': 0.5310384506628786, 'd_model_multiplier': 4, 'num_layers': 7, 'n_heads': 64, 'dim_feedforward': 375, 'batch_size': 81, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 122}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 62
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.0720[0m   [32m0.4774[0m        [35m0.4814[0m       [31m0.9226[0m        [94m0.3123[0m     +  9.5417
      2   [36m0.1987[0m   [32m0.7056[0m        [35m0.2794[0m       0.9226        [94m0.2525[0m     +  10.4207
      3   [36m0.2316[0m   [32m0.7499[0m        [35m0.2569[0m       0.9226        [94m0.2398[0m     +  10.4937
      4   [36m0.3106[0m   [32m0.7959[0m        [35m0.2457[0m       0.9202        [94m0.2278[0m     +  10.5559
      5   [36m0.3589[0m   [32m0.8107[0m        [35m0.2391[0m       [31m0.9238[0m        [94m0.2181[0m     +  10.8462
      6   [36m0.3628[0m   [32m0.8141[0m        [35m0.2362[0m       [31m0.9250[0m        [94m0.2175[0m     +  10.8840
      7   [36m0.3771[0m   [32m0.8208[0m        [35m0.2352[0m       [31m0.9262[0m        [94m0.2145[0m     +  11.0204
      8   0.3584   0.8201        [35m0.2319[0m       0.9238        0.2193        10.9873
      9   0.3304   0.8188        [35m0.2312[0m       0.9214        0.2197        11.1598
     10   0.3222   0.8182        [35m0.2271[0m       0.9214        0.2268        11.0158
     11   0.3203   0.8184        [35m0.2261[0m       0.9202        0.2242        10.8943
     12   0.3140   0.8183        [35m0.2261[0m       0.9166        0.2268        11.1105
     13   0.3240   [32m0.8242[0m        [35m0.2232[0m       0.9166        0.2224        11.2378
     14   0.3173   0.8217        0.2245       0.9166        0.2261        10.8715
     15   0.3215   0.8210        [35m0.2225[0m       0.9190        0.2269        11.2839
     16   0.3253   0.8217        0.2232       0.9178        0.2235        10.6125
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 06:54:39,409][0m Trial 562 finished with value: 0.21449680964461332 and parameters: {'lr': 0.0010463309010361654, 'dropout': 0.39750840907681245, 'd_model_multiplier': 4, 'num_layers': 5, 'n_heads': 4, 'dim_feedforward': 323, 'batch_size': 118, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.001, 'top_n_features': 62}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 49
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.2250[0m   [32m0.6219[0m        [35m0.3664[0m       [31m0.9081[0m        [94m0.2952[0m     +  9.3878
      2   [36m0.2721[0m   [32m0.6877[0m        [35m0.2566[0m       [31m0.9093[0m        [94m0.2833[0m     +  8.9986
      3   [36m0.2784[0m   [32m0.7187[0m        [35m0.2424[0m       0.9093        [94m0.2798[0m     +  9.6483
      4   [36m0.2932[0m   [32m0.7376[0m        [35m0.2394[0m       0.9069        [94m0.2770[0m     +  9.9731
      5   [36m0.3029[0m   [32m0.7539[0m        [35m0.2348[0m       0.9093        [94m0.2742[0m     +  9.7145
      6   [36m0.3167[0m   [32m0.7655[0m        [35m0.2323[0m       0.9081        [94m0.2721[0m     +  10.0770
      7   [36m0.3189[0m   [32m0.7733[0m        [35m0.2311[0m       0.9033        [94m0.2709[0m     +  9.7446
      8   [36m0.3237[0m   [32m0.7798[0m        [35m0.2296[0m       0.9033        [94m0.2701[0m     +  9.6923
      9   0.3219   [32m0.7841[0m        0.2302       0.9045        [94m0.2697[0m     +  9.9608
     10   [36m0.3240[0m   [32m0.7884[0m        [35m0.2290[0m       0.9057        [94m0.2693[0m     +  9.6470
     11   [36m0.3241[0m   [32m0.7921[0m        [35m0.2275[0m       0.9069        [94m0.2687[0m     +  10.0428
     12   [36m0.3268[0m   [32m0.7938[0m        [35m0.2263[0m       0.9057        [94m0.2682[0m     +  9.7348
     13   [36m0.3280[0m   [32m0.7970[0m        [35m0.2260[0m       0.9057        [94m0.2679[0m     +  9.7098
     14   0.3257   [32m0.7992[0m        0.2261       0.9057        0.2680        10.0730
     15   0.3251   [32m0.8007[0m        [35m0.2251[0m       0.9057        [94m0.2677[0m     +  9.7083
     16   0.3270   [32m0.8026[0m        0.2263       0.9057        [94m0.2675[0m     +  10.1608
     17   0.3271   [32m0.8040[0m        [35m0.2248[0m       0.9057        [94m0.2671[0m     +  10.5250
     18   0.3268   [32m0.8047[0m        0.2251       0.9057        0.2671        10.0708
     19   [36m0.3299[0m   [32m0.8066[0m        [35m0.2238[0m       0.9033        [94m0.2671[0m     +  9.9015
     20   [36m0.3310[0m   [32m0.8079[0m        [35m0.2229[0m       0.9033        [94m0.2669[0m     +  10.2416
     21   [36m0.3325[0m   [32m0.8092[0m        0.2239       0.9021        [94m0.2669[0m     +  10.1531
     22   0.3255   [32m0.8097[0m        [35m0.2226[0m       0.9021        [94m0.2667[0m     +  9.9543
     23   0.3323   [32m0.8111[0m        0.2233       0.9021        [94m0.2663[0m     +  10.0207
     24   0.3266   [32m0.8124[0m        [35m0.2224[0m       0.9021        [94m0.2661[0m     +  9.8665
     25   0.3279   [32m0.8139[0m        [35m0.2223[0m       0.9008        [94m0.2659[0m     +  9.8670
     26   0.3277   [32m0.8149[0m        [35m0.2222[0m       0.9008        [94m0.2657[0m     +  9.9087
     27   0.3288   [32m0.8158[0m        [35m0.2218[0m       0.9008        0.2658        9.5112
     28   0.3295   [32m0.8169[0m        [35m0.2200[0m       0.9008        0.2658        9.9124
     29   0.3310   [32m0.8183[0m        0.2202       0.8996        [94m0.2653[0m     +  9.9213
     30   0.3317   [32m0.8190[0m        0.2207       0.8996        [94m0.2652[0m     +  9.9812
     31   0.3304   [32m0.8194[0m        0.2202       0.9008        [94m0.2648[0m     +  9.7466
     32   0.3316   [32m0.8200[0m        [35m0.2194[0m       0.9021        [94m0.2643[0m     +  9.6951
     33   [36m0.3334[0m   [32m0.8208[0m        0.2195       0.9008        [94m0.2636[0m     +  9.9681
     34   [36m0.3340[0m   [32m0.8213[0m        0.2206       0.9008        [94m0.2636[0m     +  9.7235
     35   0.3339   [32m0.8215[0m        [35m0.2191[0m       0.9008        [94m0.2633[0m     +  9.7361
     36   [36m0.3351[0m   [32m0.8220[0m        [35m0.2190[0m       0.9008        0.2636        9.7042
     37   [36m0.3355[0m   [32m0.8224[0m        0.2195       0.9008        0.2633        10.3327
     38   0.3328   [32m0.8236[0m        [35m0.2167[0m       0.9021        [94m0.2630[0m     +  10.1435
     39   0.3337   [32m0.8245[0m        0.2182       0.9008        [94m0.2622[0m     +  10.1137
     40   0.3328   [32m0.8259[0m        0.2178       0.8996        [94m0.2619[0m     +  10.0590
     41   0.3332   0.8258        0.2173       0.9008        [94m0.2618[0m     +  9.4763
     42   0.3345   [32m0.8268[0m        [35m0.2165[0m       0.8996        [94m0.2616[0m     +  10.0375
     43   0.3353   [32m0.8274[0m        0.2192       0.9008        [94m0.2611[0m     +  9.8407
     44   [36m0.3374[0m   [32m0.8282[0m        0.2174       0.8996        [94m0.2600[0m     +  9.8746
     45   0.3372   [32m0.8292[0m        [35m0.2164[0m       0.8996        [94m0.2598[0m     +  9.7545
     46   [36m0.3382[0m   [32m0.8296[0m        [35m0.2161[0m       0.8984        [94m0.2595[0m     +  9.8529
     47   [36m0.3395[0m   [32m0.8299[0m        0.2162       0.8984        0.2598        10.2341
     48   [36m0.3404[0m   [32m0.8307[0m        [35m0.2151[0m       0.8996        0.2601        10.4373
     49   0.3400   [32m0.8317[0m        [35m0.2150[0m       0.8996        0.2596        10.3545
     50   0.3398   [32m0.8318[0m        0.2158       0.8984        [94m0.2592[0m     +  10.1069
[32m[I 2023-05-05 07:02:57,174][0m Trial 563 finished with value: 0.2592013486927913 and parameters: {'lr': 1.9878885744145474e-05, 'dropout': 0.4110222054309565, 'd_model_multiplier': 4, 'num_layers': 1, 'n_heads': 32, 'dim_feedforward': 313, 'batch_size': 56, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 49}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 171
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2176[0m   [32m0.7150[0m        [35m0.3015[0m       [31m0.9262[0m        [94m0.2537[0m     +  29.0981
      2   [36m0.2406[0m   [32m0.7357[0m        [35m0.2463[0m       0.9238        [94m0.2499[0m     +  28.9370
      3   [36m0.2590[0m   [32m0.7554[0m        [35m0.2432[0m       0.9214        [94m0.2427[0m     +  28.6495
      4   0.2148   0.7017        [35m0.2406[0m       0.9202        0.2616        29.0641
      5   0.1864   0.6536        [35m0.2398[0m       0.9214        0.2751        28.9575
      6   0.1977   0.6790        [35m0.2370[0m       0.9202        0.2707        29.2067
      7   0.2064   0.6736        [35m0.2349[0m       0.9226        0.2735        28.8345
      8   0.2083   0.6927        [35m0.2326[0m       0.9214        0.2657        28.7636
      9   0.1929   0.6633        [35m0.2282[0m       0.9214        0.2795        28.9029
     10   0.2314   0.6861        [35m0.2246[0m       0.9250        0.2626        28.8891
     11   0.2185   0.6757        [35m0.2189[0m       0.9238        0.2631        28.9232
     12   0.2242   0.7090        [35m0.2179[0m       0.9226        0.2700        28.8654
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 07:09:13,853][0m Trial 564 finished with value: 0.24268487493137061 and parameters: {'lr': 0.00010551052498963596, 'dropout': 0.3307951396942285, 'd_model_multiplier': 4, 'num_layers': 6, 'n_heads': 32, 'dim_feedforward': 352, 'batch_size': 15, 'pos_encoding': 'learnable', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 171}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 41
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
Warning, assumed runtime error: CUDA out of memory. Tried to allocate 1.96 GiB (GPU 0; 23.70 GiB total capacity; 18.37 GiB already allocated; 151.25 MiB free; 22.46 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[32m[I 2023-05-05 07:09:19,864][0m Trial 565 finished with value: 100.0 and parameters: {'lr': 5.1140612156846543e-05, 'dropout': 0.5018573239393297, 'd_model_multiplier': 64, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 331, 'batch_size': 166, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0, 'top_n_features': 41}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 69
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1908[0m   [32m0.7117[0m        [35m0.2708[0m       [31m0.8912[0m        [94m0.2768[0m     +  15.7648
      2   0.1843   [32m0.7189[0m        [35m0.2605[0m       [31m0.9117[0m        [94m0.2702[0m     +  16.1631
      3   0.1862   0.7060        0.2606       [31m0.9129[0m        0.2707        15.8588
      4   0.1845   0.7088        [35m0.2598[0m       [31m0.9166[0m        [94m0.2695[0m     +  16.1832
      5   0.1871   0.7054        [35m0.2575[0m       0.9166        [94m0.2679[0m     +  15.9437
      6   0.1862   0.7037        0.2576       0.9166        [94m0.2677[0m     +  16.2149
      7   0.1882   0.7000        [35m0.2574[0m       0.9166        0.2696        16.0844
      8   0.1830   0.7072        0.2592       0.9166        [94m0.2671[0m     +  16.0907
      9   [36m0.1945[0m   0.7100        [35m0.2573[0m       0.9166        [94m0.2666[0m     +  16.2657
     10   0.1922   0.7075        0.2591       0.9166        0.2684        16.2317
     11   0.1864   0.6994        0.2576       0.9166        0.2678        16.1525
     12   0.1921   0.7029        0.2579       0.9166        0.2692        16.5169
     13   0.1894   0.7073        0.2578       0.9166        0.2677        16.2426
     14   0.1792   0.7091        [35m0.2562[0m       0.9166        0.2681        16.1162
     15   0.1775   0.7068        0.2585       0.9166        0.2694        15.9361
     16   0.1784   0.7047        0.2574       0.9166        0.2681        16.0983
     17   0.1777   0.7052        0.2564       0.9166        0.2692        16.0587
     18   0.1843   0.7071        0.2589       0.9166        0.2677        16.3888
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 07:14:26,963][0m Trial 566 finished with value: 0.2666189911738128 and parameters: {'lr': 0.03259961583373089, 'dropout': 0.2652552889910674, 'd_model_multiplier': 1, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 363, 'batch_size': 45, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 69}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 92
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2177[0m   [32m0.6974[0m        [35m0.2640[0m       [31m0.9226[0m        [94m0.2585[0m     +  17.8192
      2   0.2161   [32m0.7303[0m        [35m0.2515[0m       [31m0.9262[0m        0.2817        17.4113
      3   [36m0.2275[0m   0.7299        [35m0.2508[0m       [31m0.9274[0m        0.2639        17.9922
      4   0.2214   [32m0.7365[0m        [35m0.2474[0m       0.9226        0.2847        18.0478
      5   0.2128   0.7263        [35m0.2458[0m       0.9250        0.2783        18.1451
      6   0.1811   [32m0.7384[0m        0.2504       0.9250        0.2968        17.7724
      7   0.1693   0.6899        0.2518       0.9238        0.2743        18.1321
      8   0.1689   0.6942        0.2578       0.9178        0.2896        17.7012
      9   0.1553   0.6881        0.2545       0.9262        [94m0.2567[0m     +  17.9793
     10   0.1556   0.6875        0.2564       0.9262        [94m0.2492[0m     +  18.0152
     11   0.1645   0.6882        0.2542       0.9262        [94m0.2477[0m     +  17.8711
     12   0.1603   0.6942        0.2535       0.9262        [94m0.2469[0m     +  18.1789
     13   0.1618   0.6874        0.2551       0.9214        0.2496        17.9652
     14   0.1665   0.6934        0.2549       0.9141        0.2611        17.7487
     15   0.1603   0.6905        0.2535       0.9190        0.2628        18.0850
     16   0.1695   0.6814        0.2562       0.9190        0.2554        18.1322
     17   0.1654   0.6916        0.2509       0.9178        0.2689        17.9087
     18   0.1669   0.6923        0.2524       0.9202        0.2774        18.0739
     19   0.1558   0.6904        0.2542       0.9166        0.2778        17.5961
     20   0.1559   0.6960        0.2540       0.9262        0.2930        17.7090
     21   0.1625   0.6953        0.2544       0.9202        0.3199        17.9848
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 07:21:01,419][0m Trial 567 finished with value: 0.2468721246236469 and parameters: {'lr': 0.013124108886476953, 'dropout': 0.458832898808635, 'd_model_multiplier': 2, 'num_layers': 3, 'n_heads': 32, 'dim_feedforward': 408, 'batch_size': 125, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.001, 'top_n_features': 92}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 56
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1831[0m   [32m0.6889[0m        [35m0.5121[0m       [31m0.8863[0m        [94m0.4646[0m     +  34.0622
      2   [36m0.2057[0m   [32m0.6974[0m        [35m0.2866[0m       [31m0.9141[0m        [94m0.2405[0m     +  34.2642
      3   0.1936   0.6924        [35m0.2734[0m       [31m0.9323[0m        [94m0.2313[0m     +  34.3329
      4   0.1936   0.6843        [35m0.2651[0m       0.9323        0.2634        34.5541
      5   [36m0.2107[0m   [32m0.7017[0m        [35m0.2633[0m       0.9323        0.2943        34.4489
      6   0.1838   0.6948        [35m0.2620[0m       0.9323        0.3101        34.6681
      7   0.1731   0.6933        [35m0.2582[0m       0.9323        0.3654        34.5331
      8   0.1647   0.6949        0.2610       0.9323        0.2782        34.4771
      9   0.2082   0.7000        0.2588       0.9323        0.3239        34.6325
     10   0.1707   0.6919        [35m0.2572[0m       0.9323        0.3104        34.6087
     11   0.2053   0.6949        0.2576       0.9323        0.3057        34.4746
     12   [36m0.2167[0m   0.6937        0.2687       0.9323        0.3129        34.3392
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 07:28:29,989][0m Trial 568 finished with value: 0.2312627976541796 and parameters: {'lr': 0.0051884250201862945, 'dropout': 0.4831323383986042, 'd_model_multiplier': 4, 'num_layers': 4, 'n_heads': 64, 'dim_feedforward': 395, 'batch_size': 34, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.01, 'top_n_features': 56}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 156
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2416[0m   [32m0.7918[0m        [35m0.2771[0m       [31m0.9021[0m        [94m0.2547[0m     +  16.9878
      2   0.2409   0.7765        [35m0.2460[0m       [31m0.9178[0m        0.2621        17.1926
      3   0.2348   0.7714        [35m0.2459[0m       0.9117        0.2580        17.1507
      4   0.2321   0.7697        [35m0.2429[0m       0.9105        0.2666        17.3416
      5   0.2015   0.7220        [35m0.2400[0m       0.9129        0.2808        17.3376
      6   0.2043   0.7284        0.2537       0.9141        0.2782        17.3616
      7   0.2023   0.7297        0.2497       0.9117        0.2771        17.2766
      8   0.2067   0.7260        0.2510       0.9129        0.2754        16.8898
      9   0.2051   0.7287        0.2505       0.9141        0.2746        17.1094
     10   0.2089   0.7306        0.2502       0.9141        0.2714        17.3817
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 07:31:39,649][0m Trial 569 finished with value: 0.25465711828731136 and parameters: {'lr': 0.0018999537230217813, 'dropout': 0.30801311181509294, 'd_model_multiplier': 4, 'num_layers': 6, 'n_heads': 16, 'dim_feedforward': 425, 'batch_size': 50, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 156}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 78
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
Warning, assumed runtime error: CUDA out of memory. Tried to allocate 1.57 GiB (GPU 0; 23.70 GiB total capacity; 20.91 GiB already allocated; 141.25 MiB free; 22.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[32m[I 2023-05-05 07:31:45,751][0m Trial 570 finished with value: 100.0 and parameters: {'lr': 3.561501113807288e-05, 'dropout': 0.4400342487531184, 'd_model_multiplier': 32, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 252, 'batch_size': 133, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 78}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 60
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.0604[0m   [32m0.3672[0m        [35m0.6867[0m       [31m0.9287[0m        [94m0.6077[0m     +  9.5416
      2   0.0554   0.3023        [35m0.5322[0m       [31m0.9299[0m        [94m0.4291[0m     +  10.0675
      3   0.0604   [32m0.3756[0m        [35m0.3954[0m       0.9299        [94m0.3236[0m     +  10.1159
      4   [36m0.0973[0m   [32m0.5041[0m        [35m0.3255[0m       0.9299        [94m0.2811[0m     +  9.7216
      5   [36m0.1755[0m   [32m0.5804[0m        [35m0.2918[0m       0.9299        [94m0.2589[0m     +  10.3309
      6   [36m0.2163[0m   [32m0.6285[0m        [35m0.2733[0m       [31m0.9311[0m        [94m0.2460[0m     +  10.0452
      7   [36m0.2271[0m   [32m0.6537[0m        [35m0.2612[0m       0.9311        [94m0.2386[0m     +  10.1135
      8   [36m0.2438[0m   [32m0.6717[0m        [35m0.2526[0m       0.9299        [94m0.2343[0m     +  10.1207
      9   [36m0.2515[0m   [32m0.6834[0m        [35m0.2486[0m       0.9274        [94m0.2317[0m     +  10.2636
     10   [36m0.2593[0m   [32m0.6916[0m        [35m0.2423[0m       0.9274        [94m0.2302[0m     +  10.3167
     11   [36m0.2656[0m   [32m0.6989[0m        [35m0.2406[0m       0.9299        [94m0.2290[0m     +  10.2693
     12   [36m0.2761[0m   [32m0.7042[0m        [35m0.2382[0m       0.9287        [94m0.2282[0m     +  10.4544
     13   [36m0.2796[0m   [32m0.7081[0m        [35m0.2379[0m       0.9287        [94m0.2275[0m     +  10.3418
     14   [36m0.2848[0m   [32m0.7137[0m        [35m0.2359[0m       0.9299        [94m0.2269[0m     +  10.5820
     15   [36m0.2880[0m   [32m0.7179[0m        0.2368       0.9299        [94m0.2268[0m     +  10.4720
     16   [36m0.2889[0m   [32m0.7194[0m        [35m0.2333[0m       0.9299        [94m0.2264[0m     +  10.1818
     17   [36m0.2920[0m   [32m0.7248[0m        0.2339       0.9287        [94m0.2260[0m     +  10.2236
     18   0.2919   [32m0.7267[0m        [35m0.2327[0m       0.9299        [94m0.2257[0m     +  10.4409
     19   0.2916   [32m0.7279[0m        [35m0.2324[0m       0.9299        [94m0.2255[0m     +  9.9192
     20   0.2905   [32m0.7303[0m        [35m0.2317[0m       0.9274        [94m0.2254[0m     +  9.9054
     21   0.2906   [32m0.7311[0m        0.2321       0.9299        [94m0.2252[0m     +  10.1138
     22   [36m0.2925[0m   [32m0.7342[0m        [35m0.2308[0m       0.9311        [94m0.2249[0m     +  10.3999
     23   0.2917   [32m0.7351[0m        0.2322       0.9311        [94m0.2249[0m     +  10.5388
     24   0.2898   [32m0.7358[0m        0.2315       [31m0.9323[0m        [94m0.2247[0m     +  10.4524
     25   0.2920   [32m0.7376[0m        0.2309       0.9311        [94m0.2246[0m     +  10.1025
     26   0.2912   [32m0.7379[0m        [35m0.2299[0m       0.9323        0.2248        10.3470
     27   0.2913   [32m0.7379[0m        [35m0.2291[0m       0.9311        0.2251        10.1401
     28   0.2918   [32m0.7391[0m        [35m0.2287[0m       0.9323        0.2248        10.3366
     29   [36m0.2951[0m   [32m0.7411[0m        0.2290       0.9299        [94m0.2246[0m     +  10.1456
     30   0.2931   0.7410        0.2301       0.9323        0.2246        10.1244
     31   0.2948   [32m0.7426[0m        0.2293       [31m0.9335[0m        [94m0.2242[0m     +  9.9435
     32   0.2933   [32m0.7427[0m        [35m0.2283[0m       0.9323        0.2244        10.4878
     33   0.2912   0.7424        0.2288       0.9335        0.2250        10.3016
     34   0.2916   [32m0.7436[0m        [35m0.2264[0m       0.9311        0.2255        10.0821
     35   0.2912   [32m0.7440[0m        0.2273       0.9311        0.2252        10.3497
     36   0.2893   [32m0.7449[0m        0.2268       0.9311        0.2256        10.0611
     37   0.2850   0.7442        0.2268       0.9311        0.2255        10.3595
     38   0.2851   0.7444        [35m0.2252[0m       0.9311        0.2259        10.2033
     39   0.2830   0.7448        0.2255       0.9299        0.2261        10.1564
     40   0.2828   [32m0.7458[0m        0.2252       0.9311        0.2257        10.0432
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 07:38:46,509][0m Trial 571 finished with value: 0.2242016032644454 and parameters: {'lr': 1.484478399814391e-05, 'dropout': 0.38847539062581604, 'd_model_multiplier': 16, 'num_layers': 6, 'n_heads': 4, 'dim_feedforward': 348, 'batch_size': 62, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.001, 'top_n_features': 60}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 192
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.1783[0m   [32m0.7562[0m        [35m0.3857[0m       [31m0.9407[0m        [94m0.2019[0m     +  9.1448
      2   [36m0.2344[0m   [32m0.8002[0m        [35m0.2565[0m       [31m0.9432[0m        [94m0.1943[0m     +  9.9340
      3   [36m0.2713[0m   [32m0.8008[0m        [35m0.2511[0m       0.9420        [94m0.1925[0m     +  10.1420
      4   [36m0.2759[0m   0.7896        [35m0.2440[0m       0.9420        0.1943        10.1866
      5   0.2668   0.7734        [35m0.2432[0m       [31m0.9444[0m        0.1963        9.8773
      6   0.2748   0.7797        [35m0.2393[0m       [31m0.9456[0m        0.1960        10.2498
      7   0.2635   0.7704        [35m0.2384[0m       0.9444        0.1977        10.0499
      8   0.2658   0.7811        [35m0.2353[0m       0.9444        0.1957        10.0358
      9   0.2617   0.7763        [35m0.2339[0m       0.9444        0.1985        10.7079
     10   0.2636   0.7762        0.2358       0.9432        0.1998        10.2080
     11   0.2438   0.7589        [35m0.2292[0m       0.9444        0.2062        10.2815
     12   0.2483   0.7593        0.2326       0.9432        0.2027        10.3772
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 07:40:58,345][0m Trial 572 finished with value: 0.19252316889768645 and parameters: {'lr': 0.0002826451816789614, 'dropout': 0.42630210991504847, 'd_model_multiplier': 4, 'num_layers': 1, 'n_heads': 32, 'dim_feedforward': 337, 'batch_size': 60, 'pos_encoding': 'learnable', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 192}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 200
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.2558[0m   [32m0.7236[0m        [35m0.2949[0m       [31m0.9274[0m        [94m0.2344[0m     +  8.7451
      2   [36m0.2566[0m   [32m0.7508[0m        [35m0.2526[0m       0.9274        [94m0.2309[0m     +  9.1542
      3   [36m0.3234[0m   [32m0.7632[0m        [35m0.2499[0m       [31m0.9323[0m        [94m0.2270[0m     +  9.3422
      4   [36m0.3375[0m   [32m0.7714[0m        [35m0.2440[0m       0.9299        [94m0.2221[0m     +  9.6438
      5   0.3292   [32m0.7769[0m        [35m0.2409[0m       0.9311        [94m0.2198[0m     +  9.8919
      6   [36m0.3481[0m   [32m0.7853[0m        [35m0.2401[0m       0.9299        [94m0.2174[0m     +  10.0062
      7   0.3449   [32m0.7855[0m        [35m0.2379[0m       0.9311        [94m0.2160[0m     +  9.7801
      8   0.3469   [32m0.7868[0m        [35m0.2369[0m       0.9311        0.2163        9.6179
      9   0.3473   [32m0.7924[0m        [35m0.2367[0m       [31m0.9335[0m        [94m0.2153[0m     +  9.3333
     10   [36m0.3638[0m   [32m0.7942[0m        [35m0.2342[0m       [31m0.9371[0m        [94m0.2134[0m     +  9.6690
     11   [36m0.3665[0m   [32m0.7953[0m        [35m0.2336[0m       0.9311        0.2147        9.3451
     12   0.3555   [32m0.7984[0m        [35m0.2329[0m       0.9335        0.2147        9.8139
     13   0.3304   0.7945        0.2341       0.9299        0.2179        9.7666
     14   0.3575   0.7952        [35m0.2307[0m       0.9274        0.2165        9.4396
     15   0.3571   [32m0.7998[0m        0.2310       0.9347        0.2134        9.6312
     16   0.3551   [32m0.8037[0m        [35m0.2298[0m       0.9323        0.2155        9.5763
     17   [36m0.3811[0m   0.8023        [35m0.2286[0m       0.9311        [94m0.2103[0m     +  9.6927
     18   0.3752   0.8013        [35m0.2263[0m       0.9347        0.2150        9.9191
     19   0.3640   0.8001        [35m0.2262[0m       0.9323        0.2148        9.7056
     20   0.3694   [32m0.8055[0m        0.2273       0.9311        0.2153        9.6390
     21   0.3738   0.8018        [35m0.2237[0m       0.9262        0.2198        9.5980
     22   0.3588   [32m0.8060[0m        0.2249       0.9226        0.2228        9.7840
     23   0.3301   0.7998        [35m0.2232[0m       0.9238        0.2224        9.6757
     24   0.3422   0.8026        [35m0.2220[0m       0.9238        0.2219        9.4161
     25   0.3390   0.8028        [35m0.2216[0m       0.9262        0.2173        9.7895
     26   0.3335   0.8031        [35m0.2178[0m       0.9226        0.2219        9.3425
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 07:45:18,068][0m Trial 573 finished with value: 0.21033777336183454 and parameters: {'lr': 0.0003284224081336603, 'dropout': 0.42437264292163174, 'd_model_multiplier': 4, 'num_layers': 1, 'n_heads': 8, 'dim_feedforward': 338, 'batch_size': 57, 'pos_encoding': 'learnable', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 200}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 208
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.0799[0m   [32m0.5240[0m        [35m0.4516[0m       [31m0.9226[0m        [94m0.2923[0m     +  9.6021
      2   [36m0.2084[0m   [32m0.7151[0m        [35m0.2737[0m       0.9226        [94m0.2471[0m     +  8.9658
      3   [36m0.2255[0m   [32m0.7606[0m        [35m0.2534[0m       0.9214        [94m0.2380[0m     +  9.3700
      4   [36m0.2624[0m   [32m0.7854[0m        [35m0.2459[0m       0.9190        [94m0.2317[0m     +  9.4923
      5   [36m0.2680[0m   [32m0.7927[0m        [35m0.2407[0m       0.9141        [94m0.2307[0m     +  9.5682
      6   [36m0.2777[0m   [32m0.7990[0m        [35m0.2383[0m       0.9166        [94m0.2290[0m     +  9.4840
      7   [36m0.3076[0m   [32m0.8033[0m        [35m0.2345[0m       [31m0.9238[0m        [94m0.2274[0m     +  9.6168
      8   [36m0.3171[0m   0.8026        [35m0.2323[0m       0.9226        0.2275        9.5555
      9   [36m0.3246[0m   0.7990        [35m0.2300[0m       0.9214        0.2296        9.7412
     10   0.3202   0.7976        [35m0.2280[0m       0.9178        0.2294        9.6706
     11   [36m0.3308[0m   0.8028        [35m0.2250[0m       0.9178        0.2276        9.6331
     12   [36m0.3373[0m   0.7938        [35m0.2228[0m       0.9190        0.2279        9.4214
     13   [36m0.3451[0m   0.7989        [35m0.2192[0m       0.9214        [94m0.2265[0m     +  9.7836
     14   [36m0.3504[0m   0.7931        [35m0.2170[0m       0.9226        0.2289        9.8772
     15   0.3336   0.7897        [35m0.2129[0m       0.9190        0.2319        9.9074
     16   0.3359   0.7842        [35m0.2117[0m       0.9214        0.2324        9.8691
     17   0.3347   0.7784        [35m0.2074[0m       0.9214        0.2343        10.0028
     18   0.3398   0.7936        [35m0.2043[0m       0.9178        0.2326        9.7885
     19   0.3311   0.7835        [35m0.2008[0m       0.9154        0.2378        9.8838
     20   0.3339   0.7814        [35m0.1990[0m       0.9190        0.2402        9.7938
     21   0.3329   0.7931        [35m0.1962[0m       0.9190        0.2406        9.7551
     22   0.3276   0.7929        [35m0.1915[0m       0.9154        0.2433        9.6104
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 07:49:00,837][0m Trial 574 finished with value: 0.22654883035581375 and parameters: {'lr': 0.00016388085990502377, 'dropout': 0.22794307436301917, 'd_model_multiplier': 1, 'num_layers': 1, 'n_heads': 32, 'dim_feedforward': 346, 'batch_size': 42, 'pos_encoding': 'learnable', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 208}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 189
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.2569[0m   [32m0.7736[0m        [35m0.2971[0m       [31m0.9141[0m        [94m0.2404[0m     +  9.1718
      2   [36m0.2832[0m   [32m0.7936[0m        [35m0.2491[0m       [31m0.9154[0m        [94m0.2336[0m     +  9.8326
      3   0.2812   0.7602        [35m0.2452[0m       [31m0.9190[0m        0.2532        9.7747
      4   0.2617   0.7426        [35m0.2422[0m       0.9166        0.2674        9.9435
      5   0.2417   0.7246        [35m0.2389[0m       0.9166        0.2861        10.0034
      6   0.2480   0.7119        [35m0.2376[0m       0.9166        0.2885        9.9847
      7   0.2776   0.7549        [35m0.2368[0m       0.9166        0.2667        9.9907
      8   0.2559   0.7221        [35m0.2346[0m       0.9166        0.2914        10.4291
      9   [36m0.2910[0m   0.7323        [35m0.2317[0m       [31m0.9214[0m        0.2649        10.0603
     10   [36m0.2954[0m   0.7626        [35m0.2301[0m       0.9202        0.2518        9.7775
     11   0.2852   0.7482        [35m0.2277[0m       0.9202        0.2536        10.2302
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 07:51:00,412][0m Trial 575 finished with value: 0.23360307111957773 and parameters: {'lr': 0.0008466313309069633, 'dropout': 0.2496940419647105, 'd_model_multiplier': 4, 'num_layers': 1, 'n_heads': 32, 'dim_feedforward': 352, 'batch_size': 53, 'pos_encoding': 'learnable', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 189}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 211
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.0824[0m   [32m0.3735[0m        [35m0.7591[0m       [31m0.1306[0m        [94m0.7572[0m     +  9.4755
      2   0.0798   0.3491        [35m0.7409[0m       [31m0.1644[0m        [94m0.7397[0m     +  10.4742
      3   0.0750   0.3331        [35m0.7273[0m       [31m0.2104[0m        [94m0.7229[0m     +  9.5125
      4   0.0702   0.3210        [35m0.7112[0m       [31m0.2805[0m        [94m0.7067[0m     +  9.7161
      5   0.0663   0.3139        [35m0.6966[0m       [31m0.3881[0m        [94m0.6911[0m     +  10.1600
      6   0.0640   0.3074        [35m0.6800[0m       [31m0.5224[0m        [94m0.6762[0m     +  10.2269
      7   0.0614   0.2995        [35m0.6686[0m       [31m0.6892[0m        [94m0.6618[0m     +  10.0824
      8   0.0609   0.2925        [35m0.6554[0m       [31m0.8017[0m        [94m0.6479[0m     +  10.1462
      9   0.0597   0.2864        [35m0.6428[0m       [31m0.8658[0m        [94m0.6346[0m     +  10.3153
     10   0.0616   0.2820        [35m0.6294[0m       [31m0.9045[0m        [94m0.6217[0m     +  10.2418
     11   0.0613   0.2795        [35m0.6193[0m       [31m0.9178[0m        [94m0.6094[0m     +  10.5428
     12   0.0612   0.2776        [35m0.6066[0m       0.9178        [94m0.5975[0m     +  10.1388
     13   0.0609   0.2762        [35m0.5969[0m       0.9178        [94m0.5861[0m     +  10.2856
     14   0.0608   0.2752        [35m0.5841[0m       0.9178        [94m0.5752[0m     +  10.1832
     15   0.0569   0.2748        [35m0.5751[0m       [31m0.9190[0m        [94m0.5646[0m     +  10.3703
     16   0.0553   0.2744        [35m0.5649[0m       0.9190        [94m0.5544[0m     +  10.2395
     17   0.0541   0.2736        [35m0.5563[0m       0.9190        [94m0.5446[0m     +  10.2163
     18   0.0537   0.2730        [35m0.5468[0m       0.9190        [94m0.5352[0m     +  10.6876
     19   0.0534   0.2728        [35m0.5376[0m       0.9190        [94m0.5261[0m     +  10.2814
     20   0.0533   0.2727        [35m0.5282[0m       0.9190        [94m0.5173[0m     +  10.1856
     21   0.0531   0.2726        [35m0.5202[0m       0.9190        [94m0.5089[0m     +  10.5787
     22   0.0530   0.2723        [35m0.5124[0m       0.9190        [94m0.5007[0m     +  10.2034
     23   0.0528   0.2716        [35m0.5041[0m       0.9190        [94m0.4929[0m     +  10.1532
     24   0.0527   0.2715        [35m0.4971[0m       0.9190        [94m0.4854[0m     +  10.0136
     25   0.0527   0.2717        [35m0.4906[0m       0.9190        [94m0.4781[0m     +  10.0556
     26   0.0527   0.2719        [35m0.4836[0m       0.9190        [94m0.4710[0m     +  10.1943
     27   0.0527   0.2720        [35m0.4771[0m       0.9190        [94m0.4643[0m     +  9.8732
     28   0.0527   0.2723        [35m0.4701[0m       0.9190        [94m0.4578[0m     +  10.0973
     29   0.0527   0.2726        [35m0.4628[0m       0.9190        [94m0.4515[0m     +  10.3205
     30   0.0527   0.2728        [35m0.4571[0m       0.9190        [94m0.4454[0m     +  10.5800
     31   0.0527   0.2734        [35m0.4522[0m       0.9190        [94m0.4396[0m     +  10.0626
     32   0.0527   0.2742        [35m0.4457[0m       0.9190        [94m0.4340[0m     +  9.9655
     33   0.0528   0.2748        [35m0.4414[0m       0.9190        [94m0.4285[0m     +  10.3264
     34   0.0528   0.2760        [35m0.4361[0m       0.9190        [94m0.4233[0m     +  10.1574
     35   0.0529   0.2774        [35m0.4314[0m       0.9190        [94m0.4182[0m     +  10.3637
     36   0.0530   0.2786        [35m0.4252[0m       0.9190        [94m0.4134[0m     +  10.2301
     37   0.0530   0.2798        [35m0.4199[0m       0.9190        [94m0.4087[0m     +  9.9836
     38   0.0531   0.2808        [35m0.4144[0m       0.9190        [94m0.4041[0m     +  10.0985
     39   0.0532   0.2822        [35m0.4118[0m       0.9190        [94m0.3998[0m     +  10.4375
     40   0.0532   0.2835        [35m0.4073[0m       0.9190        [94m0.3955[0m     +  9.8531
     41   0.0533   0.2849        [35m0.4020[0m       0.9190        [94m0.3914[0m     +  10.0406
     42   0.0534   0.2868        [35m0.3980[0m       0.9190        [94m0.3875[0m     +  10.3088
     43   0.0535   0.2887        [35m0.3953[0m       0.9190        [94m0.3837[0m     +  10.2577
     44   0.0537   0.2907        [35m0.3901[0m       0.9190        [94m0.3800[0m     +  10.2637
     45   0.0539   0.2936        [35m0.3866[0m       0.9190        [94m0.3765[0m     +  10.3658
     46   0.0540   0.2959        [35m0.3835[0m       0.9190        [94m0.3731[0m     +  10.2491
     47   0.0542   0.2990        [35m0.3802[0m       0.9190        [94m0.3698[0m     +  9.9797
     48   0.0544   0.3022        [35m0.3771[0m       0.9190        [94m0.3666[0m     +  10.0467
     49   0.0547   0.3052        [35m0.3732[0m       0.9190        [94m0.3635[0m     +  10.2776
     50   0.0549   0.3086        [35m0.3706[0m       0.9190        [94m0.3605[0m     +  10.1276
[32m[I 2023-05-05 07:59:32,404][0m Trial 576 finished with value: 0.360484925946705 and parameters: {'lr': 1.9217002674558002e-07, 'dropout': 0.41339547350778993, 'd_model_multiplier': 4, 'num_layers': 1, 'n_heads': 32, 'dim_feedforward': 436, 'batch_size': 60, 'pos_encoding': 'learnable', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 211}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 193
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.3109[0m   [32m0.7354[0m        [35m0.3144[0m       [31m0.9033[0m        [94m0.2937[0m     +  12.6373
      2   [36m0.3253[0m   [32m0.7636[0m        [35m0.2441[0m       0.9033        0.2959        12.8438
      3   [36m0.3346[0m   [32m0.7765[0m        [35m0.2403[0m       0.9021        [94m0.2830[0m     +  12.9533
      4   [36m0.3441[0m   [32m0.7782[0m        [35m0.2357[0m       [31m0.9069[0m        [94m0.2733[0m     +  12.9584
      5   [36m0.3454[0m   [32m0.7860[0m        [35m0.2345[0m       [31m0.9081[0m        0.2766        13.1340
      6   [36m0.3649[0m   0.7822        [35m0.2339[0m       0.9069        0.2771        12.9433
      7   0.3494   0.7748        [35m0.2315[0m       0.9033        0.2781        12.9596
      8   0.3446   0.7672        [35m0.2291[0m       [31m0.9093[0m        0.2942        12.8152
      9   0.3547   [32m0.7867[0m        0.2314       0.9057        0.2784        13.1110
     10   [36m0.3776[0m   [32m0.7905[0m        [35m0.2265[0m       0.9045        0.2776        12.9127
     11   0.3521   0.7852        [35m0.2264[0m       0.9045        0.2813        12.9432
     12   0.3697   [32m0.7917[0m        [35m0.2260[0m       0.9021        0.2796        13.0874
     13   0.3641   0.7821        0.2264       0.9057        0.3022        12.9548
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 08:02:34,202][0m Trial 577 finished with value: 0.2733155847352534 and parameters: {'lr': 0.00027808393097027953, 'dropout': 0.4314642663347937, 'd_model_multiplier': 4, 'num_layers': 1, 'n_heads': 64, 'dim_feedforward': 369, 'batch_size': 48, 'pos_encoding': 'learnable', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 193}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 215
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.2726[0m   [32m0.7368[0m        [35m0.2669[0m       [31m0.9069[0m        [94m0.2604[0m     +  9.1423
      2   [36m0.2924[0m   [32m0.7468[0m        [35m0.2476[0m       0.9045        [94m0.2567[0m     +  9.3823
      3   [36m0.2976[0m   0.7457        [35m0.2467[0m       [31m0.9141[0m        [94m0.2525[0m     +  9.5974
      4   [36m0.3054[0m   [32m0.7469[0m        [35m0.2451[0m       [31m0.9178[0m        [94m0.2478[0m     +  9.3878
      5   [36m0.3363[0m   [32m0.7536[0m        [35m0.2420[0m       [31m0.9214[0m        [94m0.2459[0m     +  9.7050
      6   0.3270   [32m0.7580[0m        0.2422       0.9154        0.2459        9.5530
      7   0.3325   [32m0.7591[0m        [35m0.2386[0m       0.9154        0.2462        9.5801
      8   [36m0.3425[0m   0.7551        0.2398       0.9129        0.2587        9.6225
      9   [36m0.3487[0m   [32m0.7604[0m        0.2412       0.9141        0.2490        9.5594
     10   0.3302   [32m0.7647[0m        0.2398       0.9178        0.2462        9.8259
     11   0.3487   [32m0.7723[0m        [35m0.2373[0m       0.9166        [94m0.2419[0m     +  9.4485
     12   [36m0.3508[0m   0.7663        0.2403       0.9129        0.2469        9.7313
     13   [36m0.3551[0m   0.7657        0.2384       0.9129        0.2508        9.8438
     14   0.3456   [32m0.7724[0m        [35m0.2369[0m       0.9129        0.2447        9.8671
     15   0.3538   0.7674        [35m0.2355[0m       0.9214        0.2461        9.9167
     16   0.3411   [32m0.7736[0m        [35m0.2344[0m       0.9081        0.2626        9.8081
     17   0.3312   0.7714        0.2352       0.9117        0.2591        9.7780
     18   0.3230   0.7711        0.2351       0.9105        0.2560        9.6490
     19   0.3118   [32m0.7753[0m        [35m0.2312[0m       0.9129        0.2529        9.6108
     20   0.3180   [32m0.7777[0m        0.2315       0.9129        0.2600        9.8169
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 08:05:57,219][0m Trial 578 finished with value: 0.2418503102293506 and parameters: {'lr': 0.0002263531725666455, 'dropout': 0.4509370974657776, 'd_model_multiplier': 4, 'num_layers': 1, 'n_heads': 32, 'dim_feedforward': 359, 'batch_size': 25, 'pos_encoding': 'learnable', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 215}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 181
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2137[0m   [32m0.7140[0m        [35m0.2927[0m       [31m0.9287[0m        [94m0.2578[0m     +  25.1307
      2   0.0867   0.5454        [35m0.2554[0m       0.9287        0.3258        25.2334
      3   0.0969   0.5469        [35m0.2500[0m       0.9274        0.3218        25.4731
      4   0.0962   0.5298        [35m0.2467[0m       0.9274        0.3328        25.5367
      5   0.1048   0.5356        [35m0.2453[0m       0.9274        0.3139        25.4670
      6   0.0738   0.4358        [35m0.2441[0m       0.9274        0.4207        25.3434
      7   0.0685   0.4224        0.2451       [31m0.9299[0m        0.3804        25.4288
      8   0.0803   0.4362        0.2480       0.9287        0.4142        25.1897
      9   0.0915   0.4298        0.2509       0.9250        0.4297        25.6699
     10   0.0998   0.4502        0.2483       0.9287        0.3491        25.3803
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 08:10:37,201][0m Trial 579 finished with value: 0.257820294532551 and parameters: {'lr': 9.305571842066025e-05, 'dropout': 0.37015557557566864, 'd_model_multiplier': 64, 'num_layers': 1, 'n_heads': 32, 'dim_feedforward': 339, 'batch_size': 54, 'pos_encoding': 'learnable', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 181}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 175
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1873[0m   [32m0.7191[0m        [35m0.2902[0m       [31m0.9190[0m        [94m0.2411[0m     +  10.0238
      2   0.1834   0.7094        [35m0.2496[0m       0.9166        0.2490        10.0832
      3   [36m0.1951[0m   0.7096        [35m0.2461[0m       0.9166        0.2499        10.3234
      4   0.1865   0.6962        [35m0.2454[0m       [31m0.9202[0m        0.2496        9.9361
      5   [36m0.1962[0m   0.6943        [35m0.2434[0m       [31m0.9238[0m        0.2519        10.1096
      6   [36m0.2056[0m   0.6949        [35m0.2420[0m       [31m0.9262[0m        0.2419        10.1247
      7   [36m0.2136[0m   [32m0.7227[0m        [35m0.2366[0m       0.9166        0.2568        9.8039
      8   0.1889   0.6957        0.2373       0.9190        0.2466        10.1841
      9   0.1984   0.7026        [35m0.2349[0m       0.9202        0.2444        10.0844
     10   0.2094   0.7225        [35m0.2340[0m       0.9238        [94m0.2398[0m     +  9.9014
     11   0.1885   0.7098        [35m0.2304[0m       0.9238        0.2456        9.9058
     12   0.1797   0.7134        [35m0.2265[0m       0.9262        0.2433        9.9752
     13   [36m0.2186[0m   [32m0.7244[0m        [35m0.2261[0m       0.9250        [94m0.2371[0m     +  9.9949
     14   0.2022   [32m0.7252[0m        [35m0.2258[0m       [31m0.9274[0m        0.2383        9.9380
     15   0.2184   [32m0.7333[0m        [35m0.2220[0m       0.9202        0.2464        10.0396
     16   0.2028   0.7207        0.2240       [31m0.9287[0m        0.2385        10.1446
     17   0.2072   0.7219        [35m0.2192[0m       0.9274        0.2419        10.1811
     18   0.2020   0.7295        0.2203       0.9250        0.2381        9.9090
     19   [36m0.2267[0m   [32m0.7468[0m        [35m0.2164[0m       0.9250        0.2373        9.9831
     20   0.2052   0.7269        [35m0.2161[0m       0.9274        [94m0.2366[0m     +  9.9273
     21   0.2200   0.7417        [35m0.2158[0m       0.9274        [94m0.2357[0m     +  9.8619
     22   0.2188   [32m0.7492[0m        [35m0.2133[0m       0.9250        0.2363        10.0938
     23   0.2023   0.7413        [35m0.2116[0m       0.9274        0.2363        10.0116
     24   0.1853   0.7367        [35m0.2101[0m       0.9250        0.2454        10.0156
     25   0.1911   0.7243        [35m0.2078[0m       0.9214        0.2539        10.1545
     26   0.2078   0.7298        [35m0.2071[0m       0.9214        0.2467        10.0564
     27   0.1988   0.7251        [35m0.2003[0m       0.9226        0.2531        10.1270
     28   0.1875   0.7317        0.2017       0.9190        0.2459        9.9521
     29   0.1747   0.7266        [35m0.1991[0m       0.9214        0.2530        9.9658
     30   0.1727   0.7109        0.2001       0.9202        0.2576        10.0290
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 08:15:49,234][0m Trial 580 finished with value: 0.23571219569817636 and parameters: {'lr': 0.0003777513635747627, 'dropout': 0.3412285688797403, 'd_model_multiplier': 2, 'num_layers': 1, 'n_heads': 32, 'dim_feedforward': 330, 'batch_size': 11, 'pos_encoding': 'learnable', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 175}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 192
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.0535[0m   [32m0.3397[0m        [35m0.4920[0m       [31m0.9287[0m        [94m0.3337[0m     +  8.6899
      2   [36m0.1741[0m   [32m0.5550[0m        [35m0.3237[0m       0.9287        [94m0.2701[0m     +  8.9717
      3   [36m0.2100[0m   [32m0.6201[0m        [35m0.2868[0m       0.9287        [94m0.2516[0m     +  9.1460
      4   [36m0.2145[0m   [32m0.6506[0m        [35m0.2727[0m       0.9287        [94m0.2442[0m     +  9.3811
      5   0.1805   [32m0.6696[0m        [35m0.2655[0m       0.9287        [94m0.2411[0m     +  9.0878
      6   0.1637   [32m0.6787[0m        [35m0.2647[0m       0.9287        [94m0.2393[0m     +  9.4647
      7   0.1668   [32m0.6893[0m        [35m0.2627[0m       0.9287        [94m0.2380[0m     +  9.2685
      8   0.1830   [32m0.6924[0m        [35m0.2598[0m       0.9287        [94m0.2370[0m     +  9.3750
      9   0.1984   [32m0.6992[0m        [35m0.2577[0m       0.9287        [94m0.2363[0m     +  9.4575
     10   0.2004   [32m0.7051[0m        0.2589       0.9287        [94m0.2356[0m     +  9.4225
     11   0.2043   [32m0.7089[0m        [35m0.2563[0m       0.9287        [94m0.2352[0m     +  9.1122
     12   0.1990   0.7083        0.2601       0.9287        [94m0.2351[0m     +  9.3738
     13   0.2060   0.7080        0.2596       0.9287        [94m0.2349[0m     +  9.4757
     14   [36m0.2152[0m   0.7085        0.2603       0.9287        [94m0.2347[0m     +  9.1944
     15   [36m0.2178[0m   0.7082        0.2588       0.9287        [94m0.2346[0m     +  9.4495
     16   [36m0.2263[0m   0.7073        0.2576       0.9287        [94m0.2345[0m     +  9.3263
     17   [36m0.2294[0m   [32m0.7096[0m        0.2586       0.9287        [94m0.2343[0m     +  9.2291
     18   [36m0.2412[0m   [32m0.7099[0m        0.2572       0.9287        [94m0.2341[0m     +  9.4587
     19   [36m0.2469[0m   0.7070        0.2582       0.9287        0.2341        9.5825
     20   0.2414   0.7091        0.2573       0.9287        [94m0.2338[0m     +  9.2220
     21   0.2428   [32m0.7125[0m        0.2567       0.9287        0.2339        9.5230
     22   0.2444   [32m0.7129[0m        [35m0.2552[0m       0.9287        [94m0.2336[0m     +  9.3013
     23   [36m0.2475[0m   [32m0.7144[0m        0.2583       0.9287        0.2337        9.6856
     24   [36m0.2481[0m   0.7132        0.2568       0.9287        0.2336        9.4013
     25   [36m0.2714[0m   [32m0.7162[0m        0.2559       0.9287        [94m0.2333[0m     +  9.2997
     26   0.2674   0.7148        0.2581       0.9287        0.2334        9.5193
     27   0.2511   0.7139        0.2585       0.9287        0.2335        9.2653
     28   0.2516   0.7142        0.2565       0.9287        0.2336        9.3440
     29   0.2511   0.7126        0.2565       0.9287        0.2335        9.4135
     30   0.2465   0.7130        0.2565       0.9287        0.2333        9.5203
     31   0.2461   0.7141        0.2581       0.9287        0.2334        9.5055
     32   0.2587   0.7115        0.2571       0.9287        0.2335        9.2941
     33   0.2523   0.7132        0.2571       0.9287        0.2335        9.4462
     34   0.2635   0.7127        0.2560       0.9287        0.2337        9.4156
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 08:21:17,307][0m Trial 581 finished with value: 0.23328259817021646 and parameters: {'lr': 0.00015091013265466016, 'dropout': 0.4399806722285701, 'd_model_multiplier': 1, 'num_layers': 2, 'n_heads': 4, 'dim_feedforward': 344, 'batch_size': 38, 'pos_encoding': 'learnable', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 192}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 206
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.2519[0m   [32m0.7688[0m        [35m0.3207[0m       [31m0.9287[0m        [94m0.2242[0m     +  9.9700
      2   [36m0.2566[0m   [32m0.7845[0m        [35m0.2493[0m       0.9262        0.2248        9.9985
      3   0.2559   0.7718        [35m0.2449[0m       0.9238        [94m0.2235[0m     +  10.1843
      4   0.2564   0.7614        [35m0.2425[0m       0.9226        0.2254        10.7410
      5   0.2400   0.7486        0.2425       0.9238        0.2321        10.4371
      6   0.2360   0.7351        [35m0.2389[0m       0.9250        0.2345        10.1936
      7   0.2336   0.7230        [35m0.2371[0m       [31m0.9299[0m        0.2365        10.1805
      8   0.2356   0.7121        [35m0.2361[0m       0.9299        0.2407        10.2440
      9   0.2227   0.6970        0.2376       [31m0.9311[0m        0.2552        10.5461
     10   0.2295   0.7019        0.2366       0.9274        0.2421        10.1565
     11   0.2267   0.6943        0.2376       0.9311        0.2495        10.4045
     12   0.2223   0.6900        [35m0.2358[0m       0.9311        0.2453        10.3492
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 08:23:31,319][0m Trial 582 finished with value: 0.22348685131972473 and parameters: {'lr': 0.0006042565533317291, 'dropout': 0.41630018379680866, 'd_model_multiplier': 4, 'num_layers': 1, 'n_heads': 32, 'dim_feedforward': 458, 'batch_size': 59, 'pos_encoding': 'learnable', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 206}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 204
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1922[0m   [32m0.6599[0m        [35m0.4162[0m       [31m0.9057[0m        [94m0.3037[0m     +  34.4842
      2   [36m0.2460[0m   [32m0.7149[0m        [35m0.2489[0m       0.9057        0.3072        34.5471
      3   0.2340   [32m0.7331[0m        [35m0.2410[0m       0.9045        0.3138        34.6982
      4   0.2298   [32m0.7424[0m        [35m0.2366[0m       0.8996        0.3243        34.6695
      5   0.2271   [32m0.7441[0m        [35m0.2332[0m       0.8996        0.3356        34.6864
      6   0.2252   [32m0.7462[0m        [35m0.2313[0m       0.9008        0.3535        34.9492
      7   0.2348   [32m0.7533[0m        [35m0.2287[0m       0.8996        0.3408        34.6287
      8   0.2352   [32m0.7577[0m        [35m0.2266[0m       0.8996        0.3375        34.8509
      9   0.2393   [32m0.7598[0m        0.2276       0.9008        0.3392        34.7316
     10   0.2444   [32m0.7634[0m        [35m0.2257[0m       0.9021        0.3413        34.7001
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 08:29:53,404][0m Trial 583 finished with value: 0.3036912743457139 and parameters: {'lr': 6.373210559637891e-05, 'dropout': 0.46517420628825285, 'd_model_multiplier': 4, 'num_layers': 4, 'n_heads': 64, 'dim_feedforward': 381, 'batch_size': 51, 'pos_encoding': 'learnable', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 204}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 222
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1692[0m   [32m0.7249[0m        [35m0.3153[0m       [31m0.9323[0m        [94m0.2474[0m     +  12.8113
      2   [36m0.1839[0m   0.7204        [35m0.2716[0m       0.9299        0.2528        13.0728
      3   [36m0.1923[0m   0.7239        [35m0.2643[0m       0.9311        0.2615        13.0868
      4   [36m0.1928[0m   0.7217        [35m0.2613[0m       0.9311        0.2577        13.1089
      5   0.1873   0.7212        [35m0.2603[0m       0.9311        [94m0.2471[0m     +  13.0456
      6   0.1891   0.7205        [35m0.2571[0m       0.9311        [94m0.2465[0m     +  12.9900
      7   0.1875   [32m0.7276[0m        0.2575       0.9299        [94m0.2397[0m     +  13.0690
      8   0.1892   0.7257        [35m0.2563[0m       0.9311        [94m0.2386[0m     +  12.9854
      9   [36m0.1945[0m   0.7245        0.2577       0.9323        [94m0.2375[0m     +  13.2147
     10   0.1932   0.7256        0.2577       0.9311        0.2394        12.9969
     11   0.1919   0.7271        0.2584       [31m0.9335[0m        [94m0.2281[0m     +  13.1189
     12   [36m0.1967[0m   [32m0.7303[0m        0.2585       0.9311        [94m0.2265[0m     +  13.1010
     13   0.1864   0.7276        [35m0.2561[0m       0.9311        [94m0.2243[0m     +  13.0215
     14   0.1925   0.7279        0.2569       0.9323        [94m0.2227[0m     +  13.7025
     15   0.1906   0.7272        0.2574       0.9299        0.2230        12.9789
     16   0.1931   0.7272        0.2563       0.9311        0.2256        13.4061
     17   0.1858   0.7250        0.2561       0.9311        0.2279        13.1881
     18   0.1856   0.7261        [35m0.2545[0m       0.9299        0.2232        13.2611
     19   0.1863   0.7218        0.2563       0.9299        0.2232        13.2144
     20   [36m0.2092[0m   0.7251        0.2583       0.9262        0.2311        13.1364
     21   0.1979   0.7257        0.2589       0.9311        0.2232        13.0544
     22   0.1934   0.7260        [35m0.2544[0m       0.9323        0.2377        12.9939
     23   0.1952   0.7266        0.2546       0.9311        0.2295        12.8740
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 08:35:09,098][0m Trial 584 finished with value: 0.22273879400043417 and parameters: {'lr': 0.0010633786038929869, 'dropout': 0.5437053547119592, 'd_model_multiplier': 4, 'num_layers': 2, 'n_heads': 32, 'dim_feedforward': 336, 'batch_size': 19, 'pos_encoding': 'learnable', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 222}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 87
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2876[0m   [32m0.7362[0m        [35m0.3353[0m       [31m0.9214[0m        [94m0.2434[0m     +  19.2347
      2   [36m0.3214[0m   [32m0.7874[0m        [35m0.2423[0m       [31m0.9238[0m        [94m0.2292[0m     +  19.3850
      3   [36m0.3318[0m   [32m0.8023[0m        [35m0.2371[0m       0.9214        [94m0.2263[0m     +  19.5460
      4   [36m0.3380[0m   [32m0.8114[0m        [35m0.2341[0m       0.9226        [94m0.2248[0m     +  19.6576
      5   0.3120   [32m0.8119[0m        [35m0.2321[0m       0.9226        0.2265        19.6513
      6   0.3210   0.8114        [35m0.2309[0m       0.9166        0.2277        19.5494
      7   0.3140   0.8089        [35m0.2281[0m       0.9178        0.2303        19.4407
      8   0.3057   0.8045        [35m0.2272[0m       0.9141        0.2330        19.5711
      9   0.3059   0.8022        [35m0.2249[0m       0.9141        0.2355        19.5595
     10   0.2960   0.7969        [35m0.2230[0m       0.9141        0.2388        19.5552
     11   0.2935   0.7972        [35m0.2213[0m       0.9117        0.2411        19.4985
     12   0.2700   0.7877        [35m0.2183[0m       0.9117        0.2486        19.6682
     13   0.2691   0.7751        [35m0.2160[0m       0.9117        0.2499        19.5066
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 08:39:43,892][0m Trial 585 finished with value: 0.22484064466143172 and parameters: {'lr': 2.3379649117396127e-05, 'dropout': 0.39722172996644356, 'd_model_multiplier': 32, 'num_layers': 4, 'n_heads': 16, 'dim_feedforward': 417, 'batch_size': 24, 'pos_encoding': 'learnable', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 87}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 184
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1347[0m   [32m0.6801[0m        [35m0.2812[0m       [31m0.9250[0m        [94m0.2697[0m     +  17.4367
      2   [36m0.2223[0m   [32m0.7638[0m        [35m0.2515[0m       0.9238        [94m0.2640[0m     +  16.7256
      3   [36m0.2261[0m   0.7547        [35m0.2471[0m       0.9214        [94m0.2605[0m     +  16.9881
      4   [36m0.2490[0m   [32m0.7756[0m        [35m0.2415[0m       0.9178        0.2625        17.3116
      5   [36m0.2566[0m   [32m0.7793[0m        [35m0.2394[0m       0.9202        0.2681        17.0541
      6   [36m0.2616[0m   [32m0.7886[0m        [35m0.2360[0m       0.9214        0.2615        16.9676
      7   0.2418   0.7612        [35m0.2322[0m       0.9238        0.2606        17.1327
      8   0.2585   0.7768        [35m0.2304[0m       0.9202        [94m0.2592[0m     +  17.0536
      9   [36m0.2718[0m   0.7785        [35m0.2289[0m       0.9238        0.2651        17.0060
     10   0.2492   0.7682        [35m0.2284[0m       0.9202        [94m0.2580[0m     +  17.2763
     11   0.2417   0.7620        [35m0.2281[0m       0.9226        0.2807        16.9942
     12   0.2507   0.7829        0.2334       0.9226        [94m0.2559[0m     +  17.4290
     13   0.2463   0.7862        0.2319       0.9190        [94m0.2519[0m     +  16.8846
     14   0.2516   [32m0.7941[0m        [35m0.2276[0m       0.9202        [94m0.2510[0m     +  17.0988
     15   0.2494   0.7572        [35m0.2232[0m       0.9250        0.2520        17.1814
     16   0.2528   0.7749        0.2278       0.9238        [94m0.2507[0m     +  17.1120
     17   0.2612   0.7840        0.2271       0.9214        0.2541        17.1431
     18   0.2562   0.7802        0.2281       0.9226        [94m0.2502[0m     +  17.2317
     19   0.2595   0.7866        0.2250       0.9250        [94m0.2490[0m     +  17.2617
     20   0.2529   0.7912        0.2258       0.9250        0.2500        17.1502
     21   0.2432   0.7804        [35m0.2223[0m       0.9214        0.2499        17.0372
     22   0.2423   [32m0.8030[0m        [35m0.2216[0m       0.9190        [94m0.2456[0m     +  17.4650
     23   0.2582   [32m0.8034[0m        0.2240       0.9238        [94m0.2440[0m     +  17.1243
     24   [36m0.2755[0m   0.7949        0.2228       [31m0.9274[0m        [94m0.2424[0m     +  17.3722
     25   0.2641   0.7966        [35m0.2211[0m       0.9274        0.2444        17.0451
     26   0.2603   0.7828        [35m0.2206[0m       0.9274        0.2434        17.3046
     27   0.2507   0.7809        0.2232       0.9226        0.2461        17.1125
     28   0.2702   0.7950        [35m0.2196[0m       0.9262        [94m0.2389[0m     +  17.2516
     29   0.2744   0.7763        [35m0.2163[0m       0.9274        0.2497        17.4785
     30   0.2580   0.7894        0.2203       0.9250        0.2450        17.2680
     31   0.2581   0.7843        0.2197       0.9262        0.2431        17.2903
     32   0.2671   0.8020        0.2212       0.9262        0.2442        17.0001
     33   0.2528   0.7953        0.2195       0.9214        0.2455        17.2095
     34   0.2512   0.7923        0.2165       0.9214        0.2480        17.1338
     35   0.2373   0.7777        0.2169       0.9226        0.2513        17.5359
     36   0.2445   0.7772        0.2186       0.9202        0.2523        17.2901
     37   0.2460   0.7925        0.2176       0.9214        0.2440        16.8745
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 08:50:37,603][0m Trial 586 finished with value: 0.23888391998687708 and parameters: {'lr': 0.0007475461376902786, 'dropout': 0.3504936390068972, 'd_model_multiplier': 4, 'num_layers': 3, 'n_heads': 32, 'dim_feedforward': 355, 'batch_size': 72, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'AdamW', 'weight_decay': 0.1, 'top_n_features': 184}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 70
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.0807[0m   [32m0.4763[0m        [35m0.4576[0m       [31m0.9190[0m        [94m0.2981[0m     +  9.4753
      2   [36m0.2253[0m   [32m0.6948[0m        [35m0.2783[0m       0.9190        [94m0.2613[0m     +  9.4308
      3   [36m0.2513[0m   [32m0.7455[0m        [35m0.2536[0m       [31m0.9202[0m        [94m0.2564[0m     +  9.5584
      4   [36m0.2702[0m   [32m0.7668[0m        [35m0.2459[0m       0.9166        [94m0.2537[0m     +  9.8229
      5   0.2696   [32m0.7781[0m        [35m0.2430[0m       0.9190        [94m0.2534[0m     +  9.6897
      6   [36m0.2798[0m   [32m0.7846[0m        [35m0.2388[0m       0.9202        0.2535        9.7607
      7   [36m0.2820[0m   [32m0.7870[0m        [35m0.2378[0m       0.9202        0.2556        9.4495
      8   0.2819   [32m0.7883[0m        [35m0.2371[0m       0.9154        0.2570        9.5460
      9   [36m0.2823[0m   0.7873        [35m0.2350[0m       0.9117        0.2597        9.5200
     10   0.2797   0.7857        [35m0.2348[0m       0.9129        0.2613        9.5307
     11   [36m0.2845[0m   0.7863        [35m0.2325[0m       0.9093        0.2636        9.8871
     12   [36m0.2847[0m   0.7869        0.2335       0.9105        0.2650        9.4944
     13   [36m0.2863[0m   0.7869        0.2339       0.9117        0.2656        9.8339
     14   [36m0.2870[0m   [32m0.7888[0m        [35m0.2321[0m       0.9117        0.2659        9.9723
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 08:53:02,821][0m Trial 587 finished with value: 0.25336404648826805 and parameters: {'lr': 4.190986333096888e-05, 'dropout': 0.570852355323844, 'd_model_multiplier': 4, 'num_layers': 1, 'n_heads': 32, 'dim_feedforward': 201, 'batch_size': 31, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 70}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 164
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2690[0m   [32m0.7877[0m        [35m0.3155[0m       [31m0.9395[0m        [94m0.1920[0m     +  16.2063
      2   0.2386   [32m0.8189[0m        [35m0.2555[0m       0.9335        0.2128        16.0971
      3   [36m0.2742[0m   0.8158        [35m0.2494[0m       0.9383        0.2092        16.2261
      4   0.2448   0.8188        [35m0.2454[0m       0.9359        0.2346        16.2331
      5   0.2429   0.8162        0.2469       0.9335        0.2370        16.2681
      6   0.2663   [32m0.8235[0m        0.2454       0.9262        0.2055        16.2063
      7   0.2701   0.8219        [35m0.2432[0m       0.9335        0.1922        16.4555
      8   [36m0.3198[0m   [32m0.8363[0m        0.2437       0.9250        0.1972        16.2315
      9   0.2224   0.7993        [35m0.2421[0m       0.9093        0.2398        16.1458
     10   0.2591   0.8147        [35m0.2391[0m       0.9129        0.2399        16.6290
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 08:56:02,106][0m Trial 588 finished with value: 0.1919541134889786 and parameters: {'lr': 0.0012240052972142335, 'dropout': 0.42846862619391746, 'd_model_multiplier': 1, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 399, 'batch_size': 41, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0, 'top_n_features': 164}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 164
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1575[0m   [32m0.6599[0m        [35m0.2899[0m       [31m0.9250[0m        [94m0.2566[0m     +  18.7140
      2   [36m0.1680[0m   [32m0.6767[0m        [35m0.2549[0m       0.9250        0.2606        19.0207
      3   [36m0.2540[0m   [32m0.7049[0m        [35m0.2516[0m       0.9250        0.2731        18.9873
      4   0.2480   [32m0.7365[0m        [35m0.2470[0m       0.9250        [94m0.2511[0m     +  19.0193
      5   [36m0.2657[0m   [32m0.7572[0m        [35m0.2434[0m       0.9214        [94m0.2504[0m     +  19.0284
      6   0.2532   [32m0.7692[0m        [35m0.2428[0m       0.9214        0.2518        18.9645
      7   [36m0.2701[0m   0.7592        [35m0.2390[0m       0.9238        0.2589        19.2702
      8   0.2583   0.7441        0.2406       0.9250        0.2812        19.2094
      9   0.2660   [32m0.7851[0m        0.2397       0.9238        [94m0.2429[0m     +  19.1649
     10   0.2206   0.7013        [35m0.2386[0m       [31m0.9299[0m        0.3070        18.8391
     11   0.1628   0.7037        0.2422       0.9250        0.2520        18.9526
     12   0.2464   0.7682        0.2394       0.9226        0.2538        19.0631
     13   0.1545   0.6787        0.2434       0.9250        0.2665        19.0956
     14   0.2099   0.7593        0.2420       0.9057        0.2496        18.9581
     15   0.2410   0.7186        [35m0.2379[0m       0.9299        0.2980        18.9859
     16   0.2636   [32m0.8009[0m        [35m0.2367[0m       0.9141        [94m0.2419[0m     +  18.9443
     17   [36m0.2714[0m   0.7430        0.2372       [31m0.9335[0m        0.2537        19.0587
     18   0.2516   0.7327        [35m0.2361[0m       0.9299        0.2940        19.2061
     19   0.1739   0.7253        0.2371       0.8839        0.2820        19.3222
     20   0.1510   0.6743        0.2369       0.9214        0.2834        19.0340
     21   0.1538   0.6725        0.2434       0.9250        0.2911        18.8495
     22   0.1588   0.6705        0.2474       0.9250        0.2796        19.0428
     23   0.1676   0.6701        0.2499       0.9250        0.2755        18.8892
     24   0.1659   0.6768        0.2468       0.9250        0.2728        19.1640
     25   0.1553   0.6751        0.2501       0.9250        0.2811        19.0087
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 09:04:17,499][0m Trial 589 finished with value: 0.24186122257987705 and parameters: {'lr': 0.0014498720783712887, 'dropout': 0.4244834123408211, 'd_model_multiplier': 1, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 395, 'batch_size': 39, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0, 'top_n_features': 164}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 65
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2441[0m   [32m0.7452[0m        [35m0.2692[0m       [31m0.9093[0m        [94m0.2629[0m     +  10.4386
      2   0.2165   [32m0.7546[0m        [35m0.2488[0m       0.9069        [94m0.2624[0m     +  10.4408
      3   0.2387   0.7518        [35m0.2446[0m       0.9093        0.2733        10.6848
      4   0.2438   [32m0.7640[0m        [35m0.2419[0m       0.8972        0.2785        10.7643
      5   [36m0.2605[0m   0.7602        0.2436       0.9008        0.2731        10.5925
      6   0.2446   0.7562        [35m0.2398[0m       0.8912        0.2864        10.7318
      7   0.2481   0.7634        [35m0.2388[0m       0.8875        0.2814        10.5505
      8   0.2522   [32m0.7649[0m        [35m0.2378[0m       0.8670        0.3127        10.6980
      9   [36m0.2618[0m   [32m0.7676[0m        0.2419       0.8875        0.2835        10.7563
     10   [36m0.2990[0m   0.7635        [35m0.2350[0m       0.9093        [94m0.2564[0m     +  10.7100
     11   0.2563   0.7657        0.2405       0.8996        0.2682        10.6739
     12   0.2389   0.7542        0.2386       0.8779        0.3096        10.7381
     13   0.2312   0.7654        0.2364       0.8779        0.2981        10.7445
     14   0.2818   0.7434        0.2373       [31m0.9166[0m        0.2593        10.6635
     15   0.2723   0.7596        0.2371       0.9069        0.2631        10.7603
     16   0.2389   0.7116        0.2365       0.9045        0.2770        10.6228
     17   0.2582   0.7461        0.2396       0.8984        0.2790        10.7825
     18   0.2576   0.7249        0.2377       0.8658        0.2988        10.6239
     19   0.2248   0.7226        0.2393       0.9008        0.2764        10.8579
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 09:07:52,023][0m Trial 590 finished with value: 0.2563927589837989 and parameters: {'lr': 0.00403462183369198, 'dropout': 0.4340606862277074, 'd_model_multiplier': 1, 'num_layers': 2, 'n_heads': 32, 'dim_feedforward': 387, 'batch_size': 36, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0, 'top_n_features': 65}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 73
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2294[0m   [32m0.7576[0m        [35m0.2824[0m       [31m0.9299[0m        [94m0.2227[0m     +  15.8296
      2   [36m0.2596[0m   [32m0.7653[0m        [35m0.2463[0m       [31m0.9311[0m        [94m0.2202[0m     +  16.1326
      3   [36m0.2808[0m   [32m0.7769[0m        [35m0.2462[0m       [31m0.9323[0m        [94m0.2169[0m     +  16.0586
      4   0.2776   0.7744        [35m0.2420[0m       0.9311        0.2235        16.1483
      5   [36m0.2825[0m   [32m0.7853[0m        [35m0.2414[0m       0.9287        0.2277        16.1091
      6   0.2495   0.7705        [35m0.2373[0m       0.9274        0.2385        16.1030
      7   [36m0.2928[0m   0.7748        0.2373       0.9287        0.2225        16.1237
      8   0.2786   [32m0.7888[0m        [35m0.2358[0m       0.9287        0.2197        16.0881
      9   0.2580   0.7041        0.2424       0.9299        0.2668        16.1799
     10   0.1960   0.6697        0.2425       0.9299        0.2884        16.0767
     11   0.1957   0.6683        0.2541       0.9299        0.2685        15.9418
     12   0.2040   0.6658        0.2545       0.9299        0.2619        16.0725
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 09:11:21,449][0m Trial 591 finished with value: 0.21689640066643867 and parameters: {'lr': 0.002821477238192501, 'dropout': 0.4056643872014219, 'd_model_multiplier': 1, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 397, 'batch_size': 40, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0, 'top_n_features': 73}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 46
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.3230[0m   [32m0.8248[0m        [35m0.2898[0m       [31m0.9262[0m        [94m0.2079[0m     +  18.7136
      2   [36m0.3304[0m   [32m0.8293[0m        [35m0.2467[0m       0.9226        0.2108        18.9394
      3   0.3184   0.8216        [35m0.2416[0m       0.9226        0.2135        18.9901
      4   0.3277   0.8238        [35m0.2404[0m       0.9226        0.2144        18.9377
      5   0.3260   0.8216        [35m0.2387[0m       0.9250        0.2198        18.8945
      6   [36m0.3375[0m   0.8216        [35m0.2365[0m       [31m0.9287[0m        0.2320        18.7518
      7   0.3206   0.8203        0.2393       0.9238        0.2322        18.8722
      8   [36m0.3448[0m   0.8200        0.2387       0.9287        0.2256        18.9920
      9   0.2771   0.8118        0.2402       0.9250        0.2368        19.0078
     10   0.3156   0.8204        0.2407       0.9274        0.2525        18.8751
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 09:14:49,669][0m Trial 592 finished with value: 0.2078768191239746 and parameters: {'lr': 0.001329527494995536, 'dropout': 0.4536060784725318, 'd_model_multiplier': 1, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 400, 'batch_size': 30, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0, 'top_n_features': 46}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 200
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2829[0m   [32m0.7695[0m        [35m0.3091[0m       [31m0.9287[0m        [94m0.2165[0m     +  13.5361
      2   [36m0.3124[0m   [32m0.8034[0m        [35m0.2588[0m       0.9287        [94m0.2097[0m     +  13.7094
      3   [36m0.3182[0m   [32m0.8158[0m        [35m0.2514[0m       [31m0.9299[0m        [94m0.2068[0m     +  13.7448
      4   [36m0.3253[0m   [32m0.8183[0m        [35m0.2486[0m       0.9287        [94m0.2051[0m     +  13.7576
      5   0.3172   0.8013        [35m0.2470[0m       [31m0.9311[0m        0.2161        13.8076
      6   0.3094   0.7956        [35m0.2435[0m       0.9299        0.2285        14.1646
      7   0.2917   0.7948        0.2447       0.9311        0.2309        13.4497
      8   0.3099   0.8099        [35m0.2406[0m       0.9262        0.2256        13.7473
      9   [36m0.3553[0m   0.8058        0.2437       0.9287        0.2199        13.5705
     10   0.3543   0.7956        0.2410       0.9274        0.2287        13.5651
     11   0.3400   0.7762        0.2423       0.9129        0.2472        13.7009
     12   [36m0.3563[0m   0.7888        0.2444       0.9093        0.2565        13.7514
     13   0.3547   0.8149        0.2447       0.9226        0.2207        13.7410
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 09:18:01,985][0m Trial 593 finished with value: 0.20513906132569482 and parameters: {'lr': 0.0017494936614377734, 'dropout': 0.43837775394163037, 'd_model_multiplier': 1, 'num_layers': 3, 'n_heads': 32, 'dim_feedforward': 410, 'batch_size': 42, 'pos_encoding': 'learnable', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0, 'top_n_features': 200}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 54
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1320[0m   [32m0.6727[0m        [35m0.2872[0m       [31m0.9347[0m        [94m0.2336[0m     +  38.1543
      2   [36m0.1337[0m   [32m0.6784[0m        [35m0.2644[0m       0.9347        0.2439        38.1140
      3   [36m0.1355[0m   0.6763        [35m0.2644[0m       0.9335        0.2359        38.2513
      4   [36m0.1563[0m   [32m0.6818[0m        [35m0.2598[0m       0.9347        0.2370        38.4599
      5   0.1423   0.6737        [35m0.2592[0m       0.9347        0.2397        38.3439
      6   0.1399   0.6779        [35m0.2570[0m       0.9347        0.2377        38.4621
      7   0.1376   0.6742        0.2594       0.9347        0.2374        38.4198
      8   0.1449   [32m0.6820[0m        [35m0.2568[0m       0.9347        0.2408        38.4050
      9   0.1341   [32m0.6857[0m        0.2582       0.9287        0.2441        38.4570
     10   0.1343   0.6812        0.2570       0.9335        0.2456        38.4824
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 09:25:04,313][0m Trial 594 finished with value: 0.23362014040422988 and parameters: {'lr': 0.0023641730555649066, 'dropout': 0.4166022037057309, 'd_model_multiplier': 1, 'num_layers': 12, 'n_heads': 32, 'dim_feedforward': 404, 'batch_size': 33, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0, 'top_n_features': 54}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 38
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0733[0m   [32m0.3635[0m        [35m0.5140[0m       [31m0.9141[0m        [94m0.3495[0m     +  16.0038
      2   [36m0.2211[0m   [32m0.7058[0m        [35m0.2940[0m       0.9141        [94m0.2704[0m     +  15.8238
      3   [36m0.3551[0m   [32m0.7862[0m        [35m0.2510[0m       [31m0.9154[0m        [94m0.2415[0m     +  16.2368
      4   [36m0.3910[0m   [32m0.7995[0m        [35m0.2398[0m       [31m0.9166[0m        [94m0.2352[0m     +  16.1374
      5   [36m0.3990[0m   [32m0.8034[0m        [35m0.2342[0m       0.9154        [94m0.2342[0m     +  16.2647
      6   [36m0.4095[0m   [32m0.8075[0m        [35m0.2321[0m       [31m0.9178[0m        [94m0.2328[0m     +  16.3554
      7   [36m0.4151[0m   [32m0.8085[0m        0.2334       0.9178        [94m0.2324[0m     +  16.1987
      8   [36m0.4166[0m   [32m0.8088[0m        [35m0.2318[0m       [31m0.9190[0m        [94m0.2322[0m     +  16.0825
      9   [36m0.4317[0m   [32m0.8141[0m        [35m0.2308[0m       0.9178        [94m0.2319[0m     +  16.0679
     10   [36m0.4391[0m   [32m0.8179[0m        [35m0.2297[0m       [31m0.9202[0m        [94m0.2281[0m     +  16.0529
     11   [36m0.4394[0m   [32m0.8193[0m        0.2308       [31m0.9214[0m        [94m0.2271[0m     +  16.2360
     12   [36m0.4464[0m   [32m0.8214[0m        0.2304       0.9190        0.2292        16.0855
     13   0.4360   [32m0.8227[0m        [35m0.2270[0m       0.9214        [94m0.2270[0m     +  16.1872
     14   0.4441   [32m0.8255[0m        0.2293       0.9214        [94m0.2237[0m     +  16.1043
     15   0.4413   0.8243        0.2278       0.9214        0.2265        16.1119
     16   0.4425   0.8244        [35m0.2263[0m       0.9202        0.2259        16.2693
     17   0.4394   [32m0.8261[0m        [35m0.2261[0m       [31m0.9226[0m        0.2248        16.2343
     18   0.4400   [32m0.8264[0m        0.2269       0.9226        0.2245        16.1983
     19   0.4388   [32m0.8274[0m        [35m0.2257[0m       0.9226        [94m0.2236[0m     +  16.3495
     20   0.4423   [32m0.8297[0m        [35m0.2236[0m       [31m0.9250[0m        [94m0.2221[0m     +  16.1537
     21   [36m0.4647[0m   [32m0.8325[0m        0.2252       0.9238        [94m0.2216[0m     +  16.1441
     22   0.4396   0.8321        0.2248       0.9202        0.2233        16.2147
     23   0.4340   0.8320        [35m0.2211[0m       0.9214        0.2242        15.9675
     24   0.4376   [32m0.8346[0m        0.2230       0.9214        0.2226        16.0888
     25   0.4460   [32m0.8355[0m        [35m0.2200[0m       0.9214        0.2225        16.1714
     26   0.4482   [32m0.8397[0m        0.2210       0.9226        [94m0.2205[0m     +  16.2336
     27   0.4473   [32m0.8402[0m        0.2207       0.9214        [94m0.2202[0m     +  16.2824
     28   0.4494   0.8401        0.2214       0.9250        [94m0.2185[0m     +  16.1493
     29   0.4594   0.8388        0.2204       [31m0.9262[0m        [94m0.2183[0m     +  16.0075
     30   0.4562   [32m0.8412[0m        0.2203       0.9262        [94m0.2173[0m     +  16.2012
     31   0.4622   [32m0.8428[0m        [35m0.2188[0m       0.9250        [94m0.2169[0m     +  16.0872
     32   0.4571   0.8421        [35m0.2161[0m       [31m0.9287[0m        0.2183        16.0026
     33   0.4504   [32m0.8438[0m        0.2170       0.9262        0.2178        16.3052
     34   0.4540   0.8425        0.2180       0.9238        0.2191        16.2584
     35   0.4612   0.8429        0.2169       0.9238        0.2187        16.0901
     36   0.4409   [32m0.8444[0m        [35m0.2155[0m       0.9274        0.2179        16.0675
     37   0.4515   [32m0.8471[0m        0.2159       0.9274        [94m0.2158[0m     +  16.1669
     38   0.4545   0.8458        0.2168       0.9262        0.2171        16.1767
     39   [36m0.4664[0m   0.8441        0.2160       0.9274        0.2187        16.2406
     40   0.4557   0.8459        [35m0.2136[0m       0.9262        0.2171        16.0779
     41   0.4528   0.8456        0.2146       0.9238        0.2185        16.2603
     42   0.4466   0.8452        [35m0.2127[0m       0.9250        0.2190        16.2496
     43   0.4595   [32m0.8476[0m        0.2141       0.9262        0.2171        16.3348
     44   [36m0.4718[0m   0.8473        0.2134       0.9274        [94m0.2154[0m     +  16.1850
     45   0.4605   [32m0.8478[0m        [35m0.2099[0m       0.9274        0.2164        16.1484
     46   0.4486   0.8456        [35m0.2097[0m       0.9250        0.2187        15.9705
     47   0.4432   0.8461        0.2104       0.9238        0.2191        16.4438
     48   0.4422   0.8468        [35m0.2094[0m       0.9250        0.2190        16.0089
     49   0.4298   0.8451        [35m0.2094[0m       0.9214        0.2205        16.1605
     50   0.4495   [32m0.8481[0m        [35m0.2065[0m       0.9238        0.2183        16.2172
[32m[I 2023-05-05 09:38:34,311][0m Trial 595 finished with value: 0.2153934873955858 and parameters: {'lr': 0.00012983013450445358, 'dropout': 0.44640411779742956, 'd_model_multiplier': 1, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 389, 'batch_size': 47, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0, 'top_n_features': 38}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 97
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0697[0m   [32m0.3693[0m        [35m0.5007[0m       [31m0.9141[0m        [94m0.3409[0m     +  18.5552
      2   [36m0.1881[0m   [32m0.6310[0m        [35m0.2867[0m       0.9141        [94m0.2808[0m     +  18.8448
      3   [36m0.2182[0m   [32m0.6861[0m        [35m0.2555[0m       0.9141        [94m0.2700[0m     +  18.9719
      4   [36m0.2583[0m   [32m0.7154[0m        [35m0.2477[0m       0.9129        [94m0.2646[0m     +  19.0227
      5   [36m0.2725[0m   [32m0.7161[0m        [35m0.2379[0m       [31m0.9154[0m        0.2673        19.0024
      6   [36m0.2887[0m   [32m0.7230[0m        [35m0.2362[0m       0.9154        0.2655        19.1144
      7   [36m0.3037[0m   [32m0.7322[0m        [35m0.2339[0m       [31m0.9166[0m        [94m0.2637[0m     +  19.0458
      8   [36m0.3081[0m   [32m0.7373[0m        [35m0.2303[0m       [31m0.9178[0m        0.2651        18.8715
      9   [36m0.3242[0m   [32m0.7426[0m        [35m0.2293[0m       [31m0.9214[0m        0.2639        19.0223
     10   [36m0.3262[0m   [32m0.7442[0m        0.2298       [31m0.9226[0m        [94m0.2637[0m     +  18.8160
     11   [36m0.3342[0m   [32m0.7491[0m        [35m0.2271[0m       0.9202        0.2651        19.0057
     12   0.3302   [32m0.7492[0m        [35m0.2269[0m       0.9202        0.2663        19.0496
     13   [36m0.3415[0m   [32m0.7551[0m        [35m0.2257[0m       0.9214        [94m0.2630[0m     +  18.9271
     14   [36m0.3439[0m   [32m0.7582[0m        [35m0.2237[0m       0.9214        [94m0.2621[0m     +  19.0171
     15   0.3404   [32m0.7602[0m        [35m0.2236[0m       0.9214        [94m0.2614[0m     +  19.0552
     16   [36m0.3488[0m   [32m0.7622[0m        [35m0.2229[0m       0.9214        [94m0.2609[0m     +  19.0124
     17   [36m0.3687[0m   [32m0.7663[0m        [35m0.2228[0m       0.9214        0.2612        18.9088
     18   0.3648   [32m0.7675[0m        [35m0.2199[0m       0.9226        [94m0.2608[0m     +  18.9270
     19   0.3620   [32m0.7694[0m        [35m0.2191[0m       0.9226        0.2636        19.0556
     20   [36m0.3745[0m   [32m0.7730[0m        [35m0.2187[0m       0.9226        0.2620        18.8831
     21   0.3666   0.7729        [35m0.2176[0m       0.9214        [94m0.2608[0m     +  18.9539
     22   [36m0.3775[0m   [32m0.7769[0m        [35m0.2154[0m       0.9214        [94m0.2574[0m     +  18.9149
     23   0.3670   0.7767        [35m0.2141[0m       0.9226        0.2588        19.5418
     24   0.3582   0.7740        0.2166       0.9190        0.2606        19.2631
     25   0.3585   [32m0.7787[0m        0.2145       0.9202        0.2598        18.7957
     26   0.3571   [32m0.7814[0m        [35m0.2125[0m       0.9190        [94m0.2561[0m     +  18.9906
     27   0.3642   [32m0.7839[0m        [35m0.2118[0m       0.9190        0.2574        18.9590
     28   0.3589   0.7837        [35m0.2106[0m       0.9202        0.2571        19.0922
     29   0.3505   0.7818        [35m0.2094[0m       0.9178        0.2616        18.9681
     30   0.3424   [32m0.7901[0m        [35m0.2078[0m       0.9202        0.2596        18.9536
     31   0.3517   0.7850        0.2082       0.9190        0.2613        18.9976
     32   0.3471   0.7870        [35m0.2045[0m       0.9190        0.2621        19.1865
     33   0.3477   0.7856        0.2050       0.9178        0.2620        18.9031
     34   0.3484   [32m0.7938[0m        [35m0.2034[0m       0.9178        0.2601        19.0345
     35   0.3401   0.7904        0.2039       0.9166        0.2607        19.1841
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 09:49:59,544][0m Trial 596 finished with value: 0.2561362267189187 and parameters: {'lr': 7.89786031955118e-05, 'dropout': 0.3808321915564501, 'd_model_multiplier': 1, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 405, 'batch_size': 34, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0, 'top_n_features': 97}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 52
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.0779[0m   [32m0.3342[0m        [35m0.6695[0m       [31m0.9105[0m        [94m0.5921[0m     +  8.6795
      2   [36m0.0798[0m   [32m0.3439[0m        [35m0.5295[0m       0.9105        [94m0.4535[0m     +  8.8083
      3   [36m0.0843[0m   [32m0.3742[0m        [35m0.4169[0m       0.9105        [94m0.3712[0m     +  9.5473
      4   [36m0.0983[0m   [32m0.4817[0m        [35m0.3524[0m       0.9105        [94m0.3252[0m     +  9.6919
      5   [36m0.1368[0m   [32m0.6087[0m        [35m0.3112[0m       0.9105        [94m0.2988[0m     +  9.5181
      6   [36m0.1894[0m   [32m0.6840[0m        [35m0.2866[0m       0.9105        [94m0.2844[0m     +  9.5153
      7   [36m0.2109[0m   [32m0.7231[0m        [35m0.2726[0m       0.9105        [94m0.2767[0m     +  9.5149
      8   [36m0.2275[0m   [32m0.7471[0m        [35m0.2626[0m       0.9105        [94m0.2720[0m     +  9.6536
      9   [36m0.2405[0m   [32m0.7602[0m        [35m0.2561[0m       0.9105        [94m0.2695[0m     +  9.7305
     10   [36m0.2445[0m   [32m0.7659[0m        [35m0.2529[0m       0.9105        [94m0.2679[0m     +  9.8390
     11   [36m0.2465[0m   [32m0.7691[0m        [35m0.2511[0m       0.9105        [94m0.2670[0m     +  9.4238
     12   [36m0.2539[0m   [32m0.7717[0m        [35m0.2480[0m       0.9105        [94m0.2657[0m     +  9.7638
     13   [36m0.2586[0m   [32m0.7730[0m        [35m0.2450[0m       0.9105        [94m0.2648[0m     +  9.4387
     14   [36m0.2684[0m   [32m0.7757[0m        [35m0.2432[0m       0.9105        [94m0.2635[0m     +  9.5341
     15   [36m0.2796[0m   [32m0.7785[0m        [35m0.2425[0m       0.9105        [94m0.2624[0m     +  9.6468
     16   [36m0.2895[0m   [32m0.7808[0m        [35m0.2410[0m       0.9105        [94m0.2615[0m     +  9.2953
     17   [36m0.2963[0m   [32m0.7836[0m        [35m0.2388[0m       0.9105        [94m0.2601[0m     +  9.3740
     18   [36m0.3023[0m   [32m0.7861[0m        [35m0.2372[0m       0.9105        [94m0.2586[0m     +  9.3383
     19   [36m0.3096[0m   [32m0.7893[0m        0.2376       0.9093        [94m0.2577[0m     +  9.4546
     20   [36m0.3162[0m   [32m0.7910[0m        [35m0.2367[0m       0.9093        [94m0.2564[0m     +  9.4503
     21   [36m0.3229[0m   [32m0.7928[0m        [35m0.2351[0m       0.9105        [94m0.2557[0m     +  9.3903
     22   [36m0.3294[0m   [32m0.7946[0m        0.2351       0.9105        [94m0.2553[0m     +  9.3640
     23   [36m0.3333[0m   [32m0.7951[0m        [35m0.2343[0m       0.9105        [94m0.2547[0m     +  9.8896
     24   [36m0.3369[0m   [32m0.7954[0m        [35m0.2330[0m       [31m0.9117[0m        [94m0.2540[0m     +  9.0345
     25   [36m0.3417[0m   [32m0.7964[0m        [35m0.2329[0m       0.9117        0.2542        9.2389
     26   [36m0.3458[0m   [32m0.7966[0m        [35m0.2328[0m       [31m0.9129[0m        [94m0.2538[0m     +  9.4753
     27   0.3456   [32m0.7969[0m        [35m0.2327[0m       [31m0.9141[0m        [94m0.2536[0m     +  9.0230
     28   [36m0.3478[0m   [32m0.7974[0m        0.2337       [31m0.9154[0m        [94m0.2533[0m     +  9.1735
     29   [36m0.3479[0m   0.7973        [35m0.2323[0m       0.9154        0.2533        9.4673
     30   [36m0.3487[0m   0.7973        [35m0.2304[0m       0.9154        0.2534        9.2523
     31   [36m0.3495[0m   [32m0.7975[0m        0.2315       [31m0.9166[0m        [94m0.2532[0m     +  9.4236
     32   0.3493   0.7974        0.2312       0.9166        [94m0.2531[0m     +  9.7444
     33   [36m0.3497[0m   0.7972        0.2314       0.9166        0.2532        9.3197
     34   [36m0.3498[0m   0.7972        0.2313       0.9154        0.2532        9.5084
     35   [36m0.3502[0m   [32m0.7978[0m        [35m0.2304[0m       0.9141        [94m0.2528[0m     +  9.4231
     36   [36m0.3517[0m   [32m0.7979[0m        [35m0.2293[0m       0.9141        [94m0.2527[0m     +  9.7153
     37   [36m0.3524[0m   [32m0.7981[0m        [35m0.2293[0m       0.9141        0.2527        9.4921
     38   [36m0.3537[0m   0.7979        0.2300       0.9141        0.2527        9.7231
     39   [36m0.3550[0m   [32m0.7984[0m        0.2299       0.9141        0.2528        9.7026
     40   0.3546   0.7984        0.2296       0.9154        [94m0.2527[0m     +  9.5309
     41   [36m0.3553[0m   0.7983        0.2294       0.9154        [94m0.2525[0m     +  9.7296
     42   [36m0.3563[0m   [32m0.7988[0m        [35m0.2278[0m       0.9154        0.2525        9.4902
     43   [36m0.3572[0m   0.7987        0.2292       0.9166        [94m0.2518[0m     +  9.5716
     44   [36m0.3579[0m   [32m0.7990[0m        [35m0.2277[0m       0.9166        [94m0.2518[0m     +  9.3806
     45   [36m0.3593[0m   [32m0.8000[0m        0.2282       0.9166        0.2519        9.4776
     46   [36m0.3607[0m   [32m0.8001[0m        0.2285       0.9166        0.2520        9.2950
     47   [36m0.3608[0m   [32m0.8005[0m        0.2284       0.9166        [94m0.2517[0m     +  9.4088
     48   0.3607   0.7999        0.2298       0.9166        [94m0.2515[0m     +  9.5637
     49   0.3607   [32m0.8006[0m        0.2281       0.9166        0.2518        9.3792
     50   [36m0.3614[0m   [32m0.8009[0m        0.2286       0.9166        [94m0.2511[0m     +  9.6700
[32m[I 2023-05-05 09:57:54,931][0m Trial 597 finished with value: 0.2511341387750592 and parameters: {'lr': 2.539530971074302e-05, 'dropout': 0.40898816678289085, 'd_model_multiplier': 1, 'num_layers': 1, 'n_heads': 32, 'dim_feedforward': 223, 'batch_size': 45, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0, 'top_n_features': 52}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 66
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0666[0m   [32m0.4228[0m        [35m0.6427[0m       [31m0.8005[0m        [94m0.6476[0m     +  31.0106
      2   0.0656   0.4191        [35m0.6419[0m       [31m0.8126[0m        [94m0.6455[0m     +  31.2136
      3   0.0649   0.4150        [35m0.6397[0m       [31m0.8247[0m        [94m0.6427[0m     +  31.3285
      4   0.0640   0.4104        [35m0.6379[0m       [31m0.8392[0m        [94m0.6396[0m     +  31.1441
      5   0.0632   0.4051        [35m0.6343[0m       [31m0.8525[0m        [94m0.6360[0m     +  31.3416
      6   0.0625   0.3990        [35m0.6313[0m       [31m0.8646[0m        [94m0.6322[0m     +  31.4187
      7   0.0618   0.3932        [35m0.6288[0m       [31m0.8815[0m        [94m0.6282[0m     +  31.4894
      8   0.0612   0.3876        [35m0.6256[0m       [31m0.8888[0m        [94m0.6239[0m     +  31.2431
      9   0.0606   0.3817        [35m0.6198[0m       [31m0.9008[0m        [94m0.6196[0m     +  31.3565
     10   0.0601   0.3759        [35m0.6160[0m       [31m0.9093[0m        [94m0.6151[0m     +  31.2924
     11   0.0597   0.3717        [35m0.6135[0m       [31m0.9129[0m        [94m0.6105[0m     +  31.3118
     12   0.0595   0.3680        [35m0.6100[0m       [31m0.9154[0m        [94m0.6058[0m     +  31.4042
     13   0.0592   0.3649        [35m0.6051[0m       [31m0.9166[0m        [94m0.6011[0m     +  31.3623
     14   0.0592   0.3626        [35m0.6006[0m       [31m0.9178[0m        [94m0.5963[0m     +  31.4118
     15   0.0590   0.3605        [35m0.5959[0m       [31m0.9190[0m        [94m0.5915[0m     +  31.3026
     16   0.0589   0.3577        [35m0.5935[0m       [31m0.9214[0m        [94m0.5867[0m     +  31.3251
     17   0.0588   0.3557        [35m0.5882[0m       0.9214        [94m0.5820[0m     +  31.3480
     18   0.0586   0.3533        [35m0.5840[0m       0.9214        [94m0.5772[0m     +  31.4789
     19   0.0585   0.3512        [35m0.5799[0m       0.9214        [94m0.5724[0m     +  31.4780
     20   0.0585   0.3496        [35m0.5748[0m       0.9214        [94m0.5677[0m     +  31.4004
     21   0.0584   0.3480        [35m0.5711[0m       0.9214        [94m0.5630[0m     +  31.3373
     22   0.0584   0.3472        [35m0.5671[0m       0.9214        [94m0.5583[0m     +  31.1814
     23   0.0585   0.3461        [35m0.5642[0m       0.9214        [94m0.5537[0m     +  31.2474
     24   0.0585   0.3453        [35m0.5601[0m       0.9214        [94m0.5491[0m     +  31.3493
     25   0.0585   0.3449        [35m0.5569[0m       0.9214        [94m0.5446[0m     +  31.1699
     26   0.0586   0.3446        [35m0.5511[0m       0.9214        [94m0.5401[0m     +  31.2876
     27   0.0588   0.3446        [35m0.5499[0m       0.9214        [94m0.5357[0m     +  31.3595
     28   0.0591   0.3445        [35m0.5455[0m       0.9214        [94m0.5314[0m     +  31.3613
     29   0.0593   0.3446        [35m0.5411[0m       0.9214        [94m0.5271[0m     +  31.3278
     30   0.0594   0.3442        [35m0.5353[0m       0.9214        [94m0.5229[0m     +  31.4768
     31   0.0594   0.3442        [35m0.5330[0m       0.9214        [94m0.5188[0m     +  31.3095
     32   0.0596   0.3441        [35m0.5290[0m       0.9214        [94m0.5147[0m     +  31.0299
     33   0.0598   0.3444        [35m0.5246[0m       0.9214        [94m0.5107[0m     +  32.0619
     34   0.0599   0.3445        [35m0.5214[0m       0.9214        [94m0.5067[0m     +  31.4596
     35   0.0599   0.3447        [35m0.5187[0m       0.9214        [94m0.5028[0m     +  31.2821
     36   0.0600   0.3450        [35m0.5158[0m       0.9214        [94m0.4990[0m     +  31.2485
     37   0.0601   0.3452        [35m0.5107[0m       0.9214        [94m0.4953[0m     +  31.3830
     38   0.0601   0.3453        [35m0.5101[0m       0.9214        [94m0.4915[0m     +  31.3340
     39   0.0603   0.3455        [35m0.5060[0m       0.9214        [94m0.4879[0m     +  31.3237
     40   0.0606   0.3460        [35m0.5030[0m       0.9214        [94m0.4844[0m     +  31.2052
     41   0.0606   0.3461        [35m0.4986[0m       0.9214        [94m0.4809[0m     +  31.2265
     42   0.0607   0.3461        [35m0.4964[0m       0.9214        [94m0.4774[0m     +  31.2516
     43   0.0607   0.3463        [35m0.4926[0m       0.9214        [94m0.4740[0m     +  31.2534
     44   0.0608   0.3464        [35m0.4897[0m       0.9214        [94m0.4708[0m     +  31.3172
     45   0.0609   0.3467        [35m0.4862[0m       0.9214        [94m0.4675[0m     +  31.3157
     46   0.0610   0.3471        [35m0.4837[0m       0.9214        [94m0.4643[0m     +  31.3713
     47   0.0612   0.3474        [35m0.4805[0m       0.9214        [94m0.4612[0m     +  31.1735
     48   0.0612   0.3474        [35m0.4781[0m       0.9214        [94m0.4581[0m     +  31.2911
     49   0.0615   0.3478        [35m0.4758[0m       0.9214        [94m0.4551[0m     +  31.4550
     50   0.0619   0.3483        [35m0.4737[0m       0.9214        [94m0.4522[0m     +  31.2602
[32m[I 2023-05-05 10:24:08,679][0m Trial 598 finished with value: 0.4521695099401647 and parameters: {'lr': 3.8295753291295734e-08, 'dropout': 0.4716408024042566, 'd_model_multiplier': 4, 'num_layers': 7, 'n_heads': 32, 'dim_feedforward': 382, 'batch_size': 42, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0, 'top_n_features': 66}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 79
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1623[0m   [32m0.6694[0m        [35m0.3187[0m       [31m0.9238[0m        [94m0.2541[0m     +  26.9160
      2   [36m0.2256[0m   [32m0.7413[0m        [35m0.2467[0m       0.9238        [94m0.2432[0m     +  26.9822
      3   [36m0.2592[0m   [32m0.7730[0m        0.2470       0.9214        [94m0.2326[0m     +  26.9540
      4   0.2502   0.7478        [35m0.2412[0m       0.9214        0.2429        26.9382
      5   0.2464   0.7600        [35m0.2405[0m       0.9214        0.2399        26.8986
      6   [36m0.2757[0m   0.7724        [35m0.2366[0m       0.9226        0.2349        26.8123
      7   0.2438   [32m0.7796[0m        0.2372       [31m0.9250[0m        0.2446        27.1763
      8   0.2582   0.7529        [35m0.2365[0m       0.9250        0.2444        27.0027
      9   0.1767   0.7008        0.2395       0.9226        0.2716        27.0177
     10   [36m0.2847[0m   0.7723        [35m0.2351[0m       0.9166        0.2493        27.0664
     11   0.2623   0.7631        0.2354       0.9141        0.2530        27.2167
     12   0.2591   0.7510        [35m0.2328[0m       0.9166        0.2542        27.2806
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 10:30:00,677][0m Trial 599 finished with value: 0.23263148259835492 and parameters: {'lr': 0.0012219456520828052, 'dropout': 0.42630745869930864, 'd_model_multiplier': 1, 'num_layers': 8, 'n_heads': 32, 'dim_feedforward': 368, 'batch_size': 47, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 79}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 104
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0770[0m   [32m0.3847[0m        [35m0.6800[0m       [31m0.7570[0m        [94m0.6696[0m     +  19.9650
      2   0.0768   0.3837        0.6801       [31m0.7594[0m        [94m0.6689[0m     +  20.2765
      3   0.0765   0.3824        [35m0.6781[0m       [31m0.7630[0m        [94m0.6681[0m     +  20.5223
      4   0.0762   0.3804        0.6787       [31m0.7703[0m        [94m0.6671[0m     +  20.3824
      5   0.0759   0.3789        [35m0.6770[0m       [31m0.7787[0m        [94m0.6659[0m     +  20.3466
      6   0.0755   0.3767        [35m0.6765[0m       [31m0.7823[0m        [94m0.6647[0m     +  20.1949
      7   0.0751   0.3742        [35m0.6737[0m       [31m0.7896[0m        [94m0.6634[0m     +  20.2263
      8   0.0749   0.3723        0.6751       [31m0.7969[0m        [94m0.6621[0m     +  20.3021
      9   [36m0.0770[0m   0.3697        [35m0.6710[0m       [31m0.8114[0m        [94m0.6607[0m     +  20.2442
     10   0.0769   0.3675        [35m0.6692[0m       [31m0.8235[0m        [94m0.6593[0m     +  20.3633
     11   0.0766   0.3653        0.6695       [31m0.8343[0m        [94m0.6578[0m     +  20.3520
     12   0.0762   0.3627        [35m0.6677[0m       [31m0.8404[0m        [94m0.6563[0m     +  20.2583
     13   0.0760   0.3607        [35m0.6669[0m       [31m0.8464[0m        [94m0.6548[0m     +  20.2306
     14   0.0758   0.3582        [35m0.6654[0m       [31m0.8501[0m        [94m0.6532[0m     +  20.3896
     15   0.0758   0.3563        [35m0.6633[0m       [31m0.8634[0m        [94m0.6517[0m     +  20.2239
     16   0.0757   0.3541        [35m0.6631[0m       [31m0.8694[0m        [94m0.6501[0m     +  20.3362
     17   0.0757   0.3524        [35m0.6600[0m       [31m0.8779[0m        [94m0.6485[0m     +  20.4098
     18   0.0755   0.3505        [35m0.6575[0m       [31m0.8827[0m        [94m0.6469[0m     +  20.4187
     19   0.0755   0.3494        0.6578       [31m0.8900[0m        [94m0.6453[0m     +  20.3714
     20   0.0753   0.3479        [35m0.6557[0m       [31m0.8960[0m        [94m0.6437[0m     +  20.2831
     21   0.0756   0.3470        [35m0.6543[0m       0.8960        [94m0.6421[0m     +  20.3549
     22   0.0754   0.3455        [35m0.6515[0m       [31m0.8984[0m        [94m0.6405[0m     +  20.1657
     23   0.0752   0.3445        0.6519       [31m0.8996[0m        [94m0.6388[0m     +  20.1353
     24   0.0758   0.3435        [35m0.6479[0m       [31m0.9033[0m        [94m0.6372[0m     +  20.2158
     25   0.0758   0.3420        0.6479       [31m0.9069[0m        [94m0.6356[0m     +  20.4134
     26   0.0769   0.3413        [35m0.6469[0m       [31m0.9081[0m        [94m0.6340[0m     +  20.2304
     27   0.0748   0.3404        [35m0.6443[0m       [31m0.9105[0m        [94m0.6324[0m     +  20.4885
     28   0.0735   0.3395        [35m0.6442[0m       [31m0.9117[0m        [94m0.6308[0m     +  20.3524
     29   0.0733   0.3386        [35m0.6418[0m       0.9117        [94m0.6292[0m     +  20.4440
     30   0.0736   0.3377        [35m0.6397[0m       0.9117        [94m0.6277[0m     +  20.2426
     31   0.0732   0.3374        [35m0.6390[0m       0.9117        [94m0.6261[0m     +  20.2986
     32   0.0738   0.3367        [35m0.6366[0m       0.9117        [94m0.6245[0m     +  20.3505
     33   0.0743   0.3356        [35m0.6361[0m       0.9117        [94m0.6229[0m     +  20.4731
     34   0.0753   0.3349        [35m0.6334[0m       [31m0.9129[0m        [94m0.6214[0m     +  20.2822
     35   0.0752   0.3341        0.6339       0.9129        [94m0.6198[0m     +  20.4677
     36   0.0741   0.3337        [35m0.6301[0m       0.9129        [94m0.6183[0m     +  20.1918
     37   0.0727   0.3331        [35m0.6288[0m       0.9129        [94m0.6167[0m     +  20.3305
     38   0.0724   0.3331        [35m0.6275[0m       0.9129        [94m0.6152[0m     +  20.3628
     39   0.0722   0.3330        [35m0.6265[0m       0.9129        [94m0.6136[0m     +  20.1901
     40   0.0721   0.3328        [35m0.6244[0m       0.9129        [94m0.6121[0m     +  20.1952
     41   0.0744   0.3325        [35m0.6227[0m       0.9129        [94m0.6106[0m     +  20.0885
     42   0.0742   0.3320        [35m0.6218[0m       0.9129        [94m0.6091[0m     +  20.3380
     43   [36m0.0810[0m   0.3318        [35m0.6192[0m       0.9129        [94m0.6076[0m     +  20.2961
     44   0.0810   0.3318        0.6195       0.9129        [94m0.6061[0m     +  20.2273
     45   0.0808   0.3315        [35m0.6169[0m       0.9129        [94m0.6046[0m     +  20.3643
     46   [36m0.0814[0m   0.3317        [35m0.6154[0m       0.9129        [94m0.6031[0m     +  20.5357
     47   0.0814   0.3317        [35m0.6140[0m       0.9129        [94m0.6016[0m     +  20.4015
     48   0.0813   0.3314        [35m0.6126[0m       0.9129        [94m0.6002[0m     +  20.4094
     49   0.0813   0.3313        [35m0.6109[0m       0.9129        [94m0.5987[0m     +  20.4112
     50   0.0811   0.3312        [35m0.6106[0m       0.9129        [94m0.5972[0m     +  20.4157
[32m[I 2023-05-05 10:47:02,629][0m Trial 600 finished with value: 0.597247504986183 and parameters: {'lr': 1.1630096819397061e-08, 'dropout': 0.3919047133069007, 'd_model_multiplier': 4, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 398, 'batch_size': 36, 'pos_encoding': 'learnable', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 104}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 62
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
Warning, assumed runtime error: CUDA out of memory. Tried to allocate 2.72 GiB (GPU 0; 23.70 GiB total capacity; 19.77 GiB already allocated; 143.25 MiB free; 22.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[32m[I 2023-05-05 10:47:08,026][0m Trial 601 finished with value: 100.0 and parameters: {'lr': 0.050389457319440896, 'dropout': 0.6037021630548183, 'd_model_multiplier': 4, 'num_layers': 5, 'n_heads': 64, 'dim_feedforward': 332, 'batch_size': 115, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 62}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 73
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.1367[0m   [32m0.5550[0m        [35m0.4630[0m       [31m0.9202[0m        [94m0.2868[0m     +  8.8446
      2   [36m0.2057[0m   [32m0.7090[0m        [35m0.2620[0m       0.9202        [94m0.2553[0m     +  8.6025
      3   [36m0.2143[0m   [32m0.7272[0m        [35m0.2429[0m       0.9154        [94m0.2550[0m     +  8.9130
      4   [36m0.2200[0m   [32m0.7312[0m        [35m0.2358[0m       0.9141        0.2588        8.7744
      5   0.2169   0.7309        [35m0.2334[0m       0.9117        0.2616        9.5458
      6   [36m0.2226[0m   [32m0.7338[0m        [35m0.2295[0m       0.9117        0.2624        9.2265
      7   [36m0.2245[0m   [32m0.7355[0m        [35m0.2281[0m       0.9129        0.2635        9.2458
      8   [36m0.2299[0m   [32m0.7381[0m        [35m0.2261[0m       0.9117        0.2637        9.2689
      9   [36m0.2325[0m   [32m0.7406[0m        [35m0.2249[0m       0.9141        0.2630        9.2226
     10   [36m0.2326[0m   [32m0.7445[0m        [35m0.2233[0m       0.9154        0.2633        9.3493
     11   [36m0.2347[0m   [32m0.7467[0m        [35m0.2215[0m       0.9154        0.2637        9.1235
     12   0.2335   [32m0.7494[0m        [35m0.2210[0m       0.9166        0.2634        9.2848
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 10:49:07,203][0m Trial 602 finished with value: 0.25495936079209774 and parameters: {'lr': 1.4708292625136608e-05, 'dropout': 0.32546494150881417, 'd_model_multiplier': 64, 'num_layers': 2, 'n_heads': 4, 'dim_feedforward': 410, 'batch_size': 28, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 73}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 57
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1450[0m   [32m0.5654[0m        [35m0.6820[0m       [31m0.7896[0m        [94m0.6698[0m     +  16.2708
      2   0.1014   0.4207        [35m0.6395[0m       [31m0.9323[0m        [94m0.6095[0m     +  16.3098
      3   0.0675   0.3368        [35m0.5864[0m       [31m0.9335[0m        [94m0.5449[0m     +  16.3896
      4   0.0658   0.3187        [35m0.5311[0m       0.9335        [94m0.4842[0m     +  16.4453
      5   0.0657   0.3190        [35m0.4853[0m       0.9335        [94m0.4317[0m     +  16.6443
      6   0.0661   0.3230        [35m0.4423[0m       0.9335        [94m0.3883[0m     +  16.5368
      7   0.0498   0.3332        [35m0.4082[0m       0.9335        [94m0.3536[0m     +  16.4172
      8   0.0506   0.3522        [35m0.3799[0m       0.9335        [94m0.3265[0m     +  16.2684
      9   0.0526   0.3821        [35m0.3574[0m       0.9335        [94m0.3056[0m     +  16.2085
     10   0.0554   0.4199        [35m0.3404[0m       0.9335        [94m0.2897[0m     +  16.3844
     11   0.0591   0.4587        [35m0.3264[0m       0.9335        [94m0.2775[0m     +  16.3531
     12   0.0637   0.4954        [35m0.3145[0m       0.9335        [94m0.2679[0m     +  16.4024
     13   0.0684   0.5258        [35m0.3069[0m       0.9335        [94m0.2605[0m     +  16.2952
     14   0.0742   0.5562        [35m0.2997[0m       0.9335        [94m0.2546[0m     +  16.3721
     15   0.0820   [32m0.5813[0m        [35m0.2928[0m       0.9335        [94m0.2498[0m     +  16.4339
     16   0.1155   [32m0.6067[0m        [35m0.2883[0m       0.9335        [94m0.2458[0m     +  16.3399
     17   0.1261   [32m0.6238[0m        [35m0.2843[0m       0.9335        [94m0.2425[0m     +  16.4200
     18   0.1395   [32m0.6386[0m        [35m0.2801[0m       0.9335        [94m0.2397[0m     +  16.2118
     19   [36m0.1475[0m   [32m0.6533[0m        [35m0.2761[0m       0.9335        [94m0.2374[0m     +  16.4100
     20   [36m0.1520[0m   [32m0.6645[0m        [35m0.2738[0m       0.9335        [94m0.2353[0m     +  16.4753
     21   [36m0.1592[0m   [32m0.6756[0m        [35m0.2703[0m       0.9335        [94m0.2336[0m     +  16.4054
     22   [36m0.1621[0m   [32m0.6844[0m        [35m0.2686[0m       0.9335        [94m0.2321[0m     +  16.3274
     23   [36m0.1639[0m   [32m0.6924[0m        [35m0.2669[0m       0.9335        [94m0.2309[0m     +  16.3614
     24   [36m0.1667[0m   [32m0.6994[0m        [35m0.2637[0m       0.9335        [94m0.2298[0m     +  16.2796
     25   [36m0.1696[0m   [32m0.7052[0m        [35m0.2636[0m       0.9335        [94m0.2288[0m     +  16.5076
     26   [36m0.1708[0m   [32m0.7102[0m        [35m0.2625[0m       0.9335        [94m0.2279[0m     +  16.2410
     27   [36m0.1730[0m   [32m0.7150[0m        [35m0.2614[0m       0.9335        [94m0.2272[0m     +  16.3968
     28   [36m0.1742[0m   [32m0.7193[0m        [35m0.2596[0m       0.9335        [94m0.2265[0m     +  16.2756
     29   [36m0.1746[0m   [32m0.7227[0m        [35m0.2585[0m       0.9335        [94m0.2259[0m     +  16.2881
     30   0.1733   [32m0.7264[0m        [35m0.2577[0m       0.9335        [94m0.2253[0m     +  16.3050
     31   [36m0.1772[0m   [32m0.7302[0m        [35m0.2559[0m       0.9335        [94m0.2248[0m     +  16.3113
     32   [36m0.1781[0m   [32m0.7330[0m        [35m0.2555[0m       0.9335        [94m0.2243[0m     +  16.3975
     33   [36m0.1782[0m   [32m0.7362[0m        [35m0.2538[0m       0.9335        [94m0.2238[0m     +  16.4293
     34   [36m0.1806[0m   [32m0.7395[0m        0.2549       0.9335        [94m0.2234[0m     +  16.3690
     35   [36m0.1819[0m   [32m0.7419[0m        0.2538       0.9335        [94m0.2230[0m     +  16.3484
     36   [36m0.1837[0m   [32m0.7443[0m        [35m0.2522[0m       0.9335        [94m0.2226[0m     +  16.2069
     37   [36m0.1878[0m   [32m0.7465[0m        [35m0.2512[0m       0.9335        [94m0.2222[0m     +  16.4453
     38   [36m0.1908[0m   [32m0.7495[0m        0.2521       0.9335        [94m0.2219[0m     +  16.3212
     39   [36m0.1920[0m   [32m0.7515[0m        [35m0.2509[0m       0.9323        [94m0.2216[0m     +  16.5246
     40   0.1900   [32m0.7530[0m        [35m0.2508[0m       0.9323        [94m0.2212[0m     +  16.5667
     41   0.1908   [32m0.7545[0m        0.2517       0.9323        [94m0.2209[0m     +  16.4115
     42   [36m0.1920[0m   [32m0.7568[0m        [35m0.2505[0m       0.9323        [94m0.2207[0m     +  16.4335
     43   [36m0.1924[0m   [32m0.7584[0m        [35m0.2490[0m       0.9323        [94m0.2203[0m     +  16.4577
     44   [36m0.1941[0m   [32m0.7599[0m        0.2494       0.9323        [94m0.2200[0m     +  16.3156
     45   [36m0.1974[0m   [32m0.7618[0m        0.2494       0.9323        [94m0.2197[0m     +  16.5018
     46   [36m0.1984[0m   [32m0.7630[0m        [35m0.2471[0m       0.9323        [94m0.2194[0m     +  16.4793
     47   0.1979   [32m0.7642[0m        0.2486       0.9323        [94m0.2191[0m     +  16.5232
     48   [36m0.2009[0m   [32m0.7658[0m        [35m0.2471[0m       0.9323        [94m0.2188[0m     +  16.4263
     49   [36m0.2013[0m   [32m0.7669[0m        0.2486       0.9323        [94m0.2185[0m     +  16.3721
     50   0.2012   [32m0.7676[0m        0.2481       0.9323        [94m0.2183[0m     +  17.0546
[32m[I 2023-05-05 11:02:53,576][0m Trial 603 finished with value: 0.21829421328157728 and parameters: {'lr': 6.610729528903615e-07, 'dropout': 0.4624335693705424, 'd_model_multiplier': 4, 'num_layers': 3, 'n_heads': 32, 'dim_feedforward': 389, 'batch_size': 26, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0, 'top_n_features': 57}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 65
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2154[0m   [32m0.7010[0m        [35m0.6916[0m       [31m0.7690[0m        [94m0.6811[0m     +  19.2693
      2   0.2135   0.6996        [35m0.6908[0m       [31m0.7944[0m        [94m0.6784[0m     +  19.2757
      3   0.2119   0.6977        [35m0.6869[0m       [31m0.8174[0m        [94m0.6749[0m     +  19.4236
      4   0.2091   0.6955        [35m0.6842[0m       [31m0.8368[0m        [94m0.6710[0m     +  19.3251
      5   0.2060   0.6930        [35m0.6786[0m       [31m0.8489[0m        [94m0.6665[0m     +  19.6602
      6   0.2017   0.6872        [35m0.6768[0m       [31m0.8609[0m        [94m0.6618[0m     +  19.3816
      7   0.1978   0.6807        [35m0.6686[0m       [31m0.8682[0m        [94m0.6567[0m     +  19.3406
      8   0.1921   0.6700        [35m0.6633[0m       [31m0.8767[0m        [94m0.6515[0m     +  19.5982
      9   0.1862   0.6581        [35m0.6588[0m       [31m0.8815[0m        [94m0.6460[0m     +  19.7486
     10   0.1799   0.6467        [35m0.6548[0m       [31m0.8863[0m        [94m0.6404[0m     +  19.3678
     11   0.1734   0.6357        [35m0.6495[0m       [31m0.8924[0m        [94m0.6347[0m     +  19.4027
     12   0.1603   0.6219        [35m0.6427[0m       0.8900        [94m0.6289[0m     +  19.5003
     13   0.1440   0.6040        [35m0.6411[0m       0.8912        [94m0.6231[0m     +  19.5577
     14   0.1246   0.5778        [35m0.6327[0m       [31m0.8960[0m        [94m0.6172[0m     +  19.3992
     15   0.1065   0.5439        [35m0.6269[0m       [31m0.9021[0m        [94m0.6114[0m     +  19.5161
     16   0.0920   0.5030        [35m0.6218[0m       0.9021        [94m0.6055[0m     +  19.3813
     17   0.0825   0.4646        [35m0.6165[0m       [31m0.9033[0m        [94m0.5997[0m     +  19.4219
     18   0.0769   0.4317        [35m0.6096[0m       [31m0.9057[0m        [94m0.5938[0m     +  19.2962
     19   0.0737   0.4061        [35m0.6052[0m       [31m0.9069[0m        [94m0.5880[0m     +  19.4293
     20   0.0713   0.3877        [35m0.6014[0m       0.9069        [94m0.5823[0m     +  19.1668
     21   0.0702   0.3753        [35m0.5937[0m       [31m0.9081[0m        [94m0.5766[0m     +  19.6242
     22   0.0692   0.3654        [35m0.5895[0m       [31m0.9093[0m        [94m0.5710[0m     +  19.3696
     23   0.0675   0.3576        [35m0.5845[0m       0.9093        [94m0.5655[0m     +  19.3425
     24   0.0668   0.3511        [35m0.5798[0m       [31m0.9105[0m        [94m0.5601[0m     +  19.4346
     25   0.0663   0.3451        [35m0.5726[0m       0.9105        [94m0.5548[0m     +  19.3492
     26   0.0660   0.3406        [35m0.5676[0m       [31m0.9117[0m        [94m0.5495[0m     +  19.4179
     27   0.0660   0.3365        [35m0.5637[0m       0.9117        [94m0.5444[0m     +  19.2597
     28   0.0661   0.3327        [35m0.5585[0m       0.9117        [94m0.5393[0m     +  19.4233
     29   0.0660   0.3300        [35m0.5530[0m       0.9117        [94m0.5344[0m     +  19.3994
     30   0.0661   0.3273        [35m0.5488[0m       0.9117        [94m0.5295[0m     +  19.4706
     31   0.0661   0.3255        [35m0.5450[0m       0.9117        [94m0.5247[0m     +  19.3876
     32   0.0663   0.3237        [35m0.5395[0m       0.9117        [94m0.5201[0m     +  19.3557
     33   0.0662   0.3224        [35m0.5344[0m       0.9117        [94m0.5155[0m     +  19.6072
     34   0.0668   0.3212        [35m0.5300[0m       0.9117        [94m0.5110[0m     +  19.5308
     35   0.0669   0.3204        [35m0.5245[0m       0.9117        [94m0.5067[0m     +  19.4426
     36   0.0669   0.3197        [35m0.5211[0m       0.9117        [94m0.5024[0m     +  19.5922
     37   0.0668   0.3186        [35m0.5161[0m       0.9117        [94m0.4982[0m     +  19.4585
     38   0.0668   0.3178        [35m0.5126[0m       0.9117        [94m0.4941[0m     +  19.6449
     39   0.0668   0.3171        [35m0.5086[0m       0.9117        [94m0.4901[0m     +  19.4784
     40   0.0667   0.3166        [35m0.5038[0m       0.9117        [94m0.4862[0m     +  19.5353
     41   0.0667   0.3161        [35m0.5015[0m       0.9117        [94m0.4824[0m     +  19.4433
     42   0.0666   0.3157        [35m0.4950[0m       0.9117        [94m0.4786[0m     +  19.7388
     43   0.0666   0.3154        [35m0.4935[0m       0.9117        [94m0.4750[0m     +  19.6150
     44   0.0666   0.3151        [35m0.4887[0m       0.9117        [94m0.4714[0m     +  19.1875
     45   0.0666   0.3147        [35m0.4848[0m       0.9117        [94m0.4679[0m     +  19.2621
     46   0.0662   0.3143        [35m0.4825[0m       0.9117        [94m0.4645[0m     +  19.6248
     47   0.0660   0.3141        [35m0.4797[0m       0.9117        [94m0.4612[0m     +  19.7134
     48   0.0659   0.3137        [35m0.4760[0m       0.9117        [94m0.4579[0m     +  19.6276
     49   0.0659   0.3134        [35m0.4714[0m       0.9117        [94m0.4547[0m     +  19.2378
     50   0.0657   0.3133        [35m0.4682[0m       0.9117        [94m0.4516[0m     +  19.5179
[32m[I 2023-05-05 11:19:10,057][0m Trial 604 finished with value: 0.45164072491958324 and parameters: {'lr': 1.4833627284959717e-07, 'dropout': 0.5859311399157043, 'd_model_multiplier': 2, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 348, 'batch_size': 50, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 65}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 49
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
Warning, assumed runtime error: CUDA out of memory. Tried to allocate 1.04 GiB (GPU 0; 23.70 GiB total capacity; 20.64 GiB already allocated; 133.25 MiB free; 22.48 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[32m[I 2023-05-05 11:19:14,017][0m Trial 605 finished with value: 100.0 and parameters: {'lr': 1.183605190684293e-06, 'dropout': 0.4919704258894233, 'd_model_multiplier': 1, 'num_layers': 14, 'n_heads': 32, 'dim_feedforward': 322, 'batch_size': 88, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 49}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 159
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0815[0m   [32m0.3537[0m        [35m0.6412[0m       [31m0.9154[0m        [94m0.4857[0m     +  24.2498
      2   [36m0.1086[0m   [32m0.5009[0m        [35m0.4105[0m       0.9154        [94m0.3136[0m     +  24.3491
      3   [36m0.2268[0m   [32m0.6742[0m        [35m0.2940[0m       0.9154        [94m0.2730[0m     +  24.9752
      4   [36m0.2627[0m   [32m0.7008[0m        [35m0.2636[0m       0.9154        [94m0.2648[0m     +  24.6329
      5   [36m0.2787[0m   [32m0.7112[0m        [35m0.2527[0m       0.9154        [94m0.2624[0m     +  25.1427
      6   [36m0.2924[0m   [32m0.7221[0m        [35m0.2468[0m       0.9154        [94m0.2596[0m     +  24.9095
      7   [36m0.3017[0m   [32m0.7324[0m        [35m0.2434[0m       [31m0.9166[0m        [94m0.2578[0m     +  25.2043
      8   [36m0.3110[0m   [32m0.7386[0m        [35m0.2391[0m       0.9166        [94m0.2568[0m     +  25.3598
      9   [36m0.3188[0m   [32m0.7420[0m        0.2396       0.9166        [94m0.2567[0m     +  24.9402
     10   [36m0.3244[0m   [32m0.7429[0m        [35m0.2363[0m       0.9166        0.2581        25.3410
     11   [36m0.3284[0m   [32m0.7473[0m        [35m0.2349[0m       [31m0.9202[0m        [94m0.2545[0m     +  24.8762
     12   [36m0.3324[0m   [32m0.7479[0m        [35m0.2329[0m       0.9202        0.2557        25.0280
     13   [36m0.3383[0m   [32m0.7488[0m        [35m0.2327[0m       0.9190        0.2559        25.1534
     14   [36m0.3441[0m   [32m0.7494[0m        0.2333       0.9190        0.2565        24.8693
     15   [36m0.3442[0m   [32m0.7496[0m        [35m0.2305[0m       [31m0.9238[0m        0.2562        25.2240
     16   0.3434   [32m0.7496[0m        0.2320       [31m0.9250[0m        0.2556        25.0554
     17   [36m0.3472[0m   [32m0.7511[0m        [35m0.2294[0m       0.9226        0.2569        24.9868
     18   0.3414   [32m0.7521[0m        0.2301       0.9202        0.2578        24.8480
     19   0.3438   [32m0.7535[0m        [35m0.2279[0m       0.9214        0.2580        24.9997
     20   0.3429   [32m0.7539[0m        [35m0.2267[0m       0.9202        0.2587        24.8516
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 11:27:59,127][0m Trial 606 finished with value: 0.2544971544840604 and parameters: {'lr': 4.504849296111599e-05, 'dropout': 0.4452983009531783, 'd_model_multiplier': 4, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 335, 'batch_size': 110, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 159}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 60
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0417[0m   [32m0.2556[0m        [35m0.7051[0m       [31m0.8162[0m        [94m0.6670[0m     +  13.5201
      2   [36m0.0432[0m   [32m0.2666[0m        [35m0.6058[0m       [31m0.9347[0m        [94m0.5357[0m     +  13.4840
      3   [36m0.0444[0m   [32m0.2791[0m        [35m0.5022[0m       0.9347        [94m0.4272[0m     +  13.8723
      4   [36m0.0448[0m   [32m0.2923[0m        [35m0.4240[0m       0.9347        [94m0.3515[0m     +  13.8613
      5   [36m0.0485[0m   [32m0.3536[0m        [35m0.3667[0m       0.9347        [94m0.3032[0m     +  13.8443
      6   [36m0.0563[0m   [32m0.4466[0m        [35m0.3300[0m       0.9347        [94m0.2732[0m     +  13.7648
      7   [36m0.0789[0m   [32m0.5310[0m        [35m0.3067[0m       0.9347        [94m0.2548[0m     +  13.8073
      8   [36m0.1043[0m   [32m0.5930[0m        [35m0.2890[0m       0.9347        [94m0.2431[0m     +  13.8214
      9   [36m0.1381[0m   [32m0.6327[0m        [35m0.2772[0m       0.9347        [94m0.2359[0m     +  13.7373
     10   [36m0.1520[0m   [32m0.6603[0m        [35m0.2701[0m       0.9347        [94m0.2311[0m     +  13.9169
     11   [36m0.1603[0m   [32m0.6792[0m        [35m0.2640[0m       0.9347        [94m0.2279[0m     +  13.5830
     12   [36m0.1683[0m   [32m0.6932[0m        [35m0.2592[0m       0.9347        [94m0.2257[0m     +  13.7361
     13   [36m0.1715[0m   [32m0.7036[0m        [35m0.2560[0m       0.9347        [94m0.2242[0m     +  13.7924
     14   [36m0.1778[0m   [32m0.7132[0m        [35m0.2530[0m       0.9347        [94m0.2232[0m     +  13.6365
     15   [36m0.1800[0m   [32m0.7201[0m        [35m0.2518[0m       [31m0.9359[0m        [94m0.2224[0m     +  13.7037
     16   [36m0.1818[0m   [32m0.7250[0m        [35m0.2491[0m       0.9359        [94m0.2220[0m     +  13.7808
     17   [36m0.1839[0m   [32m0.7304[0m        [35m0.2487[0m       0.9359        [94m0.2216[0m     +  13.7667
     18   [36m0.1866[0m   [32m0.7344[0m        [35m0.2484[0m       0.9359        [94m0.2214[0m     +  13.6469
     19   [36m0.1893[0m   [32m0.7386[0m        [35m0.2473[0m       0.9359        [94m0.2212[0m     +  13.6611
     20   [36m0.1900[0m   [32m0.7407[0m        [35m0.2461[0m       0.9359        [94m0.2211[0m     +  13.7466
     21   [36m0.1921[0m   [32m0.7440[0m        [35m0.2439[0m       0.9359        [94m0.2210[0m     +  13.8056
     22   0.1920   [32m0.7461[0m        [35m0.2434[0m       0.9347        0.2211        13.8005
     23   0.1920   [32m0.7484[0m        [35m0.2433[0m       0.9347        0.2212        13.7550
     24   [36m0.1948[0m   [32m0.7505[0m        [35m0.2427[0m       0.9347        0.2211        13.8322
     25   [36m0.1957[0m   [32m0.7526[0m        [35m0.2423[0m       0.9335        0.2212        13.6631
     26   [36m0.1963[0m   [32m0.7545[0m        [35m0.2420[0m       0.9323        0.2212        13.6619
     27   [36m0.1965[0m   [32m0.7553[0m        [35m0.2394[0m       0.9323        0.2214        13.8532
     28   [36m0.1967[0m   [32m0.7559[0m        0.2412       0.9323        0.2215        14.0171
     29   [36m0.1974[0m   [32m0.7569[0m        [35m0.2389[0m       0.9311        0.2217        13.5265
     30   0.1971   0.7561        0.2398       0.9311        0.2218        13.6278
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 11:35:09,987][0m Trial 607 finished with value: 0.22101243854722205 and parameters: {'lr': 4.768929644452972e-07, 'dropout': 0.4301722742020925, 'd_model_multiplier': 32, 'num_layers': 1, 'n_heads': 32, 'dim_feedforward': 372, 'batch_size': 44, 'pos_encoding': 'learnable', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 60}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 44
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
Warning, assumed runtime error: CUDA out of memory. Tried to allocate 3.43 GiB (GPU 0; 23.70 GiB total capacity; 13.81 GiB already allocated; 259.25 MiB free; 22.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[32m[I 2023-05-05 11:35:14,977][0m Trial 608 finished with value: 100.0 and parameters: {'lr': 7.456154797443086e-07, 'dropout': 0.1992005360185679, 'd_model_multiplier': 16, 'num_layers': 5, 'n_heads': 64, 'dim_feedforward': 378, 'batch_size': 145, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 44}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 69
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.0653[0m   [32m0.4042[0m        [35m0.6797[0m       [31m0.3664[0m        [94m0.7000[0m     +  9.5683
      2   [36m0.0659[0m   0.4015        [35m0.6778[0m       0.3253        0.7011        10.2160
      3   [36m0.0662[0m   [32m0.4069[0m        0.6778       0.3579        [94m0.6995[0m     +  9.6907
      4   0.0658   0.4045        0.6788       0.3591        0.6998        10.4834
      5   0.0634   0.3958        0.6801       [31m0.3881[0m        [94m0.6986[0m     +  10.7325
      6   0.0656   0.4032        0.6814       0.3543        0.6999        10.3798
      7   0.0641   0.4023        0.6803       0.3712        0.6997        10.0899
      8   0.0614   0.3899        0.6779       0.3857        [94m0.6983[0m     +  10.4466
      9   0.0662   [32m0.4080[0m        0.6780       0.3446        0.7002        10.3305
     10   0.0660   0.4046        [35m0.6768[0m       0.3652        0.6996        10.2929
     11   0.0627   0.3945        [35m0.6760[0m       0.3821        0.6988        10.3937
     12   0.0626   0.3931        [35m0.6744[0m       [31m0.3942[0m        [94m0.6977[0m     +  10.3074
     13   0.0640   0.3955        0.6745       0.3736        0.6996        10.4183
     14   0.0622   0.3915        [35m0.6732[0m       0.3821        0.6984        10.3658
     15   0.0630   0.3936        0.6753       [31m0.4015[0m        0.6978        10.3692
     16   0.0617   0.3889        [35m0.6704[0m       0.3954        [94m0.6975[0m     +  10.3597
     17   0.0611   0.3868        0.6726       0.3954        0.6979        10.3763
     18   0.0614   0.3869        0.6713       [31m0.4027[0m        0.6976        10.4339
     19   0.0620   0.3821        0.6705       [31m0.4111[0m        [94m0.6972[0m     +  10.5794
     20   0.0633   0.3931        0.6712       0.3966        0.6979        10.3770
     21   0.0624   0.3897        [35m0.6684[0m       0.3507        0.6997        10.5216
     22   0.0622   0.3870        0.6698       0.3978        0.6975        10.5968
     23   0.0626   0.3794        0.6697       [31m0.4389[0m        [94m0.6954[0m     +  10.2614
     24   0.0614   0.3782        [35m0.6683[0m       0.4184        0.6966        10.6800
     25   0.0618   0.3816        0.6684       0.4220        0.6963        10.3858
     26   0.0622   0.3759        [35m0.6639[0m       0.4281        0.6959        10.4637
     27   0.0632   0.3834        [35m0.6632[0m       0.4075        0.6977        10.5382
     28   0.0622   0.3835        0.6634       0.4256        0.6967        10.4343
     29   0.0605   0.3749        0.6641       [31m0.4498[0m        [94m0.6943[0m     +  10.3293
     30   0.0618   0.3777        0.6642       0.4220        0.6967        10.3883
     31   0.0606   0.3719        0.6636       0.4389        0.6953        10.7562
     32   0.0610   0.3772        [35m0.6606[0m       0.4426        0.6948        10.4233
     33   0.0610   0.3756        0.6610       0.4377        0.6954        10.5960
     34   0.0611   0.3754        [35m0.6592[0m       0.4401        0.6952        10.5237
     35   0.0602   0.3728        [35m0.6591[0m       [31m0.4607[0m        [94m0.6942[0m     +  10.5800
     36   0.0598   0.3687        0.6598       [31m0.4643[0m        [94m0.6936[0m     +  10.3458
     37   0.0596   0.3677        [35m0.6573[0m       0.4619        0.6942        10.4631
     38   0.0606   0.3680        0.6587       0.4643        [94m0.6936[0m     +  10.7405
     39   0.0594   0.3663        0.6581       [31m0.4776[0m        [94m0.6931[0m     +  10.8278
     40   0.0598   0.3668        [35m0.6557[0m       0.4571        0.6947        10.8854
     41   0.0626   0.3700        [35m0.6552[0m       0.4498        0.6950        10.4127
     42   0.0584   0.3611        [35m0.6543[0m       [31m0.5006[0m        [94m0.6920[0m     +  10.2779
     43   0.0614   0.3689        0.6545       0.4595        0.6942        10.7093
     44   0.0638   0.3643        [35m0.6528[0m       0.4800        0.6933        10.6628
     45   0.0605   0.3634        [35m0.6523[0m       0.4982        0.6923        10.5732
     46   0.0604   0.3616        [35m0.6505[0m       0.4728        0.6942        10.5434
     47   0.0603   0.3597        0.6539       [31m0.5030[0m        [94m0.6920[0m     +  10.2686
     48   0.0606   0.3623        0.6508       0.4982        0.6930        10.4984
     49   0.0600   0.3556        [35m0.6497[0m       0.4909        0.6930        10.3097
     50   0.0609   0.3604        0.6505       [31m0.5127[0m        [94m0.6906[0m     +  10.4275
[32m[I 2023-05-05 11:43:58,060][0m Trial 609 finished with value: 0.6906050790322015 and parameters: {'lr': 5.808925823005522e-08, 'dropout': 0.4047100680543312, 'd_model_multiplier': 4, 'num_layers': 4, 'n_heads': 8, 'dim_feedforward': 361, 'batch_size': 76, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 69}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 83
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1958[0m   [32m0.6951[0m        [35m0.2957[0m       [31m0.9129[0m        [94m0.3287[0m     +  16.2570
      2   [36m0.2402[0m   [32m0.7537[0m        [35m0.2699[0m       [31m0.9190[0m        0.3308        16.2996
      3   0.2011   0.6867        [35m0.2663[0m       0.9190        [94m0.3181[0m     +  16.3208
      4   0.1867   0.6849        0.2665       0.9166        [94m0.2942[0m     +  16.4284
      5   0.1931   0.6912        [35m0.2661[0m       0.9154        [94m0.2765[0m     +  16.6948
      6   0.1958   0.6937        [35m0.2643[0m       0.8767        0.2965        16.3470
      7   0.1952   0.6905        [35m0.2607[0m       0.8839        0.2889        16.4864
      8   0.2037   0.6884        [35m0.2597[0m       0.8803        0.3020        16.4930
      9   0.1908   0.6906        [35m0.2589[0m       0.9093        0.2810        16.4296
     10   0.1943   0.6923        [35m0.2560[0m       0.9045        0.2913        16.5561
     11   0.2176   0.6921        [35m0.2547[0m       0.8960        0.2822        16.6521
     12   0.1928   0.6864        [35m0.2536[0m       0.9117        [94m0.2747[0m     +  16.8294
     13   0.1842   0.6858        [35m0.2500[0m       0.9141        0.2763        16.3952
     14   0.2110   0.6869        0.2550       0.9166        [94m0.2690[0m     +  16.2630
     15   0.2036   0.6891        0.2526       [31m0.9202[0m        [94m0.2667[0m     +  16.7652
     16   0.1837   0.6801        0.2528       0.9166        [94m0.2664[0m     +  16.3915
     17   0.1926   0.6842        [35m0.2498[0m       0.9202        [94m0.2608[0m     +  16.3589
     18   0.1958   0.6893        0.2510       0.9190        [94m0.2590[0m     +  16.3679
     19   0.1894   0.6854        [35m0.2489[0m       0.9141        0.2659        16.5926
     20   0.2039   0.6879        0.2494       0.9166        0.2599        16.7313
     21   0.1908   0.6886        0.2492       0.9202        0.2612        16.3582
     22   0.1939   0.6933        0.2504       0.9117        0.2665        16.6407
     23   0.1928   0.6846        [35m0.2477[0m       0.9202        0.2648        16.5965
     24   0.1880   0.6888        0.2503       0.9166        0.2620        16.4885
     25   0.1940   0.6837        [35m0.2471[0m       0.9154        0.2642        16.5334
     26   0.1909   0.6802        0.2495       0.9190        0.2610        16.4763
     27   0.1879   0.6830        0.2493       0.9154        0.2627        16.7837
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 11:51:41,074][0m Trial 610 finished with value: 0.2590332809732067 and parameters: {'lr': 0.00973516822301833, 'dropout': 0.3583189471848149, 'd_model_multiplier': 4, 'num_layers': 3, 'n_heads': 32, 'dim_feedforward': 341, 'batch_size': 55, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0, 'top_n_features': 83}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 168
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.1663[0m   [32m0.6776[0m        [35m0.2706[0m       [31m0.9262[0m        [94m0.2477[0m     +  8.7606
      2   0.1035   0.5414        [35m0.2637[0m       [31m0.9299[0m        0.2720        9.7044
      3   [36m0.1797[0m   [32m0.7180[0m        [35m0.2547[0m       0.9287        [94m0.2362[0m     +  9.6287
      4   [36m0.2155[0m   [32m0.7556[0m        [35m0.2528[0m       0.9238        [94m0.2316[0m     +  9.6751
      5   0.2149   [32m0.7605[0m        [35m0.2430[0m       0.9154        0.2387        9.7848
      6   0.2065   0.7529        [35m0.2423[0m       0.9166        0.2392        10.2892
      7   0.2006   [32m0.7613[0m        [35m0.2420[0m       0.9226        0.2326        9.8674
      8   0.1882   0.7555        [35m0.2415[0m       0.9226        0.2397        9.8833
      9   0.2122   [32m0.7914[0m        [35m0.2395[0m       0.9238        [94m0.2248[0m     +  9.8302
     10   0.2040   0.7763        0.2400       0.9202        0.2300        9.4651
     11   0.2089   0.7816        0.2396       0.9226        0.2294        9.9350
     12   0.2110   0.7847        0.2431       0.9262        [94m0.2232[0m     +  10.0715
     13   0.2149   [32m0.7932[0m        [35m0.2387[0m       0.9226        0.2303        9.7944
     14   [36m0.2207[0m   0.7893        [35m0.2343[0m       0.9154        0.2407        10.1420
     15   [36m0.2301[0m   [32m0.7976[0m        0.2362       0.9262        [94m0.2206[0m     +  10.1301
     16   [36m0.2356[0m   [32m0.7977[0m        0.2361       0.9202        0.2267        9.8310
     17   0.2010   0.7962        0.2373       0.9202        0.2370        9.9412
     18   0.2192   0.7955        [35m0.2336[0m       0.9226        0.2262        10.0992
     19   0.2266   [32m0.8001[0m        [35m0.2326[0m       0.9226        0.2278        10.0197
     20   0.2105   0.7951        [35m0.2306[0m       0.9141        0.2448        9.8668
     21   0.2274   [32m0.8035[0m        0.2332       0.9226        0.2257        9.9614
     22   0.2078   0.7878        [35m0.2296[0m       0.9190        0.2461        9.9736
     23   0.2170   0.7992        0.2318       0.9190        0.2312        9.9984
     24   0.2131   0.7988        0.2317       0.9226        0.2281        9.9966
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 11:55:48,078][0m Trial 611 finished with value: 0.22055425369681875 and parameters: {'lr': 0.0019018458006026312, 'dropout': 0.2502281956183293, 'd_model_multiplier': 4, 'num_layers': 1, 'n_heads': 32, 'dim_feedforward': 164, 'batch_size': 38, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 168}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 148
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0768[0m   [32m0.4388[0m        [35m0.6508[0m       [31m0.9262[0m        [94m0.5974[0m     +  10.8173
      2   0.0622   0.3494        [35m0.5462[0m       [31m0.9287[0m        [94m0.4796[0m     +  10.7359
      3   0.0587   0.3342        [35m0.4543[0m       0.9287        [94m0.3962[0m     +  11.1450
      4   0.0630   0.3444        [35m0.3875[0m       0.9287        [94m0.3372[0m     +  11.2436
      5   0.0693   0.3819        [35m0.3464[0m       0.9287        [94m0.3034[0m     +  10.9919
      6   [36m0.0790[0m   [32m0.4692[0m        [35m0.3182[0m       0.9287        [94m0.2829[0m     +  10.7344
      7   [36m0.0974[0m   [32m0.5248[0m        [35m0.2995[0m       0.9287        [94m0.2700[0m     +  11.1753
      8   [36m0.1134[0m   [32m0.5738[0m        [35m0.2883[0m       0.9287        [94m0.2616[0m     +  11.0465
      9   [36m0.1269[0m   [32m0.6080[0m        [35m0.2826[0m       0.9287        [94m0.2555[0m     +  10.9744
     10   [36m0.1376[0m   [32m0.6343[0m        [35m0.2752[0m       0.9287        [94m0.2512[0m     +  10.8754
     11   [36m0.1446[0m   [32m0.6454[0m        [35m0.2703[0m       0.9287        [94m0.2481[0m     +  11.1359
     12   [36m0.1491[0m   [32m0.6526[0m        [35m0.2695[0m       0.9287        [94m0.2460[0m     +  11.0400
     13   [36m0.1649[0m   [32m0.6603[0m        [35m0.2659[0m       0.9287        [94m0.2443[0m     +  11.0603
     14   [36m0.1662[0m   [32m0.6642[0m        [35m0.2635[0m       0.9287        [94m0.2428[0m     +  11.0130
     15   0.1592   [32m0.6680[0m        0.2640       0.9287        [94m0.2418[0m     +  11.1398
     16   [36m0.1702[0m   [32m0.6741[0m        [35m0.2629[0m       0.9287        [94m0.2409[0m     +  10.8603
     17   [36m0.1704[0m   [32m0.6754[0m        [35m0.2619[0m       0.9287        [94m0.2401[0m     +  10.9491
     18   [36m0.1710[0m   [32m0.6783[0m        [35m0.2606[0m       0.9287        [94m0.2396[0m     +  11.0834
     19   [36m0.1750[0m   [32m0.6807[0m        [35m0.2601[0m       0.9287        [94m0.2390[0m     +  10.9140
     20   0.1748   [32m0.6826[0m        [35m0.2582[0m       0.9287        [94m0.2386[0m     +  10.9800
     21   [36m0.1772[0m   [32m0.6879[0m        0.2586       0.9287        [94m0.2382[0m     +  11.0106
     22   0.1640   [32m0.6899[0m        [35m0.2575[0m       0.9287        [94m0.2380[0m     +  10.9080
     23   0.1635   [32m0.6946[0m        [35m0.2573[0m       0.9287        [94m0.2377[0m     +  10.7849
     24   0.1659   [32m0.6968[0m        0.2602       0.9287        [94m0.2374[0m     +  10.7282
     25   0.1661   [32m0.7030[0m        0.2598       0.9287        [94m0.2374[0m     +  10.9837
     26   0.1660   0.7022        [35m0.2563[0m       0.9287        [94m0.2373[0m     +  10.7843
     27   0.1660   [32m0.7048[0m        0.2575       0.9287        [94m0.2370[0m     +  10.8684
     28   0.1699   [32m0.7089[0m        0.2569       0.9287        [94m0.2368[0m     +  10.8155
     29   0.1720   [32m0.7098[0m        0.2589       0.9287        [94m0.2367[0m     +  11.1358
     30   0.1726   [32m0.7111[0m        0.2568       0.9287        [94m0.2364[0m     +  11.0939
     31   0.1687   [32m0.7144[0m        [35m0.2562[0m       0.9287        [94m0.2364[0m     +  10.7362
     32   0.1710   [32m0.7146[0m        0.2572       0.9287        [94m0.2363[0m     +  10.8785
     33   0.1741   [32m0.7151[0m        0.2564       0.9287        [94m0.2362[0m     +  10.9860
     34   0.1762   [32m0.7191[0m        0.2580       0.9287        [94m0.2360[0m     +  11.2679
     35   0.1762   [32m0.7217[0m        0.2581       0.9287        0.2360        10.9676
     36   0.1770   [32m0.7218[0m        0.2574       0.9287        [94m0.2358[0m     +  11.0536
     37   0.1765   [32m0.7251[0m        0.2570       0.9287        [94m0.2356[0m     +  11.0117
     38   [36m0.1810[0m   [32m0.7282[0m        0.2566       0.9287        [94m0.2355[0m     +  11.0523
     39   [36m0.1811[0m   0.7279        0.2573       0.9287        0.2355        10.6730
     40   0.1801   [32m0.7287[0m        [35m0.2550[0m       0.9287        [94m0.2354[0m     +  10.8669
     41   0.1808   [32m0.7310[0m        0.2585       0.9287        [94m0.2353[0m     +  10.8116
     42   0.1788   [32m0.7354[0m        [35m0.2534[0m       0.9287        [94m0.2351[0m     +  10.9253
     43   [36m0.1814[0m   [32m0.7380[0m        0.2564       0.9287        [94m0.2348[0m     +  10.9819
     44   [36m0.1916[0m   [32m0.7422[0m        0.2539       0.9287        [94m0.2343[0m     +  10.9739
     45   0.1808   [32m0.7451[0m        0.2544       0.9287        [94m0.2341[0m     +  10.8600
     46   0.1837   [32m0.7503[0m        0.2560       0.9287        [94m0.2335[0m     +  10.8918
     47   0.1881   [32m0.7514[0m        [35m0.2534[0m       0.9287        [94m0.2331[0m     +  10.9698
     48   [36m0.1969[0m   [32m0.7578[0m        [35m0.2525[0m       0.9287        [94m0.2327[0m     +  11.1893
     49   0.1866   [32m0.7592[0m        0.2533       0.9287        [94m0.2324[0m     +  10.8286
     50   0.1862   [32m0.7617[0m        [35m0.2524[0m       0.9287        [94m0.2319[0m     +  11.0585
[32m[I 2023-05-05 12:05:01,693][0m Trial 612 finished with value: 0.23191094594898362 and parameters: {'lr': 3.1118343044199617e-05, 'dropout': 0.4199130708207631, 'd_model_multiplier': 1, 'num_layers': 5, 'n_heads': 4, 'dim_feedforward': 402, 'batch_size': 20, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 148}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 115
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2480[0m   [32m0.8133[0m        [35m0.2756[0m       [31m0.9190[0m        [94m0.2169[0m     +  19.6636
      2   [36m0.2612[0m   [32m0.8380[0m        [35m0.2636[0m       [31m0.9347[0m        [94m0.1975[0m     +  20.3387
      3   0.2421   0.8152        [35m0.2543[0m       [31m0.9359[0m        0.2000        20.0953
      4   0.2300   0.8166        [35m0.2513[0m       [31m0.9395[0m        [94m0.1953[0m     +  20.2626
      5   [36m0.2643[0m   0.8255        [35m0.2465[0m       0.9311        [94m0.1952[0m     +  20.2048
      6   [36m0.2721[0m   [32m0.8429[0m        0.2485       [31m0.9456[0m        [94m0.1739[0m     +  20.0185
      7   0.2269   0.8140        [35m0.2437[0m       0.9407        0.1930        20.1063
      8   0.2514   0.8095        0.2447       0.9444        0.1933        20.1642
      9   0.2410   0.8131        0.2505       0.9395        0.1856        20.1091
     10   0.2357   0.8051        0.2585       0.9407        0.1825        20.2623
     11   0.2272   0.8074        0.2603       0.9407        0.1813        20.3351
     12   0.2401   0.8258        0.2586       0.9407        0.1784        21.2808
     13   0.2531   0.8266        0.2491       0.9395        0.1749        20.4768
     14   0.2475   [32m0.8606[0m        0.2462       0.9250        0.1833        20.2437
     15   0.2207   0.8121        0.2530       0.9444        0.2042        20.1513
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 12:10:26,217][0m Trial 613 finished with value: 0.17388480932349307 and parameters: {'lr': 0.00321880894156341, 'dropout': 0.4572503814253615, 'd_model_multiplier': 4, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 183, 'batch_size': 69, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 115}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 98
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2385[0m   [32m0.7093[0m        [35m0.2747[0m       [31m0.9347[0m        [94m0.2472[0m     +  20.2390
      2   [36m0.2795[0m   [32m0.7465[0m        [35m0.2615[0m       0.9347        [94m0.2351[0m     +  20.0798
      3   0.2325   [32m0.7755[0m        [35m0.2551[0m       0.9287        [94m0.2120[0m     +  20.7703
      4   0.2264   [32m0.7844[0m        [35m0.2538[0m       0.9238        0.2219        20.2889
      5   0.2525   [32m0.7870[0m        [35m0.2491[0m       0.9323        [94m0.2105[0m     +  20.2549
      6   [36m0.3240[0m   0.7867        [35m0.2437[0m       0.9335        [94m0.2090[0m     +  20.5129
      7   0.2915   0.7849        0.2468       0.9347        0.2111        20.6448
      8   0.2680   0.7829        [35m0.2422[0m       0.9274        0.2251        20.3173
      9   0.3039   0.7836        [35m0.2411[0m       0.9274        0.2189        20.7208
     10   0.2100   0.7435        0.2468       0.9105        0.2390        20.6054
     11   0.2131   0.7425        0.2433       0.9190        0.2328        20.3873
     12   0.1945   0.7179        0.2579       0.9299        0.2279        20.2709
     13   0.1926   0.7162        0.2556       0.9262        0.2348        20.4916
     14   0.1883   0.7196        0.2565       0.9262        0.2326        20.1067
     15   0.1969   0.7180        0.2557       0.9311        0.2371        20.3942
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 12:15:53,420][0m Trial 614 finished with value: 0.20896398230143873 and parameters: {'lr': 0.0041696650700593614, 'dropout': 0.47447014268151705, 'd_model_multiplier': 4, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 184, 'batch_size': 79, 'pos_encoding': 'learnable', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 98}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 128
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2323[0m   [32m0.7115[0m        [35m0.2832[0m       [31m0.9250[0m        [94m0.2611[0m     +  20.3536
      2   [36m0.2422[0m   [32m0.7144[0m        [35m0.2578[0m       0.9238        0.2621        20.3476
      3   [36m0.2561[0m   0.6922        [35m0.2567[0m       0.9238        0.3082        20.8359
      4   0.1939   0.7093        [35m0.2545[0m       0.9141        0.3782        20.9198
      5   0.2133   0.7081        0.2570       0.9190        0.3470        20.9045
      6   0.2512   [32m0.7393[0m        0.2548       0.9238        [94m0.2435[0m     +  20.7593
      7   [36m0.2609[0m   [32m0.7465[0m        [35m0.2449[0m       0.9238        [94m0.2373[0m     +  21.9314
      8   0.2601   [32m0.7843[0m        [35m0.2387[0m       0.9202        [94m0.2316[0m     +  20.6813
      9   0.1993   0.7115        0.2405       0.8839        0.2918        20.7250
     10   0.2083   0.7252        0.2481       0.9238        0.2475        20.7838
     11   0.2422   0.7376        0.2757       0.9238        0.2468        20.8273
     12   0.2074   0.7415        0.2887       0.9190        0.2616        20.8318
     13   0.1880   0.7238        0.2821       0.9238        0.2539        20.9453
     14   0.1870   0.7236        0.2730       0.9214        0.2615        20.9078
     15   0.1919   0.7203        0.2701       0.9226        0.2520        20.8497
     16   0.1715   0.7193        0.2654       0.9238        0.2827        20.5893
     17   0.1831   0.7158        0.2642       0.9069        0.2807        20.6928
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 12:22:08,500][0m Trial 615 finished with value: 0.23157341403497028 and parameters: {'lr': 0.0030920683930648852, 'dropout': 0.45472985006390104, 'd_model_multiplier': 4, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 201, 'batch_size': 70, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 128}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 112
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2193[0m   [32m0.7243[0m        [35m0.2852[0m       [31m0.8912[0m        [94m0.3118[0m     +  34.5035
      2   [36m0.2273[0m   0.7002        [35m0.2728[0m       [31m0.9057[0m        0.3147        34.2884
      3   0.2234   0.6773        [35m0.2713[0m       [31m0.9069[0m        0.3661        35.0334
      4   [36m0.3192[0m   [32m0.7268[0m        [35m0.2566[0m       0.9069        0.3216        34.7948
      5   0.2595   [32m0.7271[0m        [35m0.2548[0m       0.9069        0.3369        34.6087
      6   0.2213   0.6789        [35m0.2513[0m       0.9069        0.3670        34.6107
      7   0.2251   0.6806        0.2521       0.9069        0.3480        34.4902
      8   0.2286   0.6864        0.2517       0.9069        0.3450        34.6775
      9   0.2218   0.6826        [35m0.2487[0m       0.9069        0.3384        34.5978
     10   0.2241   0.6845        0.2494       0.9069        0.3253        34.7782
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 12:28:29,740][0m Trial 616 finished with value: 0.3118120632984618 and parameters: {'lr': 0.005762835256916846, 'dropout': 0.4397951035342859, 'd_model_multiplier': 4, 'num_layers': 4, 'n_heads': 64, 'dim_feedforward': 167, 'batch_size': 83, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 112}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 108
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2980[0m   [32m0.7221[0m        [35m0.2660[0m       [31m0.9117[0m        [94m0.2750[0m     +  19.9603
      2   [36m0.3044[0m   [32m0.7304[0m        [35m0.2421[0m       0.9057        0.2840        20.2538
      3   0.2821   [32m0.7348[0m        0.2463       0.8972        0.2795        20.3950
      4   [36m0.3205[0m   [32m0.7390[0m        [35m0.2380[0m       0.8996        0.2771        20.8154
      5   0.2741   [32m0.7404[0m        0.2382       0.9093        [94m0.2706[0m     +  20.2227
      6   0.3176   [32m0.7447[0m        [35m0.2372[0m       0.9021        0.2746        20.2357
      7   0.2987   [32m0.7485[0m        [35m0.2311[0m       0.8984        0.2857        20.4923
      8   0.3190   [32m0.7558[0m        [35m0.2311[0m       0.9069        0.2772        20.5106
      9   0.2969   0.7455        [35m0.2274[0m       0.9021        0.2801        20.5555
     10   0.3064   0.7365        0.2286       0.9069        0.2924        20.4777
     11   [36m0.3213[0m   0.7479        0.2307       0.9033        0.2784        20.4645
     12   0.2795   0.7330        0.2307       0.9057        0.2824        20.4527
     13   0.3016   0.7345        [35m0.2272[0m       0.8960        0.2946        20.3969
     14   0.2976   0.7261        0.2288       0.9057        0.2787        20.4482
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 12:33:36,367][0m Trial 617 finished with value: 0.2706386031810433 and parameters: {'lr': 0.0022421894198176233, 'dropout': 0.4642932584212474, 'd_model_multiplier': 4, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 316, 'batch_size': 69, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 108}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 104
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2061[0m   [32m0.6174[0m        [35m0.6963[0m       [31m0.5490[0m        [94m0.6871[0m     +  20.0489
      2   0.2042   0.6105        [35m0.6925[0m       [31m0.6058[0m        [94m0.6815[0m     +  20.6217
      3   0.2007   0.5999        [35m0.6880[0m       [31m0.6614[0m        [94m0.6745[0m     +  20.2025
      4   0.1986   0.5883        [35m0.6814[0m       [31m0.7412[0m        [94m0.6664[0m     +  20.4392
      5   0.1986   0.5776        [35m0.6720[0m       [31m0.8283[0m        [94m0.6574[0m     +  20.3518
      6   0.1866   0.5655        [35m0.6623[0m       [31m0.8730[0m        [94m0.6478[0m     +  20.6294
      7   0.1684   0.5458        [35m0.6521[0m       [31m0.8936[0m        [94m0.6378[0m     +  20.7250
      8   0.1440   0.5193        [35m0.6423[0m       [31m0.9021[0m        [94m0.6274[0m     +  20.5691
      9   0.1163   0.4898        [35m0.6311[0m       [31m0.9057[0m        [94m0.6168[0m     +  20.6600
     10   0.1105   0.4595        [35m0.6215[0m       0.9057        [94m0.6061[0m     +  20.6926
     11   0.1064   0.4364        [35m0.6121[0m       0.9045        [94m0.5953[0m     +  20.5077
     12   0.1034   0.4178        [35m0.5981[0m       0.9045        [94m0.5846[0m     +  20.5520
     13   0.0897   0.4023        [35m0.5892[0m       0.9045        [94m0.5739[0m     +  20.7018
     14   0.0875   0.3887        [35m0.5786[0m       0.9045        [94m0.5634[0m     +  20.9588
     15   0.0860   0.3777        [35m0.5684[0m       0.9045        [94m0.5531[0m     +  20.5366
     16   0.0852   0.3693        [35m0.5591[0m       0.9045        [94m0.5429[0m     +  20.7988
     17   0.0844   0.3620        [35m0.5469[0m       0.9045        [94m0.5331[0m     +  20.4422
     18   0.0840   0.3567        [35m0.5360[0m       0.9045        [94m0.5234[0m     +  20.8964
     19   0.0836   0.3521        [35m0.5268[0m       0.9045        [94m0.5140[0m     +  20.6315
     20   0.0834   0.3492        [35m0.5172[0m       0.9045        [94m0.5050[0m     +  20.4827
     21   0.0771   0.3475        [35m0.5083[0m       0.9045        [94m0.4962[0m     +  20.3754
     22   0.0750   0.3460        [35m0.4981[0m       0.9045        [94m0.4877[0m     +  20.5899
     23   0.0740   0.3452        [35m0.4906[0m       0.9045        [94m0.4795[0m     +  20.6610
     24   0.0740   0.3450        [35m0.4812[0m       0.9045        [94m0.4716[0m     +  20.8587
     25   0.0741   0.3448        [35m0.4725[0m       0.9045        [94m0.4641[0m     +  20.5847
     26   0.0742   0.3451        [35m0.4649[0m       0.9045        [94m0.4568[0m     +  20.7601
     27   0.0736   0.3458        [35m0.4573[0m       0.9045        [94m0.4498[0m     +  20.6273
     28   0.0736   0.3463        [35m0.4490[0m       0.9045        [94m0.4431[0m     +  20.6844
     29   0.0738   0.3474        [35m0.4415[0m       0.9045        [94m0.4368[0m     +  20.5647
     30   0.0739   0.3486        [35m0.4342[0m       0.9045        [94m0.4307[0m     +  20.7123
     31   0.0741   0.3500        [35m0.4282[0m       0.9045        [94m0.4249[0m     +  20.7004
     32   0.0740   0.3519        [35m0.4214[0m       0.9045        [94m0.4194[0m     +  20.5305
     33   0.0739   0.3537        [35m0.4143[0m       0.9045        [94m0.4141[0m     +  20.5167
     34   0.0738   0.3562        [35m0.4090[0m       0.9045        [94m0.4090[0m     +  20.6732
     35   0.0738   0.3585        [35m0.4036[0m       0.9045        [94m0.4042[0m     +  20.5721
     36   0.0741   0.3610        [35m0.3967[0m       0.9045        [94m0.3996[0m     +  20.3960
     37   0.0744   0.3649        [35m0.3929[0m       0.9045        [94m0.3952[0m     +  20.3254
     38   0.0748   0.3685        [35m0.3878[0m       0.9045        [94m0.3910[0m     +  20.5281
     39   0.0753   0.3730        [35m0.3818[0m       0.9045        [94m0.3870[0m     +  20.9554
     40   0.0757   0.3770        [35m0.3776[0m       0.9045        [94m0.3832[0m     +  20.7671
     41   0.0763   0.3816        [35m0.3725[0m       0.9045        [94m0.3796[0m     +  20.6081
     42   0.0768   0.3868        [35m0.3682[0m       0.9045        [94m0.3761[0m     +  20.7149
     43   0.0774   0.3922        [35m0.3634[0m       0.9045        [94m0.3728[0m     +  20.5765
     44   0.0780   0.3970        [35m0.3603[0m       0.9045        [94m0.3697[0m     +  20.6559
     45   0.0785   0.4019        [35m0.3554[0m       0.9045        [94m0.3667[0m     +  20.5660
     46   0.0791   0.4073        [35m0.3514[0m       0.9045        [94m0.3639[0m     +  20.4786
     47   0.0796   0.4121        [35m0.3497[0m       0.9045        [94m0.3612[0m     +  20.7455
     48   0.0803   0.4176        [35m0.3466[0m       0.9045        [94m0.3586[0m     +  20.5958
     49   0.0809   0.4232        [35m0.3426[0m       0.9045        [94m0.3562[0m     +  20.5496
     50   0.0816   0.4281        [35m0.3387[0m       0.9045        [94m0.3538[0m     +  20.5303
[32m[I 2023-05-05 12:50:50,596][0m Trial 618 finished with value: 0.35382953167249964 and parameters: {'lr': 2.6801543299196064e-07, 'dropout': 0.48316653320909425, 'd_model_multiplier': 4, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 327, 'batch_size': 77, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 104}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 115
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2933[0m   [32m0.7784[0m        [35m0.2751[0m       [31m0.9069[0m        [94m0.2597[0m     +  19.9814
      2   0.2822   0.7741        [35m0.2558[0m       0.9045        0.2818        20.1204
      3   0.2342   0.7533        [35m0.2556[0m       [31m0.9141[0m        0.2632        20.4210
      4   0.2759   0.6780        [35m0.2468[0m       0.9105        0.2684        20.4550
      5   [36m0.3338[0m   0.7612        0.2494       0.9141        [94m0.2595[0m     +  20.2646
      6   0.1964   0.7069        0.2585       [31m0.9214[0m        [94m0.2591[0m     +  20.3963
      7   0.2027   0.7054        0.2576       [31m0.9226[0m        0.2606        20.1851
      8   0.2012   0.7137        0.2560       0.9202        [94m0.2574[0m     +  20.2668
      9   0.2084   0.7137        0.2535       0.9202        [94m0.2510[0m     +  20.4198
     10   0.2053   0.7154        0.2546       0.9202        0.2534        20.2606
     11   0.2682   0.6988        [35m0.2465[0m       0.9226        0.2749        20.3577
     12   0.2172   0.7200        [35m0.2433[0m       0.9214        0.2582        20.2821
     13   0.2005   0.7106        0.2533       0.9226        0.2542        20.4190
     14   0.2066   0.7164        0.2556       0.9202        [94m0.2504[0m     +  20.1047
     15   0.2793   0.7478        0.2487       [31m0.9250[0m        [94m0.2436[0m     +  20.0374
     16   0.2034   0.7125        0.2461       0.9202        0.2541        20.3373
     17   0.2025   0.7098        0.2539       0.9202        0.2504        20.4838
     18   0.2006   0.7114        0.2544       0.9202        0.2535        20.4977
     19   0.2030   0.7121        0.2530       0.9238        0.2512        20.5759
     20   0.2019   0.7168        0.2538       0.9214        0.2530        20.5013
     21   0.2028   0.7127        0.2531       0.9214        0.2526        20.3080
     22   0.2012   0.7094        0.2535       0.9202        0.2531        20.2350
     23   0.2023   0.7117        0.2525       0.9226        0.2606        20.2146
     24   0.2047   0.7123        0.2538       0.9202        0.2686        20.0552
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 12:59:18,840][0m Trial 619 finished with value: 0.2435811486505107 and parameters: {'lr': 0.0031396967963481805, 'dropout': 0.44776985754239, 'd_model_multiplier': 4, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 177, 'batch_size': 71, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 115}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 126
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.1274[0m   [32m0.6002[0m        [35m0.4224[0m       [31m0.9262[0m        [94m0.2630[0m     +  9.0723
      2   [36m0.2034[0m   [32m0.6647[0m        [35m0.2603[0m       0.9262        [94m0.2482[0m     +  8.4620
      3   [36m0.2461[0m   [32m0.7048[0m        [35m0.2467[0m       0.9262        [94m0.2424[0m     +  8.9938
      4   [36m0.2778[0m   [32m0.7235[0m        [35m0.2399[0m       [31m0.9287[0m        [94m0.2387[0m     +  9.2348
      5   [36m0.2954[0m   [32m0.7348[0m        [35m0.2347[0m       [31m0.9299[0m        0.2394        9.3097
      6   0.2946   [32m0.7460[0m        [35m0.2316[0m       [31m0.9323[0m        [94m0.2363[0m     +  9.7480
      7   0.2921   [32m0.7550[0m        [35m0.2273[0m       0.9311        0.2368        9.4916
      8   0.2846   [32m0.7667[0m        [35m0.2225[0m       0.9311        [94m0.2329[0m     +  8.9627
      9   0.2759   0.7599        [35m0.2208[0m       0.9311        0.2351        9.3528
     10   0.2834   0.7605        [35m0.2174[0m       0.9287        0.2340        9.5528
     11   0.2841   0.7621        [35m0.2130[0m       0.9299        0.2383        9.2143
     12   0.2902   0.7635        [35m0.2111[0m       [31m0.9335[0m        0.2380        9.4803
     13   0.2812   0.7567        [35m0.2072[0m       0.9311        0.2412        9.2664
     14   0.2918   0.7649        [35m0.2042[0m       0.9287        0.2403        9.5306
     15   0.2877   0.7652        [35m0.2020[0m       0.9287        0.2417        9.6835
     16   0.2831   0.7630        [35m0.1988[0m       0.9299        0.2447        9.7382
     17   0.2889   0.7664        [35m0.1956[0m       0.9287        0.2459        9.5305
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 13:02:07,473][0m Trial 620 finished with value: 0.23294426098988452 and parameters: {'lr': 0.0008809816122762438, 'dropout': 0.23173499548997517, 'd_model_multiplier': 4, 'num_layers': 2, 'n_heads': 4, 'dim_feedforward': 187, 'batch_size': 66, 'pos_encoding': 'learnable', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 126}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 122
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2199[0m   [32m0.7370[0m        [35m0.3265[0m       [31m0.8936[0m        [94m0.5227[0m     +  16.7810
      2   0.2172   0.7244        [35m0.2684[0m       [31m0.9021[0m        [94m0.3901[0m     +  17.0208
      3   [36m0.2254[0m   0.7345        [35m0.2660[0m       [31m0.9081[0m        [94m0.3297[0m     +  16.9264
      4   [36m0.2258[0m   0.7235        [35m0.2611[0m       [31m0.9093[0m        [94m0.3110[0m     +  17.2189
      5   [36m0.2272[0m   0.7297        [35m0.2580[0m       [31m0.9129[0m        0.3298        16.9308
      6   [36m0.2359[0m   0.7266        0.2609       0.9093        0.4080        16.9378
      7   0.2079   0.6698        [35m0.2569[0m       0.9117        0.3973        17.0005
      8   0.2232   0.7327        0.2606       0.8972        0.4492        16.8570
      9   0.1914   0.6276        0.2675       0.9093        0.3919        16.9876
     10   0.1849   0.6272        0.2599       0.9045        0.3388        16.8655
     11   [36m0.2366[0m   [32m0.7463[0m        0.2635       0.9045        [94m0.2854[0m     +  16.7940
     12   0.2187   0.7131        [35m0.2508[0m       0.9033        0.2979        17.6830
     13   0.2325   [32m0.7681[0m        [35m0.2501[0m       0.9093        [94m0.2557[0m     +  16.7565
     14   0.2349   [32m0.7706[0m        [35m0.2443[0m       0.9045        [94m0.2541[0m     +  16.8135
     15   0.2312   0.7586        0.2487       0.8948        0.2705        16.8982
     16   0.2353   0.7556        0.2460       [31m0.9141[0m        0.2557        16.7177
     17   0.2304   0.7625        0.2489       0.9117        0.2563        16.8769
     18   0.2219   0.7535        0.2496       [31m0.9154[0m        0.2617        17.0215
     19   0.2265   0.7542        0.2554       0.9154        0.2574        16.9979
     20   0.2199   0.7437        0.2496       0.9008        0.2742        16.8911
     21   0.2298   0.7457        0.2495       0.9141        0.2560        16.8744
     22   0.2165   0.7414        0.2493       0.9093        0.2607        16.9553
     23   0.2172   0.7410        0.2509       0.9069        0.2629        16.8871
     24   [36m0.2380[0m   0.7524        0.2469       0.9117        [94m0.2527[0m     +  16.9489
     25   0.2320   0.7431        0.2446       0.9105        0.2645        17.0372
     26   [36m0.2445[0m   0.7511        0.2481       0.9141        [94m0.2522[0m     +  17.0385
     27   0.2239   0.7477        0.2504       0.8912        0.2757        17.0426
     28   0.2282   0.7483        0.2476       0.9093        0.2633        17.0783
     29   0.2265   0.7409        [35m0.2438[0m       0.9093        0.2638        17.0191
     30   [36m0.2523[0m   0.7448        [35m0.2399[0m       0.9057        0.2756        17.0364
     31   0.2199   0.7385        0.2410       0.9069        0.2789        16.9305
     32   0.2307   0.7471        0.2417       0.9081        0.2692        16.9333
     33   0.2269   0.7414        0.2426       0.9093        0.2747        16.9509
     34   0.2214   0.7479        0.2426       0.9117        0.2648        17.0832
     35   0.2325   0.7440        [35m0.2391[0m       [31m0.9166[0m        0.2661        17.2269
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 13:12:19,476][0m Trial 621 finished with value: 0.25220930356500804 and parameters: {'lr': 0.0016477753065163602, 'dropout': 0.6764092493863441, 'd_model_multiplier': 4, 'num_layers': 3, 'n_heads': 32, 'dim_feedforward': 177, 'batch_size': 63, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 122}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 117
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2377[0m   [32m0.6778[0m        [35m0.4310[0m       [31m0.9057[0m        [94m0.2760[0m     +  16.4377
      2   [36m0.2952[0m   [32m0.7639[0m        [35m0.2536[0m       [31m0.9117[0m        [94m0.2515[0m     +  16.5317
      3   [36m0.3366[0m   [32m0.8028[0m        [35m0.2429[0m       [31m0.9129[0m        [94m0.2419[0m     +  16.4711
      4   [36m0.3759[0m   [32m0.8187[0m        [35m0.2359[0m       [31m0.9154[0m        [94m0.2364[0m     +  16.5936
      5   0.3747   [32m0.8221[0m        [35m0.2329[0m       0.9117        [94m0.2355[0m     +  16.7418
      6   [36m0.3780[0m   [32m0.8264[0m        [35m0.2312[0m       0.9093        [94m0.2331[0m     +  16.6554
      7   [36m0.3825[0m   [32m0.8283[0m        [35m0.2283[0m       0.9093        [94m0.2319[0m     +  16.5424
      8   0.3803   [32m0.8304[0m        [35m0.2268[0m       0.9117        [94m0.2313[0m     +  16.8638
      9   [36m0.3932[0m   [32m0.8327[0m        [35m0.2260[0m       0.9093        [94m0.2293[0m     +  16.5339
     10   [36m0.3937[0m   [32m0.8373[0m        [35m0.2228[0m       0.9045        [94m0.2278[0m     +  16.6932
     11   [36m0.3992[0m   [32m0.8396[0m        [35m0.2191[0m       0.8996        0.2283        16.5572
     12   [36m0.4024[0m   [32m0.8404[0m        [35m0.2178[0m       0.8996        0.2313        16.8850
     13   0.3804   0.8385        [35m0.2173[0m       0.8972        0.2336        16.7374
     14   0.3833   0.8376        [35m0.2157[0m       0.9021        0.2330        16.7202
     15   0.3601   0.8361        [35m0.2128[0m       0.8996        0.2377        17.0248
     16   0.3456   0.8352        [35m0.2108[0m       0.9021        0.2473        16.7736
     17   0.3593   0.8384        [35m0.2078[0m       0.9057        0.2412        16.6587
     18   0.3589   [32m0.8412[0m        [35m0.2059[0m       0.9021        0.2485        16.7494
     19   0.3461   0.8373        0.2060       0.9057        0.2498        16.8389
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 13:17:54,359][0m Trial 622 finished with value: 0.2277948610763884 and parameters: {'lr': 0.00017907491292421118, 'dropout': 0.3782092459489498, 'd_model_multiplier': 4, 'num_layers': 3, 'n_heads': 32, 'dim_feedforward': 158, 'batch_size': 74, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 117}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 99
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1301[0m   [32m0.5765[0m        [35m0.4407[0m       [31m0.9226[0m        [94m0.2727[0m     +  33.9717
      2   [36m0.2892[0m   [32m0.7416[0m        [35m0.2643[0m       0.9226        [94m0.2423[0m     +  34.0056
      3   [36m0.3378[0m   [32m0.7790[0m        [35m0.2485[0m       0.9226        [94m0.2342[0m     +  33.9822
      4   [36m0.3543[0m   [32m0.7902[0m        [35m0.2434[0m       [31m0.9262[0m        [94m0.2300[0m     +  33.8555
      5   [36m0.3557[0m   [32m0.7957[0m        [35m0.2395[0m       0.9250        [94m0.2269[0m     +  33.9229
      6   [36m0.3580[0m   0.7924        [35m0.2356[0m       0.9262        [94m0.2269[0m     +  34.0649
      7   [36m0.3610[0m   0.7880        0.2358       [31m0.9274[0m        0.2279        34.0449
      8   [36m0.3621[0m   0.7846        [35m0.2342[0m       0.9274        0.2293        33.8755
      9   [36m0.3717[0m   0.7882        [35m0.2333[0m       0.9274        0.2290        34.0863
     10   [36m0.3754[0m   0.7888        [35m0.2299[0m       [31m0.9311[0m        0.2297        33.7882
     11   0.3715   0.7869        [35m0.2295[0m       0.9311        0.2304        33.8472
     12   [36m0.3768[0m   0.7868        0.2296       0.9299        0.2310        34.0628
     13   [36m0.3787[0m   0.7880        0.2299       0.9274        0.2312        34.1350
     14   0.3760   0.7860        [35m0.2277[0m       0.9262        0.2323        33.9430
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 13:26:24,701][0m Trial 623 finished with value: 0.22689976822472252 and parameters: {'lr': 1.803708247732462e-05, 'dropout': 0.43259889882215574, 'd_model_multiplier': 4, 'num_layers': 4, 'n_heads': 64, 'dim_feedforward': 207, 'batch_size': 29, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 99}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 92
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0728[0m   [32m0.4257[0m        [35m0.5175[0m       [31m0.9250[0m        [94m0.3187[0m     +  20.5910
      2   [36m0.1432[0m   [32m0.6951[0m        [35m0.2946[0m       0.9250        [94m0.2528[0m     +  19.8269
      3   [36m0.1823[0m   [32m0.7424[0m        [35m0.2575[0m       0.9250        [94m0.2442[0m     +  20.6470
      4   [36m0.2020[0m   [32m0.7713[0m        [35m0.2458[0m       0.9178        0.2447        20.3874
      5   [36m0.2139[0m   [32m0.7776[0m        [35m0.2407[0m       0.9166        0.2472        20.3273
      6   [36m0.2194[0m   [32m0.7802[0m        [35m0.2361[0m       0.9129        0.2470        20.6511
      7   0.2190   [32m0.7831[0m        [35m0.2359[0m       0.9117        0.2478        20.6032
      8   0.2189   [32m0.7854[0m        [35m0.2338[0m       0.9129        0.2477        20.4114
      9   [36m0.2213[0m   [32m0.7863[0m        0.2343       0.9129        0.2480        20.3957
     10   0.2213   [32m0.7887[0m        [35m0.2313[0m       0.9129        0.2483        20.6949
     11   0.2178   0.7885        [35m0.2296[0m       0.9057        0.2541        20.3070
     12   0.2197   0.7877        [35m0.2292[0m       0.9081        0.2523        20.4425
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 13:30:50,923][0m Trial 624 finished with value: 0.24422834633411492 and parameters: {'lr': 6.041012384242411e-05, 'dropout': 0.5026120359567511, 'd_model_multiplier': 4, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 416, 'batch_size': 68, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 92}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 211
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1959[0m   [32m0.6319[0m        [35m0.3883[0m       [31m0.9021[0m        [94m0.3171[0m     +  24.3500
      2   [36m0.2517[0m   [32m0.7146[0m        [35m0.2466[0m       0.9021        [94m0.3138[0m     +  24.2977
      3   [36m0.2881[0m   [32m0.7371[0m        [35m0.2423[0m       0.9021        0.3210        24.3371
      4   [36m0.3080[0m   [32m0.7519[0m        [35m0.2373[0m       0.9021        0.3300        24.2996
      5   [36m0.3210[0m   [32m0.7595[0m        [35m0.2356[0m       0.9021        0.3206        24.4221
      6   [36m0.3440[0m   [32m0.7665[0m        [35m0.2341[0m       [31m0.9033[0m        [94m0.3084[0m     +  24.5123
      7   0.3429   [32m0.7676[0m        [35m0.2329[0m       0.9033        0.3195        24.5147
      8   [36m0.3491[0m   0.7674        [35m0.2279[0m       0.9021        0.3267        24.2564
      9   [36m0.3559[0m   [32m0.7680[0m        0.2297       0.9033        0.3158        24.4478
     10   [36m0.3840[0m   [32m0.7742[0m        [35m0.2261[0m       0.9021        0.3107        24.4849
     11   0.3771   0.7729        [35m0.2259[0m       0.9021        0.3174        24.3458
     12   0.3775   0.7671        [35m0.2219[0m       0.9033        0.3213        24.2377
     13   0.3747   0.7721        0.2245       0.9008        0.3171        24.6100
     14   [36m0.3919[0m   0.7684        0.2230       0.9021        0.3106        24.3777
     15   0.3731   0.7685        [35m0.2197[0m       0.9021        0.3257        24.6049
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 13:37:21,920][0m Trial 625 finished with value: 0.3084299602200071 and parameters: {'lr': 0.0003091120029892647, 'dropout': 0.45960444298359426, 'd_model_multiplier': 4, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 212, 'batch_size': 98, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 211}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 51
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0539[0m   [32m0.3440[0m        [35m0.5515[0m       [31m0.9287[0m        [94m0.3492[0m     +  19.6260
      2   [36m0.1663[0m   [32m0.6640[0m        [35m0.3192[0m       0.9287        [94m0.2485[0m     +  19.6225
      3   [36m0.2524[0m   [32m0.7437[0m        [35m0.2659[0m       0.9287        [94m0.2277[0m     +  19.6976
      4   [36m0.3161[0m   [32m0.7728[0m        [35m0.2516[0m       0.9287        [94m0.2190[0m     +  19.7715
      5   [36m0.3273[0m   [32m0.7892[0m        [35m0.2448[0m       [31m0.9311[0m        [94m0.2139[0m     +  19.7104
      6   0.3259   [32m0.7997[0m        [35m0.2386[0m       0.9299        [94m0.2128[0m     +  19.7142
      7   0.3249   [32m0.8038[0m        [35m0.2371[0m       0.9287        0.2130        19.6505
      8   0.3216   [32m0.8057[0m        [35m0.2360[0m       0.9274        0.2138        19.8989
      9   0.3183   [32m0.8070[0m        [35m0.2341[0m       0.9250        0.2136        19.7410
     10   0.3214   [32m0.8089[0m        0.2342       0.9274        0.2135        19.6760
     11   0.3205   [32m0.8101[0m        [35m0.2328[0m       0.9274        0.2136        19.7051
     12   0.3171   [32m0.8108[0m        [35m0.2328[0m       0.9262        0.2138        19.9681
     13   0.3142   [32m0.8111[0m        [35m0.2326[0m       0.9262        0.2139        19.8357
     14   0.3160   [32m0.8124[0m        [35m0.2308[0m       0.9262        0.2134        19.9138
     15   0.3149   [32m0.8130[0m        [35m0.2308[0m       0.9262        0.2131        20.1603
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 13:42:39,245][0m Trial 626 finished with value: 0.212809831908529 and parameters: {'lr': 1.0302516067004354e-05, 'dropout': 0.42528128066743276, 'd_model_multiplier': 4, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 194, 'batch_size': 23, 'pos_encoding': 'learnable', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 51}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 91
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0698[0m   [32m0.3901[0m        [35m0.6358[0m       [31m0.9287[0m        [94m0.5828[0m     +  33.4706
      2   [36m0.0826[0m   [32m0.4766[0m        [35m0.4542[0m       0.9287        [94m0.4056[0m     +  33.5053
      3   [36m0.1251[0m   [32m0.6377[0m        [35m0.3287[0m       0.9287        [94m0.3007[0m     +  33.4361
      4   [36m0.1634[0m   [32m0.7017[0m        [35m0.2889[0m       0.9287        [94m0.2729[0m     +  33.7096
      5   [36m0.1774[0m   [32m0.7223[0m        [35m0.2797[0m       0.9287        [94m0.2518[0m     +  33.6565
      6   [36m0.1948[0m   [32m0.7396[0m        [35m0.2719[0m       0.9287        [94m0.2422[0m     +  33.4191
      7   [36m0.2035[0m   [32m0.7479[0m        [35m0.2706[0m       0.9287        [94m0.2394[0m     +  33.7272
      8   [36m0.2183[0m   [32m0.7546[0m        [35m0.2661[0m       0.9287        [94m0.2345[0m     +  33.2852
      9   [36m0.2383[0m   [32m0.7592[0m        [35m0.2641[0m       0.9287        [94m0.2267[0m     +  33.5024
     10   [36m0.2475[0m   0.7592        [35m0.2594[0m       [31m0.9299[0m        0.2267        33.4425
     11   0.2346   [32m0.7600[0m        [35m0.2574[0m       0.9299        0.2270        33.2565
     12   [36m0.2545[0m   [32m0.7686[0m        0.2575       0.9299        [94m0.2237[0m     +  33.1985
     13   0.2443   0.7673        [35m0.2504[0m       0.9287        0.2249        33.2819
     14   [36m0.2598[0m   [32m0.7730[0m        0.2522       0.9299        [94m0.2227[0m     +  33.2329
     15   [36m0.2667[0m   [32m0.7766[0m        0.2535       0.9287        [94m0.2214[0m     +  33.1388
     16   0.2645   0.7752        [35m0.2489[0m       [31m0.9311[0m        0.2225        33.3939
     17   [36m0.2858[0m   [32m0.7821[0m        0.2501       0.9299        [94m0.2191[0m     +  32.9977
     18   0.2822   [32m0.7852[0m        [35m0.2467[0m       0.9287        [94m0.2188[0m     +  33.2148
     19   0.2802   0.7814        0.2469       0.9299        [94m0.2185[0m     +  33.5327
     20   [36m0.3007[0m   [32m0.7899[0m        0.2467       0.9287        [94m0.2169[0m     +  33.4336
     21   0.2815   0.7878        [35m0.2423[0m       0.9299        0.2178        33.4816
     22   0.2920   [32m0.7926[0m        [35m0.2393[0m       0.9287        [94m0.2162[0m     +  33.4768
     23   0.2896   [32m0.7948[0m        [35m0.2389[0m       0.9287        0.2168        33.5595
     24   0.2927   0.7940        0.2404       0.9287        0.2171        33.3396
     25   0.2859   [32m0.7959[0m        0.2432       0.9274        0.2170        33.4141
     26   0.2773   0.7950        [35m0.2379[0m       0.9274        0.2163        33.4460
     27   0.2727   0.7935        [35m0.2350[0m       0.9274        0.2176        33.3067
     28   0.2875   [32m0.8030[0m        [35m0.2349[0m       0.9262        [94m0.2149[0m     +  33.2967
     29   0.2809   0.7974        [35m0.2344[0m       0.9274        0.2165        33.3753
     30   0.2725   0.7983        0.2357       0.9250        0.2163        33.1763
     31   0.2771   0.7974        [35m0.2335[0m       0.9250        0.2162        33.0914
     32   0.2767   0.8015        [35m0.2315[0m       0.9274        0.2155        33.5217
     33   0.2718   [32m0.8047[0m        0.2319       0.9238        0.2154        33.3437
     34   0.2797   [32m0.8087[0m        0.2335       0.9262        [94m0.2144[0m     +  33.5671
     35   0.2606   0.7983        [35m0.2313[0m       0.9238        0.2190        33.3443
     36   0.2666   0.8006        [35m0.2286[0m       0.9250        0.2169        33.3026
     37   0.2568   0.7982        [35m0.2278[0m       0.9238        0.2190        33.1827
     38   0.2511   0.7946        [35m0.2243[0m       0.9250        0.2203        33.1897
     39   0.2486   0.7912        0.2284       0.9226        0.2220        33.3603
     40   0.2532   0.7936        0.2303       0.9250        0.2209        33.1120
     41   0.2400   0.7922        0.2261       0.9238        0.2230        33.4182
     42   0.2324   0.7866        [35m0.2238[0m       0.9226        0.2255        32.9839
     43   0.2457   0.7873        [35m0.2223[0m       0.9238        0.2234        33.3667
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 14:07:09,256][0m Trial 627 finished with value: 0.21435253561768308 and parameters: {'lr': 0.00010576372980923381, 'dropout': 0.44998637697121197, 'd_model_multiplier': 1, 'num_layers': 10, 'n_heads': 32, 'dim_feedforward': 279, 'batch_size': 62, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 91}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 109
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.3315[0m   [32m0.7572[0m        [35m0.2730[0m       [31m0.8924[0m        [94m0.3026[0m     +  9.4664
      2   [36m0.3347[0m   [32m0.7652[0m        [35m0.2497[0m       [31m0.9081[0m        [94m0.2804[0m     +  9.8418
      3   [36m0.3350[0m   [32m0.7659[0m        [35m0.2446[0m       0.9033        [94m0.2738[0m     +  9.8618
      4   0.3148   0.7607        [35m0.2377[0m       0.9081        0.2784        9.9097
      5   0.3307   [32m0.7705[0m        [35m0.2344[0m       [31m0.9105[0m        0.2738        9.8347
      6   0.3177   0.7633        0.2350       0.9045        0.2873        10.0844
      7   [36m0.3538[0m   [32m0.7761[0m        [35m0.2303[0m       0.9105        0.2757        9.8879
      8   [36m0.3556[0m   [32m0.7907[0m        [35m0.2274[0m       0.8984        0.2859        9.8221
      9   0.3487   0.7731        [35m0.2263[0m       [31m0.9117[0m        [94m0.2733[0m     +  9.9891
     10   0.3138   0.7656        0.2300       0.9021        0.2915        10.1558
     11   0.3257   0.7720        0.2285       0.9069        0.2799        9.9177
     12   [36m0.3635[0m   [32m0.7918[0m        [35m0.2262[0m       0.9057        0.2862        9.8983
     13   0.2764   0.7441        0.2342       0.9008        0.3004        10.0589
     14   0.2903   0.7633        0.2299       0.9008        0.2915        9.9340
     15   0.2592   0.7398        0.2264       0.9021        0.2952        10.0314
     16   0.3062   0.7511        0.2263       0.9033        0.2903        10.1876
     17   0.2718   0.7433        0.2285       0.8984        0.3002        10.0752
     18   0.2707   0.7344        0.2292       0.8984        0.3047        9.8788
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 14:10:18,477][0m Trial 628 finished with value: 0.2732597738053951 and parameters: {'lr': 0.00239614983226298, 'dropout': 0.25876562335352865, 'd_model_multiplier': 4, 'num_layers': 1, 'n_heads': 32, 'dim_feedforward': 393, 'batch_size': 72, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 109}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 197
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.0576[0m   [32m0.3773[0m        [35m0.4595[0m       [31m0.9238[0m        [94m0.3259[0m     +  8.4487
      2   [36m0.2474[0m   [32m0.7038[0m        [35m0.2988[0m       0.9238        [94m0.2507[0m     +  9.5265
      3   [36m0.2686[0m   [32m0.7355[0m        [35m0.2661[0m       0.9238        [94m0.2388[0m     +  9.2770
      4   0.2593   [32m0.7531[0m        [35m0.2586[0m       0.9238        [94m0.2365[0m     +  9.6192
      5   0.2436   [32m0.7534[0m        [35m0.2562[0m       0.9238        0.2367        9.5025
      6   0.2606   [32m0.7544[0m        [35m0.2550[0m       0.9238        [94m0.2347[0m     +  9.5415
      7   0.2410   0.7490        [35m0.2549[0m       0.9238        0.2353        9.6211
      8   0.2333   0.7461        [35m0.2532[0m       0.9238        0.2358        9.6257
      9   0.2391   0.7472        0.2534       0.9238        0.2364        9.6436
     10   0.2290   0.7501        0.2533       0.9238        0.2417        9.6250
     11   [36m0.2902[0m   [32m0.7762[0m        [35m0.2471[0m       0.9238        0.2354        9.9129
     12   0.2784   0.7726        [35m0.2380[0m       0.9238        0.2370        9.7061
     13   0.2744   0.7705        0.2385       0.9238        0.2365        9.7650
     14   0.2736   0.7759        [35m0.2331[0m       0.9214        [94m0.2343[0m     +  9.4944
     15   0.2830   0.7733        [35m0.2316[0m       0.9238        0.2404        9.6916
     16   0.2838   0.7736        [35m0.2273[0m       0.9226        0.2399        9.8101
     17   0.2747   [32m0.7777[0m        0.2281       0.9226        0.2440        9.8833
     18   0.2691   0.7620        [35m0.2248[0m       [31m0.9274[0m        0.2483        9.8083
     19   0.2855   [32m0.7788[0m        0.2268       0.9274        0.2474        9.8986
     20   0.2705   0.7695        [35m0.2218[0m       0.9262        0.2446        10.0539
     21   0.2833   0.7705        0.2222       0.9262        0.2524        9.9020
     22   0.2664   0.7589        [35m0.2186[0m       0.9250        0.2516        9.7578
     23   [36m0.2987[0m   0.7730        0.2207       [31m0.9287[0m        0.2548        9.8500
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 14:14:10,886][0m Trial 629 finished with value: 0.23425529208011753 and parameters: {'lr': 0.0012300334682868745, 'dropout': 0.3613087133330714, 'd_model_multiplier': 2, 'num_layers': 3, 'n_heads': 4, 'dim_feedforward': 332, 'batch_size': 82, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0, 'top_n_features': 197}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 134
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
Warning, assumed runtime error: CUDA out of memory. Tried to allocate 3.24 GiB (GPU 0; 23.70 GiB total capacity; 11.27 GiB already allocated; 2.29 GiB free; 20.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[32m[I 2023-05-05 14:14:16,094][0m Trial 630 finished with value: 100.0 and parameters: {'lr': 0.0004628365041758846, 'dropout': 0.3901683314072248, 'd_model_multiplier': 4, 'num_layers': 5, 'n_heads': 64, 'dim_feedforward': 296, 'batch_size': 137, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 134}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 102
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0732[0m   [32m0.3381[0m        [35m0.5639[0m       [31m0.9069[0m        [94m0.4502[0m     +  38.1459
      2   [36m0.0874[0m   [32m0.4588[0m        [35m0.3842[0m       0.9069        [94m0.3380[0m     +  38.4794
      3   [36m0.1426[0m   [32m0.6264[0m        [35m0.3000[0m       0.9069        [94m0.3018[0m     +  38.4716
      4   [36m0.2274[0m   [32m0.7030[0m        [35m0.2692[0m       0.9069        [94m0.2881[0m     +  38.6145
      5   [36m0.2566[0m   [32m0.7239[0m        [35m0.2565[0m       0.9069        [94m0.2824[0m     +  38.5738
      6   [36m0.2745[0m   [32m0.7348[0m        [35m0.2498[0m       0.9069        [94m0.2789[0m     +  38.6157
      7   [36m0.2921[0m   [32m0.7442[0m        [35m0.2466[0m       0.9069        [94m0.2765[0m     +  38.6224
      8   [36m0.3004[0m   [32m0.7523[0m        [35m0.2441[0m       0.9069        [94m0.2742[0m     +  38.6542
      9   [36m0.3080[0m   [32m0.7594[0m        [35m0.2427[0m       [31m0.9081[0m        [94m0.2728[0m     +  38.6791
     10   [36m0.3135[0m   [32m0.7642[0m        [35m0.2408[0m       0.9081        [94m0.2719[0m     +  38.5836
     11   [36m0.3156[0m   [32m0.7690[0m        [35m0.2390[0m       0.9069        [94m0.2706[0m     +  38.5632
     12   [36m0.3214[0m   [32m0.7735[0m        [35m0.2370[0m       0.9057        [94m0.2692[0m     +  38.6690
     13   [36m0.3235[0m   [32m0.7768[0m        [35m0.2352[0m       0.9069        [94m0.2679[0m     +  38.6864
     14   [36m0.3278[0m   [32m0.7793[0m        [35m0.2341[0m       0.9045        [94m0.2672[0m     +  38.6167
     15   [36m0.3324[0m   [32m0.7818[0m        0.2346       0.9069        [94m0.2660[0m     +  38.6877
     16   [36m0.3403[0m   [32m0.7847[0m        [35m0.2324[0m       0.9081        [94m0.2655[0m     +  38.7284
     17   [36m0.3432[0m   [32m0.7864[0m        [35m0.2320[0m       0.9045        [94m0.2650[0m     +  38.6112
     18   [36m0.3434[0m   [32m0.7878[0m        [35m0.2310[0m       0.9057        [94m0.2645[0m     +  38.5374
     19   [36m0.3459[0m   [32m0.7892[0m        [35m0.2294[0m       0.9057        [94m0.2643[0m     +  38.4694
     20   [36m0.3491[0m   [32m0.7900[0m        0.2298       0.9057        [94m0.2637[0m     +  38.5525
     21   [36m0.3519[0m   0.7899        [35m0.2292[0m       [31m0.9093[0m        [94m0.2631[0m     +  38.6059
     22   [36m0.3530[0m   [32m0.7908[0m        [35m0.2286[0m       0.9081        0.2633        38.8264
     23   [36m0.3536[0m   [32m0.7910[0m        [35m0.2285[0m       0.9093        [94m0.2630[0m     +  38.5232
     24   [36m0.3549[0m   [32m0.7910[0m        [35m0.2284[0m       0.9081        0.2630        39.0260
     25   [36m0.3564[0m   [32m0.7912[0m        [35m0.2271[0m       0.9081        [94m0.2625[0m     +  38.6813
     26   [36m0.3579[0m   [32m0.7920[0m        0.2271       [31m0.9105[0m        0.2630        38.6479
     27   [36m0.3584[0m   0.7917        0.2273       0.9093        [94m0.2624[0m     +  38.6999
     28   [36m0.3602[0m   0.7917        0.2272       0.9081        [94m0.2620[0m     +  38.5965
     29   [36m0.3621[0m   [32m0.7921[0m        [35m0.2269[0m       0.9105        0.2623        38.7645
     30   [36m0.3621[0m   [32m0.7922[0m        [35m0.2260[0m       0.9093        0.2621        38.6268
     31   0.3611   0.7919        [35m0.2245[0m       0.9105        [94m0.2620[0m     +  38.6267
     32   [36m0.3637[0m   0.7921        0.2247       0.9093        0.2620        38.4773
     33   0.3621   [32m0.7922[0m        0.2263       0.9093        [94m0.2619[0m     +  38.6734
     34   0.3627   [32m0.7924[0m        0.2251       0.9093        0.2623        38.6887
     35   0.3628   0.7922        [35m0.2244[0m       0.9057        0.2620        38.5956
     36   0.3636   [32m0.7928[0m        0.2261       0.9057        0.2623        38.4262
     37   [36m0.3645[0m   [32m0.7931[0m        [35m0.2230[0m       0.9057        0.2622        38.6791
     38   [36m0.3653[0m   0.7931        0.2244       0.9057        0.2624        38.5099
     39   0.3641   0.7929        0.2236       0.9057        0.2626        38.6846
     40   0.3550   0.7926        0.2240       0.9069        0.2624        38.5541
     41   0.3548   0.7926        [35m0.2226[0m       0.9057        0.2623        38.7305
     42   0.3567   0.7930        0.2227       0.9057        0.2622        38.5736
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 14:42:14,497][0m Trial 631 finished with value: 0.2618878276876772 and parameters: {'lr': 9.123785906173506e-07, 'dropout': 0.4107103904644361, 'd_model_multiplier': 32, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 413, 'batch_size': 33, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 102}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 95
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
Warning, assumed runtime error: CUDA out of memory. Tried to allocate 1.52 GiB (GPU 0; 23.70 GiB total capacity; 19.38 GiB already allocated; 797.25 MiB free; 21.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[32m[I 2023-05-05 14:42:19,148][0m Trial 632 finished with value: 100.0 and parameters: {'lr': 0.004673847384492758, 'dropout': 0.4711872048879583, 'd_model_multiplier': 4, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 320, 'batch_size': 128, 'pos_encoding': 'learnable', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 95}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 40
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0611[0m   [32m0.3018[0m        [35m0.5576[0m       [31m0.9117[0m        [94m0.5560[0m     +  10.8438
      2   [36m0.0771[0m   [32m0.4093[0m        [35m0.4125[0m       0.9117        [94m0.4189[0m     +  10.6839
      3   [36m0.2098[0m   [32m0.6120[0m        [35m0.3132[0m       0.9105        [94m0.3475[0m     +  11.0690
      4   [36m0.2642[0m   [32m0.7169[0m        [35m0.2745[0m       [31m0.9141[0m        [94m0.3168[0m     +  11.1492
      5   [36m0.2903[0m   [32m0.7549[0m        [35m0.2601[0m       [31m0.9166[0m        [94m0.2917[0m     +  11.1797
      6   [36m0.3181[0m   [32m0.7898[0m        [35m0.2514[0m       0.9117        [94m0.2837[0m     +  11.1030
      7   [36m0.3257[0m   [32m0.7979[0m        [35m0.2500[0m       0.9117        [94m0.2789[0m     +  11.0958
      8   [36m0.3418[0m   [32m0.8049[0m        [35m0.2435[0m       0.9117        [94m0.2681[0m     +  10.9898
      9   0.3331   [32m0.8070[0m        0.2445       0.9117        [94m0.2661[0m     +  10.8508
     10   0.3247   [32m0.8103[0m        [35m0.2416[0m       0.9129        [94m0.2602[0m     +  10.8904
     11   0.3226   0.8083        [35m0.2394[0m       0.9141        [94m0.2590[0m     +  10.9982
     12   0.3229   0.8097        [35m0.2386[0m       0.9141        0.2610        11.1991
     13   0.3263   0.8063        [35m0.2384[0m       0.9141        0.2603        10.9132
     14   0.3224   0.8015        [35m0.2372[0m       0.9129        0.2612        11.0077
     15   0.3272   0.8075        0.2384       0.9141        [94m0.2543[0m     +  11.1119
     16   0.3321   0.8037        [35m0.2347[0m       0.9154        0.2554        11.1125
     17   0.3359   0.8041        0.2367       0.9154        0.2607        11.0166
     18   0.3413   0.8059        [35m0.2323[0m       0.9141        0.2582        10.9679
     19   [36m0.3450[0m   0.8069        [35m0.2320[0m       0.9154        0.2577        11.0241
     20   [36m0.3465[0m   0.8099        [35m0.2311[0m       0.9129        0.2548        10.9662
     21   [36m0.3524[0m   0.8083        0.2332       0.9129        0.2560        10.9832
     22   [36m0.3533[0m   0.8102        [35m0.2300[0m       0.9129        0.2562        11.1037
     23   [36m0.3575[0m   0.8090        [35m0.2286[0m       0.9154        [94m0.2525[0m     +  11.3127
     24   0.3555   0.8100        0.2293       0.9141        0.2542        11.0296
     25   0.3565   [32m0.8113[0m        0.2298       0.9154        0.2538        11.0023
     26   0.3574   [32m0.8117[0m        [35m0.2267[0m       0.9154        [94m0.2511[0m     +  11.0806
     27   [36m0.3671[0m   [32m0.8133[0m        0.2271       0.9154        0.2524        11.0913
     28   0.3630   0.8119        [35m0.2260[0m       0.9141        0.2526        11.1832
     29   0.3614   0.8113        [35m0.2228[0m       0.9141        0.2516        10.8191
     30   0.3653   [32m0.8144[0m        0.2228       0.9154        [94m0.2499[0m     +  11.1651
     31   0.3623   0.8141        0.2241       0.9141        [94m0.2494[0m     +  11.3108
     32   0.3564   0.8128        0.2233       0.9129        0.2527        11.1267
     33   0.3524   0.8125        [35m0.2220[0m       0.9129        0.2501        11.1822
     34   0.3659   [32m0.8149[0m        [35m0.2197[0m       0.9129        [94m0.2474[0m     +  11.1081
     35   0.3658   [32m0.8151[0m        [35m0.2196[0m       0.9129        [94m0.2474[0m     +  11.1422
     36   0.3640   [32m0.8164[0m        0.2202       0.9141        [94m0.2472[0m     +  11.2211
     37   0.3625   0.8161        0.2200       0.9154        [94m0.2466[0m     +  11.2005
     38   0.3618   [32m0.8170[0m        [35m0.2193[0m       0.9129        0.2500        11.3054
     39   0.3662   [32m0.8187[0m        0.2200       0.9129        0.2476        11.0940
     40   0.3647   [32m0.8189[0m        [35m0.2181[0m       0.9129        0.2490        11.0298
     41   0.3577   0.8178        [35m0.2164[0m       0.9117        0.2484        10.9709
     42   0.3648   [32m0.8195[0m        0.2179       0.9117        0.2477        11.2819
     43   0.3587   [32m0.8198[0m        [35m0.2159[0m       0.9129        0.2469        11.3129
     44   0.3632   0.8196        [35m0.2157[0m       0.9117        0.2470        11.0919
     45   0.3638   [32m0.8209[0m        [35m0.2152[0m       0.9105        0.2470        11.3295
     46   0.3540   0.8195        [35m0.2139[0m       0.9105        0.2485        10.9211
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 14:51:01,096][0m Trial 633 finished with value: 0.2466019164982557 and parameters: {'lr': 3.607357878045125e-05, 'dropout': 0.24115353507879272, 'd_model_multiplier': 1, 'num_layers': 2, 'n_heads': 32, 'dim_feedforward': 328, 'batch_size': 63, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 40}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 115
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0947[0m   [32m0.3983[0m        [35m0.5778[0m       [31m0.9081[0m        [94m0.4472[0m     +  19.9032
      2   [36m0.0996[0m   [32m0.4898[0m        [35m0.3679[0m       0.9081        [94m0.3230[0m     +  20.2368
      3   [36m0.1658[0m   [32m0.6210[0m        [35m0.2802[0m       0.9081        [94m0.2992[0m     +  20.4903
      4   [36m0.1825[0m   [32m0.6719[0m        [35m0.2561[0m       0.9081        [94m0.2945[0m     +  20.3745
      5   [36m0.2010[0m   [32m0.7043[0m        [35m0.2464[0m       0.9081        [94m0.2922[0m     +  20.3643
      6   [36m0.2118[0m   [32m0.7246[0m        [35m0.2406[0m       0.9081        [94m0.2904[0m     +  20.5161
      7   [36m0.2256[0m   [32m0.7403[0m        [35m0.2375[0m       0.9069        [94m0.2892[0m     +  20.3808
      8   [36m0.2360[0m   [32m0.7491[0m        [35m0.2341[0m       0.9045        0.2894        20.3128
      9   [36m0.2438[0m   [32m0.7529[0m        [35m0.2310[0m       0.9033        [94m0.2884[0m     +  20.8034
     10   [36m0.2507[0m   [32m0.7532[0m        [35m0.2297[0m       0.9021        0.2899        20.3322
     11   [36m0.2528[0m   [32m0.7552[0m        [35m0.2283[0m       0.9057        [94m0.2881[0m     +  20.2279
     12   [36m0.2565[0m   [32m0.7561[0m        [35m0.2264[0m       0.9057        [94m0.2876[0m     +  20.3480
     13   [36m0.2604[0m   [32m0.7564[0m        [35m0.2250[0m       0.9045        [94m0.2874[0m     +  20.4679
     14   [36m0.2618[0m   [32m0.7574[0m        [35m0.2250[0m       0.9045        [94m0.2870[0m     +  20.2165
     15   [36m0.2649[0m   [32m0.7582[0m        [35m0.2244[0m       0.9045        [94m0.2865[0m     +  20.1906
     16   [36m0.2686[0m   0.7580        [35m0.2220[0m       0.9045        0.2875        20.4812
     17   0.2680   0.7575        [35m0.2215[0m       0.9033        0.2876        20.5175
     18   [36m0.2718[0m   [32m0.7585[0m        [35m0.2206[0m       0.9045        0.2886        20.3044
     19   [36m0.2747[0m   [32m0.7592[0m        [35m0.2190[0m       0.9045        0.2890        20.3154
     20   0.2736   [32m0.7599[0m        [35m0.2187[0m       0.9033        0.2888        20.4326
     21   0.2731   0.7598        [35m0.2186[0m       0.9045        0.2889        20.3471
     22   [36m0.2762[0m   [32m0.7601[0m        [35m0.2173[0m       0.9021        0.2896        20.1760
     23   0.2726   0.7593        0.2174       0.9021        0.2897        20.2406
     24   0.2746   0.7598        [35m0.2166[0m       0.9033        0.2879        20.3799
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 14:59:31,108][0m Trial 634 finished with value: 0.28654523166808715 and parameters: {'lr': 2.7416802351692752e-05, 'dropout': 0.34889445133396596, 'd_model_multiplier': 4, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 311, 'batch_size': 76, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 115}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 86
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1474[0m   [32m0.6477[0m        [35m0.2774[0m       [31m0.9323[0m        [94m0.2401[0m     +  18.4522
      2   [36m0.1582[0m   [32m0.6483[0m        [35m0.2595[0m       0.9323        [94m0.2359[0m     +  18.3350
      3   [36m0.1790[0m   0.6422        [35m0.2584[0m       0.9323        0.2406        18.7179
      4   [36m0.1868[0m   [32m0.6496[0m        [35m0.2569[0m       0.9311        0.2431        18.8819
      5   [36m0.1956[0m   0.6491        [35m0.2552[0m       0.9299        0.2381        17.9347
      6   [36m0.1962[0m   0.6447        [35m0.2542[0m       0.9311        0.2460        18.0318
      7   [36m0.2215[0m   0.6464        0.2545       [31m0.9347[0m        0.2458        17.8095
      8   0.2060   0.6467        0.2542       0.9311        0.2470        17.8943
      9   0.1968   0.6439        [35m0.2530[0m       0.9311        0.2439        18.4083
     10   0.1989   0.6456        0.2542       0.9335        0.2487        17.6841
     11   0.2107   0.6457        [35m0.2528[0m       0.9335        0.2486        17.8210
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 15:03:09,710][0m Trial 635 finished with value: 0.23592048870825277 and parameters: {'lr': 0.0010270702525721153, 'dropout': 0.12199645424533856, 'd_model_multiplier': 4, 'num_layers': 8, 'n_heads': 8, 'dim_feedforward': 384, 'batch_size': 14, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 86}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 55
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0771[0m   [32m0.3772[0m        [35m0.6957[0m       [31m0.9154[0m        [94m0.6397[0m     +  22.0597
      2   0.0613   0.3277        [35m0.5945[0m       [31m0.9202[0m        [94m0.5148[0m     +  22.6276
      3   0.0611   0.3308        [35m0.4825[0m       0.9202        [94m0.4100[0m     +  22.3888
      4   0.0628   0.3602        [35m0.3936[0m       0.9202        [94m0.3384[0m     +  22.5563
      5   [36m0.0913[0m   [32m0.4601[0m        [35m0.3328[0m       0.9202        [94m0.2995[0m     +  22.8517
      6   [36m0.1242[0m   [32m0.5426[0m        [35m0.2977[0m       0.9202        [94m0.2813[0m     +  22.2275
      7   [36m0.1587[0m   [32m0.5999[0m        [35m0.2790[0m       0.9202        [94m0.2719[0m     +  22.5253
      8   [36m0.1811[0m   [32m0.6378[0m        [35m0.2691[0m       0.9202        [94m0.2667[0m     +  22.2101
      9   [36m0.1926[0m   [32m0.6634[0m        [35m0.2630[0m       0.9202        [94m0.2636[0m     +  22.3061
     10   [36m0.1971[0m   [32m0.6808[0m        [35m0.2585[0m       0.9202        [94m0.2617[0m     +  22.2758
     11   [36m0.2103[0m   [32m0.6921[0m        [35m0.2537[0m       0.9202        [94m0.2608[0m     +  22.2154
     12   [36m0.2153[0m   [32m0.7008[0m        [35m0.2520[0m       0.9202        [94m0.2602[0m     +  22.4224
     13   [36m0.2222[0m   [32m0.7075[0m        [35m0.2479[0m       0.9202        [94m0.2600[0m     +  22.6433
     14   [36m0.2223[0m   [32m0.7122[0m        [35m0.2467[0m       0.9202        [94m0.2600[0m     +  22.8075
     15   [36m0.2247[0m   [32m0.7172[0m        [35m0.2448[0m       0.9202        [94m0.2597[0m     +  22.5343
     16   [36m0.2364[0m   [32m0.7221[0m        [35m0.2438[0m       0.9190        [94m0.2594[0m     +  22.6284
     17   [36m0.2382[0m   [32m0.7255[0m        [35m0.2406[0m       0.9166        [94m0.2591[0m     +  22.4218
     18   [36m0.2402[0m   [32m0.7290[0m        [35m0.2381[0m       0.9178        0.2592        22.3964
     19   [36m0.2425[0m   [32m0.7312[0m        [35m0.2364[0m       0.9178        0.2597        22.2483
     20   [36m0.2428[0m   [32m0.7339[0m        [35m0.2334[0m       0.9178        0.2594        22.4033
     21   [36m0.2444[0m   [32m0.7363[0m        [35m0.2333[0m       0.9166        0.2601        22.0188
     22   [36m0.2456[0m   [32m0.7382[0m        [35m0.2325[0m       0.9178        0.2605        22.3206
     23   [36m0.2456[0m   [32m0.7392[0m        [35m0.2305[0m       0.9166        0.2599        22.2687
     24   [36m0.2466[0m   [32m0.7402[0m        0.2311       0.9178        0.2604        22.7191
     25   [36m0.2471[0m   [32m0.7412[0m        [35m0.2300[0m       0.9178        0.2610        22.2700
     26   0.2450   [32m0.7416[0m        0.2303       0.9178        0.2608        22.2667
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 15:13:16,741][0m Trial 636 finished with value: 0.2591072720764914 and parameters: {'lr': 2.093423480599479e-05, 'dropout': 0.43932351266173575, 'd_model_multiplier': 4, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 338, 'batch_size': 159, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0, 'top_n_features': 55}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 47
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
Warning, assumed runtime error: CUDA out of memory. Tried to allocate 1.59 GiB (GPU 0; 23.70 GiB total capacity; 18.83 GiB already allocated; 419.25 MiB free; 22.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[32m[I 2023-05-05 15:13:20,524][0m Trial 637 finished with value: 100.0 and parameters: {'lr': 0.006818904863612808, 'dropout': 0.40189586049273746, 'd_model_multiplier': 4, 'num_layers': 5, 'n_heads': 64, 'dim_feedforward': 220, 'batch_size': 67, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'PlainRAdam', 'weight_decay': 0.1, 'top_n_features': 47}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 187
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2545[0m   [32m0.7315[0m        [35m0.3750[0m       [31m0.9226[0m        [94m0.2434[0m     +  30.0805
      2   [36m0.2821[0m   [32m0.7813[0m        [35m0.2572[0m       0.9226        [94m0.2345[0m     +  30.4039
      3   [36m0.2842[0m   [32m0.7919[0m        [35m0.2477[0m       0.9226        [94m0.2306[0m     +  30.4211
      4   [36m0.3107[0m   [32m0.7942[0m        [35m0.2469[0m       [31m0.9238[0m        [94m0.2274[0m     +  30.4813
      5   [36m0.3202[0m   [32m0.7945[0m        [35m0.2430[0m       [31m0.9250[0m        [94m0.2274[0m     +  30.3771
      6   [36m0.3317[0m   [32m0.7966[0m        [35m0.2409[0m       [31m0.9262[0m        [94m0.2251[0m     +  30.5344
      7   [36m0.3334[0m   [32m0.8017[0m        [35m0.2379[0m       0.9238        [94m0.2217[0m     +  30.4404
      8   [36m0.3496[0m   0.7971        [35m0.2371[0m       0.9238        0.2229        30.4970
      9   0.3472   0.7938        [35m0.2358[0m       [31m0.9287[0m        0.2249        30.5294
     10   0.3465   0.7962        [35m0.2343[0m       0.9250        [94m0.2208[0m     +  30.3973
     11   0.3145   0.7961        [35m0.2301[0m       0.9238        0.2289        30.4994
     12   0.3365   0.7929        0.2302       0.9238        0.2240        30.3026
     13   0.3401   0.7988        [35m0.2296[0m       0.9262        [94m0.2193[0m     +  30.6918
     14   0.3353   0.7970        [35m0.2287[0m       0.9226        0.2237        30.5689
     15   [36m0.3517[0m   0.8015        [35m0.2260[0m       0.9238        0.2201        30.5975
     16   0.3427   0.7943        0.2272       0.9287        0.2252        30.5004
     17   [36m0.3634[0m   [32m0.8257[0m        0.2267       0.9250        [94m0.2183[0m     +  30.6194
     18   0.3513   0.8170        [35m0.2249[0m       0.9238        0.2215        30.3915
     19   0.3208   0.7936        [35m0.2236[0m       0.9226        0.2286        30.6241
     20   0.3283   0.8002        0.2268       0.9274        0.2261        30.5260
     21   0.3399   0.7877        0.2239       0.9274        0.2277        30.6409
     22   0.3539   0.8092        0.2242       0.9226        0.2251        30.5340
     23   0.3254   0.7871        0.2238       0.9190        0.2302        30.5426
     24   0.3083   0.7962        [35m0.2200[0m       0.9238        0.2350        30.7182
     25   0.3369   0.7882        0.2224       0.9274        0.2289        30.5698
     26   0.3455   0.7930        0.2203       [31m0.9299[0m        0.2275        30.4917
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 15:27:05,680][0m Trial 638 finished with value: 0.21834994400733182 and parameters: {'lr': 0.00020862928270283643, 'dropout': 0.41799734476902517, 'd_model_multiplier': 1, 'num_layers': 9, 'n_heads': 32, 'dim_feedforward': 403, 'batch_size': 25, 'pos_encoding': 'learnable', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 187}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 57
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1598[0m   [32m0.5782[0m        [35m0.4685[0m       [31m0.9323[0m        [94m0.3110[0m     +  16.7549
      2   [36m0.2856[0m   [32m0.7296[0m        [35m0.2761[0m       [31m0.9347[0m        [94m0.2448[0m     +  16.4872
      3   [36m0.3124[0m   [32m0.7430[0m        [35m0.2588[0m       [31m0.9359[0m        [94m0.2329[0m     +  16.7966
      4   [36m0.3150[0m   [32m0.7544[0m        [35m0.2497[0m       [31m0.9371[0m        [94m0.2272[0m     +  17.0403
      5   0.3064   [32m0.7596[0m        0.2510       0.9347        [94m0.2238[0m     +  16.7668
      6   0.3093   [32m0.7632[0m        [35m0.2470[0m       0.9347        [94m0.2223[0m     +  16.8423
      7   0.3045   [32m0.7647[0m        [35m0.2444[0m       0.9359        [94m0.2180[0m     +  16.9783
      8   0.3109   0.7645        [35m0.2437[0m       0.9359        [94m0.2174[0m     +  16.7147
      9   0.3050   [32m0.7703[0m        [35m0.2382[0m       0.9347        [94m0.2145[0m     +  16.9335
     10   0.3111   0.7677        [35m0.2355[0m       0.9347        [94m0.2136[0m     +  16.8767
     11   [36m0.3161[0m   [32m0.7739[0m        0.2358       0.9347        [94m0.2103[0m     +  16.8145
     12   [36m0.3201[0m   [32m0.7759[0m        [35m0.2308[0m       0.9371        [94m0.2093[0m     +  17.0478
     13   [36m0.3203[0m   [32m0.7769[0m        0.2315       0.9347        [94m0.2081[0m     +  16.9958
     14   [36m0.3255[0m   [32m0.7791[0m        [35m0.2281[0m       0.9335        [94m0.2070[0m     +  16.9541
     15   [36m0.3294[0m   0.7787        0.2284       0.9359        [94m0.2060[0m     +  17.0558
     16   [36m0.3334[0m   [32m0.7835[0m        [35m0.2243[0m       0.9359        [94m0.2048[0m     +  16.9998
     17   [36m0.3345[0m   [32m0.7846[0m        0.2250       0.9371        [94m0.2038[0m     +  16.8267
     18   [36m0.3463[0m   [32m0.7853[0m        [35m0.2213[0m       0.9359        [94m0.2034[0m     +  16.8206
     19   0.3429   0.7852        [35m0.2212[0m       [31m0.9395[0m        0.2040        16.9092
     20   [36m0.3524[0m   [32m0.7895[0m        [35m0.2201[0m       0.9395        [94m0.2016[0m     +  16.7339
     21   [36m0.3579[0m   0.7894        [35m0.2185[0m       0.9395        0.2017        16.8264
     22   0.3563   [32m0.7915[0m        0.2193       [31m0.9407[0m        [94m0.2013[0m     +  16.6190
     23   0.3453   [32m0.7957[0m        [35m0.2141[0m       0.9371        [94m0.2010[0m     +  16.8897
     24   0.3575   0.7934        0.2174       0.9371        0.2014        16.7824
     25   0.3474   0.7923        0.2154       0.9371        0.2016        17.0248
     26   0.3496   0.7897        [35m0.2122[0m       0.9347        0.2024        16.8917
     27   0.3578   0.7920        0.2147       0.9383        0.2018        17.0131
     28   0.3410   0.7934        0.2141       0.9347        0.2032        17.0705
     29   0.3513   [32m0.7963[0m        [35m0.2117[0m       0.9359        0.2011        16.7838
     30   0.3452   0.7926        [35m0.2103[0m       0.9347        0.2027        16.7097
     31   0.3081   0.7892        [35m0.2076[0m       0.9335        0.2102        16.8760
     32   0.3139   0.7927        0.2094       0.9359        0.2133        17.6756
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 15:36:24,998][0m Trial 639 finished with value: 0.20095402401440665 and parameters: {'lr': 6.644634082174403e-05, 'dropout': 0.37341176553533284, 'd_model_multiplier': 4, 'num_layers': 3, 'n_heads': 32, 'dim_feedforward': 324, 'batch_size': 57, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'BatchNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 57}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 52
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1094[0m   [32m0.4332[0m        [35m0.5440[0m       [31m0.9117[0m        [94m0.6313[0m     +  16.3239
      2   [36m0.2119[0m   [32m0.7020[0m        [35m0.3412[0m       0.9117        [94m0.5714[0m     +  16.4930
      3   [36m0.2891[0m   [32m0.7702[0m        [35m0.2948[0m       [31m0.9129[0m        [94m0.5456[0m     +  16.8118
      4   [36m0.3012[0m   [32m0.7745[0m        [35m0.2798[0m       0.9105        [94m0.5190[0m     +  16.9142
      5   [36m0.3115[0m   [32m0.7769[0m        [35m0.2741[0m       0.9105        [94m0.4968[0m     +  16.8228
      6   [36m0.3122[0m   [32m0.7780[0m        [35m0.2697[0m       0.9105        [94m0.4803[0m     +  16.8638
      7   [36m0.3173[0m   [32m0.7788[0m        [35m0.2670[0m       0.9069        [94m0.4715[0m     +  17.0283
      8   [36m0.3198[0m   0.7778        [35m0.2664[0m       0.9081        [94m0.4514[0m     +  16.9747
      9   0.3187   0.7778        [35m0.2621[0m       0.9081        [94m0.4402[0m     +  16.9221
     10   [36m0.3219[0m   0.7767        [35m0.2611[0m       0.9057        [94m0.4319[0m     +  16.8523
     11   0.3200   0.7749        [35m0.2578[0m       [31m0.9166[0m        [94m0.4069[0m     +  16.7594
     12   [36m0.3226[0m   0.7710        0.2596       [31m0.9190[0m        [94m0.3959[0m     +  16.7104
     13   0.3218   0.7684        [35m0.2566[0m       0.9166        [94m0.3827[0m     +  16.9746
     14   0.3195   0.7684        [35m0.2540[0m       0.9141        [94m0.3666[0m     +  16.8418
     15   0.3212   0.7671        0.2547       0.9129        [94m0.3568[0m     +  16.9176
     16   0.3188   0.7624        [35m0.2514[0m       0.9141        [94m0.3439[0m     +  16.9331
     17   0.3182   0.7656        0.2536       0.9154        [94m0.3331[0m     +  16.8596
     18   0.3209   0.7652        [35m0.2485[0m       0.9166        [94m0.3282[0m     +  16.8674
     19   0.3183   0.7644        0.2486       0.9141        [94m0.3180[0m     +  16.9165
     20   [36m0.3264[0m   0.7694        [35m0.2481[0m       0.9166        [94m0.3165[0m     +  16.8954
     21   0.3217   0.7678        0.2490       0.9141        [94m0.3101[0m     +  16.8050
     22   0.3229   0.7677        0.2484       0.9141        [94m0.3034[0m     +  16.8333
     23   0.3213   0.7665        [35m0.2454[0m       0.9129        [94m0.2944[0m     +  17.0093
     24   0.3200   0.7669        0.2457       0.9129        [94m0.2890[0m     +  16.8561
     25   0.3195   0.7671        [35m0.2432[0m       0.9129        [94m0.2808[0m     +  16.7443
     26   0.3192   0.7690        [35m0.2417[0m       0.9141        [94m0.2780[0m     +  16.7595
     27   [36m0.3292[0m   0.7697        [35m0.2410[0m       0.9129        [94m0.2746[0m     +  16.9155
     28   0.3268   0.7758        0.2459       0.9141        0.2767        16.8750
     29   [36m0.3362[0m   0.7767        [35m0.2409[0m       0.9129        [94m0.2718[0m     +  16.9426
     30   0.3273   0.7772        [35m0.2403[0m       0.9129        [94m0.2716[0m     +  16.9616
     31   0.3297   [32m0.7792[0m        0.2417       0.9129        [94m0.2680[0m     +  16.8726
     32   0.3350   0.7787        0.2412       0.9129        0.2684        16.9790
     33   [36m0.3394[0m   [32m0.7806[0m        [35m0.2391[0m       0.9129        [94m0.2676[0m     +  16.8598
     34   [36m0.3444[0m   [32m0.7828[0m        0.2412       0.9141        0.2682        16.9522
     35   0.3411   [32m0.7832[0m        0.2405       0.9129        [94m0.2638[0m     +  16.7038
     36   0.3399   [32m0.7841[0m        [35m0.2373[0m       0.9141        [94m0.2613[0m     +  16.7270
     37   [36m0.3449[0m   [32m0.7853[0m        0.2390       0.9154        [94m0.2600[0m     +  16.7712
     38   [36m0.3498[0m   [32m0.7891[0m        0.2391       0.9154        0.2609        16.8600
     39   0.3408   0.7887        [35m0.2372[0m       0.9154        [94m0.2595[0m     +  16.8498
     40   0.3481   [32m0.7894[0m        [35m0.2367[0m       0.9154        0.2613        17.0072
     41   0.3490   [32m0.7894[0m        0.2370       0.9141        [94m0.2594[0m     +  17.0057
     42   0.3485   [32m0.7895[0m        [35m0.2360[0m       0.9141        [94m0.2578[0m     +  16.9876
     43   [36m0.3504[0m   [32m0.7900[0m        0.2388       0.9154        0.2586        16.8233
     44   0.3473   [32m0.7905[0m        [35m0.2321[0m       0.9141        0.2579        16.8608
     45   0.3480   0.7901        0.2360       0.9141        [94m0.2560[0m     +  16.8893
     46   [36m0.3548[0m   [32m0.7911[0m        0.2340       0.9154        [94m0.2555[0m     +  16.8420
     47   0.3546   0.7902        0.2348       0.9154        0.2571        16.7165
     48   0.3506   0.7905        0.2366       0.9154        [94m0.2542[0m     +  16.9007
     49   [36m0.3553[0m   0.7903        0.2367       0.9154        0.2548        16.8131
     50   0.3493   0.7903        0.2341       0.9141        0.2563        16.9868
[32m[I 2023-05-05 15:50:31,417][0m Trial 640 finished with value: 0.25415385663149004 and parameters: {'lr': 5.8171839268185475e-05, 'dropout': 0.6253455065677199, 'd_model_multiplier': 4, 'num_layers': 3, 'n_heads': 32, 'dim_feedforward': 322, 'batch_size': 58, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 52}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 58
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0715[0m   [32m0.3733[0m        [35m0.6745[0m       [31m0.8331[0m        [94m0.6691[0m     +  20.7602
      2   0.0643   0.3382        [35m0.6543[0m       [31m0.8972[0m        [94m0.6546[0m     +  21.0169
      3   0.0594   0.3082        [35m0.6293[0m       [31m0.9190[0m        [94m0.6391[0m     +  21.2465
      4   0.0557   0.2921        [35m0.6021[0m       [31m0.9226[0m        [94m0.6212[0m     +  21.0844
      5   0.0558   0.2983        [35m0.5736[0m       [31m0.9238[0m        [94m0.6006[0m     +  21.1537
      6   0.0545   0.2868        [35m0.5461[0m       0.9238        [94m0.5849[0m     +  21.2869
      7   0.0544   0.2929        [35m0.5219[0m       0.9238        [94m0.5655[0m     +  21.2055
      8   0.0544   0.2906        [35m0.4963[0m       0.9238        [94m0.5492[0m     +  21.1107
      9   0.0546   0.2924        [35m0.4734[0m       0.9238        [94m0.5314[0m     +  21.3157
     10   0.0548   0.2955        [35m0.4514[0m       0.9238        [94m0.5157[0m     +  21.0616
     11   0.0551   0.2978        [35m0.4322[0m       0.9238        [94m0.4986[0m     +  21.1231
     12   0.0556   0.3033        [35m0.4150[0m       0.9238        [94m0.4840[0m     +  21.0761
     13   0.0560   0.3070        [35m0.4008[0m       0.9238        [94m0.4714[0m     +  21.0935
     14   0.0565   0.3135        [35m0.3875[0m       0.9238        [94m0.4600[0m     +  21.1345
     15   0.0568   0.3197        [35m0.3744[0m       0.9238        [94m0.4483[0m     +  21.1993
     16   0.0575   0.3318        [35m0.3639[0m       0.9238        [94m0.4371[0m     +  21.1740
     17   0.0588   0.3492        [35m0.3552[0m       0.9238        [94m0.4259[0m     +  21.3298
     18   0.0595   0.3581        [35m0.3441[0m       0.9238        [94m0.4194[0m     +  21.1364
     19   0.0610   [32m0.3784[0m        [35m0.3386[0m       0.9238        [94m0.4084[0m     +  21.1919
     20   0.0627   [32m0.3987[0m        [35m0.3316[0m       0.9238        [94m0.4031[0m     +  21.1161
     21   0.0644   [32m0.4163[0m        [35m0.3256[0m       0.9238        [94m0.3953[0m     +  21.3868
     22   0.0656   [32m0.4286[0m        [35m0.3180[0m       0.9238        [94m0.3859[0m     +  21.1183
     23   0.0677   [32m0.4479[0m        [35m0.3139[0m       0.9238        [94m0.3809[0m     +  20.9856
     24   0.0704   [32m0.4692[0m        [35m0.3088[0m       0.9238        [94m0.3755[0m     +  20.9683
     25   [36m0.0727[0m   [32m0.4851[0m        [35m0.3048[0m       0.9238        [94m0.3695[0m     +  21.2644
     26   [36m0.0762[0m   [32m0.5047[0m        [35m0.3028[0m       0.9238        [94m0.3658[0m     +  21.1982
     27   [36m0.0783[0m   [32m0.5162[0m        [35m0.2987[0m       0.9238        [94m0.3630[0m     +  21.0143
     28   [36m0.0829[0m   [32m0.5355[0m        [35m0.2962[0m       0.9238        [94m0.3552[0m     +  21.1007
     29   [36m0.0888[0m   [32m0.5556[0m        [35m0.2935[0m       0.9238        [94m0.3533[0m     +  21.2221
     30   [36m0.0925[0m   [32m0.5657[0m        [35m0.2910[0m       0.9238        [94m0.3494[0m     +  21.3231
     31   [36m0.0975[0m   [32m0.5763[0m        [35m0.2874[0m       0.9238        [94m0.3457[0m     +  21.3944
     32   [36m0.1039[0m   [32m0.5871[0m        [35m0.2864[0m       0.9238        [94m0.3423[0m     +  21.0439
     33   [36m0.1104[0m   [32m0.5978[0m        [35m0.2844[0m       0.9238        [94m0.3389[0m     +  21.0688
     34   [36m0.1183[0m   [32m0.6078[0m        [35m0.2825[0m       0.9238        [94m0.3374[0m     +  21.3098
     35   [36m0.1254[0m   [32m0.6176[0m        [35m0.2807[0m       0.9238        [94m0.3329[0m     +  21.2474
     36   [36m0.1328[0m   [32m0.6255[0m        [35m0.2797[0m       0.9238        0.3331        21.2624
     37   [36m0.1384[0m   [32m0.6338[0m        [35m0.2774[0m       0.9238        [94m0.3283[0m     +  21.1037
     38   [36m0.1446[0m   [32m0.6395[0m        [35m0.2762[0m       0.9238        0.3283        21.0298
     39   [36m0.1516[0m   [32m0.6502[0m        [35m0.2733[0m       0.9238        [94m0.3236[0m     +  21.3152
     40   [36m0.1542[0m   [32m0.6560[0m        0.2746       0.9238        [94m0.3216[0m     +  21.2101
     41   [36m0.1600[0m   [32m0.6626[0m        [35m0.2731[0m       0.9238        [94m0.3198[0m     +  21.2838
     42   [36m0.1638[0m   [32m0.6668[0m        [35m0.2712[0m       0.9238        [94m0.3172[0m     +  20.9226
     43   [36m0.1670[0m   [32m0.6695[0m        0.2713       0.9238        [94m0.3167[0m     +  21.1994
     44   [36m0.1697[0m   [32m0.6755[0m        [35m0.2692[0m       0.9238        [94m0.3143[0m     +  21.2568
     45   [36m0.1753[0m   [32m0.6806[0m        0.2700       0.9238        0.3166        21.2502
     46   [36m0.1781[0m   [32m0.6857[0m        [35m0.2684[0m       0.9238        [94m0.3111[0m     +  21.2465
     47   [36m0.1800[0m   [32m0.6883[0m        0.2691       0.9226        [94m0.3107[0m     +  21.1088
     48   [36m0.1831[0m   [32m0.6939[0m        [35m0.2660[0m       0.9226        [94m0.3077[0m     +  21.2405
     49   [36m0.1859[0m   [32m0.6973[0m        [35m0.2657[0m       0.9226        [94m0.3064[0m     +  21.2178
     50   [36m0.1879[0m   [32m0.7002[0m        0.2665       0.9226        0.3079        21.5298
[32m[I 2023-05-05 16:08:20,149][0m Trial 641 finished with value: 0.306356156283163 and parameters: {'lr': 1.1391772624289673e-07, 'dropout': 0.37134047381398194, 'd_model_multiplier': 16, 'num_layers': 3, 'n_heads': 32, 'dim_feedforward': 306, 'batch_size': 42, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'BatchNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 58}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 56
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0871[0m   [32m0.5079[0m        [35m0.5044[0m       [31m0.9214[0m        [94m0.3946[0m     +  19.4231
      2   [36m0.2556[0m   [32m0.6970[0m        [35m0.2804[0m       0.9214        [94m0.3142[0m     +  19.8233
      3   [36m0.3057[0m   [32m0.7299[0m        [35m0.2567[0m       [31m0.9250[0m        [94m0.2949[0m     +  19.5968
      4   [36m0.3187[0m   [32m0.7429[0m        [35m0.2517[0m       [31m0.9262[0m        [94m0.2784[0m     +  19.8537
      5   0.3149   [32m0.7512[0m        [35m0.2503[0m       [31m0.9274[0m        [94m0.2673[0m     +  19.9784
      6   0.3153   [32m0.7519[0m        [35m0.2492[0m       0.9262        [94m0.2580[0m     +  19.8465
      7   [36m0.3250[0m   [32m0.7549[0m        [35m0.2453[0m       0.9262        [94m0.2528[0m     +  19.6147
      8   [36m0.3294[0m   [32m0.7553[0m        [35m0.2434[0m       0.9262        [94m0.2481[0m     +  19.9720
      9   0.3164   [32m0.7576[0m        [35m0.2401[0m       0.9262        [94m0.2454[0m     +  19.7785
     10   0.3168   [32m0.7591[0m        0.2419       0.9226        [94m0.2433[0m     +  19.9347
     11   0.3181   [32m0.7623[0m        [35m0.2375[0m       0.9214        [94m0.2398[0m     +  19.8546
     12   0.3194   [32m0.7640[0m        [35m0.2322[0m       0.9214        [94m0.2358[0m     +  19.8328
     13   0.3078   [32m0.7641[0m        0.2356       0.9238        0.2365        20.0011
     14   0.3054   0.7637        0.2346       0.9202        0.2366        19.9226
     15   0.3069   [32m0.7648[0m        [35m0.2295[0m       0.9226        [94m0.2357[0m     +  19.9295
     16   0.2981   [32m0.7687[0m        0.2310       0.9214        0.2359        20.2731
     17   0.2979   [32m0.7692[0m        [35m0.2276[0m       0.9202        0.2358        19.7714
     18   0.2955   [32m0.7707[0m        [35m0.2269[0m       0.9214        [94m0.2345[0m     +  19.9427
     19   0.2943   [32m0.7732[0m        [35m0.2258[0m       0.9214        0.2351        19.9244
     20   0.2859   0.7726        [35m0.2242[0m       0.9190        0.2364        20.1991
     21   0.2833   0.7731        [35m0.2224[0m       0.9190        0.2366        19.8416
     22   0.2741   0.7700        0.2225       0.9202        0.2384        20.0364
     23   0.2766   0.7705        0.2239       0.9238        0.2383        20.2865
     24   0.2779   [32m0.7735[0m        [35m0.2201[0m       0.9214        0.2379        19.8374
     25   0.2830   [32m0.7752[0m        0.2209       0.9202        0.2389        19.8991
     26   0.2802   [32m0.7755[0m        [35m0.2190[0m       0.9202        0.2397        19.8813
     27   0.2875   [32m0.7758[0m        0.2202       0.9226        0.2366        19.8535
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 16:17:38,287][0m Trial 642 finished with value: 0.23447708511655246 and parameters: {'lr': 8.896782643915159e-05, 'dropout': 0.38573922981330455, 'd_model_multiplier': 2, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 316, 'batch_size': 49, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'BatchNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 56}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 61
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0604[0m   [32m0.3893[0m        [35m0.5148[0m       [31m0.9311[0m        [94m0.5595[0m     +  20.1389
      2   [36m0.2488[0m   [32m0.7149[0m        [35m0.3325[0m       0.9311        [94m0.4851[0m     +  20.6457
      3   [36m0.3196[0m   [32m0.7544[0m        [35m0.2936[0m       0.9311        [94m0.4652[0m     +  20.3525
      4   [36m0.3460[0m   [32m0.7659[0m        [35m0.2839[0m       0.9287        [94m0.4586[0m     +  20.6942
      5   [36m0.3634[0m   [32m0.7749[0m        [35m0.2799[0m       [31m0.9335[0m        [94m0.4419[0m     +  20.7862
      6   0.3600   [32m0.7797[0m        [35m0.2758[0m       [31m0.9347[0m        0.4436        20.4360
      7   0.3611   [32m0.7853[0m        [35m0.2735[0m       0.9347        [94m0.4294[0m     +  20.6109
      8   [36m0.3670[0m   [32m0.7895[0m        [35m0.2729[0m       [31m0.9383[0m        [94m0.4147[0m     +  20.5340
      9   0.3657   [32m0.7895[0m        [35m0.2713[0m       [31m0.9395[0m        [94m0.3992[0m     +  20.4758
     10   0.3466   [32m0.7919[0m        [35m0.2680[0m       0.9383        [94m0.3825[0m     +  20.9218
     11   0.3476   0.7908        [35m0.2659[0m       0.9371        [94m0.3742[0m     +  20.6460
     12   0.3421   0.7915        0.2684       0.9383        [94m0.3635[0m     +  20.6162
     13   0.3365   0.7916        [35m0.2631[0m       0.9383        [94m0.3455[0m     +  20.7037
     14   0.3273   0.7901        0.2634       0.9371        [94m0.3328[0m     +  20.6409
     15   0.3331   0.7872        0.2631       0.9371        [94m0.3115[0m     +  20.7024
     16   0.3346   0.7897        [35m0.2621[0m       0.9371        [94m0.3110[0m     +  20.6758
     17   0.3253   0.7884        [35m0.2581[0m       0.9383        [94m0.2941[0m     +  20.5769
     18   0.3159   0.7864        0.2600       0.9371        [94m0.2843[0m     +  20.6664
     19   0.3111   0.7840        0.2586       0.9347        [94m0.2738[0m     +  20.5360
     20   0.3082   0.7831        0.2598       0.9347        [94m0.2657[0m     +  20.6545
     21   0.3154   0.7842        [35m0.2580[0m       0.9347        [94m0.2646[0m     +  20.4489
     22   0.3019   0.7755        [35m0.2553[0m       0.9311        [94m0.2556[0m     +  20.7584
     23   0.3045   0.7734        0.2565       0.9335        [94m0.2511[0m     +  20.5778
     24   0.3020   0.7729        [35m0.2545[0m       0.9311        [94m0.2480[0m     +  20.5810
     25   0.3046   0.7767        [35m0.2530[0m       0.9323        [94m0.2452[0m     +  20.6388
     26   0.3042   0.7771        0.2554       0.9311        [94m0.2367[0m     +  20.7716
     27   0.3047   0.7774        [35m0.2512[0m       0.9323        0.2371        20.6341
     28   0.2951   0.7722        [35m0.2498[0m       0.9323        [94m0.2331[0m     +  20.6698
     29   0.2969   0.7736        0.2520       0.9311        [94m0.2279[0m     +  20.6407
     30   0.2874   0.7677        0.2499       0.9323        [94m0.2272[0m     +  20.7169
     31   0.2848   0.7693        [35m0.2490[0m       0.9323        [94m0.2267[0m     +  21.1346
     32   0.2955   0.7741        [35m0.2454[0m       0.9323        0.2281        20.6238
     33   0.2845   0.7715        0.2484       0.9323        [94m0.2266[0m     +  20.5470
     34   0.2927   0.7742        0.2485       0.9311        [94m0.2265[0m     +  20.6309
     35   0.2934   0.7792        0.2457       0.9311        [94m0.2256[0m     +  20.6593
     36   0.2922   0.7791        0.2473       0.9311        0.2281        20.7992
     37   0.2996   0.7817        [35m0.2431[0m       0.9311        0.2276        20.9024
     38   0.2837   0.7769        0.2457       0.9311        0.2260        20.8616
     39   0.2844   0.7752        0.2445       0.9311        [94m0.2232[0m     +  20.7154
     40   0.2840   0.7760        [35m0.2405[0m       0.9311        0.2266        20.5705
     41   0.2814   0.7739        0.2450       0.9323        0.2266        20.6219
     42   0.2928   0.7794        0.2417       0.9311        [94m0.2229[0m     +  20.7661
     43   0.2750   0.7719        0.2418       0.9299        [94m0.2216[0m     +  20.7008
     44   0.2913   0.7795        [35m0.2401[0m       0.9311        [94m0.2175[0m     +  20.7935
     45   0.2858   0.7803        0.2416       0.9287        [94m0.2156[0m     +  20.5500
     46   0.2939   0.7830        0.2424       0.9311        0.2167        20.6403
     47   0.2917   0.7784        [35m0.2389[0m       0.9311        0.2157        20.6427
     48   0.2860   0.7784        0.2412       0.9311        0.2164        20.4702
     49   0.2930   0.7820        0.2396       0.9323        0.2165        20.7380
     50   0.2849   0.7796        [35m0.2374[0m       0.9311        0.2180        20.6197
[32m[I 2023-05-05 16:34:54,890][0m Trial 643 finished with value: 0.21559264371227901 and parameters: {'lr': 4.285831837930343e-05, 'dropout': 0.5987950122983727, 'd_model_multiplier': 4, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 325, 'batch_size': 52, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'BatchNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 61}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 52
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2865[0m   [32m0.7522[0m        [35m0.3232[0m       [31m0.9141[0m        [94m0.2861[0m     +  51.4014
      2   0.2841   [32m0.7688[0m        [35m0.2443[0m       0.9081        [94m0.2838[0m     +  51.4810
      3   0.2749   [32m0.7691[0m        [35m0.2405[0m       0.9105        [94m0.2725[0m     +  51.4071
      4   0.2812   0.7678        [35m0.2379[0m       [31m0.9166[0m        [94m0.2662[0m     +  51.8047
      5   0.2854   [32m0.7808[0m        [35m0.2340[0m       0.9117        [94m0.2588[0m     +  52.0068
      6   0.2765   [32m0.7848[0m        [35m0.2301[0m       0.9141        [94m0.2579[0m     +  51.5197
      7   0.2804   [32m0.7861[0m        [35m0.2254[0m       0.9141        [94m0.2566[0m     +  51.3634
      8   [36m0.2919[0m   [32m0.7864[0m        [35m0.2245[0m       0.9141        0.2619        51.6479
      9   0.2810   [32m0.7870[0m        [35m0.2216[0m       0.9129        0.2626        51.6604
     10   0.2777   0.7859        0.2221       0.9105        0.2673        51.7899
     11   0.2902   [32m0.7931[0m        [35m0.2186[0m       0.9129        0.2604        51.6595
     12   0.2826   0.7796        0.2189       0.9117        0.2800        51.5808
     13   0.2733   0.7819        [35m0.2181[0m       0.9093        0.2711        51.5603
     14   0.2874   0.7842        [35m0.2146[0m       0.9093        0.2776        51.4741
     15   0.2902   0.7854        [35m0.2100[0m       0.9105        0.2737        51.5415
     16   0.2867   0.7893        0.2111       0.9093        0.2783        51.4517
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 16:49:37,518][0m Trial 644 finished with value: 0.25661941723584986 and parameters: {'lr': 7.076918019824115e-05, 'dropout': 0.37246170772677534, 'd_model_multiplier': 32, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 336, 'batch_size': 58, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'BatchNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 52}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 44
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0616[0m   [32m0.3249[0m        [35m0.7597[0m       [31m0.9190[0m        [94m0.3557[0m     +  20.4221
      2   [36m0.0810[0m   [32m0.5000[0m        0.7709       0.9190        [94m0.2822[0m     +  20.3217
      3   0.0810   0.5000        [35m0.2768[0m       0.9190        0.2827        20.6278
      4   0.0810   0.5000        [35m0.2757[0m       0.9190        0.2829        20.6401
      5   0.0810   0.5000        [35m0.2748[0m       0.9190        0.2829        20.3440
      6   0.0810   0.5000        [35m0.2740[0m       0.9190        0.2830        20.4590
      7   0.0810   0.5000        0.2740       0.9190        0.2831        20.6610
      8   0.0810   0.5000        0.2740       0.9190        0.2831        20.5238
      9   0.0810   0.5000        0.2748       0.9190        0.2830        20.4482
     10   0.0810   0.5000        0.2771       0.9190        0.2832        20.6289
     11   0.0810   0.5000        0.2741       0.9190        0.2832        20.7249
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 16:53:44,390][0m Trial 645 finished with value: 0.2822361147479394 and parameters: {'lr': 0.0825223960040623, 'dropout': 0.5879350328886668, 'd_model_multiplier': 4, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 326, 'batch_size': 31, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'BatchNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0, 'top_n_features': 44}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 58
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1417[0m   [32m0.6166[0m        [35m0.4205[0m       [31m0.9238[0m        [94m0.5209[0m     +  19.9156
      2   [36m0.2063[0m   [32m0.7264[0m        [35m0.2700[0m       0.9238        [94m0.4032[0m     +  20.1193
      3   [36m0.2229[0m   [32m0.7267[0m        [35m0.2587[0m       0.9238        [94m0.3290[0m     +  19.8657
      4   [36m0.2340[0m   0.7211        [35m0.2552[0m       0.9238        [94m0.2901[0m     +  19.9402
      5   0.2172   0.7014        [35m0.2512[0m       0.9226        [94m0.2733[0m     +  19.9595
      6   0.2240   0.7222        [35m0.2470[0m       0.9202        [94m0.2581[0m     +  19.8701
      7   0.2190   0.7157        [35m0.2439[0m       0.9202        [94m0.2514[0m     +  19.8707
      8   0.2290   0.7266        [35m0.2414[0m       0.9214        [94m0.2458[0m     +  19.9320
      9   [36m0.2379[0m   [32m0.7344[0m        [35m0.2365[0m       0.9214        [94m0.2433[0m     +  19.9403
     10   0.2320   0.7303        [35m0.2365[0m       0.9202        [94m0.2422[0m     +  20.0271
     11   0.2220   0.7183        [35m0.2343[0m       0.9226        0.2447        19.9581
     12   0.2357   [32m0.7365[0m        [35m0.2308[0m       0.9214        [94m0.2409[0m     +  19.8589
     13   [36m0.2413[0m   [32m0.7392[0m        0.2333       0.9226        [94m0.2388[0m     +  19.9792
     14   0.2342   0.7376        [35m0.2287[0m       0.9238        0.2398        19.9060
     15   0.2393   0.7333        0.2294       0.9238        0.2414        20.0091
     16   0.2399   [32m0.7413[0m        [35m0.2285[0m       0.9202        0.2390        19.9163
     17   0.2363   [32m0.7468[0m        [35m0.2270[0m       0.9214        [94m0.2385[0m     +  19.8790
     18   [36m0.2480[0m   [32m0.7504[0m        [35m0.2236[0m       0.9238        0.2388        19.9749
     19   [36m0.2693[0m   0.7478        [35m0.2231[0m       0.9226        0.2402        20.0873
     20   0.2655   0.7491        0.2234       [31m0.9250[0m        [94m0.2380[0m     +  19.9423
     21   0.2619   0.7452        [35m0.2202[0m       0.9226        0.2402        19.8141
     22   0.2626   0.7330        0.2228       0.9226        0.2419        20.0354
     23   0.2643   0.7418        0.2204       0.9238        [94m0.2372[0m     +  19.9012
     24   0.2674   0.7392        [35m0.2163[0m       [31m0.9262[0m        0.2405        19.9210
     25   [36m0.2772[0m   [32m0.7511[0m        0.2165       0.9250        [94m0.2356[0m     +  19.8778
     26   0.2725   0.7384        [35m0.2159[0m       [31m0.9274[0m        0.2413        20.0680
     27   0.2616   0.7481        [35m0.2129[0m       0.9178        0.2435        19.9364
     28   0.2674   0.7409        0.2161       0.9178        0.2457        20.0226
     29   [36m0.2805[0m   0.7486        [35m0.2121[0m       0.9166        0.2428        19.9767
     30   0.2669   0.7388        [35m0.2100[0m       0.9190        0.2425        19.8909
     31   0.2640   0.7362        [35m0.2091[0m       0.9178        0.2433        19.8558
     32   0.2515   0.7403        [35m0.2088[0m       0.9190        0.2446        20.1479
     33   0.2643   0.7368        [35m0.2082[0m       0.9202        0.2439        20.1500
     34   0.2641   0.7355        [35m0.2052[0m       0.9178        0.2441        19.9767
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 17:05:24,912][0m Trial 646 finished with value: 0.2356130636805058 and parameters: {'lr': 0.00012256824905890502, 'dropout': 0.47962027357302767, 'd_model_multiplier': 1, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 318, 'batch_size': 17, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 58}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 63
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0657[0m   [32m0.5763[0m        [35m0.4749[0m       [31m0.9456[0m        [94m0.3585[0m     +  16.5425
      2   [36m0.1985[0m   [32m0.8072[0m        [35m0.2806[0m       0.9432        [94m0.2885[0m     +  16.4580
      3   [36m0.2077[0m   [32m0.8169[0m        [35m0.2654[0m       0.9407        [94m0.2741[0m     +  16.5099
      4   [36m0.2148[0m   0.8131        [35m0.2574[0m       0.9407        [94m0.2645[0m     +  16.7170
      5   [36m0.2184[0m   0.8126        0.2584       0.9432        [94m0.2588[0m     +  16.8294
      6   [36m0.2268[0m   0.8139        [35m0.2534[0m       0.9456        [94m0.2507[0m     +  16.6914
      7   0.2224   0.8138        [35m0.2520[0m       0.9444        [94m0.2455[0m     +  16.5018
      8   0.2224   0.8119        [35m0.2508[0m       0.9444        [94m0.2356[0m     +  16.6190
      9   0.2254   0.8132        [35m0.2478[0m       0.9420        [94m0.2291[0m     +  16.6020
     10   [36m0.2338[0m   0.8154        [35m0.2476[0m       0.9420        [94m0.2231[0m     +  16.6059
     11   0.2329   [32m0.8177[0m        [35m0.2453[0m       0.9407        [94m0.2204[0m     +  16.7956
     12   0.2316   0.8165        [35m0.2447[0m       0.9407        [94m0.2117[0m     +  16.7113
     13   0.2268   0.8130        [35m0.2431[0m       0.9407        [94m0.2117[0m     +  16.7230
     14   [36m0.2359[0m   [32m0.8200[0m        [35m0.2421[0m       0.9420        [94m0.2099[0m     +  16.6193
     15   [36m0.2455[0m   [32m0.8332[0m        [35m0.2382[0m       0.9420        0.2115        16.8905
     16   0.2336   0.8222        0.2390       0.9420        [94m0.2071[0m     +  16.7606
     17   0.2367   0.8273        [35m0.2375[0m       0.9407        [94m0.2061[0m     +  16.5948
     18   0.2346   0.8307        [35m0.2343[0m       0.9407        [94m0.2024[0m     +  16.7151
     19   0.2285   0.8240        0.2358       0.9407        [94m0.2021[0m     +  16.6750
     20   0.2310   0.8321        [35m0.2338[0m       0.9407        [94m0.1980[0m     +  16.6632
     21   0.2273   [32m0.8367[0m        [35m0.2310[0m       0.9359        0.1987        16.6318
     22   0.2297   [32m0.8413[0m        [35m0.2310[0m       0.9359        [94m0.1971[0m     +  16.7043
     23   0.2407   [32m0.8421[0m        [35m0.2304[0m       0.9371        0.1983        16.7664
     24   0.2282   0.8408        0.2326       0.9371        0.2012        16.9409
     25   0.2268   0.8391        0.2317       0.9383        0.1988        16.6681
     26   0.2319   0.8380        [35m0.2274[0m       0.9359        0.1984        16.9367
     27   0.2382   0.8412        [35m0.2259[0m       0.9347        0.1976        16.4813
     28   0.2256   0.8359        0.2265       0.9335        0.1983        16.5865
     29   0.2192   0.8403        [35m0.2244[0m       0.9371        [94m0.1941[0m     +  16.6422
     30   0.2199   [32m0.8441[0m        [35m0.2227[0m       0.9371        [94m0.1937[0m     +  16.6127
     31   0.2257   0.8436        0.2260       0.9347        0.1977        17.4542
     32   0.2269   0.8404        [35m0.2222[0m       0.9347        0.1949        16.6935
     33   0.2229   0.8374        [35m0.2217[0m       0.9359        0.1957        16.6608
     34   0.2276   0.8369        [35m0.2207[0m       0.9347        0.1992        16.7885
     35   0.2201   0.8271        [35m0.2186[0m       0.9371        0.1991        16.8385
     36   0.2186   0.8328        0.2195       0.9359        0.1960        16.7666
     37   0.2210   0.8339        0.2206       0.9371        0.1984        16.6490
     38   0.2235   0.8224        0.2197       0.9371        0.1968        16.7434
     39   0.2172   0.8241        [35m0.2180[0m       0.9359        [94m0.1922[0m     +  16.6210
     40   0.2184   0.8232        [35m0.2166[0m       0.9347        0.1934        16.9005
     41   0.2187   0.8325        [35m0.2161[0m       0.9335        0.1959        16.6275
     42   0.2115   0.8253        0.2170       0.9311        0.1944        16.8972
     43   0.2034   0.8234        [35m0.2144[0m       0.9323        0.1935        16.7039
     44   0.2182   0.8296        0.2145       0.9335        0.1944        16.7676
     45   0.2216   0.8355        0.2145       0.9311        0.1942        16.6017
     46   0.2214   0.8354        [35m0.2114[0m       0.9311        0.1965        16.8357
     47   0.2181   0.8330        0.2147       0.9335        0.1936        16.7010
     48   0.2132   0.8245        0.2126       0.9311        0.1984        16.7657
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 17:19:06,105][0m Trial 647 finished with value: 0.1921737744881712 and parameters: {'lr': 4.886944338218949e-05, 'dropout': 0.45914627529739144, 'd_model_multiplier': 4, 'num_layers': 3, 'n_heads': 32, 'dim_feedforward': 332, 'batch_size': 37, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'BatchNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 63}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 63
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0732[0m   [32m0.4352[0m        [35m0.4617[0m       [31m0.9117[0m        [94m0.4179[0m     +  16.6678
      2   [36m0.2670[0m   [32m0.7577[0m        [35m0.2844[0m       0.9117        [94m0.3402[0m     +  16.8703
      3   [36m0.3088[0m   [32m0.7897[0m        [35m0.2634[0m       0.9117        [94m0.3172[0m     +  16.9135
      4   [36m0.3254[0m   [32m0.7920[0m        [35m0.2586[0m       0.9117        [94m0.3035[0m     +  16.7494
      5   [36m0.3472[0m   0.7901        [35m0.2525[0m       [31m0.9129[0m        [94m0.2964[0m     +  16.8662
      6   0.3469   0.7833        [35m0.2511[0m       [31m0.9178[0m        [94m0.2871[0m     +  16.8204
      7   0.3392   0.7825        [35m0.2485[0m       0.9166        [94m0.2833[0m     +  16.7442
      8   0.3435   0.7883        [35m0.2463[0m       0.9166        [94m0.2772[0m     +  16.9845
      9   0.3385   0.7857        [35m0.2449[0m       0.9166        [94m0.2692[0m     +  16.6687
     10   0.3371   0.7888        [35m0.2429[0m       0.9166        [94m0.2655[0m     +  16.7930
     11   0.3367   0.7894        0.2430       0.9141        [94m0.2636[0m     +  16.8346
     12   0.3306   0.7839        [35m0.2407[0m       0.9129        [94m0.2602[0m     +  16.8647
     13   0.3369   0.7903        [35m0.2398[0m       0.9129        [94m0.2562[0m     +  16.9448
     14   0.3416   [32m0.7983[0m        [35m0.2371[0m       0.9141        [94m0.2525[0m     +  16.8666
     15   0.3451   0.7968        [35m0.2365[0m       0.9154        [94m0.2507[0m     +  16.9156
     16   0.3381   [32m0.8008[0m        0.2366       0.9154        [94m0.2486[0m     +  16.8480
     17   0.3411   [32m0.8022[0m        0.2366       0.9154        [94m0.2472[0m     +  16.8602
     18   0.3404   [32m0.8058[0m        [35m0.2352[0m       0.9154        [94m0.2459[0m     +  16.8791
     19   [36m0.3482[0m   0.8023        [35m0.2339[0m       0.9154        0.2463        16.7204
     20   0.3458   [32m0.8065[0m        [35m0.2327[0m       0.9154        [94m0.2448[0m     +  16.8093
     21   [36m0.3507[0m   [32m0.8073[0m        [35m0.2288[0m       0.9154        [94m0.2442[0m     +  16.9140
     22   0.3507   [32m0.8094[0m        0.2341       0.9154        0.2443        16.7101
     23   [36m0.3566[0m   [32m0.8115[0m        0.2326       0.9154        [94m0.2423[0m     +  16.6989
     24   [36m0.3604[0m   [32m0.8146[0m        [35m0.2280[0m       0.9129        0.2423        16.8716
     25   0.3559   0.8103        0.2283       0.9129        0.2440        16.8220
     26   [36m0.3645[0m   [32m0.8182[0m        [35m0.2267[0m       0.9129        [94m0.2410[0m     +  16.7780
     27   0.3601   0.8132        [35m0.2246[0m       0.9129        0.2422        16.9392
     28   0.3567   0.8128        0.2260       0.9129        0.2425        16.7639
     29   0.3583   0.8153        [35m0.2245[0m       0.9129        0.2422        16.9306
     30   0.3562   [32m0.8187[0m        [35m0.2245[0m       0.9105        0.2411        16.9090
     31   0.3525   0.8156        0.2250       0.9129        0.2422        16.7850
     32   0.3549   0.8185        0.2262       0.9105        0.2414        16.8239
     33   0.3444   [32m0.8204[0m        [35m0.2214[0m       0.9117        [94m0.2408[0m     +  16.8780
     34   0.3443   [32m0.8210[0m        0.2232       0.9093        0.2419        16.9408
     35   0.3385   0.8135        0.2227       0.9105        0.2435        16.9044
     36   0.3432   0.8174        [35m0.2209[0m       0.9117        0.2422        16.8037
     37   0.3345   0.8135        [35m0.2204[0m       0.9081        0.2452        16.8457
     38   0.3406   0.8150        0.2219       0.9081        0.2445        16.6659
     39   0.3389   0.8149        [35m0.2200[0m       0.9081        0.2442        16.8233
     40   0.3420   0.8180        [35m0.2181[0m       0.9081        0.2425        16.9315
     41   0.3434   0.8178        0.2184       0.9093        0.2431        16.8638
     42   0.3404   0.8184        [35m0.2168[0m       0.9069        0.2432        16.8719
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 17:31:12,169][0m Trial 648 finished with value: 0.24082536152152945 and parameters: {'lr': 4.5762979236658727e-05, 'dropout': 0.49474055589009436, 'd_model_multiplier': 4, 'num_layers': 3, 'n_heads': 32, 'dim_feedforward': 333, 'batch_size': 42, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'BatchNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 63}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 68
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1855[0m   [32m0.6532[0m        [35m0.3985[0m       [31m0.9214[0m        [94m0.3955[0m     +  16.6685
      2   [36m0.3026[0m   [32m0.7775[0m        [35m0.2647[0m       [31m0.9238[0m        [94m0.3296[0m     +  16.5633
      3   0.3024   [32m0.7813[0m        [35m0.2549[0m       0.9202        [94m0.2975[0m     +  16.5883
      4   [36m0.3155[0m   0.7784        [35m0.2504[0m       0.9202        [94m0.2895[0m     +  16.8117
      5   0.3063   0.7768        [35m0.2478[0m       0.9202        [94m0.2753[0m     +  16.8085
      6   0.3029   0.7686        [35m0.2437[0m       0.9202        [94m0.2683[0m     +  16.7791
      7   0.3060   0.7689        [35m0.2404[0m       0.9202        [94m0.2621[0m     +  16.7291
      8   0.3038   0.7625        [35m0.2398[0m       0.9202        [94m0.2597[0m     +  16.7934
      9   0.2996   0.7529        [35m0.2369[0m       0.9226        [94m0.2528[0m     +  16.7162
     10   0.2877   0.7515        0.2383       [31m0.9250[0m        [94m0.2502[0m     +  16.7150
     11   0.2888   0.7602        [35m0.2345[0m       0.9214        [94m0.2481[0m     +  16.8157
     12   0.2878   0.7538        [35m0.2340[0m       0.9226        [94m0.2470[0m     +  17.0520
     13   0.2877   0.7518        0.2343       0.9226        [94m0.2459[0m     +  17.0257
     14   0.2839   0.7499        [35m0.2304[0m       0.9226        [94m0.2458[0m     +  16.6963
     15   0.2743   0.7425        [35m0.2301[0m       0.9214        [94m0.2452[0m     +  16.8737
     16   0.2749   0.7481        0.2309       0.9214        0.2468        16.6615
     17   0.2633   0.7354        [35m0.2300[0m       0.9214        0.2472        16.8542
     18   0.2686   0.7351        [35m0.2271[0m       0.9226        0.2478        17.0066
     19   0.2783   0.7423        [35m0.2241[0m       0.9226        0.2463        16.7574
     20   0.2684   0.7323        [35m0.2232[0m       0.9226        0.2469        16.6849
     21   0.2729   0.7384        [35m0.2227[0m       0.9202        0.2454        16.7755
     22   0.2706   0.7303        0.2236       0.9190        0.2471        16.8654
     23   0.2806   0.7314        [35m0.2208[0m       0.9202        0.2487        16.8096
     24   0.2773   0.7342        0.2230       0.9214        0.2468        16.6813
     25   0.2850   0.7442        [35m0.2192[0m       0.9214        [94m0.2445[0m     +  16.7635
     26   0.2650   0.7395        0.2195       0.9202        0.2468        16.6557
     27   0.2788   0.7285        0.2209       0.9202        0.2475        16.7266
     28   0.2677   0.7229        [35m0.2159[0m       0.9214        0.2486        16.8022
     29   0.2834   0.7354        [35m0.2156[0m       0.9202        0.2456        16.5991
     30   0.2700   0.7215        0.2163       0.9178        0.2501        16.8239
     31   0.2689   0.7321        0.2169       0.9190        0.2479        16.8102
     32   0.2706   0.7306        [35m0.2118[0m       0.9190        0.2471        16.7155
     33   0.2676   0.7349        0.2130       0.9190        0.2472        16.6279
     34   0.2574   0.7267        0.2131       0.9178        0.2482        16.6378
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 17:41:00,392][0m Trial 649 finished with value: 0.2445330130974644 and parameters: {'lr': 6.368689176154221e-05, 'dropout': 0.46294741940981815, 'd_model_multiplier': 4, 'num_layers': 3, 'n_heads': 32, 'dim_feedforward': 343, 'batch_size': 36, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'BatchNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 68}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 75
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0944[0m   [32m0.4929[0m        [35m0.6672[0m       [31m0.6252[0m        [94m0.6845[0m     +  16.6666
      2   0.0905   0.4802        [35m0.6624[0m       [31m0.6626[0m        [94m0.6814[0m     +  16.7279
      3   0.0873   0.4669        [35m0.6602[0m       [31m0.6917[0m        [94m0.6786[0m     +  16.8113
      4   0.0870   0.4629        [35m0.6524[0m       [31m0.7545[0m        [94m0.6746[0m     +  16.6636
      5   0.0802   0.4441        [35m0.6431[0m       [31m0.7920[0m        [94m0.6700[0m     +  16.9888
      6   0.0778   0.4341        [35m0.6325[0m       [31m0.8452[0m        [94m0.6658[0m     +  16.7724
      7   0.0730   0.4175        [35m0.6238[0m       [31m0.8646[0m        [94m0.6605[0m     +  16.8089
      8   0.0733   0.4181        [35m0.6135[0m       [31m0.8936[0m        [94m0.6540[0m     +  16.8103
      9   0.0715   0.4027        [35m0.6039[0m       [31m0.9045[0m        [94m0.6504[0m     +  16.6157
     10   0.0680   0.3942        [35m0.5942[0m       [31m0.9093[0m        [94m0.6453[0m     +  16.5924
     11   0.0662   0.3833        [35m0.5835[0m       [31m0.9154[0m        [94m0.6391[0m     +  16.6288
     12   0.0646   0.3752        [35m0.5736[0m       [31m0.9166[0m        [94m0.6351[0m     +  16.7611
     13   0.0646   0.3756        [35m0.5624[0m       [31m0.9178[0m        [94m0.6291[0m     +  16.7122
     14   0.0626   0.3651        [35m0.5528[0m       0.9178        [94m0.6250[0m     +  16.7205
     15   0.0626   0.3660        [35m0.5430[0m       0.9178        [94m0.6178[0m     +  16.6712
     16   0.0620   0.3602        [35m0.5338[0m       0.9178        [94m0.6135[0m     +  16.4812
     17   0.0621   0.3612        [35m0.5251[0m       0.9178        [94m0.6079[0m     +  16.7671
     18   0.0615   0.3571        [35m0.5172[0m       0.9178        [94m0.6024[0m     +  17.0119
     19   0.0620   0.3607        [35m0.5071[0m       0.9178        [94m0.5960[0m     +  16.6924
     20   0.0610   0.3525        [35m0.5022[0m       0.9178        [94m0.5918[0m     +  16.6992
     21   0.0612   0.3543        [35m0.4929[0m       0.9178        [94m0.5885[0m     +  16.7998
     22   0.0607   0.3502        [35m0.4832[0m       0.9178        [94m0.5835[0m     +  16.7834
     23   0.0611   0.3531        [35m0.4767[0m       0.9178        [94m0.5771[0m     +  16.6334
     24   0.0607   0.3502        [35m0.4701[0m       0.9178        [94m0.5745[0m     +  16.6753
     25   0.0609   0.3522        [35m0.4614[0m       0.9178        [94m0.5700[0m     +  16.6672
     26   0.0609   0.3522        [35m0.4589[0m       0.9178        [94m0.5622[0m     +  16.9259
     27   0.0609   0.3516        [35m0.4495[0m       0.9178        [94m0.5612[0m     +  16.8055
     28   0.0607   0.3510        [35m0.4422[0m       0.9178        [94m0.5565[0m     +  16.7348
     29   0.0611   0.3529        [35m0.4385[0m       0.9178        [94m0.5511[0m     +  16.6459
     30   0.0610   0.3531        [35m0.4315[0m       0.9178        [94m0.5506[0m     +  16.8195
     31   0.0612   0.3543        [35m0.4287[0m       0.9178        [94m0.5439[0m     +  16.8252
     32   0.0613   0.3555        [35m0.4215[0m       0.9178        [94m0.5406[0m     +  16.6871
     33   0.0614   0.3567        [35m0.4173[0m       0.9178        [94m0.5361[0m     +  16.7863
     34   0.0613   0.3558        [35m0.4121[0m       0.9178        [94m0.5334[0m     +  16.6453
     35   0.0615   0.3572        [35m0.4093[0m       0.9178        [94m0.5303[0m     +  16.9017
     36   0.0614   0.3571        [35m0.4046[0m       0.9178        [94m0.5272[0m     +  16.7464
     37   0.0617   0.3594        [35m0.4002[0m       0.9178        [94m0.5218[0m     +  16.6705
     38   0.0619   0.3607        [35m0.3947[0m       0.9178        [94m0.5165[0m     +  16.8361
     39   0.0622   0.3628        [35m0.3895[0m       0.9178        [94m0.5163[0m     +  16.6790
     40   0.0623   0.3641        [35m0.3860[0m       0.9178        [94m0.5112[0m     +  16.6729
     41   0.0622   0.3645        [35m0.3844[0m       0.9178        0.5126        16.7708
     42   0.0627   0.3685        [35m0.3775[0m       0.9178        [94m0.5037[0m     +  17.3947
     43   0.0626   0.3693        [35m0.3760[0m       0.9178        0.5042        16.8187
     44   0.0630   0.3718        [35m0.3720[0m       0.9178        [94m0.5034[0m     +  16.7632
     45   0.0629   0.3707        [35m0.3709[0m       0.9178        [94m0.4987[0m     +  16.8235
     46   0.0634   0.3760        [35m0.3664[0m       0.9178        [94m0.4969[0m     +  16.7724
     47   0.0641   0.3806        [35m0.3634[0m       0.9178        [94m0.4913[0m     +  16.9198
     48   0.0636   0.3803        [35m0.3623[0m       0.9178        [94m0.4900[0m     +  16.8245
     49   0.0643   0.3853        [35m0.3598[0m       0.9178        [94m0.4857[0m     +  16.8116
     50   0.0644   0.3870        [35m0.3581[0m       0.9178        0.4890        16.8093
[32m[I 2023-05-05 17:55:03,365][0m Trial 650 finished with value: 0.4857480017853475 and parameters: {'lr': 9.31540899744667e-08, 'dropout': 0.4743527187775141, 'd_model_multiplier': 4, 'num_layers': 3, 'n_heads': 32, 'dim_feedforward': 329, 'batch_size': 36, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'BatchNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 75}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 67
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0475[0m   [32m0.3452[0m        [35m0.5723[0m       [31m0.9359[0m        [94m0.5427[0m     +  13.6702
      2   [36m0.0681[0m   [32m0.5306[0m        [35m0.3744[0m       0.9359        [94m0.4126[0m     +  13.7209
      3   [36m0.1678[0m   [32m0.6716[0m        [35m0.3011[0m       0.9347        [94m0.3453[0m     +  13.4560
      4   [36m0.2096[0m   [32m0.7159[0m        [35m0.2757[0m       0.9347        [94m0.3193[0m     +  13.6850
      5   [36m0.2205[0m   [32m0.7284[0m        [35m0.2693[0m       0.9347        [94m0.2973[0m     +  13.7160
      6   [36m0.2238[0m   [32m0.7327[0m        [35m0.2614[0m       0.9347        [94m0.2883[0m     +  13.6088
      7   [36m0.2342[0m   [32m0.7365[0m        0.2616       0.9347        [94m0.2724[0m     +  13.6041
      8   0.2325   [32m0.7392[0m        [35m0.2557[0m       0.9359        [94m0.2606[0m     +  13.6521
      9   [36m0.2358[0m   0.7378        [35m0.2522[0m       0.9335        [94m0.2547[0m     +  13.6564
     10   0.2356   0.7337        0.2523       [31m0.9371[0m        [94m0.2490[0m     +  13.4192
     11   0.2340   0.7350        0.2523       0.9371        [94m0.2450[0m     +  13.6599
     12   [36m0.2370[0m   0.7344        [35m0.2495[0m       0.9371        [94m0.2388[0m     +  13.6399
     13   [36m0.2409[0m   0.7364        [35m0.2486[0m       0.9371        0.2396        13.6795
     14   [36m0.2436[0m   0.7329        [35m0.2463[0m       0.9359        [94m0.2336[0m     +  13.6183
     15   [36m0.2473[0m   0.7328        0.2495       0.9359        [94m0.2326[0m     +  13.6697
     16   [36m0.2535[0m   0.7349        0.2496       0.9359        0.2328        13.6581
     17   0.2463   0.7326        0.2469       0.9371        [94m0.2316[0m     +  13.5950
     18   0.2389   0.7307        [35m0.2438[0m       0.9371        [94m0.2296[0m     +  13.7539
     19   0.2485   0.7319        0.2440       0.9371        [94m0.2274[0m     +  13.7076
     20   [36m0.2591[0m   0.7359        0.2472       0.9371        0.2277        13.6560
     21   0.2516   0.7363        [35m0.2422[0m       0.9371        [94m0.2229[0m     +  13.7005
     22   0.2486   0.7362        0.2447       [31m0.9383[0m        [94m0.2225[0m     +  13.6122
     23   0.2513   0.7355        0.2432       0.9383        [94m0.2193[0m     +  13.6083
     24   0.2519   0.7369        0.2439       [31m0.9395[0m        0.2194        13.6253
     25   0.2552   [32m0.7399[0m        [35m0.2416[0m       0.9383        0.2200        13.5541
     26   0.2540   [32m0.7411[0m        [35m0.2398[0m       0.9383        0.2198        13.4516
     27   0.2546   0.7403        0.2431       0.9395        [94m0.2176[0m     +  13.6612
     28   0.2488   0.7396        0.2406       0.9383        [94m0.2173[0m     +  13.5898
     29   0.2533   0.7393        [35m0.2389[0m       0.9395        0.2187        13.8007
     30   0.2571   [32m0.7413[0m        [35m0.2365[0m       0.9383        [94m0.2162[0m     +  13.7444
     31   0.2546   0.7398        0.2419       0.9395        [94m0.2158[0m     +  13.8643
     32   0.2552   [32m0.7427[0m        0.2388       0.9395        [94m0.2150[0m     +  13.7828
     33   0.2538   [32m0.7437[0m        0.2396       0.9395        0.2151        13.6610
     34   0.2544   0.7435        0.2377       0.9395        0.2151        13.5637
     35   0.2512   0.7424        0.2391       0.9395        [94m0.2149[0m     +  13.6369
     36   [36m0.2602[0m   [32m0.7447[0m        [35m0.2353[0m       0.9395        [94m0.2147[0m     +  13.6208
     37   0.2566   [32m0.7455[0m        0.2355       0.9395        [94m0.2142[0m     +  13.5010
     38   0.2556   [32m0.7485[0m        [35m0.2333[0m       0.9395        0.2147        13.7043
     39   0.2587   0.7478        0.2375       0.9383        0.2145        13.5753
     40   0.2586   [32m0.7492[0m        0.2339       0.9383        [94m0.2141[0m     +  13.6031
     41   [36m0.2633[0m   [32m0.7496[0m        [35m0.2331[0m       0.9395        [94m0.2135[0m     +  13.6071
     42   0.2585   [32m0.7514[0m        [35m0.2329[0m       0.9395        0.2142        13.8248
     43   0.2602   [32m0.7516[0m        0.2337       0.9383        0.2144        13.8407
     44   0.2629   [32m0.7530[0m        0.2351       0.9371        [94m0.2134[0m     +  13.7718
     45   [36m0.2657[0m   [32m0.7546[0m        0.2331       0.9371        0.2135        13.6102
     46   0.2657   [32m0.7555[0m        0.2331       0.9383        [94m0.2133[0m     +  13.5982
     47   0.2601   0.7545        [35m0.2322[0m       0.9395        [94m0.2132[0m     +  13.6103
     48   [36m0.2702[0m   [32m0.7563[0m        [35m0.2320[0m       0.9371        [94m0.2125[0m     +  13.5594
     49   0.2677   [32m0.7564[0m        [35m0.2308[0m       0.9383        0.2128        13.6119
     50   0.2630   [32m0.7569[0m        [35m0.2307[0m       0.9359        0.2125        13.7491
[32m[I 2023-05-05 18:06:28,773][0m Trial 651 finished with value: 0.21245130219979902 and parameters: {'lr': 3.588665130435651e-05, 'dropout': 0.4859790850372103, 'd_model_multiplier': 1, 'num_layers': 3, 'n_heads': 32, 'dim_feedforward': 304, 'batch_size': 31, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'BatchNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0, 'top_n_features': 67}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 63
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1069[0m   [32m0.5612[0m        [35m0.4473[0m       [31m0.9335[0m        [94m0.5047[0m     +  16.5378
      2   [36m0.2261[0m   [32m0.7021[0m        [35m0.2800[0m       0.9299        [94m0.4605[0m     +  16.5598
      3   [36m0.2398[0m   [32m0.7163[0m        [35m0.2621[0m       0.9287        [94m0.4528[0m     +  16.5805
      4   [36m0.2555[0m   [32m0.7297[0m        [35m0.2570[0m       0.9323        [94m0.4240[0m     +  16.7128
      5   [36m0.2669[0m   [32m0.7317[0m        [35m0.2525[0m       [31m0.9359[0m        [94m0.4049[0m     +  16.5124
      6   [36m0.2758[0m   [32m0.7353[0m        [35m0.2507[0m       [31m0.9395[0m        [94m0.3973[0m     +  16.5872
      7   0.2681   0.7324        [35m0.2502[0m       0.9359        [94m0.3702[0m     +  17.2633
      8   0.2732   0.7288        [35m0.2458[0m       0.9311        [94m0.3561[0m     +  16.8359
      9   [36m0.2862[0m   [32m0.7378[0m        0.2467       0.9299        [94m0.3425[0m     +  16.6085
     10   0.2861   0.7373        [35m0.2457[0m       0.9323        [94m0.3172[0m     +  16.8938
     11   0.2801   0.7333        [35m0.2423[0m       0.9335        [94m0.3152[0m     +  16.7120
     12   [36m0.2879[0m   0.7355        [35m0.2418[0m       0.9323        [94m0.2961[0m     +  16.5971
     13   0.2839   [32m0.7389[0m        0.2422       0.9323        [94m0.2864[0m     +  16.5728
     14   0.2815   0.7337        [35m0.2403[0m       0.9323        [94m0.2842[0m     +  16.7981
     15   0.2787   0.7342        [35m0.2370[0m       0.9335        [94m0.2650[0m     +  16.6675
     16   0.2858   0.7361        [35m0.2349[0m       0.9335        [94m0.2587[0m     +  16.7811
     17   0.2763   0.7365        0.2354       0.9335        [94m0.2533[0m     +  16.6824
     18   0.2845   [32m0.7400[0m        [35m0.2340[0m       0.9323        [94m0.2523[0m     +  16.7422
     19   0.2867   [32m0.7422[0m        [35m0.2322[0m       0.9335        [94m0.2389[0m     +  16.6553
     20   [36m0.2895[0m   0.7403        0.2332       0.9323        [94m0.2362[0m     +  16.7302
     21   0.2745   0.7320        [35m0.2301[0m       0.9335        [94m0.2343[0m     +  16.6021
     22   0.2732   0.7329        0.2316       0.9335        0.2366        16.5754
     23   [36m0.2912[0m   0.7389        [35m0.2299[0m       0.9335        [94m0.2303[0m     +  16.6662
     24   [36m0.2936[0m   0.7390        [35m0.2290[0m       0.9335        [94m0.2273[0m     +  16.6347
     25   [36m0.3086[0m   [32m0.7428[0m        [35m0.2288[0m       0.9335        [94m0.2248[0m     +  16.7323
     26   0.3065   [32m0.7489[0m        [35m0.2278[0m       0.9335        [94m0.2165[0m     +  16.7978
     27   [36m0.3091[0m   0.7467        [35m0.2252[0m       0.9335        [94m0.2157[0m     +  16.7701
     28   [36m0.3240[0m   [32m0.7515[0m        0.2252       0.9323        [94m0.2143[0m     +  16.7913
     29   [36m0.3321[0m   0.7483        [35m0.2219[0m       0.9335        [94m0.2125[0m     +  16.6417
     30   0.3202   0.7470        0.2238       0.9335        0.2137        16.6390
     31   [36m0.3388[0m   [32m0.7553[0m        0.2231       0.9323        [94m0.2094[0m     +  16.5874
     32   0.3325   0.7490        [35m0.2208[0m       0.9347        0.2123        16.6568
     33   0.3006   0.7402        0.2237       0.9335        0.2128        16.7981
     34   0.3035   0.7398        [35m0.2192[0m       0.9347        0.2128        16.7760
     35   0.3074   0.7442        0.2197       0.9347        0.2129        16.7949
     36   0.3198   0.7486        [35m0.2191[0m       0.9347        0.2098        16.5591
     37   0.3163   0.7472        [35m0.2182[0m       0.9347        0.2105        16.8131
     38   0.3113   0.7387        [35m0.2170[0m       0.9323        0.2120        16.6583
     39   0.3306   0.7488        [35m0.2169[0m       0.9347        0.2109        16.7119
     40   0.3236   0.7488        [35m0.2163[0m       0.9347        [94m0.2094[0m     +  16.8169
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 18:17:56,297][0m Trial 652 finished with value: 0.20943408105364733 and parameters: {'lr': 4.5880626918837665e-05, 'dropout': 0.4559499143068721, 'd_model_multiplier': 4, 'num_layers': 3, 'n_heads': 32, 'dim_feedforward': 311, 'batch_size': 40, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'BatchNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 63}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 69
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2163[0m   [32m0.7738[0m        [35m0.3234[0m       [31m0.9190[0m        [94m0.2633[0m     +  16.7010
      2   0.1930   0.7361        [35m0.3099[0m       [31m0.9202[0m        [94m0.2602[0m     +  16.6468
      3   0.1935   0.7505        [35m0.2704[0m       0.9202        0.2634        16.8967
      4   0.1930   0.7349        0.2823       0.9202        0.2743        16.7644
      5   0.1902   0.7263        0.2836       0.9202        0.2782        16.8122
      6   0.1802   0.6875        0.2908       0.9202        0.2645        16.6065
      7   0.2126   0.7362        0.3038       0.9202        0.2773        16.5344
      8   [36m0.2377[0m   [32m0.7769[0m        0.3076       0.9202        [94m0.2571[0m     +  16.6077
      9   0.1781   0.6374        0.2992       0.9202        0.2766        16.6176
     10   [36m0.2559[0m   0.7384        0.3147       0.9202        [94m0.2529[0m     +  16.8707
     11   0.2195   0.7397        0.3301       0.9141        0.2805        16.6839
     12   0.1922   0.7188        0.3436       0.9202        0.2800        16.5264
     13   0.2177   0.7676        0.3482       0.9021        0.2644        16.6803
     14   0.2342   0.7468        0.3477       0.8972        0.2745        16.6244
     15   0.2027   0.7643        0.3336       0.9057        0.2580        16.6494
     16   0.2052   0.7484        0.3181       0.9202        [94m0.2528[0m     +  16.6355
     17   0.2241   0.7512        0.3370       0.9141        [94m0.2509[0m     +  16.8386
     18   0.1837   0.7502        0.3230       0.9129        0.2588        16.7437
     19   0.2171   0.7574        0.3279       0.9141        0.2584        16.7759
     20   0.1841   0.7007        0.3376       0.9057        0.2702        16.6033
     21   0.2380   0.7493        0.3530       0.9093        0.2903        16.7761
     22   0.2363   0.7608        0.3321       0.9202        0.2616        16.7362
     23   0.2172   0.7536        0.3276       0.9117        0.2617        16.6907
     24   0.1451   0.5363        0.3400       0.9202        0.2956        16.7376
     25   0.2190   0.7534        0.3290       0.9190        [94m0.2509[0m     +  16.7110
     26   0.2054   0.7257        0.3484       0.9129        0.2692        16.7626
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 18:25:28,323][0m Trial 653 finished with value: 0.2508793847664668 and parameters: {'lr': 0.01931898838093246, 'dropout': 0.3613407794604051, 'd_model_multiplier': 4, 'num_layers': 3, 'n_heads': 32, 'dim_feedforward': 325, 'batch_size': 28, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 69}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 62
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2635[0m   [32m0.6996[0m        [35m0.3907[0m       [31m0.8996[0m        [94m0.4009[0m     +  12.7124
      2   [36m0.3144[0m   [32m0.7435[0m        [35m0.2457[0m       0.8972        [94m0.3750[0m     +  12.7048
      3   [36m0.3183[0m   [32m0.7468[0m        [35m0.2417[0m       [31m0.9008[0m        [94m0.3538[0m     +  12.9077
      4   0.3048   0.7416        [35m0.2359[0m       0.9008        [94m0.3502[0m     +  12.8839
      5   0.3117   0.7447        [35m0.2335[0m       0.9008        [94m0.3440[0m     +  12.8823
      6   0.3157   0.7449        0.2349       0.8996        [94m0.3334[0m     +  13.3045
      7   0.3170   0.7431        [35m0.2317[0m       0.8996        [94m0.3330[0m     +  12.9515
      8   0.3152   0.7381        [35m0.2271[0m       0.8996        [94m0.3224[0m     +  13.0162
      9   0.3133   0.7456        [35m0.2258[0m       0.8996        [94m0.3219[0m     +  12.8535
     10   0.3099   0.7434        [35m0.2228[0m       0.8984        [94m0.3150[0m     +  13.0658
     11   0.3094   0.7461        0.2231       0.8984        [94m0.3125[0m     +  13.0609
     12   0.3099   0.7351        [35m0.2202[0m       0.9008        [94m0.3103[0m     +  12.8613
     13   0.3083   0.7468        [35m0.2176[0m       0.8996        [94m0.3096[0m     +  13.0262
     14   0.3089   0.7387        0.2188       0.8996        [94m0.3075[0m     +  13.0660
     15   0.3086   [32m0.7507[0m        [35m0.2168[0m       0.8996        [94m0.3025[0m     +  13.0330
     16   0.3014   0.7468        [35m0.2119[0m       0.8984        [94m0.3017[0m     +  12.9641
     17   0.3100   [32m0.7569[0m        0.2133       0.8984        [94m0.2994[0m     +  12.9883
     18   0.3110   [32m0.7610[0m        0.2132       0.8948        [94m0.2958[0m     +  13.0262
     19   0.3080   [32m0.7633[0m        [35m0.2099[0m       0.8984        [94m0.2910[0m     +  12.8889
     20   0.3044   0.7565        0.2100       0.8984        [94m0.2906[0m     +  12.9836
     21   0.3070   0.7568        [35m0.2071[0m       0.8984        [94m0.2898[0m     +  12.9001
     22   0.3137   0.7590        0.2084       0.8996        0.2928        12.8443
     23   0.3179   0.7606        [35m0.2070[0m       0.8948        [94m0.2888[0m     +  12.9420
     24   [36m0.3208[0m   [32m0.7688[0m        [35m0.2060[0m       0.8948        0.2898        12.8739
     25   [36m0.3222[0m   0.7606        [35m0.2042[0m       0.8960        [94m0.2862[0m     +  13.0967
     26   0.3196   0.7609        [35m0.2031[0m       0.8984        [94m0.2844[0m     +  12.9377
     27   0.3131   0.7620        [35m0.2010[0m       0.8972        0.2866        13.0663
     28   0.3046   0.7648        [35m0.2007[0m       0.8960        [94m0.2839[0m     +  13.0058
     29   0.3138   0.7632        [35m0.1999[0m       0.8984        [94m0.2832[0m     +  12.7360
     30   0.3119   [32m0.7738[0m        [35m0.1973[0m       0.8984        [94m0.2820[0m     +  12.9780
     31   0.3035   0.7704        0.1976       0.8948        0.2838        12.7632
     32   0.2923   0.7516        0.1991       0.8984        0.2883        12.7728
     33   0.2903   0.7564        [35m0.1970[0m       0.8948        0.2880        13.0037
     34   0.2969   0.7564        [35m0.1930[0m       0.8960        0.2870        12.8350
     35   0.2877   0.7495        [35m0.1908[0m       0.8948        0.2889        12.8809
     36   0.2775   0.7493        [35m0.1888[0m       0.8948        0.2909        12.8995
     37   0.2828   0.7547        [35m0.1868[0m       0.8948        0.2901        13.0156
     38   0.2707   0.7429        0.1876       0.8948        0.2938        13.0220
     39   0.2758   0.7453        0.1881       0.8972        0.2935        12.8738
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 18:34:08,182][0m Trial 654 finished with value: 0.2819662377004035 and parameters: {'lr': 7.987185261680512e-05, 'dropout': 0.39809713239418415, 'd_model_multiplier': 4, 'num_layers': 2, 'n_heads': 32, 'dim_feedforward': 340, 'batch_size': 36, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'BatchNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 62}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 142
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0623[0m   [32m0.3315[0m        [35m0.6400[0m       [31m0.9262[0m        [94m0.6281[0m     +  16.1176
      2   0.0558   [32m0.3552[0m        [35m0.4389[0m       0.9262        [94m0.5333[0m     +  16.1075
      3   [36m0.0661[0m   [32m0.4598[0m        [35m0.3369[0m       0.9262        [94m0.4740[0m     +  16.0352
      4   [36m0.0944[0m   [32m0.5861[0m        [35m0.2988[0m       0.9262        [94m0.4433[0m     +  16.3982
      5   [36m0.1303[0m   [32m0.6522[0m        [35m0.2826[0m       0.9262        [94m0.4219[0m     +  16.3331
      6   [36m0.1503[0m   [32m0.6808[0m        [35m0.2702[0m       0.9262        [94m0.4045[0m     +  16.4732
      7   [36m0.1629[0m   [32m0.6968[0m        [35m0.2653[0m       0.9262        [94m0.3935[0m     +  16.3106
      8   [36m0.1732[0m   [32m0.7056[0m        [35m0.2629[0m       0.9262        [94m0.3794[0m     +  16.2576
      9   [36m0.1819[0m   [32m0.7118[0m        [35m0.2600[0m       0.9262        [94m0.3736[0m     +  16.3343
     10   [36m0.1885[0m   [32m0.7168[0m        0.2608       0.9262        [94m0.3659[0m     +  16.2658
     11   [36m0.1940[0m   [32m0.7182[0m        [35m0.2587[0m       0.9262        [94m0.3592[0m     +  16.0625
     12   [36m0.1977[0m   [32m0.7199[0m        [35m0.2576[0m       0.9262        [94m0.3504[0m     +  16.2334
     13   [36m0.1979[0m   0.7195        [35m0.2560[0m       0.9262        [94m0.3449[0m     +  16.1859
     14   [36m0.1984[0m   0.7199        [35m0.2551[0m       0.9262        [94m0.3423[0m     +  16.3016
     15   0.1973   0.7177        0.2560       0.9262        [94m0.3342[0m     +  16.2785
     16   0.1927   0.7173        [35m0.2537[0m       0.9262        [94m0.3283[0m     +  16.3758
     17   0.1933   0.7173        0.2542       0.9262        [94m0.3251[0m     +  16.3578
     18   0.1911   0.7176        [35m0.2536[0m       0.9262        [94m0.3233[0m     +  16.4105
     19   0.1909   0.7175        [35m0.2524[0m       0.9262        [94m0.3225[0m     +  16.4797
     20   0.1918   0.7170        [35m0.2490[0m       0.9262        [94m0.3143[0m     +  16.2292
     21   0.1882   0.7168        0.2528       0.9262        0.3171        16.4635
     22   0.1871   0.7155        0.2503       0.9262        [94m0.3103[0m     +  16.4005
     23   0.1869   0.7162        0.2491       0.9262        [94m0.3071[0m     +  16.1739
     24   0.1894   0.7144        0.2504       0.9262        [94m0.3056[0m     +  16.2900
     25   0.1892   0.7117        [35m0.2479[0m       0.9262        [94m0.2997[0m     +  16.3956
     26   0.1867   0.7136        [35m0.2477[0m       0.9250        [94m0.2985[0m     +  16.3958
     27   0.1885   0.7159        [35m0.2474[0m       0.9238        0.2992        16.1714
     28   0.1845   0.7144        0.2485       0.9238        [94m0.2941[0m     +  16.4698
     29   0.1870   0.7173        [35m0.2458[0m       0.9250        [94m0.2926[0m     +  16.4013
     30   0.1837   0.7175        0.2463       0.9250        0.2934        16.2566
     31   0.1871   0.7163        [35m0.2455[0m       0.9238        [94m0.2898[0m     +  16.1824
     32   0.1855   0.7153        0.2458       0.9238        [94m0.2858[0m     +  16.3535
     33   0.1918   0.7159        0.2475       0.9238        0.2866        16.2423
     34   0.1929   0.7173        [35m0.2450[0m       0.9238        0.2914        16.3902
     35   0.1959   0.7163        [35m0.2403[0m       0.9238        0.2860        16.2793
     36   0.1961   0.7145        0.2435       0.9238        [94m0.2831[0m     +  16.1958
     37   [36m0.1997[0m   0.7189        0.2442       0.9238        0.2854        16.3240
     38   [36m0.2008[0m   [32m0.7206[0m        0.2416       0.9238        0.2869        16.5926
     39   0.1972   0.7157        0.2423       0.9238        [94m0.2773[0m     +  16.1826
     40   0.1952   0.7181        0.2412       0.9238        0.2798        16.2817
     41   0.1956   0.7171        0.2420       0.9226        0.2799        16.2210
     42   0.1955   0.7157        0.2448       0.9238        [94m0.2770[0m     +  16.4079
     43   0.1971   0.7178        0.2438       0.9226        [94m0.2758[0m     +  16.6404
     44   0.1949   0.7182        0.2413       0.9226        0.2759        16.3566
     45   0.1969   0.7197        0.2417       0.9226        0.2759        16.5032
     46   0.1969   0.7175        0.2414       0.9226        [94m0.2735[0m     +  16.3900
     47   0.1977   [32m0.7217[0m        [35m0.2398[0m       0.9226        [94m0.2730[0m     +  16.5099
     48   0.1995   [32m0.7237[0m        0.2430       0.9214        0.2767        16.2587
     49   [36m0.2019[0m   [32m0.7254[0m        [35m0.2387[0m       0.9226        0.2744        16.1541
     50   0.2009   0.7247        0.2402       0.9226        0.2740        16.2863
[32m[I 2023-05-05 18:47:46,472][0m Trial 655 finished with value: 0.27296793804996106 and parameters: {'lr': 2.8617603675645636e-05, 'dropout': 0.6355816313115255, 'd_model_multiplier': 2, 'num_layers': 3, 'n_heads': 32, 'dim_feedforward': 317, 'batch_size': 47, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'BatchNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 142}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 76
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0989[0m   [32m0.5649[0m        [35m0.4596[0m       [31m0.9202[0m        [94m0.3441[0m     +  25.1524
      2   [36m0.2253[0m   [32m0.7241[0m        [35m0.2726[0m       0.9202        [94m0.2732[0m     +  25.3022
      3   [36m0.2507[0m   [32m0.7479[0m        [35m0.2539[0m       0.9190        [94m0.2541[0m     +  25.4708
      4   [36m0.2759[0m   [32m0.7590[0m        [35m0.2474[0m       0.9190        [94m0.2449[0m     +  25.6167
      5   0.2541   0.7579        [35m0.2427[0m       0.9117        0.2465        25.4331
      6   0.2680   [32m0.7694[0m        0.2457       0.9178        [94m0.2393[0m     +  25.5706
      7   0.2714   [32m0.7723[0m        [35m0.2395[0m       0.9166        [94m0.2390[0m     +  25.5514
      8   [36m0.2792[0m   [32m0.7810[0m        [35m0.2374[0m       0.9178        [94m0.2357[0m     +  25.5497
      9   0.2707   0.7805        [35m0.2337[0m       0.9141        0.2375        25.4465
     10   [36m0.2810[0m   [32m0.7872[0m        0.2359       0.9166        0.2362        25.9954
     11   [36m0.2886[0m   [32m0.7919[0m        [35m0.2313[0m       0.9166        [94m0.2327[0m     +  25.5352
     12   0.2873   [32m0.7933[0m        0.2321       0.9178        0.2340        25.5325
     13   [36m0.2918[0m   [32m0.7961[0m        [35m0.2311[0m       0.9190        [94m0.2322[0m     +  25.4858
     14   [36m0.2965[0m   [32m0.8016[0m        [35m0.2272[0m       0.9202        [94m0.2319[0m     +  25.3498
     15   0.2908   0.7986        [35m0.2267[0m       0.9190        0.2340        25.4841
     16   [36m0.3091[0m   [32m0.8065[0m        [35m0.2249[0m       [31m0.9226[0m        0.2319        25.5308
     17   0.3036   0.8038        [35m0.2229[0m       0.9190        0.2346        25.6229
     18   [36m0.3103[0m   [32m0.8093[0m        [35m0.2226[0m       0.9202        [94m0.2303[0m     +  25.3279
     19   [36m0.3160[0m   [32m0.8103[0m        0.2242       [31m0.9238[0m        [94m0.2285[0m     +  25.4149
     20   [36m0.3237[0m   [32m0.8127[0m        [35m0.2221[0m       0.9238        0.2301        25.4940
     21   0.3129   [32m0.8151[0m        [35m0.2202[0m       0.9226        0.2307        25.4576
     22   0.3151   [32m0.8187[0m        [35m0.2173[0m       0.9214        0.2295        25.6563
     23   0.3206   0.8171        [35m0.2171[0m       0.9214        0.2301        25.5449
     24   0.3205   0.8179        0.2181       0.9202        0.2328        25.5409
     25   [36m0.3238[0m   [32m0.8211[0m        [35m0.2147[0m       0.9214        0.2302        25.5210
     26   [36m0.3258[0m   0.8177        [35m0.2143[0m       0.9202        0.2308        25.6856
     27   0.3246   0.8119        [35m0.2095[0m       0.9190        0.2348        25.5370
     28   0.3071   0.8109        0.2111       0.9190        0.2365        25.5344
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 19:00:07,821][0m Trial 656 finished with value: 0.22847634080928067 and parameters: {'lr': 6.148908058314186e-05, 'dropout': 0.3824833921574659, 'd_model_multiplier': 1, 'num_layers': 7, 'n_heads': 32, 'dim_feedforward': 335, 'batch_size': 22, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'BatchNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 76}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 72
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0697[0m   [32m0.3328[0m        [35m0.5727[0m       [31m0.9287[0m        [94m0.5712[0m     +  24.2696
      2   [36m0.0852[0m   [32m0.5318[0m        [35m0.3588[0m       0.9287        [94m0.4813[0m     +  24.4818
      3   [36m0.1971[0m   [32m0.7088[0m        [35m0.3071[0m       0.9287        [94m0.4433[0m     +  24.3852
      4   [36m0.2319[0m   [32m0.7471[0m        [35m0.2886[0m       0.9274        [94m0.4254[0m     +  24.5512
      5   [36m0.2444[0m   [32m0.7576[0m        [35m0.2843[0m       0.9262        [94m0.4124[0m     +  24.4272
      6   0.2426   0.7564        [35m0.2796[0m       0.9262        [94m0.4001[0m     +  24.6478
      7   0.2423   0.7523        [35m0.2771[0m       0.9262        [94m0.3902[0m     +  24.5667
      8   [36m0.2480[0m   0.7499        [35m0.2746[0m       0.9262        [94m0.3821[0m     +  24.4471
      9   0.2475   0.7489        [35m0.2694[0m       0.9262        [94m0.3791[0m     +  24.4198
     10   [36m0.2489[0m   0.7499        0.2728       0.9238        [94m0.3682[0m     +  24.7258
     11   [36m0.2492[0m   0.7426        0.2695       0.9274        [94m0.3456[0m     +  24.6153
     12   0.2489   0.7426        [35m0.2683[0m       0.9262        [94m0.3360[0m     +  24.7684
     13   [36m0.2517[0m   0.7443        [35m0.2680[0m       0.9250        [94m0.3344[0m     +  24.4665
     14   0.2495   0.7403        [35m0.2669[0m       0.9262        [94m0.3200[0m     +  24.5239
     15   0.2490   0.7357        [35m0.2645[0m       0.9262        [94m0.3089[0m     +  24.5993
     16   0.2484   0.7345        [35m0.2628[0m       0.9274        [94m0.2978[0m     +  24.5421
     17   [36m0.2519[0m   0.7390        0.2643       0.9274        [94m0.2902[0m     +  24.4632
     18   [36m0.2527[0m   0.7400        0.2629       0.9274        [94m0.2787[0m     +  24.4185
     19   [36m0.2531[0m   0.7404        [35m0.2610[0m       0.9274        [94m0.2748[0m     +  24.5541
     20   [36m0.2611[0m   0.7459        0.2632       0.9250        [94m0.2729[0m     +  24.5863
     21   0.2597   0.7453        [35m0.2610[0m       0.9274        [94m0.2663[0m     +  24.5116
     22   0.2599   0.7449        [35m0.2582[0m       0.9287        [94m0.2607[0m     +  24.5338
     23   0.2523   0.7391        0.2590       0.9274        [94m0.2525[0m     +  24.4823
     24   0.2588   0.7479        [35m0.2562[0m       0.9274        0.2559        24.6560
     25   [36m0.2613[0m   0.7512        0.2582       0.9274        [94m0.2509[0m     +  24.6349
     26   0.2522   0.7507        0.2564       0.9287        [94m0.2472[0m     +  24.6452
     27   0.2530   0.7488        0.2578       0.9287        0.2475        24.6872
     28   0.2530   0.7546        [35m0.2553[0m       0.9287        [94m0.2441[0m     +  24.6783
     29   0.2474   0.7487        0.2574       0.9274        [94m0.2431[0m     +  24.5015
     30   0.2581   [32m0.7628[0m        [35m0.2549[0m       0.9274        [94m0.2429[0m     +  24.6636
     31   0.2524   0.7552        [35m0.2535[0m       0.9287        [94m0.2392[0m     +  24.5408
     32   0.2573   0.7553        0.2547       0.9274        [94m0.2388[0m     +  24.6892
     33   0.2599   [32m0.7643[0m        [35m0.2507[0m       0.9287        [94m0.2361[0m     +  24.6333
     34   0.2596   0.7622        0.2532       0.9287        0.2394        24.6179
     35   [36m0.2670[0m   [32m0.7711[0m        [35m0.2486[0m       0.9287        [94m0.2355[0m     +  24.5608
     36   0.2650   0.7682        0.2548       0.9287        [94m0.2339[0m     +  24.7195
     37   0.2622   0.7625        0.2499       0.9287        [94m0.2311[0m     +  24.5984
     38   0.2644   0.7675        0.2511       0.9287        0.2344        24.5955
     39   0.2642   0.7689        0.2516       0.9287        0.2319        24.3986
     40   [36m0.2717[0m   [32m0.7793[0m        0.2535       0.9287        [94m0.2303[0m     +  24.3354
     41   0.2711   [32m0.7819[0m        [35m0.2479[0m       0.9287        [94m0.2300[0m     +  24.4681
     42   0.2698   [32m0.7826[0m        [35m0.2467[0m       0.9287        [94m0.2288[0m     +  24.4987
     43   0.2691   0.7810        0.2504       0.9287        [94m0.2276[0m     +  24.5270
     44   [36m0.2768[0m   [32m0.7850[0m        0.2511       0.9287        [94m0.2275[0m     +  24.5523
     45   [36m0.2881[0m   [32m0.7883[0m        0.2494       0.9274        [94m0.2269[0m     +  24.5746
     46   0.2856   [32m0.7904[0m        0.2481       0.9274        [94m0.2262[0m     +  24.4845
     47   0.2861   0.7866        0.2482       0.9287        [94m0.2259[0m     +  24.5489
     48   0.2850   [32m0.7920[0m        [35m0.2465[0m       0.9287        [94m0.2226[0m     +  24.4426
     49   0.2864   [32m0.7972[0m        0.2468       0.9287        0.2228        24.5841
     50   0.2879   [32m0.8009[0m        0.2515       0.9262        [94m0.2214[0m     +  24.6035
[32m[I 2023-05-05 19:20:40,720][0m Trial 657 finished with value: 0.22137002392864574 and parameters: {'lr': 2.7534554463300896e-05, 'dropout': 0.6200705640479384, 'd_model_multiplier': 4, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 343, 'batch_size': 42, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'BatchNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 72}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 152
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.3500[0m   [32m0.7840[0m        [35m0.3419[0m       [31m0.9069[0m        [94m0.2701[0m     +  31.8893
      2   [36m0.3784[0m   [32m0.8019[0m        [35m0.2455[0m       [31m0.9117[0m        [94m0.2589[0m     +  31.7408
      3   [36m0.3802[0m   [32m0.8019[0m        [35m0.2374[0m       0.9069        [94m0.2563[0m     +  31.9848
      4   [36m0.3808[0m   0.7993        [35m0.2337[0m       0.9117        [94m0.2562[0m     +  32.0597
      5   0.3753   0.7935        [35m0.2283[0m       0.9117        0.2592        31.7888
      6   0.3582   0.7880        [35m0.2247[0m       0.9105        0.2605        32.0617
      7   0.3745   0.7935        [35m0.2210[0m       0.9105        0.2580        31.9964
      8   0.3670   0.7866        [35m0.2179[0m       0.9093        0.2609        31.8904
      9   0.3529   0.7891        [35m0.2135[0m       0.9081        0.2641        32.1067
     10   0.3560   0.7794        [35m0.2117[0m       0.9057        0.2683        31.8888
     11   0.3502   0.7738        [35m0.2072[0m       0.9045        0.2708        31.9503
     12   0.3594   0.7780        [35m0.2052[0m       0.9081        0.2654        32.1063
     13   0.3656   0.7824        [35m0.1995[0m       0.9081        0.2637        32.1414
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 19:28:10,730][0m Trial 658 finished with value: 0.2561586303174712 and parameters: {'lr': 1.3503930271166195e-05, 'dropout': 0.27245543903722497, 'd_model_multiplier': 32, 'num_layers': 3, 'n_heads': 32, 'dim_feedforward': 328, 'batch_size': 32, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'BatchNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 152}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 56
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.3021[0m   [32m0.7650[0m        [35m0.5200[0m       [31m0.8948[0m        [94m0.2793[0m     +  31.7584
      2   0.1439   0.6447        1.0196       [31m0.9178[0m        0.3801        31.9422
      3   0.1322   0.4039        1.9403       0.9069        3.7089        31.8935
      4   0.2527   0.7646        4.0881       0.8694        2.0920        31.9230
      5   0.1413   0.5924        1.4854       0.9141        0.3614        31.7966
      6   0.2451   0.7342        [35m0.3368[0m       0.9166        [94m0.2521[0m     +  32.1028
      7   0.2590   0.7485        [35m0.2773[0m       0.9141        [94m0.2500[0m     +  32.0798
      8   0.2194   0.6869        [35m0.2690[0m       0.9178        0.3031        32.1714
      9   0.2709   [32m0.7757[0m        [35m0.2667[0m       0.8924        0.2575        32.0723
     10   0.2414   0.7520        0.2695       0.9178        [94m0.2451[0m     +  32.2866
     11   0.2644   0.7538        0.2719       0.9178        0.2539        32.0158
     12   0.2827   0.7611        0.2679       [31m0.9190[0m        0.2484        32.2013
     13   0.2975   [32m0.7878[0m        [35m0.2634[0m       0.9117        0.2461        31.9187
     14   0.2568   0.7700        0.2743       0.9178        0.2628        31.8559
     15   [36m0.3111[0m   0.7818        0.2747       0.9178        0.2487        32.0856
     16   0.2270   0.7299        0.2713       0.9178        0.2730        32.1727
     17   0.3018   0.7805        0.2801       0.8948        0.3101        31.9240
     18   0.2531   0.6477        0.2769       0.9178        0.2848        32.0916
     19   0.2760   0.7693        0.2761       0.8912        0.2658        32.0477
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 19:38:51,880][0m Trial 659 finished with value: 0.24514288139927057 and parameters: {'lr': 0.09898005968827893, 'dropout': 0.6442810006011096, 'd_model_multiplier': 4, 'num_layers': 7, 'n_heads': 32, 'dim_feedforward': 350, 'batch_size': 45, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0, 'top_n_features': 56}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 59
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.2362[0m   [32m0.7423[0m        [35m0.3850[0m       [31m0.9383[0m        [94m0.3253[0m     +  8.7382
      2   [36m0.2637[0m   [32m0.7719[0m        [35m0.2631[0m       0.9383        [94m0.2901[0m     +  9.4448
      3   [36m0.2905[0m   [32m0.7821[0m        [35m0.2568[0m       0.9383        [94m0.2791[0m     +  9.3938
      4   [36m0.2916[0m   [32m0.7850[0m        [35m0.2521[0m       [31m0.9407[0m        [94m0.2734[0m     +  9.8444
      5   0.2858   [32m0.7867[0m        [35m0.2498[0m       0.9371        [94m0.2701[0m     +  9.6941
      6   0.2881   0.7819        0.2505       0.9359        [94m0.2652[0m     +  9.4736
      7   [36m0.2972[0m   0.7824        0.2505       0.9359        [94m0.2610[0m     +  9.8139
      8   [36m0.3099[0m   0.7790        [35m0.2476[0m       0.9359        [94m0.2606[0m     +  9.7378
      9   [36m0.3239[0m   [32m0.7869[0m        [35m0.2449[0m       0.9359        [94m0.2590[0m     +  9.7586
     10   0.3112   0.7775        [35m0.2447[0m       0.9383        [94m0.2586[0m     +  9.6146
     11   0.3054   0.7858        [35m0.2440[0m       0.9347        0.2662        9.6679
     12   0.3150   0.7861        [35m0.2435[0m       0.9371        0.2669        9.3330
     13   0.3151   0.7803        [35m0.2435[0m       0.9395        0.2602        9.4953
     14   0.3189   0.7866        [35m0.2419[0m       0.9359        0.2638        9.4423
     15   0.3160   0.7846        [35m0.2413[0m       0.9383        0.2668        9.3940
     16   0.3158   0.7706        0.2421       0.9371        0.2607        9.6215
     17   0.3211   0.7681        [35m0.2404[0m       0.9395        0.2597        9.6753
     18   [36m0.3278[0m   0.7730        0.2406       0.9371        0.2602        9.4274
     19   0.3194   0.7812        [35m0.2380[0m       0.9383        [94m0.2562[0m     +  9.4719
     20   0.3196   0.7634        [35m0.2378[0m       0.9383        [94m0.2551[0m     +  9.2218
     21   0.3168   0.7636        0.2380       0.9395        [94m0.2545[0m     +  9.7595
     22   [36m0.3280[0m   0.7717        0.2388       0.9383        0.2562        9.6756
     23   0.3193   0.7668        0.2399       0.9395        0.2590        9.5986
     24   [36m0.3304[0m   0.7681        [35m0.2355[0m       0.9383        0.2550        9.5004
     25   0.2979   0.7646        0.2378       0.9395        0.2604        9.6476
     26   0.3120   0.7597        0.2372       0.9395        0.2550        9.6457
     27   0.2936   0.7582        0.2400       0.9407        0.2546        9.4943
     28   0.3105   0.7638        [35m0.2344[0m       0.9407        0.2562        9.3972
     29   0.2968   0.7545        0.2356       [31m0.9420[0m        0.2558        9.5444
     30   0.3166   0.7675        0.2346       0.9395        0.2578        9.5439
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 19:43:48,417][0m Trial 660 finished with value: 0.254503334411831 and parameters: {'lr': 0.00012684310180393905, 'dropout': 0.6010949826082808, 'd_model_multiplier': 4, 'num_layers': 1, 'n_heads': 32, 'dim_feedforward': 333, 'batch_size': 37, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'BatchNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 59}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 67
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2257[0m   [32m0.7478[0m        [35m0.3477[0m       [31m0.9311[0m        [94m0.2274[0m     +  12.5807
      2   [36m0.2516[0m   [32m0.7597[0m        [35m0.2440[0m       0.9311        [94m0.2202[0m     +  12.6666
      3   [36m0.2583[0m   0.7536        [35m0.2344[0m       0.9274        [94m0.2175[0m     +  12.7085
      4   0.2571   0.7592        [35m0.2314[0m       0.9311        [94m0.2166[0m     +  12.9732
      5   [36m0.2626[0m   0.7564        [35m0.2241[0m       [31m0.9323[0m        0.2186        12.9885
      6   0.2587   [32m0.7720[0m        [35m0.2158[0m       0.9323        0.2235        12.7703
      7   0.2574   [32m0.7771[0m        [35m0.2110[0m       [31m0.9347[0m        0.2234        12.7955
      8   [36m0.2673[0m   0.7769        [35m0.2075[0m       0.9311        0.2267        12.8022
      9   0.2484   0.7715        [35m0.2004[0m       0.9287        0.2290        12.8326
     10   0.2478   0.7766        [35m0.1945[0m       0.9274        0.2252        12.8718
     11   0.2366   0.7696        [35m0.1902[0m       0.9214        0.2363        12.8345
     12   0.2339   0.7719        [35m0.1844[0m       0.9226        0.2322        12.7722
     13   0.2207   0.7644        [35m0.1789[0m       0.9202        0.2353        12.8362
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 19:46:48,026][0m Trial 661 finished with value: 0.2165729380882781 and parameters: {'lr': 9.366309726670187e-05, 'dropout': 0.13394092591425372, 'd_model_multiplier': 4, 'num_layers': 2, 'n_heads': 32, 'dim_feedforward': 324, 'batch_size': 29, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'BatchNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 67}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 63
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0666[0m   [32m0.4274[0m        [35m0.5344[0m       [31m0.9274[0m        [94m0.5253[0m     +  19.4207
      2   [36m0.1353[0m   [32m0.6681[0m        [35m0.3178[0m       0.9274        [94m0.3864[0m     +  19.5267
      3   [36m0.1993[0m   [32m0.7351[0m        [35m0.2702[0m       0.9274        [94m0.3312[0m     +  19.4390
      4   [36m0.2383[0m   [32m0.7559[0m        [35m0.2589[0m       0.9274        [94m0.3039[0m     +  19.4570
      5   0.2033   0.7360        [35m0.2550[0m       0.9274        [94m0.2979[0m     +  19.3896
      6   0.2132   0.7393        [35m0.2517[0m       [31m0.9287[0m        [94m0.2801[0m     +  19.9638
      7   0.2155   0.7464        [35m0.2507[0m       0.9262        [94m0.2695[0m     +  19.6465
      8   0.2142   0.7435        [35m0.2460[0m       0.9262        0.2848        19.4507
      9   0.2168   0.7374        [35m0.2455[0m       0.9250        0.2795        19.6390
     10   0.2238   0.7388        0.2473       0.9250        0.2743        19.6800
     11   0.2219   0.7467        [35m0.2446[0m       0.9262        [94m0.2570[0m     +  19.5074
     12   0.2303   0.7536        [35m0.2421[0m       0.9250        0.2570        19.4827
     13   0.2272   0.7445        [35m0.2383[0m       0.9238        [94m0.2533[0m     +  19.5639
     14   0.2288   0.7540        [35m0.2375[0m       0.9250        0.2533        19.7311
     15   0.2305   0.7452        0.2376       0.9250        [94m0.2515[0m     +  19.6644
     16   0.2346   0.7461        [35m0.2358[0m       0.9250        [94m0.2467[0m     +  19.4760
     17   0.2348   0.7495        0.2380       0.9262        0.2501        19.6805
     18   0.2256   0.7524        [35m0.2336[0m       0.9250        [94m0.2446[0m     +  19.4560
     19   [36m0.2426[0m   0.7431        [35m0.2323[0m       0.9262        0.2490        19.7268
     20   [36m0.2455[0m   0.7430        0.2342       0.9274        0.2493        19.5247
     21   [36m0.2515[0m   [32m0.7566[0m        0.2332       0.9262        [94m0.2421[0m     +  19.4816
     22   0.2503   [32m0.7590[0m        0.2325       0.9287        [94m0.2406[0m     +  19.4160
     23   0.2440   [32m0.7615[0m        [35m0.2310[0m       0.9274        [94m0.2384[0m     +  19.5613
     24   0.2432   0.7604        [35m0.2300[0m       0.9250        0.2406        19.4639
     25   0.2419   0.7538        [35m0.2297[0m       0.9274        0.2412        19.4140
     26   [36m0.2555[0m   [32m0.7641[0m        [35m0.2273[0m       0.9274        [94m0.2365[0m     +  19.6698
     27   0.2550   0.7633        0.2301       0.9274        [94m0.2342[0m     +  19.6313
     28   0.2483   0.7465        0.2288       [31m0.9299[0m        0.2349        19.5123
     29   0.2514   0.7476        0.2274       0.9274        [94m0.2329[0m     +  19.7042
     30   [36m0.2645[0m   0.7638        [35m0.2250[0m       0.9287        [94m0.2294[0m     +  19.7509
     31   0.2629   0.7629        0.2268       0.9299        0.2310        19.6293
     32   [36m0.2657[0m   0.7612        [35m0.2247[0m       0.9299        [94m0.2283[0m     +  19.5108
     33   0.2655   [32m0.7664[0m        [35m0.2229[0m       0.9287        [94m0.2281[0m     +  19.4784
     34   0.2634   0.7615        0.2239       0.9274        0.2293        19.5326
     35   [36m0.2693[0m   0.7637        [35m0.2217[0m       0.9299        [94m0.2279[0m     +  19.6482
     36   0.2653   0.7583        [35m0.2206[0m       0.9226        0.2313        19.5624
     37   0.2636   0.7591        0.2217       0.9287        0.2292        19.4814
     38   0.2659   0.7638        0.2220       0.9250        0.2299        19.5707
     39   0.2643   0.7599        0.2212       0.9287        0.2308        19.6499
     40   0.2635   0.7545        [35m0.2178[0m       0.9274        0.2327        19.6953
     41   0.2584   0.7608        0.2187       0.9274        0.2299        19.7220
     42   0.2633   0.7593        0.2191       0.9250        0.2311        19.5374
     43   [36m0.2749[0m   0.7596        0.2191       0.9238        0.2301        19.5773
     44   0.2654   0.7555        [35m0.2154[0m       0.9274        0.2312        19.4072
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 20:01:31,228][0m Trial 662 finished with value: 0.22794078036404003 and parameters: {'lr': 4.588662057212854e-05, 'dropout': 0.46682098608240646, 'd_model_multiplier': 1, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 344, 'batch_size': 24, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'BatchNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 63}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 60
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0639[0m   [32m0.2426[0m        [35m0.5572[0m       [31m0.9057[0m        [94m0.5588[0m     +  16.3856
      2   [36m0.0879[0m   [32m0.4798[0m        [35m0.3673[0m       0.9057        [94m0.4481[0m     +  16.7321
      3   [36m0.3749[0m   [32m0.7310[0m        [35m0.2972[0m       0.9045        [94m0.3823[0m     +  16.6880
      4   [36m0.4152[0m   [32m0.7827[0m        [35m0.2734[0m       0.9045        [94m0.3574[0m     +  16.7351
      5   [36m0.4291[0m   [32m0.8047[0m        [35m0.2674[0m       0.9057        [94m0.3333[0m     +  16.6853
      6   [36m0.4383[0m   [32m0.8142[0m        [35m0.2618[0m       [31m0.9093[0m        [94m0.3241[0m     +  16.6145
      7   0.4376   [32m0.8173[0m        [35m0.2586[0m       0.9093        [94m0.3194[0m     +  16.9008
      8   0.4344   0.8150        [35m0.2563[0m       [31m0.9105[0m        [94m0.3157[0m     +  17.0228
      9   0.4349   0.8158        [35m0.2529[0m       [31m0.9141[0m        [94m0.3027[0m     +  16.7400
     10   0.4330   0.8145        [35m0.2524[0m       0.9141        [94m0.2983[0m     +  16.5978
     11   0.4321   0.8153        [35m0.2499[0m       [31m0.9154[0m        0.3008        17.0344
     12   0.4274   0.8095        [35m0.2475[0m       0.9117        [94m0.2944[0m     +  16.7965
     13   0.4291   0.8116        0.2489       0.9105        [94m0.2907[0m     +  16.8708
     14   0.4284   0.8098        [35m0.2465[0m       0.9081        [94m0.2894[0m     +  17.0331
     15   0.4258   0.8082        [35m0.2446[0m       0.9117        [94m0.2813[0m     +  16.7600
     16   0.4227   0.8066        [35m0.2434[0m       0.9081        0.2831        16.7767
     17   0.4221   0.8053        [35m0.2424[0m       0.9093        [94m0.2805[0m     +  16.8561
     18   0.4220   0.8006        [35m0.2422[0m       0.9093        [94m0.2762[0m     +  16.7934
     19   0.4216   0.8002        [35m0.2411[0m       0.9093        0.2766        16.9726
     20   0.4229   0.8022        [35m0.2405[0m       0.9093        [94m0.2711[0m     +  16.8478
     21   0.4188   0.7982        0.2429       0.9093        0.2751        16.7223
     22   0.4207   0.7996        [35m0.2389[0m       0.9093        0.2724        16.8247
     23   0.4235   0.7987        [35m0.2377[0m       0.9093        0.2724        16.8306
     24   0.4235   0.7956        [35m0.2369[0m       0.9093        0.2716        16.6434
     25   0.4280   0.7983        0.2377       0.9093        [94m0.2687[0m     +  16.8366
     26   0.4325   0.7994        [35m0.2364[0m       0.9081        [94m0.2679[0m     +  17.2908
     27   0.4286   0.7943        [35m0.2346[0m       0.9069        0.2684        16.6415
     28   0.4307   0.7935        [35m0.2344[0m       0.9081        [94m0.2664[0m     +  16.6275
     29   0.4327   0.7950        [35m0.2342[0m       0.9081        [94m0.2651[0m     +  17.1019
     30   0.4322   0.7925        [35m0.2302[0m       0.9081        [94m0.2646[0m     +  16.9395
     31   0.4294   0.7911        [35m0.2300[0m       0.9081        [94m0.2639[0m     +  16.8958
     32   0.4297   0.7893        0.2313       0.9081        0.2639        17.0420
     33   0.4254   0.7867        0.2301       0.9045        0.2649        16.9990
     34   0.4270   0.7870        [35m0.2290[0m       0.9057        0.2650        16.7372
     35   0.4246   0.7858        [35m0.2282[0m       0.9057        [94m0.2625[0m     +  16.7140
     36   0.4273   0.7856        [35m0.2282[0m       0.9057        0.2628        16.8466
     37   0.4271   0.7850        0.2287       0.9057        [94m0.2619[0m     +  16.9362
     38   0.4277   0.7827        0.2283       0.9045        [94m0.2615[0m     +  16.6921
     39   0.4311   0.7891        [35m0.2258[0m       0.9057        [94m0.2608[0m     +  16.7568
     40   0.4236   0.7827        0.2260       0.9033        0.2621        16.8390
     41   0.4168   0.7764        [35m0.2237[0m       0.9033        0.2657        16.9386
     42   0.4219   0.7803        0.2239       0.9045        [94m0.2604[0m     +  16.7365
     43   0.4189   0.7820        0.2245       0.9045        0.2616        16.8994
     44   0.4251   0.7854        [35m0.2230[0m       0.9045        [94m0.2601[0m     +  16.7141
     45   0.4279   0.7855        0.2238       0.9057        [94m0.2596[0m     +  16.8475
     46   0.4240   0.7814        [35m0.2204[0m       0.9045        0.2609        16.7425
     47   0.4179   0.7735        0.2240       0.9045        0.2625        16.8755
     48   0.4214   0.7790        0.2218       0.9057        0.2599        16.8668
     49   0.4206   0.7803        0.2228       0.9057        0.2615        16.8895
     50   0.4247   0.7844        0.2213       0.9081        [94m0.2595[0m     +  16.8752
[32m[I 2023-05-05 20:15:35,339][0m Trial 663 finished with value: 0.259451251877911 and parameters: {'lr': 2.122698360413277e-05, 'dropout': 0.44477259472700803, 'd_model_multiplier': 4, 'num_layers': 3, 'n_heads': 32, 'dim_feedforward': 355, 'batch_size': 51, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'BatchNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 60}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 81
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.3272[0m   [32m0.7524[0m        [35m0.3400[0m       [31m0.9262[0m        [94m0.2555[0m     +  27.8661
      2   [36m0.3507[0m   [32m0.7655[0m        [35m0.2719[0m       [31m0.9287[0m        [94m0.2283[0m     +  27.8754
      3   [36m0.3632[0m   0.7615        [35m0.2680[0m       [31m0.9311[0m        [94m0.2233[0m     +  27.9006
      4   0.3525   0.7611        [35m0.2654[0m       0.9287        [94m0.2227[0m     +  27.8004
      5   0.3534   0.7565        [35m0.2584[0m       0.9299        0.2249        28.1132
      6   0.3391   0.7414        [35m0.2537[0m       0.9262        0.2380        27.9903
      7   0.3180   0.7375        0.2547       0.9274        0.2412        28.1195
      8   0.3200   0.7354        [35m0.2536[0m       0.9274        0.2388        28.1625
      9   0.3110   0.7343        [35m0.2482[0m       0.9287        0.2429        28.1939
     10   0.3172   0.7307        [35m0.2477[0m       0.9274        0.2391        27.6454
     11   0.3128   0.7315        [35m0.2474[0m       0.9274        0.2395        27.7757
     12   0.3142   0.7319        [35m0.2457[0m       0.9274        0.2433        27.7508
     13   0.3211   0.7359        [35m0.2411[0m       0.9274        0.2441        27.6926
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 20:22:06,952][0m Trial 664 finished with value: 0.22268838016935674 and parameters: {'lr': 3.581390742194266e-05, 'dropout': 0.45331335734223144, 'd_model_multiplier': 4, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 468, 'batch_size': 8, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'BatchNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 81}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 178
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0559[0m   [32m0.3281[0m        [35m0.5577[0m       [31m0.9238[0m        [94m0.5431[0m     +  35.8684
      2   [36m0.1083[0m   [32m0.5791[0m        [35m0.3333[0m       0.9238        [94m0.4411[0m     +  35.9455
      3   [36m0.1782[0m   [32m0.6719[0m        [35m0.2931[0m       0.9214        [94m0.3940[0m     +  36.1300
      4   [36m0.2063[0m   [32m0.7025[0m        [35m0.2875[0m       0.9214        [94m0.3555[0m     +  35.8466
      5   [36m0.2306[0m   [32m0.7236[0m        [35m0.2810[0m       0.9238        [94m0.2959[0m     +  36.1727
      6   [36m0.2512[0m   [32m0.7389[0m        [35m0.2784[0m       0.9238        [94m0.2839[0m     +  36.2841
      7   0.2510   0.7359        [35m0.2746[0m       0.9238        [94m0.2739[0m     +  35.8658
      8   [36m0.2527[0m   0.7339        [35m0.2685[0m       0.9226        [94m0.2530[0m     +  36.3152
      9   [36m0.2737[0m   [32m0.7394[0m        [35m0.2653[0m       0.9238        0.2533        36.0059
     10   0.2553   0.7324        [35m0.2621[0m       0.9238        [94m0.2486[0m     +  36.1613
     11   0.2348   0.7201        0.2627       0.9238        0.2499        36.0541
     12   0.2519   0.7288        [35m0.2617[0m       0.9226        0.2499        36.1416
     13   0.2441   0.7287        [35m0.2602[0m       0.9226        [94m0.2452[0m     +  36.0621
     14   0.2290   0.7201        [35m0.2577[0m       0.9226        0.2464        36.1620
     15   0.2322   0.7235        [35m0.2533[0m       0.9214        0.2461        35.8527
     16   0.2464   0.7230        0.2535       0.9226        0.2467        35.9744
     17   0.2418   0.7266        [35m0.2525[0m       0.9226        0.2453        36.2961
     18   0.2255   0.7182        [35m0.2487[0m       0.9214        0.2493        36.3180
     19   0.2236   0.7174        0.2492       0.9214        0.2471        36.3646
     20   0.2276   0.7224        0.2494       0.9214        0.2467        36.4600
     21   0.2269   0.7198        [35m0.2482[0m       0.9214        0.2474        36.0857
     22   0.2282   0.7245        [35m0.2461[0m       0.9214        0.2467        36.2049
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 20:35:59,153][0m Trial 665 finished with value: 0.24519357778933779 and parameters: {'lr': 7.145107806203633e-05, 'dropout': 0.5796882446209098, 'd_model_multiplier': 4, 'num_layers': 8, 'n_heads': 32, 'dim_feedforward': 376, 'batch_size': 46, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 178}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 73
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1537[0m   [32m0.5934[0m        [35m0.4466[0m       [31m0.9238[0m        [94m0.3301[0m     +  12.4039
      2   [36m0.2458[0m   [32m0.7541[0m        [35m0.2559[0m       [31m0.9262[0m        [94m0.2769[0m     +  12.8322
      3   [36m0.2662[0m   [32m0.7937[0m        [35m0.2442[0m       0.9238        [94m0.2648[0m     +  12.9220
      4   [36m0.2704[0m   [32m0.8034[0m        [35m0.2381[0m       0.9202        [94m0.2587[0m     +  12.5493
      5   0.2698   [32m0.8091[0m        [35m0.2341[0m       0.9214        [94m0.2543[0m     +  12.8925
      6   0.2686   [32m0.8111[0m        [35m0.2325[0m       0.9154        0.2546        12.9637
      7   0.2652   [32m0.8119[0m        [35m0.2288[0m       0.9154        [94m0.2523[0m     +  12.8455
      8   0.2665   [32m0.8127[0m        [35m0.2274[0m       0.9178        [94m0.2458[0m     +  12.9029
      9   0.2634   [32m0.8168[0m        [35m0.2240[0m       0.9166        0.2481        12.8697
     10   0.2644   0.8138        0.2241       0.9154        [94m0.2429[0m     +  12.7831
     11   0.2612   0.8112        [35m0.2220[0m       0.9141        [94m0.2423[0m     +  13.0507
     12   0.2503   0.8135        [35m0.2201[0m       0.9117        0.2508        12.7957
     13   0.2525   0.8080        [35m0.2188[0m       0.9141        0.2427        13.1565
     14   0.2538   0.8106        [35m0.2184[0m       0.9141        0.2432        12.7426
     15   0.2539   0.8114        [35m0.2156[0m       0.9154        0.2437        13.2055
     16   0.2548   0.8085        [35m0.2129[0m       0.9129        [94m0.2395[0m     +  12.9706
     17   0.2502   0.8038        [35m0.2122[0m       0.9117        0.2404        13.0534
     18   0.2463   0.8049        [35m0.2096[0m       0.9129        0.2435        13.0789
     19   0.2474   0.8116        [35m0.2081[0m       0.9081        0.2465        12.9441
     20   0.2616   0.8094        [35m0.2071[0m       0.9093        [94m0.2380[0m     +  12.9504
     21   0.2544   0.8077        [35m0.2046[0m       0.9093        0.2381        12.6962
     22   0.2505   0.8072        [35m0.2036[0m       0.9129        [94m0.2359[0m     +  13.0844
     23   0.2475   0.8100        [35m0.2008[0m       0.9117        0.2424        13.0244
     24   0.2408   0.8117        0.2020       0.9069        0.2495        12.7575
     25   0.2483   0.8097        [35m0.1988[0m       0.9081        0.2463        12.7564
     26   0.2421   0.8060        [35m0.1969[0m       0.9081        0.2549        13.0850
     27   0.2424   0.8031        [35m0.1958[0m       0.9093        0.2468        13.7972
     28   0.2432   0.8028        [35m0.1954[0m       0.9117        0.2471        12.7816
     29   0.2389   0.8023        [35m0.1952[0m       0.9057        0.2533        12.9856
     30   0.2386   0.7983        [35m0.1938[0m       0.9105        0.2555        13.0875
     31   0.2374   0.7950        [35m0.1930[0m       0.9117        0.2550        12.6025
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 20:42:53,788][0m Trial 666 finished with value: 0.23591121258225378 and parameters: {'lr': 5.362254820104083e-05, 'dropout': 0.3379355981290488, 'd_model_multiplier': 16, 'num_layers': 1, 'n_heads': 32, 'dim_feedforward': 318, 'batch_size': 122, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'BatchNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0, 'top_n_features': 73}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 64
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1465[0m   [32m0.6731[0m        [35m0.2921[0m       [31m0.9154[0m        [94m0.3716[0m     +  19.9433
      2   [36m0.3372[0m   [32m0.7467[0m        [35m0.2681[0m       [31m0.9226[0m        [94m0.2890[0m     +  19.7178
      3   0.3308   0.6948        [35m0.2543[0m       0.9178        0.2975        19.9333
      4   0.2964   0.6071        [35m0.2492[0m       0.9154        0.3267        19.9403
      5   0.2195   0.5208        [35m0.2442[0m       0.9154        0.4474        20.1551
      6   0.3048   0.6718        [35m0.2438[0m       0.9154        0.3639        19.9165
      7   0.2981   0.6170        0.2438       0.9154        0.3678        19.8898
      8   0.3200   0.7357        0.2491       0.9154        0.3066        19.9868
      9   [36m0.3611[0m   [32m0.7714[0m        0.2474       0.9154        [94m0.2556[0m     +  20.1208
     10   [36m0.3643[0m   [32m0.7807[0m        0.2443       0.9154        [94m0.2526[0m     +  19.8816
     11   0.3640   [32m0.7844[0m        0.2452       0.9154        [94m0.2514[0m     +  19.9487
     12   [36m0.3780[0m   0.7803        [35m0.2437[0m       0.9154        [94m0.2511[0m     +  19.8057
     13   0.3604   [32m0.7847[0m        0.2470       0.9154        [94m0.2509[0m     +  19.9593
     14   0.3378   0.7784        0.2470       0.9154        0.2514        19.8181
     15   0.3000   0.7554        0.2473       0.9154        0.2930        19.9905
     16   0.3442   [32m0.7880[0m        0.2465       0.9154        [94m0.2473[0m     +  19.8720
     17   0.3619   0.7870        0.2450       0.9154        0.2569        19.9653
     18   0.3649   0.7707        0.2454       0.9154        0.2541        20.0186
     19   [36m0.3782[0m   [32m0.7999[0m        0.2457       0.9154        0.2477        19.8534
     20   [36m0.3845[0m   [32m0.8072[0m        0.2442       0.9154        [94m0.2458[0m     +  19.9252
     21   0.3798   0.7916        [35m0.2430[0m       0.9154        0.2528        19.9581
     22   0.3543   0.7883        0.2444       0.9154        0.2461        19.9635
     23   0.3756   0.8035        0.2455       0.9154        0.2493        19.8964
     24   0.3762   0.8022        [35m0.2411[0m       0.9154        0.2501        20.0717
     25   [36m0.4087[0m   [32m0.8169[0m        0.2495       0.9154        0.2659        19.8340
     26   [36m0.4164[0m   [32m0.8177[0m        0.2452       0.9154        0.2577        19.9835
     27   0.3660   0.7993        0.2454       0.9154        [94m0.2433[0m     +  19.9483
     28   0.3617   0.8127        0.2451       0.9154        0.2455        19.8423
     29   0.3838   0.7982        0.2432       0.9154        0.2465        19.9164
     30   0.3762   0.8007        0.2441       0.9154        0.2508        19.9553
     31   0.3867   0.8116        0.2445       0.9154        0.2496        20.0133
     32   0.3612   0.8128        0.2480       0.9154        0.2713        19.8770
     33   0.3691   0.8008        0.2449       0.9154        0.2450        19.8618
     34   0.3343   0.7995        0.2436       0.9154        0.2677        19.8951
     35   0.3585   0.7948        0.2460       0.9154        0.2553        19.9138
     36   0.3916   0.8029        0.2458       0.9154        0.2480        19.8639
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 20:55:12,719][0m Trial 667 finished with value: 0.24327802309426316 and parameters: {'lr': 0.003926799160230386, 'dropout': 0.6143740162321216, 'd_model_multiplier': 1, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 335, 'batch_size': 18, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'BatchNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 64}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 54
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1000[0m   [32m0.3812[0m        [35m0.5206[0m       [31m0.9008[0m        [94m0.4407[0m     +  32.0475
      2   [36m0.2148[0m   [32m0.6720[0m        [35m0.2934[0m       0.8996        [94m0.3360[0m     +  32.0807
      3   [36m0.2669[0m   [32m0.7236[0m        [35m0.2584[0m       0.8972        [94m0.3086[0m     +  32.2421
      4   [36m0.2935[0m   [32m0.7350[0m        [35m0.2523[0m       0.8984        [94m0.2991[0m     +  32.2110
      5   [36m0.3056[0m   [32m0.7439[0m        [35m0.2490[0m       0.8984        [94m0.2933[0m     +  32.3808
      6   [36m0.3131[0m   [32m0.7474[0m        [35m0.2488[0m       0.9008        [94m0.2906[0m     +  32.0875
      7   [36m0.3192[0m   [32m0.7476[0m        [35m0.2442[0m       0.9008        [94m0.2874[0m     +  32.1096
      8   [36m0.3269[0m   0.7474        [35m0.2438[0m       0.9008        0.2884        32.0956
      9   0.3156   [32m0.7501[0m        [35m0.2416[0m       [31m0.9033[0m        [94m0.2856[0m     +  32.1196
     10   0.3177   [32m0.7511[0m        0.2418       0.9021        [94m0.2830[0m     +  32.3444
     11   0.3248   [32m0.7522[0m        [35m0.2391[0m       [31m0.9045[0m        0.2832        31.9684
     12   0.3203   0.7512        [35m0.2380[0m       0.9045        0.2841        32.1667
     13   0.3240   0.7519        [35m0.2346[0m       0.9033        0.2835        32.4820
     14   0.3217   0.7496        [35m0.2339[0m       0.9021        0.2863        32.2691
     15   0.3267   0.7521        [35m0.2336[0m       0.9033        [94m0.2827[0m     +  32.2486
     16   0.3168   0.7502        0.2336       0.9021        0.2842        32.2289
     17   0.3226   0.7501        [35m0.2316[0m       0.9033        [94m0.2818[0m     +  32.0689
     18   0.3225   0.7501        [35m0.2307[0m       0.9033        0.2820        32.2371
     19   0.3127   [32m0.7540[0m        [35m0.2272[0m       0.9033        0.2825        32.0625
     20   0.3220   0.7520        0.2287       0.9033        0.2828        32.3357
     21   [36m0.3288[0m   0.7491        [35m0.2271[0m       [31m0.9057[0m        0.2820        32.1857
     22   [36m0.3297[0m   [32m0.7593[0m        0.2281       0.9057        [94m0.2800[0m     +  32.2755
     23   [36m0.3335[0m   0.7576        0.2273       0.9045        [94m0.2794[0m     +  32.3515
     24   0.3316   [32m0.7603[0m        [35m0.2258[0m       0.9045        [94m0.2781[0m     +  32.3190
     25   0.3290   [32m0.7634[0m        [35m0.2231[0m       0.9057        [94m0.2774[0m     +  32.3196
     26   0.3330   0.7590        0.2259       [31m0.9069[0m        0.2788        32.2254
     27   [36m0.3354[0m   [32m0.7665[0m        [35m0.2215[0m       0.9057        [94m0.2769[0m     +  32.1378
     28   0.3318   0.7654        0.2243       0.9021        0.2787        32.2664
     29   0.3305   0.7587        0.2225       0.9021        0.2793        32.2385
     30   0.3285   0.7547        [35m0.2213[0m       0.9045        0.2811        32.3131
     31   0.3324   0.7527        0.2214       0.9057        0.2804        32.1751
     32   0.3257   0.7529        [35m0.2173[0m       0.9045        0.2827        32.0938
     33   0.3328   0.7528        0.2214       [31m0.9081[0m        0.2797        32.2438
     34   0.3310   0.7549        0.2192       0.9057        0.2814        32.1902
     35   0.3312   0.7615        [35m0.2170[0m       0.9045        0.2799        32.3493
     36   0.3253   0.7561        [35m0.2168[0m       0.9033        0.2808        32.2006
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 21:15:07,177][0m Trial 668 finished with value: 0.27691796562547694 and parameters: {'lr': 3.1090720089302994e-05, 'dropout': 0.4293156689370064, 'd_model_multiplier': 4, 'num_layers': 7, 'n_heads': 32, 'dim_feedforward': 365, 'batch_size': 40, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'BatchNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 54}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 69
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0577[0m   [32m0.3645[0m        [35m0.6635[0m       [31m0.6675[0m        [94m0.6724[0m     +  12.9373
      2   [36m0.0588[0m   0.3619        [35m0.6405[0m       [31m0.8501[0m        [94m0.6561[0m     +  12.8583
      3   [36m0.0602[0m   0.3587        [35m0.6060[0m       [31m0.9274[0m        [94m0.6364[0m     +  12.8937
      4   [36m0.0650[0m   0.3578        [35m0.5699[0m       [31m0.9323[0m        [94m0.6150[0m     +  12.8012
      5   0.0638   0.3595        [35m0.5341[0m       [31m0.9335[0m        [94m0.5940[0m     +  12.8539
      6   0.0646   0.3624        [35m0.5011[0m       0.9335        [94m0.5721[0m     +  12.9862
      7   0.0648   [32m0.3669[0m        [35m0.4732[0m       0.9335        [94m0.5522[0m     +  12.8252
      8   [36m0.0653[0m   [32m0.3741[0m        [35m0.4445[0m       0.9335        [94m0.5317[0m     +  12.7895
      9   0.0652   [32m0.3795[0m        [35m0.4226[0m       0.9335        [94m0.5143[0m     +  12.9329
     10   0.0653   [32m0.3863[0m        [35m0.4026[0m       0.9335        [94m0.4976[0m     +  12.8245
     11   0.0653   [32m0.3961[0m        [35m0.3870[0m       0.9335        [94m0.4822[0m     +  12.9176
     12   [36m0.0672[0m   [32m0.4045[0m        [35m0.3718[0m       0.9335        [94m0.4670[0m     +  12.8568
     13   [36m0.0676[0m   [32m0.4157[0m        [35m0.3582[0m       0.9335        [94m0.4533[0m     +  12.9393
     14   [36m0.0686[0m   [32m0.4265[0m        [35m0.3484[0m       0.9335        [94m0.4411[0m     +  12.9400
     15   [36m0.0694[0m   [32m0.4372[0m        [35m0.3398[0m       0.9335        [94m0.4303[0m     +  12.9066
     16   [36m0.0727[0m   [32m0.4527[0m        [35m0.3320[0m       0.9335        [94m0.4199[0m     +  12.9640
     17   [36m0.0738[0m   [32m0.4667[0m        [35m0.3232[0m       0.9335        [94m0.4131[0m     +  12.9623
     18   [36m0.0762[0m   [32m0.4821[0m        [35m0.3161[0m       0.9335        [94m0.4046[0m     +  12.8209
     19   [36m0.0793[0m   [32m0.4992[0m        [35m0.3128[0m       0.9335        [94m0.3972[0m     +  12.8403
     20   [36m0.0821[0m   [32m0.5115[0m        [35m0.3095[0m       0.9335        [94m0.3901[0m     +  12.8616
     21   [36m0.0897[0m   [32m0.5252[0m        [35m0.3035[0m       0.9335        [94m0.3828[0m     +  12.9552
     22   [36m0.0922[0m   [32m0.5382[0m        [35m0.2984[0m       0.9335        [94m0.3779[0m     +  12.7378
     23   [36m0.1004[0m   [32m0.5506[0m        [35m0.2976[0m       0.9335        [94m0.3732[0m     +  12.8743
     24   [36m0.1105[0m   [32m0.5599[0m        [35m0.2944[0m       0.9335        [94m0.3701[0m     +  12.8096
     25   [36m0.1210[0m   [32m0.5685[0m        [35m0.2904[0m       0.9335        [94m0.3631[0m     +  12.8179
     26   [36m0.1294[0m   [32m0.5782[0m        [35m0.2874[0m       0.9335        [94m0.3604[0m     +  12.9144
     27   [36m0.1449[0m   [32m0.5883[0m        [35m0.2869[0m       0.9335        [94m0.3566[0m     +  12.8830
     28   [36m0.1550[0m   [32m0.5937[0m        [35m0.2867[0m       0.9335        [94m0.3526[0m     +  12.9573
     29   [36m0.1619[0m   [32m0.5998[0m        [35m0.2854[0m       0.9335        [94m0.3514[0m     +  13.0290
     30   [36m0.1665[0m   [32m0.6064[0m        [35m0.2797[0m       0.9335        [94m0.3488[0m     +  12.9422
     31   [36m0.1708[0m   [32m0.6101[0m        0.2810       0.9335        [94m0.3460[0m     +  12.9947
     32   [36m0.1795[0m   [32m0.6156[0m        [35m0.2777[0m       0.9335        [94m0.3441[0m     +  12.7896
     33   [36m0.1861[0m   [32m0.6206[0m        0.2793       0.9335        [94m0.3403[0m     +  13.2334
     34   [36m0.1891[0m   [32m0.6241[0m        [35m0.2758[0m       0.9335        [94m0.3380[0m     +  12.8319
     35   [36m0.1915[0m   [32m0.6269[0m        [35m0.2751[0m       0.9335        [94m0.3357[0m     +  12.7583
     36   0.1914   [32m0.6320[0m        [35m0.2742[0m       0.9335        [94m0.3340[0m     +  12.8966
     37   [36m0.1958[0m   [32m0.6360[0m        [35m0.2721[0m       0.9335        [94m0.3304[0m     +  12.8506
     38   [36m0.1977[0m   [32m0.6378[0m        [35m0.2710[0m       0.9335        0.3313        12.9432
     39   [36m0.1982[0m   [32m0.6412[0m        0.2720       0.9335        [94m0.3299[0m     +  12.8526
     40   [36m0.1995[0m   [32m0.6438[0m        [35m0.2690[0m       0.9335        [94m0.3269[0m     +  12.8720
     41   [36m0.2008[0m   [32m0.6467[0m        0.2699       0.9335        [94m0.3247[0m     +  12.8923
     42   0.2003   [32m0.6473[0m        [35m0.2681[0m       0.9335        [94m0.3246[0m     +  12.8775
     43   [36m0.2015[0m   [32m0.6489[0m        [35m0.2674[0m       0.9335        0.3248        12.8568
     44   [36m0.2027[0m   [32m0.6523[0m        0.2688       0.9335        [94m0.3218[0m     +  12.8360
     45   [36m0.2053[0m   [32m0.6536[0m        [35m0.2661[0m       0.9335        [94m0.3187[0m     +  12.9110
     46   [36m0.2063[0m   [32m0.6567[0m        0.2672       0.9335        0.3198        12.7959
     47   0.2062   [32m0.6578[0m        [35m0.2650[0m       0.9335        0.3198        12.7195
     48   0.2053   [32m0.6583[0m        0.2651       0.9335        [94m0.3169[0m     +  12.7124
     49   [36m0.2066[0m   [32m0.6598[0m        0.2668       0.9335        0.3169        12.6519
     50   [36m0.2076[0m   [32m0.6602[0m        0.2669       0.9335        [94m0.3158[0m     +  12.8712
[32m[I 2023-05-05 21:25:55,836][0m Trial 669 finished with value: 0.3158468049483547 and parameters: {'lr': 3.9076970175996206e-07, 'dropout': 0.5057954724759872, 'd_model_multiplier': 4, 'num_layers': 2, 'n_heads': 32, 'dim_feedforward': 328, 'batch_size': 26, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'BatchNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 69}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 107
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0614[0m   [32m0.4271[0m        [35m0.4988[0m       [31m0.9274[0m        [94m0.3097[0m     +  15.8764
      2   [36m0.2065[0m   [32m0.6922[0m        [35m0.2878[0m       0.9274        [94m0.2431[0m     +  16.1963
      3   [36m0.2294[0m   [32m0.7332[0m        [35m0.2565[0m       0.9274        [94m0.2349[0m     +  16.2381
      4   [36m0.2715[0m   [32m0.7534[0m        [35m0.2491[0m       0.9274        [94m0.2296[0m     +  16.3172
      5   [36m0.3185[0m   [32m0.7734[0m        [35m0.2436[0m       0.9274        [94m0.2241[0m     +  16.1048
      6   [36m0.3391[0m   [32m0.7857[0m        [35m0.2393[0m       [31m0.9299[0m        [94m0.2208[0m     +  16.1158
      7   [36m0.3551[0m   [32m0.7920[0m        [35m0.2373[0m       [31m0.9311[0m        [94m0.2168[0m     +  16.3068
      8   [36m0.3632[0m   [32m0.7953[0m        [35m0.2345[0m       [31m0.9335[0m        [94m0.2158[0m     +  16.4719
      9   [36m0.3683[0m   [32m0.7998[0m        [35m0.2332[0m       [31m0.9347[0m        [94m0.2146[0m     +  16.2124
     10   [36m0.3745[0m   [32m0.8032[0m        [35m0.2319[0m       0.9347        [94m0.2131[0m     +  16.4456
     11   0.3696   [32m0.8057[0m        [35m0.2314[0m       0.9347        [94m0.2120[0m     +  16.1854
     12   0.3723   [32m0.8087[0m        0.2321       0.9311        [94m0.2101[0m     +  16.3771
     13   0.3740   [32m0.8106[0m        [35m0.2300[0m       0.9323        [94m0.2094[0m     +  16.4866
     14   [36m0.3757[0m   [32m0.8135[0m        [35m0.2285[0m       0.9311        [94m0.2087[0m     +  16.4119
     15   0.3755   [32m0.8164[0m        [35m0.2282[0m       0.9323        [94m0.2080[0m     +  16.4805
     16   [36m0.3844[0m   [32m0.8190[0m        [35m0.2278[0m       0.9323        [94m0.2074[0m     +  16.3364
     17   0.3820   [32m0.8208[0m        [35m0.2272[0m       0.9311        [94m0.2067[0m     +  16.6194
     18   0.3822   [32m0.8227[0m        [35m0.2259[0m       0.9323        [94m0.2059[0m     +  16.1880
     19   0.3808   [32m0.8239[0m        [35m0.2255[0m       0.9323        0.2068        16.2118
     20   0.3801   0.8238        [35m0.2252[0m       0.9323        0.2068        16.1777
     21   0.3764   [32m0.8241[0m        0.2254       0.9323        0.2060        16.2929
     22   0.3800   [32m0.8252[0m        [35m0.2247[0m       0.9323        0.2063        16.1219
     23   0.3755   0.8241        [35m0.2246[0m       0.9311        [94m0.2059[0m     +  16.3656
     24   0.3787   [32m0.8255[0m        [35m0.2213[0m       0.9299        [94m0.2058[0m     +  16.3736
     25   0.3770   0.8253        [35m0.2205[0m       0.9299        [94m0.2056[0m     +  16.2413
     26   0.3790   [32m0.8255[0m        [35m0.2204[0m       0.9311        0.2059        16.4403
     27   0.3716   [32m0.8268[0m        [35m0.2197[0m       0.9311        0.2058        16.1924
     28   0.3764   [32m0.8286[0m        0.2202       0.9323        [94m0.2053[0m     +  16.1021
     29   0.3708   [32m0.8301[0m        [35m0.2167[0m       0.9311        0.2057        16.3722
     30   0.3748   [32m0.8304[0m        0.2172       0.9311        0.2058        16.3260
     31   0.3697   [32m0.8316[0m        0.2172       0.9311        0.2056        16.3424
     32   0.3691   0.8314        [35m0.2162[0m       0.9311        0.2066        16.3128
     33   0.3667   0.8308        [35m0.2148[0m       0.9323        0.2084        16.2723
     34   0.3661   [32m0.8331[0m        0.2148       0.9323        0.2074        16.3520
     35   0.3682   [32m0.8344[0m        [35m0.2138[0m       0.9299        0.2072        16.2841
     36   0.3652   0.8331        [35m0.2129[0m       0.9299        0.2072        16.2936
     37   0.3679   0.8341        [35m0.2114[0m       0.9299        0.2074        16.2527
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 21:36:16,751][0m Trial 670 finished with value: 0.20533404908308528 and parameters: {'lr': 2.2502934158655092e-05, 'dropout': 0.35045020735628024, 'd_model_multiplier': 4, 'num_layers': 3, 'n_heads': 32, 'dim_feedforward': 310, 'batch_size': 34, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 107}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 90
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1847[0m   [32m0.7295[0m        [35m0.4230[0m       [31m0.9323[0m        [94m0.2395[0m     +  19.8586
      2   [36m0.2359[0m   [32m0.7579[0m        [35m0.2519[0m       [31m0.9347[0m        [94m0.2232[0m     +  19.8490
      3   [36m0.2708[0m   [32m0.7720[0m        [35m0.2417[0m       [31m0.9359[0m        [94m0.2151[0m     +  20.3184
      4   [36m0.2926[0m   [32m0.7795[0m        [35m0.2369[0m       0.9347        [94m0.2102[0m     +  19.8836
      5   0.2921   0.7783        [35m0.2338[0m       0.9347        [94m0.2091[0m     +  19.9116
      6   [36m0.3046[0m   0.7753        [35m0.2283[0m       0.9359        [94m0.2076[0m     +  20.2096
      7   0.3030   [32m0.7816[0m        [35m0.2258[0m       [31m0.9371[0m        [94m0.2074[0m     +  20.1865
      8   [36m0.3073[0m   0.7724        [35m0.2211[0m       0.9371        [94m0.2070[0m     +  20.2998
      9   [36m0.3253[0m   0.7667        [35m0.2179[0m       0.9371        [94m0.2056[0m     +  19.9962
     10   0.3189   0.7609        [35m0.2133[0m       [31m0.9383[0m        0.2068        20.0923
     11   0.3079   0.7635        [35m0.2129[0m       0.9371        0.2075        20.1005
     12   0.3077   0.7617        [35m0.2087[0m       0.9371        0.2073        19.9600
     13   0.3177   0.7702        [35m0.2038[0m       0.9359        [94m0.2054[0m     +  20.0529
     14   0.3238   0.7610        [35m0.1998[0m       0.9383        0.2074        20.1331
     15   0.3222   0.7635        [35m0.1983[0m       0.9383        0.2079        20.0260
     16   [36m0.3364[0m   0.7524        [35m0.1950[0m       0.9371        0.2092        19.9308
     17   [36m0.3365[0m   0.7457        [35m0.1913[0m       [31m0.9395[0m        0.2109        19.8703
     18   0.3149   0.7496        [35m0.1866[0m       0.9359        0.2130        19.9681
     19   0.3012   0.7364        [35m0.1820[0m       0.9383        0.2177        19.9092
     20   0.2755   0.7295        [35m0.1783[0m       0.9347        0.2231        20.2038
     21   0.2857   0.7317        [35m0.1744[0m       0.9371        0.2249        20.3682
     22   0.2986   0.7429        [35m0.1703[0m       0.9323        0.2179        19.9982
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 21:43:58,519][0m Trial 671 finished with value: 0.20544632253976947 and parameters: {'lr': 0.00017064943584804327, 'dropout': 0.2171013600790333, 'd_model_multiplier': 2, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 342, 'batch_size': 53, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'BatchNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 90}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 59
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1244[0m   [32m0.5780[0m        [35m0.8012[0m       [31m0.9190[0m        [94m0.6622[0m     +  45.7502
      2   0.0692   0.4233        [35m0.3432[0m       0.9190        [94m0.5186[0m     +  45.7657
      3   0.1078   0.5300        0.4004       0.8900        [94m0.3525[0m     +  46.0664
      4   0.1048   0.4560        0.3700       0.9190        0.5667        46.0943
      5   0.1184   0.5317        0.5892       0.8791        0.5091        46.2307
      6   0.0817   0.4892        0.8250       0.9178        1.3789        46.2080
      7   0.1081   0.5206        1.1186       0.9190        1.3019        45.9386
      8   0.0552   0.3086        0.6941       0.9190        0.9679        46.2534
      9   [36m0.1886[0m   [32m0.6926[0m        0.7070       0.8368        2.1543        46.0618
     10   0.0635   0.3834        1.3552       0.9190        1.3239        45.8903
     11   0.0625   0.3903        0.9245       0.9190        2.9078        46.0925
     12   0.0546   0.3019        0.8356       0.9190        1.0943        45.9291
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 21:53:59,451][0m Trial 672 finished with value: 0.3525168383460235 and parameters: {'lr': 0.04107855123778288, 'dropout': 0.241146022917698, 'd_model_multiplier': 32, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 320, 'batch_size': 47, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 59}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 66
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0516[0m   [32m0.3200[0m        [35m0.6324[0m       [31m0.9347[0m        [94m0.4573[0m     +  27.1830
      2   [36m0.1046[0m   [32m0.5545[0m        [35m0.3841[0m       0.9347        [94m0.2772[0m     +  27.7344
      3   [36m0.1869[0m   [32m0.6921[0m        [35m0.2812[0m       0.9335        [94m0.2331[0m     +  27.5972
      4   [36m0.2002[0m   [32m0.7316[0m        [35m0.2539[0m       0.9323        [94m0.2219[0m     +  27.6436
      5   [36m0.2066[0m   [32m0.7484[0m        [35m0.2417[0m       0.9287        [94m0.2187[0m     +  27.3928
      6   [36m0.2173[0m   [32m0.7583[0m        [35m0.2366[0m       0.9274        [94m0.2174[0m     +  27.3568
      7   [36m0.2184[0m   [32m0.7622[0m        [35m0.2331[0m       0.9238        0.2181        27.4086
      8   [36m0.2193[0m   [32m0.7658[0m        [35m0.2304[0m       0.9250        [94m0.2167[0m     +  27.4481
      9   0.2077   [32m0.7684[0m        [35m0.2296[0m       0.9214        0.2177        27.4830
     10   0.2088   [32m0.7694[0m        [35m0.2272[0m       0.9250        0.2171        27.5197
     11   0.2084   [32m0.7713[0m        [35m0.2271[0m       0.9226        0.2176        27.1601
     12   0.2108   [32m0.7727[0m        [35m0.2254[0m       0.9226        0.2177        27.5935
     13   0.2048   [32m0.7747[0m        [35m0.2251[0m       0.9226        0.2186        27.4703
     14   0.2066   [32m0.7764[0m        [35m0.2237[0m       0.9214        0.2179        27.4541
     15   0.2092   [32m0.7784[0m        [35m0.2221[0m       0.9214        0.2174        27.4385
     16   0.2102   [32m0.7792[0m        [35m0.2215[0m       0.9214        0.2182        27.5540
     17   0.2128   [32m0.7795[0m        [35m0.2203[0m       0.9238        0.2190        27.6752
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 22:02:14,903][0m Trial 673 finished with value: 0.2167143877347911 and parameters: {'lr': 1.7482508287067695e-05, 'dropout': 0.1447312716859156, 'd_model_multiplier': 4, 'num_layers': 6, 'n_heads': 32, 'dim_feedforward': 334, 'batch_size': 59, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0, 'top_n_features': 66}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 54
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0511[0m   [32m0.2838[0m        [35m0.6481[0m       [31m0.9262[0m        [94m0.4800[0m     +  15.9179
      2   [36m0.0682[0m   [32m0.4422[0m        [35m0.3794[0m       0.9262        [94m0.3081[0m     +  15.9379
      3   [36m0.2074[0m   [32m0.6602[0m        [35m0.2890[0m       0.9262        [94m0.2571[0m     +  16.0015
      4   [36m0.2767[0m   [32m0.7376[0m        [35m0.2580[0m       0.9262        [94m0.2352[0m     +  16.0191
      5   [36m0.2785[0m   [32m0.7602[0m        [35m0.2426[0m       [31m0.9274[0m        [94m0.2278[0m     +  16.0755
      6   [36m0.2873[0m   [32m0.7740[0m        [35m0.2384[0m       0.9262        [94m0.2248[0m     +  16.1357
      7   [36m0.2878[0m   [32m0.7817[0m        [35m0.2353[0m       0.9262        [94m0.2234[0m     +  16.0137
      8   [36m0.2882[0m   [32m0.7859[0m        [35m0.2330[0m       0.9238        [94m0.2227[0m     +  16.0600
      9   0.2817   [32m0.7906[0m        [35m0.2306[0m       0.9238        [94m0.2220[0m     +  16.0282
     10   0.2815   [32m0.7930[0m        [35m0.2303[0m       0.9238        [94m0.2218[0m     +  15.9807
     11   0.2757   [32m0.7958[0m        [35m0.2292[0m       0.9238        [94m0.2216[0m     +  16.1875
     12   0.2768   [32m0.7991[0m        [35m0.2277[0m       0.9238        [94m0.2211[0m     +  16.0606
     13   0.2778   [32m0.8016[0m        0.2280       0.9238        [94m0.2207[0m     +  16.2087
     14   0.2787   [32m0.8029[0m        [35m0.2265[0m       0.9238        0.2209        16.2467
     15   0.2777   [32m0.8052[0m        [35m0.2248[0m       0.9238        [94m0.2206[0m     +  16.0187
     16   0.2796   [32m0.8062[0m        [35m0.2236[0m       0.9238        0.2208        16.0372
     17   0.2760   [32m0.8074[0m        [35m0.2236[0m       0.9238        0.2209        15.9260
     18   0.2820   [32m0.8099[0m        [35m0.2229[0m       0.9238        [94m0.2203[0m     +  15.9400
     19   0.2767   0.8090        [35m0.2208[0m       0.9238        0.2209        16.0973
     20   0.2747   [32m0.8107[0m        [35m0.2206[0m       0.9250        0.2206        16.1051
     21   0.2734   0.8105        [35m0.2193[0m       0.9250        0.2207        16.4044
     22   0.2723   0.8102        [35m0.2183[0m       0.9250        0.2210        16.0830
     23   0.2714   [32m0.8113[0m        [35m0.2167[0m       0.9250        0.2215        16.0325
     24   0.2726   [32m0.8115[0m        [35m0.2162[0m       0.9250        0.2215        16.1679
     25   0.2762   0.8110        [35m0.2160[0m       0.9250        0.2216        15.9848
     26   0.2757   0.8110        [35m0.2148[0m       0.9250        0.2217        15.9593
     27   0.2682   0.8076        [35m0.2130[0m       0.9250        0.2228        15.9651
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 22:09:45,654][0m Trial 674 finished with value: 0.22031081820125303 and parameters: {'lr': 3.8152549181594606e-05, 'dropout': 0.1054883431303385, 'd_model_multiplier': 1, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 348, 'batch_size': 40, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 54}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 77
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0567[0m   [32m0.2810[0m        [35m0.5645[0m       [31m0.9141[0m        [94m0.4058[0m     +  23.6477
      2   [36m0.1588[0m   [32m0.6171[0m        [35m0.3200[0m       0.9141        [94m0.2872[0m     +  24.0665
      3   [36m0.2448[0m   [32m0.7301[0m        [35m0.2625[0m       0.9141        [94m0.2637[0m     +  23.9653
      4   [36m0.2906[0m   [32m0.7682[0m        [35m0.2475[0m       0.9117        [94m0.2538[0m     +  23.9847
      5   [36m0.3026[0m   [32m0.7792[0m        [35m0.2378[0m       [31m0.9154[0m        [94m0.2502[0m     +  23.8600
      6   [36m0.3070[0m   [32m0.7848[0m        [35m0.2340[0m       0.9141        [94m0.2493[0m     +  23.9191
      7   [36m0.3176[0m   [32m0.7913[0m        [35m0.2306[0m       [31m0.9166[0m        [94m0.2470[0m     +  23.7951
      8   [36m0.3207[0m   [32m0.7952[0m        [35m0.2290[0m       0.9154        [94m0.2466[0m     +  23.9009
      9   [36m0.3366[0m   [32m0.7995[0m        [35m0.2275[0m       0.9154        [94m0.2453[0m     +  23.9380
     10   0.3356   [32m0.8041[0m        [35m0.2263[0m       0.9166        [94m0.2438[0m     +  23.8283
     11   0.3312   [32m0.8072[0m        [35m0.2254[0m       0.9166        0.2443        23.9839
     12   0.3352   [32m0.8097[0m        [35m0.2234[0m       0.9141        [94m0.2431[0m     +  23.9626
     13   0.3365   [32m0.8126[0m        [35m0.2225[0m       0.9129        [94m0.2423[0m     +  23.9850
     14   0.3338   [32m0.8147[0m        [35m0.2222[0m       0.9129        [94m0.2415[0m     +  23.9762
     15   [36m0.3403[0m   [32m0.8170[0m        [35m0.2213[0m       0.9129        0.2415        24.0054
     16   [36m0.3453[0m   [32m0.8204[0m        [35m0.2202[0m       0.9129        [94m0.2398[0m     +  24.0250
     17   [36m0.3500[0m   [32m0.8239[0m        [35m0.2201[0m       0.9117        [94m0.2385[0m     +  24.1495
     18   0.3413   [32m0.8249[0m        [35m0.2185[0m       0.9117        0.2388        23.8333
     19   0.3402   [32m0.8256[0m        [35m0.2184[0m       0.9117        [94m0.2384[0m     +  23.8587
     20   0.3447   [32m0.8276[0m        [35m0.2172[0m       0.9141        [94m0.2379[0m     +  23.9614
     21   0.3438   [32m0.8296[0m        0.2176       0.9129        0.2384        24.0600
     22   0.3438   [32m0.8303[0m        [35m0.2171[0m       0.9117        [94m0.2378[0m     +  24.0110
     23   0.3471   [32m0.8313[0m        [35m0.2153[0m       0.9141        [94m0.2372[0m     +  23.9274
     24   0.3455   [32m0.8331[0m        [35m0.2150[0m       0.9141        [94m0.2366[0m     +  23.8956
     25   0.3490   [32m0.8337[0m        [35m0.2146[0m       0.9129        [94m0.2358[0m     +  23.8605
     26   0.3416   0.8337        [35m0.2136[0m       0.9141        [94m0.2358[0m     +  23.9144
     27   0.3484   [32m0.8369[0m        [35m0.2126[0m       0.9129        [94m0.2349[0m     +  23.9897
     28   0.3495   [32m0.8380[0m        [35m0.2122[0m       0.9154        0.2352        23.8071
     29   [36m0.3569[0m   [32m0.8385[0m        [35m0.2108[0m       0.9105        [94m0.2344[0m     +  23.9619
     30   0.3526   [32m0.8391[0m        [35m0.2089[0m       0.9105        0.2359        23.9142
     31   0.3410   [32m0.8397[0m        0.2094       0.9117        0.2358        23.9005
     32   0.3447   [32m0.8401[0m        0.2099       0.9117        0.2350        23.9896
     33   0.3489   0.8398        [35m0.2078[0m       0.9093        0.2354        23.9470
     34   0.3426   0.8396        0.2078       0.9141        0.2364        24.0301
     35   0.3478   [32m0.8418[0m        [35m0.2076[0m       0.9093        0.2357        23.8877
     36   0.3452   0.8415        [35m0.2061[0m       0.9105        0.2358        24.0628
     37   0.3369   0.8407        0.2066       0.9081        0.2376        23.7010
     38   0.3382   0.8417        [35m0.2047[0m       0.9093        0.2379        24.0460
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 22:25:22,232][0m Trial 675 finished with value: 0.2344318797773577 and parameters: {'lr': 9.853959513743325e-06, 'dropout': 0.20221387090352816, 'd_model_multiplier': 4, 'num_layers': 5, 'n_heads': 32, 'dim_feedforward': 359, 'batch_size': 31, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 77}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 120
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.2000[0m   [32m0.6799[0m        [35m0.3638[0m       [31m0.9154[0m        [94m0.3748[0m     +  9.0053
      2   [36m0.2512[0m   [32m0.7458[0m        [35m0.2643[0m       0.9154        [94m0.3485[0m     +  9.5224
      3   [36m0.2548[0m   [32m0.7652[0m        [35m0.2599[0m       0.9129        [94m0.3445[0m     +  9.7241
      4   0.2503   [32m0.7663[0m        [35m0.2573[0m       0.9117        [94m0.3370[0m     +  9.4670
      5   0.2410   0.7628        [35m0.2562[0m       0.9093        [94m0.3341[0m     +  9.4028
      6   0.2322   0.7433        [35m0.2522[0m       0.9105        [94m0.3250[0m     +  9.6837
      7   0.2301   0.7383        [35m0.2480[0m       0.9105        0.3297        9.9235
      8   0.2133   0.7126        0.2482       0.9081        0.3295        9.5394
      9   0.2151   0.7179        [35m0.2469[0m       0.9057        0.3331        9.6103
     10   0.2073   0.7050        0.2501       0.9081        0.3328        9.4297
     11   0.2027   0.7043        0.2484       0.9069        0.3312        9.5806
     12   0.2020   0.6978        0.2498       0.9045        0.3351        9.4720
     13   0.1907   0.6811        0.2473       0.9057        0.3338        9.6443
     14   0.1930   0.6796        [35m0.2456[0m       0.9045        0.3367        9.4543
     15   0.1886   0.6759        0.2470       0.9093        0.3324        9.5869
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 22:27:55,685][0m Trial 676 finished with value: 0.324975796647487 and parameters: {'lr': 0.00010304074240319833, 'dropout': 0.6593234406390046, 'd_model_multiplier': 4, 'num_layers': 1, 'n_heads': 32, 'dim_feedforward': 328, 'batch_size': 21, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'BatchNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 120}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 63
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.1435[0m   [32m0.6010[0m        [35m0.4384[0m       [31m0.9311[0m        [94m0.2609[0m     +  9.0797
      2   [36m0.2411[0m   [32m0.7444[0m        [35m0.2619[0m       0.9287        [94m0.2223[0m     +  9.1128
      3   [36m0.2681[0m   [32m0.7812[0m        [35m0.2432[0m       0.9299        [94m0.2138[0m     +  9.1024
      4   [36m0.2858[0m   [32m0.7906[0m        [35m0.2383[0m       0.9287        [94m0.2099[0m     +  9.3396
      5   [36m0.2971[0m   [32m0.7985[0m        [35m0.2346[0m       [31m0.9323[0m        [94m0.2069[0m     +  9.5995
      6   [36m0.3150[0m   [32m0.8046[0m        [35m0.2311[0m       [31m0.9335[0m        [94m0.2045[0m     +  9.6271
      7   0.3098   [32m0.8083[0m        [35m0.2308[0m       0.9311        [94m0.2033[0m     +  9.7968
      8   [36m0.3312[0m   [32m0.8134[0m        [35m0.2277[0m       0.9311        [94m0.2015[0m     +  9.8878
      9   [36m0.3483[0m   [32m0.8162[0m        [35m0.2243[0m       0.9299        [94m0.2000[0m     +  9.5007
     10   0.3339   [32m0.8183[0m        [35m0.2228[0m       0.9311        0.2009        9.7557
     11   0.3422   0.8158        0.2236       0.9299        0.2012        9.5210
     12   0.3365   [32m0.8219[0m        [35m0.2185[0m       0.9335        0.2001        9.6965
     13   0.3301   0.8195        [35m0.2152[0m       0.9323        0.2022        9.5880
     14   0.3333   0.8138        0.2155       0.9323        0.2016        9.8591
     15   0.3459   0.8124        [35m0.2123[0m       [31m0.9359[0m        0.2023        9.6109
     16   0.3220   0.8067        [35m0.2103[0m       0.9311        0.2065        9.7739
     17   0.3337   0.8101        0.2105       0.9359        0.2054        9.9134
     18   0.3221   0.8056        [35m0.2052[0m       0.9335        0.2072        9.7677
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 22:30:58,559][0m Trial 677 finished with value: 0.19998523739347868 and parameters: {'lr': 0.00028624975562960644, 'dropout': 0.2609050596978033, 'd_model_multiplier': 4, 'num_layers': 3, 'n_heads': 8, 'dim_feedforward': 342, 'batch_size': 56, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 63}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 57
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.1260[0m   [32m0.6153[0m        [35m0.4462[0m       [31m0.9178[0m        [94m0.2851[0m     +  9.0530
      2   [36m0.2290[0m   [32m0.7276[0m        [35m0.2573[0m       [31m0.9190[0m        [94m0.2563[0m     +  8.8016
      3   [36m0.2414[0m   [32m0.7495[0m        [35m0.2367[0m       0.9190        [94m0.2546[0m     +  9.5730
      4   [36m0.2484[0m   [32m0.7599[0m        [35m0.2324[0m       0.9166        [94m0.2529[0m     +  9.3904
      5   0.2448   [32m0.7668[0m        [35m0.2291[0m       0.9166        [94m0.2513[0m     +  9.3641
      6   [36m0.2485[0m   [32m0.7738[0m        [35m0.2259[0m       0.9154        [94m0.2494[0m     +  9.1325
      7   [36m0.2553[0m   [32m0.7765[0m        [35m0.2244[0m       0.9141        [94m0.2490[0m     +  9.2541
      8   [36m0.2570[0m   [32m0.7804[0m        [35m0.2234[0m       0.9154        [94m0.2473[0m     +  9.4584
      9   [36m0.2597[0m   [32m0.7835[0m        [35m0.2214[0m       0.9129        [94m0.2468[0m     +  9.4818
     10   [36m0.2687[0m   [32m0.7854[0m        [35m0.2203[0m       0.9166        [94m0.2450[0m     +  9.4712
     11   [36m0.2807[0m   [32m0.7886[0m        [35m0.2195[0m       0.9178        [94m0.2432[0m     +  9.6670
     12   [36m0.2863[0m   [32m0.7903[0m        [35m0.2176[0m       [31m0.9202[0m        [94m0.2431[0m     +  9.5611
     13   [36m0.2954[0m   [32m0.7925[0m        [35m0.2159[0m       0.9166        [94m0.2416[0m     +  9.5092
     14   [36m0.2988[0m   [32m0.7951[0m        [35m0.2156[0m       0.9154        [94m0.2409[0m     +  9.7571
     15   [36m0.3068[0m   [32m0.7976[0m        [35m0.2121[0m       0.9166        [94m0.2389[0m     +  9.4178
     16   0.3003   0.7958        [35m0.2113[0m       0.9178        0.2397        9.4927
     17   0.3055   [32m0.7982[0m        [35m0.2096[0m       0.9154        0.2395        9.2914
     18   [36m0.3155[0m   [32m0.7995[0m        [35m0.2087[0m       0.9141        0.2398        9.4056
     19   0.3127   [32m0.8008[0m        [35m0.2078[0m       0.9129        [94m0.2389[0m     +  9.2021
     20   0.3048   0.7953        [35m0.2051[0m       0.9141        0.2429        9.3627
     21   0.3016   0.7940        [35m0.2038[0m       0.9117        0.2430        9.5203
     22   0.2957   0.7915        [35m0.2031[0m       0.9117        0.2452        9.4066
     23   0.2951   0.7917        [35m0.2021[0m       0.9129        0.2462        9.5772
     24   0.2879   0.7894        [35m0.2001[0m       0.9154        0.2463        9.5759
     25   0.2992   0.7879        [35m0.1984[0m       0.9141        0.2463        9.5450
     26   0.2971   0.7892        [35m0.1943[0m       0.9154        0.2480        9.4383
     27   0.2907   0.7850        0.1962       0.9141        0.2493        9.6251
     28   0.2836   0.7806        [35m0.1929[0m       0.9154        0.2536        9.8042
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 22:35:33,017][0m Trial 678 finished with value: 0.23886399663885674 and parameters: {'lr': 0.0002492781269172211, 'dropout': 0.2615684827692206, 'd_model_multiplier': 4, 'num_layers': 3, 'n_heads': 8, 'dim_feedforward': 355, 'batch_size': 51, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 57}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 95
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.0799[0m   [32m0.3294[0m        [35m0.5730[0m       [31m0.9166[0m        [94m0.4298[0m     +  9.0009
      2   [36m0.1071[0m   [32m0.4776[0m        [35m0.3592[0m       0.9166        [94m0.3170[0m     +  8.7396
      3   [36m0.1786[0m   [32m0.5794[0m        [35m0.2936[0m       0.9166        [94m0.2870[0m     +  9.0558
      4   [36m0.1960[0m   [32m0.6225[0m        [35m0.2702[0m       0.9166        [94m0.2766[0m     +  9.1585
      5   [36m0.2085[0m   [32m0.6557[0m        [35m0.2611[0m       0.9166        [94m0.2715[0m     +  9.5367
      6   [36m0.2545[0m   [32m0.7276[0m        [35m0.2528[0m       0.9154        [94m0.2561[0m     +  9.5834
      7   [36m0.2964[0m   [32m0.7518[0m        [35m0.2466[0m       [31m0.9178[0m        [94m0.2475[0m     +  9.4679
      8   [36m0.2974[0m   [32m0.7551[0m        [35m0.2438[0m       0.9154        [94m0.2456[0m     +  9.4324
      9   [36m0.3185[0m   [32m0.7597[0m        [35m0.2428[0m       [31m0.9226[0m        [94m0.2432[0m     +  9.5216
     10   [36m0.3310[0m   [32m0.7620[0m        [35m0.2388[0m       [31m0.9238[0m        [94m0.2422[0m     +  9.1338
     11   0.3222   [32m0.7637[0m        [35m0.2350[0m       0.9226        0.2429        9.1652
     12   0.3155   [32m0.7704[0m        [35m0.2349[0m       0.9190        [94m0.2417[0m     +  9.4875
     13   [36m0.3344[0m   [32m0.7752[0m        [35m0.2340[0m       0.9166        [94m0.2401[0m     +  9.8632
     14   [36m0.3347[0m   0.7732        [35m0.2308[0m       0.9202        [94m0.2394[0m     +  9.4513
     15   0.3308   [32m0.7819[0m        [35m0.2307[0m       0.9166        [94m0.2387[0m     +  9.0213
     16   0.3331   0.7804        [35m0.2303[0m       0.9178        0.2397        9.5586
     17   [36m0.3542[0m   [32m0.7870[0m        [35m0.2278[0m       0.9166        [94m0.2377[0m     +  9.6028
     18   0.3443   0.7859        0.2291       0.9141        0.2382        9.6084
     19   0.3536   [32m0.7915[0m        [35m0.2258[0m       0.9166        [94m0.2369[0m     +  9.3482
     20   0.3477   [32m0.7993[0m        [35m0.2252[0m       0.9166        [94m0.2367[0m     +  9.6268
     21   0.3454   0.7899        [35m0.2242[0m       0.9154        0.2405        9.2871
     22   0.3473   0.7980        [35m0.2231[0m       0.9166        0.2391        9.2655
     23   [36m0.3560[0m   [32m0.8012[0m        0.2233       0.9154        0.2388        9.3177
     24   0.3323   0.7986        0.2245       0.9141        0.2398        9.5555
     25   0.3530   [32m0.8017[0m        0.2232       0.9166        0.2391        9.3779
     26   [36m0.3616[0m   0.8014        [35m0.2203[0m       0.9154        0.2407        9.7344
     27   [36m0.3619[0m   [32m0.8060[0m        [35m0.2202[0m       0.9166        [94m0.2361[0m     +  9.2247
     28   0.3512   [32m0.8063[0m        [35m0.2178[0m       0.9166        0.2381        9.4980
     29   0.3589   0.8024        [35m0.2161[0m       0.9129        0.2426        9.8109
     30   0.3498   [32m0.8116[0m        0.2200       0.9141        0.2387        9.2961
     31   0.3555   [32m0.8123[0m        0.2179       0.9129        0.2383        9.9623
     32   0.3558   0.8099        [35m0.2138[0m       0.9117        0.2408        9.4747
     33   0.3595   [32m0.8156[0m        0.2174       0.9141        0.2365        9.4140
     34   0.3522   [32m0.8209[0m        0.2147       0.9093        0.2367        9.3848
     35   0.3538   0.8161        0.2145       0.9141        0.2378        9.4756
     36   0.3429   0.8170        0.2147       0.9117        0.2394        9.4815
     37   0.3538   [32m0.8224[0m        0.2144       0.9141        [94m0.2349[0m     +  9.2048
     38   0.3588   [32m0.8230[0m        [35m0.2137[0m       0.9129        0.2374        9.1389
     39   [36m0.3655[0m   0.8223        [35m0.2129[0m       0.9117        0.2375        9.1659
     40   0.3654   0.8198        [35m0.2105[0m       0.9141        0.2381        9.2043
     41   0.3561   [32m0.8240[0m        0.2110       0.9129        0.2353        9.3441
     42   0.3510   0.8225        0.2108       0.9117        0.2373        9.6049
     43   0.3612   [32m0.8252[0m        [35m0.2092[0m       0.9166        0.2359        9.5248
     44   0.3557   [32m0.8255[0m        0.2103       0.9154        0.2363        9.3285
     45   0.3527   0.8203        0.2098       0.9154        0.2385        9.8952
     46   0.3616   0.8245        [35m0.2086[0m       0.9166        0.2369        9.6299
     47   [36m0.3688[0m   0.8247        0.2089       0.9166        [94m0.2348[0m     +  9.6819
     48   0.3578   0.8217        [35m0.2085[0m       0.9154        [94m0.2344[0m     +  9.4025
     49   0.3660   0.8227        [35m0.2076[0m       0.9154        0.2347        9.4179
     50   0.3504   0.8209        [35m0.2054[0m       0.9129        0.2408        9.8864
[32m[I 2023-05-05 22:43:25,916][0m Trial 679 finished with value: 0.23439731934482846 and parameters: {'lr': 0.0003198282405310437, 'dropout': 0.3773423464098107, 'd_model_multiplier': 1, 'num_layers': 3, 'n_heads': 8, 'dim_feedforward': 345, 'batch_size': 52, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 95}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 61
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.0945[0m   [32m0.5659[0m        [35m0.5019[0m       [31m0.9214[0m        [94m0.2930[0m     +  8.4914
      2   [36m0.2691[0m   [32m0.7369[0m        [35m0.2734[0m       0.9214        [94m0.2468[0m     +  9.9475
      3   [36m0.3008[0m   [32m0.7716[0m        [35m0.2476[0m       [31m0.9226[0m        [94m0.2348[0m     +  9.9692
      4   0.2891   [32m0.7737[0m        [35m0.2373[0m       0.9190        0.2353        9.3814
      5   0.2842   [32m0.7747[0m        [35m0.2325[0m       0.9178        0.2358        9.6860
      6   0.2785   [32m0.7770[0m        [35m0.2276[0m       0.9178        0.2357        9.6574
      7   0.2811   [32m0.7815[0m        [35m0.2271[0m       0.9190        [94m0.2346[0m     +  9.8832
      8   0.2871   0.7780        [35m0.2251[0m       0.9202        0.2347        10.1899
      9   0.2809   0.7809        [35m0.2242[0m       0.9202        0.2349        9.4764
     10   0.2816   0.7786        [35m0.2217[0m       0.9190        0.2363        9.9525
     11   0.2861   0.7797        0.2229       0.9190        0.2371        9.7347
     12   0.2879   [32m0.7894[0m        [35m0.2192[0m       0.9214        0.2348        9.9187
     13   0.2843   0.7834        [35m0.2174[0m       0.9202        0.2381        9.8857
     14   [36m0.3037[0m   [32m0.7909[0m        [35m0.2161[0m       0.9178        0.2365        9.7073
     15   0.2999   [32m0.7919[0m        [35m0.2156[0m       0.9190        0.2361        9.9898
     16   [36m0.3096[0m   [32m0.7982[0m        [35m0.2111[0m       0.9202        [94m0.2337[0m     +  10.2492
     17   0.3015   0.7918        0.2115       0.9202        0.2367        10.2206
     18   [36m0.3193[0m   [32m0.7983[0m        [35m0.2094[0m       0.9226        0.2346        9.6149
     19   0.3186   0.7981        [35m0.2079[0m       0.9202        0.2353        10.1274
     20   0.3139   0.7951        [35m0.2046[0m       0.9154        0.2373        9.8652
     21   0.3127   0.7892        [35m0.2042[0m       0.9190        0.2381        9.8303
     22   [36m0.3270[0m   [32m0.7998[0m        [35m0.2034[0m       0.9214        0.2363        9.8672
     23   0.3020   0.7974        [35m0.2002[0m       0.9202        0.2391        9.9490
     24   0.3093   0.7920        [35m0.1989[0m       0.9178        0.2388        9.8942
     25   0.3037   0.7970        [35m0.1977[0m       0.9190        0.2401        9.7370
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 22:47:41,617][0m Trial 680 finished with value: 0.2337145502740988 and parameters: {'lr': 0.00020734894788759572, 'dropout': 0.2494256610535842, 'd_model_multiplier': 4, 'num_layers': 3, 'n_heads': 8, 'dim_feedforward': 368, 'batch_size': 58, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 61}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 51
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.3208[0m   [32m0.7543[0m        [35m0.3193[0m       [31m0.9335[0m        [94m0.2502[0m     +  16.4469
      2   [36m0.3552[0m   [32m0.7572[0m        [35m0.2485[0m       [31m0.9420[0m        [94m0.2352[0m     +  17.0572
      3   0.3467   [32m0.7634[0m        [35m0.2484[0m       0.9420        [94m0.2296[0m     +  16.5234
      4   0.3424   [32m0.7657[0m        [35m0.2429[0m       [31m0.9468[0m        [94m0.2257[0m     +  17.1664
      5   [36m0.3696[0m   [32m0.7700[0m        [35m0.2410[0m       0.9395        [94m0.2188[0m     +  17.0785
      6   0.3119   0.7639        [35m0.2370[0m       0.9395        [94m0.2163[0m     +  16.8476
      7   0.3455   [32m0.7738[0m        [35m0.2346[0m       0.9407        0.2196        16.9764
      8   0.3509   0.7704        [35m0.2317[0m       0.9407        [94m0.2130[0m     +  17.2097
      9   0.3414   [32m0.7753[0m        0.2317       0.9371        [94m0.2126[0m     +  16.8037
     10   0.3560   [32m0.7764[0m        [35m0.2306[0m       0.9432        0.2151        16.9035
     11   0.3600   [32m0.7766[0m        [35m0.2281[0m       0.9432        0.2169        16.7491
     12   0.3430   [32m0.7776[0m        [35m0.2241[0m       0.9383        0.2181        16.9666
     13   0.3322   [32m0.7783[0m        0.2252       0.9395        [94m0.2118[0m     +  16.9070
     14   0.3403   0.7771        0.2244       0.9444        0.2139        16.8849
     15   0.3478   0.7763        [35m0.2228[0m       0.9420        0.2136        16.7516
     16   0.3372   [32m0.7832[0m        0.2231       0.9383        [94m0.2107[0m     +  16.9143
     17   0.3611   [32m0.7841[0m        [35m0.2184[0m       0.9359        0.2159        16.9594
     18   0.3609   [32m0.7870[0m        [35m0.2161[0m       0.9383        0.2118        16.9672
     19   0.3510   0.7784        0.2180       0.9347        0.2139        16.9452
     20   0.3241   0.7750        0.2163       0.9359        0.2108        16.9019
     21   0.3389   0.7721        0.2175       0.9407        0.2162        16.8049
     22   0.3556   0.7785        [35m0.2123[0m       0.9383        [94m0.2090[0m     +  16.9129
     23   0.3581   0.7786        0.2131       0.9383        0.2101        17.0239
     24   0.3456   0.7797        [35m0.2083[0m       0.9407        0.2134        17.0364
     25   0.3317   0.7851        0.2099       0.9323        0.2103        16.7712
     26   0.3173   0.7835        [35m0.2062[0m       0.9383        0.2115        17.1005
     27   0.3284   0.7789        [35m0.2053[0m       0.9359        0.2176        16.8070
     28   0.3373   0.7837        0.2069       0.9335        0.2191        17.1103
     29   0.3412   0.7782        [35m0.2047[0m       0.9371        0.2140        17.1397
     30   0.3627   [32m0.7904[0m        [35m0.2031[0m       0.9395        0.2110        16.8068
     31   0.3623   0.7762        [35m0.2019[0m       0.9371        0.2127        16.8524
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 22:56:43,990][0m Trial 681 finished with value: 0.2090326543298281 and parameters: {'lr': 0.0004119685106582237, 'dropout': 0.40973674108975855, 'd_model_multiplier': 4, 'num_layers': 3, 'n_heads': 32, 'dim_feedforward': 351, 'batch_size': 54, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'BatchNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0, 'top_n_features': 51}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 58
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.1146[0m   [32m0.5397[0m        [35m0.6916[0m       [31m0.5756[0m        [94m0.6880[0m     +  8.3232
      2   [36m0.1146[0m   0.5395        0.6917       0.5744        [94m0.6878[0m     +  9.5751
      3   0.1145   0.5392        [35m0.6915[0m       [31m0.5768[0m        [94m0.6877[0m     +  9.7087
      4   [36m0.1146[0m   0.5388        [35m0.6907[0m       [31m0.5792[0m        [94m0.6875[0m     +  9.4656
      5   0.1144   0.5381        0.6913       0.5792        [94m0.6873[0m     +  9.5520
      6   0.1143   0.5377        0.6908       [31m0.5804[0m        [94m0.6871[0m     +  9.5362
      7   0.1141   0.5372        0.6916       [31m0.5816[0m        [94m0.6868[0m     +  10.2314
      8   0.1138   0.5364        [35m0.6903[0m       [31m0.5840[0m        [94m0.6866[0m     +  9.6498
      9   0.1137   0.5359        [35m0.6900[0m       [31m0.5889[0m        [94m0.6863[0m     +  9.7379
     10   0.1137   0.5354        0.6903       0.5877        [94m0.6860[0m     +  9.5908
     11   0.1136   0.5350        [35m0.6884[0m       [31m0.5901[0m        [94m0.6858[0m     +  9.5528
     12   0.1133   0.5339        0.6900       [31m0.5913[0m        [94m0.6855[0m     +  9.1686
     13   0.1131   0.5334        0.6889       [31m0.5937[0m        [94m0.6852[0m     +  9.2020
     14   0.1130   0.5325        [35m0.6884[0m       0.5937        [94m0.6849[0m     +  9.4830
     15   0.1130   0.5318        [35m0.6879[0m       0.5937        [94m0.6846[0m     +  9.5771
     16   0.1128   0.5311        0.6883       [31m0.5949[0m        [94m0.6843[0m     +  9.6182
     17   0.1128   0.5305        0.6881       [31m0.5961[0m        [94m0.6839[0m     +  9.4418
     18   0.1129   0.5299        [35m0.6874[0m       [31m0.5985[0m        [94m0.6836[0m     +  9.6721
     19   0.1128   0.5291        [35m0.6873[0m       [31m0.6010[0m        [94m0.6833[0m     +  9.4043
     20   0.1127   0.5281        [35m0.6869[0m       [31m0.6034[0m        [94m0.6830[0m     +  9.6851
     21   0.1131   0.5270        0.6876       0.6034        [94m0.6826[0m     +  9.4919
     22   0.1131   0.5262        [35m0.6859[0m       0.6034        [94m0.6823[0m     +  9.5862
     23   0.1129   0.5254        0.6862       [31m0.6070[0m        [94m0.6820[0m     +  9.5877
     24   0.1127   0.5245        0.6867       0.6070        [94m0.6816[0m     +  9.7448
     25   0.1127   0.5238        [35m0.6842[0m       [31m0.6082[0m        [94m0.6813[0m     +  9.5016
     26   0.1121   0.5230        0.6854       [31m0.6131[0m        [94m0.6810[0m     +  9.5570
     27   0.1119   0.5219        0.6859       [31m0.6167[0m        [94m0.6806[0m     +  9.4149
     28   0.1119   0.5213        0.6853       0.6167        [94m0.6803[0m     +  9.6745
     29   0.1117   0.5203        0.6849       [31m0.6179[0m        [94m0.6799[0m     +  9.2625
     30   0.1116   0.5195        [35m0.6831[0m       [31m0.6191[0m        [94m0.6796[0m     +  9.5486
     31   0.1114   0.5188        [35m0.6829[0m       0.6191        [94m0.6793[0m     +  9.6592
     32   0.1116   0.5183        0.6837       0.6191        [94m0.6789[0m     +  9.6636
     33   0.1114   0.5175        0.6832       [31m0.6203[0m        [94m0.6786[0m     +  9.8056
     34   0.1114   0.5169        [35m0.6827[0m       0.6203        [94m0.6782[0m     +  9.5472
     35   0.1112   0.5160        [35m0.6825[0m       [31m0.6215[0m        [94m0.6779[0m     +  9.7288
     36   0.1110   0.5150        0.6826       [31m0.6264[0m        [94m0.6776[0m     +  9.5299
     37   0.1109   0.5142        [35m0.6814[0m       [31m0.6276[0m        [94m0.6772[0m     +  9.4488
     38   0.1108   0.5134        0.6822       [31m0.6288[0m        [94m0.6769[0m     +  9.6258
     39   0.1107   0.5130        [35m0.6802[0m       0.6288        [94m0.6765[0m     +  9.5142
     40   0.1107   0.5124        0.6804       [31m0.6324[0m        [94m0.6762[0m     +  9.5565
     41   0.1106   0.5116        0.6805       [31m0.6336[0m        [94m0.6758[0m     +  9.5396
     42   0.1104   0.5107        0.6803       [31m0.6360[0m        [94m0.6755[0m     +  9.6775
     43   0.1105   0.5100        [35m0.6792[0m       [31m0.6372[0m        [94m0.6751[0m     +  9.5411
     44   0.1098   0.5089        [35m0.6792[0m       [31m0.6397[0m        [94m0.6748[0m     +  9.8138
     45   0.1097   0.5082        [35m0.6791[0m       [31m0.6421[0m        [94m0.6745[0m     +  9.6912
     46   0.1096   0.5075        [35m0.6788[0m       [31m0.6445[0m        [94m0.6741[0m     +  9.7766
     47   0.1095   0.5068        0.6793       [31m0.6457[0m        [94m0.6738[0m     +  9.6621
     48   0.1089   0.5061        [35m0.6776[0m       [31m0.6481[0m        [94m0.6734[0m     +  9.9015
     49   0.1087   0.5052        [35m0.6772[0m       0.6481        [94m0.6731[0m     +  9.5113
     50   0.1082   0.5045        0.6774       [31m0.6530[0m        [94m0.6728[0m     +  9.4951
[32m[I 2023-05-05 23:04:45,700][0m Trial 682 finished with value: 0.6727533837039352 and parameters: {'lr': 1.9451756457997176e-08, 'dropout': 0.19830622644373136, 'd_model_multiplier': 2, 'num_layers': 3, 'n_heads': 8, 'dim_feedforward': 339, 'batch_size': 43, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 58}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 64
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.0961[0m   [32m0.5433[0m        [35m0.4626[0m       [31m0.9371[0m        [94m0.2586[0m     +  9.2126
      2   [36m0.2139[0m   [32m0.7672[0m        [35m0.2772[0m       0.9371        [94m0.2083[0m     +  9.0447
      3   [36m0.2579[0m   [32m0.8011[0m        [35m0.2538[0m       0.9287        [94m0.2052[0m     +  9.5380
      4   0.2432   0.7955        [35m0.2432[0m       0.9250        0.2115        9.2547
      5   0.2424   0.7975        [35m0.2385[0m       0.9250        0.2146        9.5359
      6   0.2416   0.7971        [35m0.2348[0m       0.9274        0.2147        9.3173
      7   0.2300   0.8009        0.2360       0.9250        0.2171        10.0085
      8   0.2298   0.8010        [35m0.2321[0m       0.9287        0.2164        9.6349
      9   0.2262   [32m0.8035[0m        [35m0.2304[0m       0.9299        0.2179        9.5561
     10   0.2255   0.8029        [35m0.2293[0m       0.9262        0.2224        9.2664
     11   0.2200   0.8034        0.2299       0.9299        0.2187        9.6438
     12   0.2268   [32m0.8038[0m        [35m0.2258[0m       0.9311        0.2199        9.6404
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 23:06:49,014][0m Trial 683 finished with value: 0.20515068817744087 and parameters: {'lr': 0.0002206567657891672, 'dropout': 0.39174260623406343, 'd_model_multiplier': 4, 'num_layers': 3, 'n_heads': 8, 'dim_feedforward': 362, 'batch_size': 48, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 64}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 71
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.1641[0m   [32m0.6500[0m        [35m0.4306[0m       [31m0.9274[0m        [94m0.2556[0m     +  13.2856
      2   [36m0.2877[0m   [32m0.7640[0m        [35m0.2591[0m       0.9274        [94m0.2199[0m     +  13.6572
      3   0.2865   [32m0.7827[0m        [35m0.2401[0m       [31m0.9287[0m        [94m0.2160[0m     +  13.6613
      4   [36m0.2942[0m   [32m0.7894[0m        [35m0.2349[0m       [31m0.9299[0m        [94m0.2146[0m     +  13.5537
      5   [36m0.3018[0m   [32m0.7920[0m        [35m0.2318[0m       0.9262        [94m0.2134[0m     +  13.4663
      6   [36m0.3079[0m   [32m0.7985[0m        [35m0.2279[0m       0.9262        [94m0.2125[0m     +  13.8793
      7   0.3070   0.7985        [35m0.2261[0m       0.9262        [94m0.2123[0m     +  13.6210
      8   [36m0.3100[0m   [32m0.8052[0m        [35m0.2249[0m       0.9287        0.2143        13.4274
      9   0.3079   [32m0.8146[0m        [35m0.2227[0m       0.9287        0.2125        13.3666
     10   [36m0.3103[0m   0.8053        [35m0.2202[0m       0.9299        0.2124        13.5426
     11   0.3029   0.7883        [35m0.2184[0m       0.9287        0.2171        13.6526
     12   [36m0.3188[0m   0.8103        [35m0.2154[0m       0.9287        0.2126        13.5868
     13   0.3008   0.8051        [35m0.2143[0m       0.9274        0.2170        13.5051
     14   0.3031   0.8074        [35m0.2109[0m       0.9299        0.2167        13.3164
     15   0.3043   0.8064        [35m0.2098[0m       [31m0.9311[0m        0.2188        13.6696
     16   [36m0.3199[0m   [32m0.8174[0m        [35m0.2079[0m       0.9287        0.2148        13.5385
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 23:10:39,954][0m Trial 684 finished with value: 0.2122778449986837 and parameters: {'lr': 0.00034674551544106995, 'dropout': 0.2364497261361287, 'd_model_multiplier': 1, 'num_layers': 3, 'n_heads': 32, 'dim_feedforward': 264, 'batch_size': 60, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 71}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 62
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.1050[0m   [32m0.5279[0m        [35m0.4555[0m       [31m0.9274[0m        [94m0.3465[0m     +  9.2509
      2   [36m0.2519[0m   [32m0.7477[0m        [35m0.2907[0m       0.9274        [94m0.2681[0m     +  10.2977
      3   [36m0.2692[0m   [32m0.7821[0m        [35m0.2648[0m       0.9274        [94m0.2478[0m     +  9.9717
      4   [36m0.2849[0m   [32m0.8093[0m        [35m0.2610[0m       [31m0.9287[0m        [94m0.2414[0m     +  10.2782
      5   [36m0.3073[0m   [32m0.8137[0m        [35m0.2543[0m       0.9274        [94m0.2336[0m     +  9.9150
      6   [36m0.3260[0m   0.8107        [35m0.2543[0m       [31m0.9299[0m        [94m0.2329[0m     +  10.0106
      7   [36m0.3292[0m   0.8101        [35m0.2511[0m       0.9299        [94m0.2289[0m     +  9.9451
      8   [36m0.3410[0m   0.8085        [35m0.2494[0m       [31m0.9311[0m        0.2304        10.0093
      9   [36m0.3537[0m   [32m0.8162[0m        [35m0.2462[0m       0.9299        0.2318        10.3029
     10   [36m0.3537[0m   0.8093        [35m0.2439[0m       0.9299        [94m0.2280[0m     +  10.0720
     11   [36m0.3551[0m   0.8137        [35m0.2436[0m       0.9299        0.2297        10.5045
     12   0.3519   [32m0.8260[0m        [35m0.2411[0m       [31m0.9323[0m        0.2355        10.4085
     13   [36m0.3576[0m   0.8207        [35m0.2403[0m       0.9299        0.2326        10.5292
     14   [36m0.3656[0m   [32m0.8293[0m        [35m0.2391[0m       [31m0.9347[0m        0.2310        10.4509
     15   [36m0.3674[0m   [32m0.8303[0m        [35m0.2363[0m       0.9311        0.2303        10.3273
     16   [36m0.3714[0m   [32m0.8312[0m        [35m0.2350[0m       [31m0.9359[0m        [94m0.2278[0m     +  10.4922
     17   0.3633   [32m0.8325[0m        0.2352       0.9335        0.2290        10.6424
     18   0.3624   0.8288        [35m0.2321[0m       0.9335        0.2280        10.5988
     19   0.3616   [32m0.8332[0m        0.2356       0.9335        0.2307        9.8116
     20   0.3644   [32m0.8353[0m        0.2339       0.9287        0.2287        10.4024
     21   0.3562   0.8340        0.2324       0.9274        [94m0.2255[0m     +  10.1378
     22   0.3546   0.8307        [35m0.2289[0m       0.9274        [94m0.2230[0m     +  10.3020
     23   0.3511   0.8353        0.2302       0.9262        0.2243        10.2780
     24   0.3483   0.8314        0.2292       0.9250        0.2233        10.2244
     25   0.3421   0.8305        [35m0.2271[0m       0.9262        [94m0.2228[0m     +  10.2534
     26   0.3496   0.8292        [35m0.2259[0m       0.9250        0.2246        10.4739
     27   0.3360   0.8270        [35m0.2253[0m       0.9250        0.2245        10.4288
     28   0.3296   0.8289        0.2259       0.9250        0.2257        10.6035
     29   0.3276   0.8282        [35m0.2251[0m       0.9250        0.2233        10.4951
     30   0.3044   0.8281        0.2264       0.9250        [94m0.2211[0m     +  10.5214
     31   0.3117   0.8291        [35m0.2195[0m       0.9226        0.2247        10.4906
     32   0.3256   0.8280        0.2220       0.9214        0.2214        10.4771
     33   0.3024   0.8294        0.2224       0.9178        0.2335        10.3091
     34   0.2999   0.8289        0.2222       0.9178        0.2262        10.4672
     35   0.3054   0.8296        0.2197       0.9154        0.2244        10.6014
     36   0.3052   0.8261        0.2202       0.9190        0.2214        10.7072
     37   0.3079   0.8254        [35m0.2187[0m       0.9178        0.2262        10.6012
     38   0.2765   0.8072        [35m0.2171[0m       0.9178        0.2315        10.5134
     39   0.2796   0.8027        [35m0.2167[0m       0.9154        0.2335        10.4847
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 23:17:34,071][0m Trial 685 finished with value: 0.22110457774229175 and parameters: {'lr': 0.0006495532348498386, 'dropout': 0.4880773753411232, 'd_model_multiplier': 4, 'num_layers': 2, 'n_heads': 8, 'dim_feedforward': 347, 'batch_size': 131, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'BatchNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 62}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 101
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.1836[0m   [32m0.7246[0m        [35m0.2827[0m       [31m0.9311[0m        [94m0.2324[0m     +  9.7652
      2   [36m0.2102[0m   [32m0.7823[0m        [35m0.2691[0m       0.9299        [94m0.2242[0m     +  9.2892
      3   0.1923   0.7504        [35m0.2560[0m       0.9287        0.2278        9.8010
      4   0.2018   0.7671        [35m0.2471[0m       0.9262        0.2267        9.7369
      5   0.2094   0.7798        0.2476       0.9226        [94m0.2233[0m     +  9.8330
      6   [36m0.2217[0m   [32m0.7876[0m        [35m0.2445[0m       0.9178        0.2292        10.1641
      7   [36m0.2945[0m   [32m0.8073[0m        [35m0.2416[0m       0.9214        [94m0.2165[0m     +  9.9997
      8   0.2221   0.7909        0.2463       0.9190        0.2327        10.2317
      9   0.1820   0.7639        0.2501       0.9311        0.2605        9.8444
     10   0.2325   0.6874        0.2534       0.8730        0.3034        9.8171
     11   0.2625   0.7950        0.2521       0.9129        0.2363        9.7038
     12   0.1834   0.7572        0.2561       0.9214        0.2307        9.6632
     13   0.1793   0.7585        0.2548       0.9226        0.2310        9.5453
     14   0.1840   0.7509        0.2513       0.9178        0.2313        9.7022
     15   0.1796   0.7504        0.2540       0.9250        0.2304        9.5476
     16   0.1808   0.7512        0.2536       0.9238        0.2323        9.9030
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 23:20:20,955][0m Trial 686 finished with value: 0.21648499942604257 and parameters: {'lr': 0.0026679039694979074, 'dropout': 0.3694137109594752, 'd_model_multiplier': 32, 'num_layers': 2, 'n_heads': 8, 'dim_feedforward': 381, 'batch_size': 56, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 101}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 57
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0659[0m   [32m0.3685[0m        [35m0.6321[0m       [31m0.9238[0m        [94m0.5701[0m     +  11.2352
      2   0.0630   0.3548        [35m0.5385[0m       0.9238        [94m0.4624[0m     +  11.8183
      3   [36m0.0698[0m   0.3603        [35m0.4455[0m       0.9238        [94m0.3792[0m     +  12.2219
      4   [36m0.0804[0m   [32m0.4442[0m        [35m0.3762[0m       0.9238        [94m0.3236[0m     +  12.0557
      5   [36m0.1162[0m   [32m0.5723[0m        [35m0.3306[0m       0.9238        [94m0.2894[0m     +  12.5127
      6   [36m0.1511[0m   [32m0.6303[0m        [35m0.3024[0m       0.9238        [94m0.2704[0m     +  12.0588
      7   [36m0.1603[0m   [32m0.6647[0m        [35m0.2826[0m       0.9238        [94m0.2598[0m     +  12.1979
      8   [36m0.1819[0m   [32m0.6942[0m        [35m0.2709[0m       0.9238        [94m0.2526[0m     +  12.3327
      9   [36m0.1938[0m   [32m0.7194[0m        [35m0.2632[0m       0.9214        [94m0.2477[0m     +  12.4947
     10   [36m0.2064[0m   [32m0.7424[0m        [35m0.2551[0m       0.9238        [94m0.2449[0m     +  12.2658
     11   [36m0.2106[0m   [32m0.7520[0m        [35m0.2490[0m       0.9202        [94m0.2439[0m     +  12.5153
     12   [36m0.2156[0m   [32m0.7535[0m        [35m0.2436[0m       0.9214        0.2442        12.2885
     13   [36m0.2253[0m   [32m0.7555[0m        [35m0.2409[0m       0.9190        0.2442        12.1437
     14   [36m0.2278[0m   [32m0.7571[0m        [35m0.2391[0m       0.9178        0.2441        12.1401
     15   [36m0.2322[0m   [32m0.7589[0m        [35m0.2370[0m       0.9154        0.2444        12.2750
     16   [36m0.2377[0m   [32m0.7602[0m        [35m0.2344[0m       0.9154        0.2441        12.1797
     17   [36m0.2459[0m   [32m0.7621[0m        [35m0.2341[0m       0.9154        [94m0.2436[0m     +  12.2880
     18   [36m0.2464[0m   0.7618        [35m0.2317[0m       0.9154        0.2438        12.4394
     19   [36m0.2473[0m   [32m0.7643[0m        [35m0.2313[0m       0.9154        0.2441        12.3375
     20   0.2470   0.7630        0.2316       0.9154        0.2437        12.5652
     21   0.2472   0.7634        [35m0.2310[0m       0.9166        0.2445        12.6332
     22   [36m0.2540[0m   0.7619        [35m0.2294[0m       0.9129        0.2444        12.3041
     23   [36m0.2543[0m   0.7616        [35m0.2286[0m       0.9190        0.2449        12.2938
     24   [36m0.2577[0m   0.7621        [35m0.2286[0m       0.9190        0.2450        12.4907
     25   [36m0.2578[0m   0.7627        0.2303       0.9190        0.2446        12.3778
     26   [36m0.2621[0m   0.7627        [35m0.2272[0m       0.9166        0.2448        12.7216
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 23:25:53,469][0m Trial 687 finished with value: 0.2436006777664194 and parameters: {'lr': 0.00012715672375329616, 'dropout': 0.41896451757790476, 'd_model_multiplier': 4, 'num_layers': 3, 'n_heads': 8, 'dim_feedforward': 339, 'batch_size': 202, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 57}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 85
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.2760[0m   [32m0.7281[0m        [35m0.3566[0m       [31m0.9238[0m        [94m0.2450[0m     +  19.7302
      2   [36m0.3184[0m   [32m0.7667[0m        [35m0.2443[0m       0.9093        [94m0.2402[0m     +  19.8607
      3   [36m0.3421[0m   [32m0.7802[0m        [35m0.2358[0m       0.9178        [94m0.2357[0m     +  20.0428
      4   [36m0.3535[0m   [32m0.7918[0m        [35m0.2343[0m       0.9141        [94m0.2337[0m     +  20.0080
      5   [36m0.3627[0m   [32m0.7974[0m        [35m0.2326[0m       0.9226        [94m0.2273[0m     +  20.0705
      6   [36m0.3882[0m   0.7957        [35m0.2291[0m       0.9238        [94m0.2253[0m     +  20.0084
      7   0.3732   0.7916        0.2315       0.9238        0.2266        20.0279
      8   0.3726   0.7929        [35m0.2286[0m       0.9226        0.2260        19.9696
      9   0.3794   0.7938        [35m0.2286[0m       0.9226        [94m0.2253[0m     +  20.1462
     10   0.3878   0.7952        [35m0.2246[0m       [31m0.9287[0m        [94m0.2214[0m     +  20.0221
     11   0.3695   0.7913        [35m0.2229[0m       0.9250        0.2247        20.0696
     12   0.3735   0.7921        [35m0.2226[0m       0.9250        0.2243        19.9941
     13   0.3771   0.7834        [35m0.2200[0m       0.9274        0.2259        20.1085
     14   0.3531   0.7830        [35m0.2195[0m       0.9250        0.2285        19.9267
     15   0.3430   0.7760        [35m0.2177[0m       0.9250        0.2314        19.9934
     16   0.3714   0.7882        [35m0.2170[0m       0.9287        0.2260        20.0764
     17   0.3298   0.7641        [35m0.2146[0m       0.9250        0.2345        19.9775
     18   0.3390   0.7756        [35m0.2137[0m       0.9262        0.2336        19.9862
     19   0.3458   0.7799        [35m0.2131[0m       0.9226        0.2324        20.1855
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 23:32:34,774][0m Trial 688 finished with value: 0.22136106325410299 and parameters: {'lr': 0.00016280468296406224, 'dropout': 0.47214732734294496, 'd_model_multiplier': 4, 'num_layers': 4, 'n_heads': 32, 'dim_feedforward': 351, 'batch_size': 37, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.001, 'top_n_features': 85}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 52
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.1551[0m   [32m0.6360[0m        [35m0.4259[0m       [31m0.9274[0m        [94m0.2570[0m     +  9.0283
      2   [36m0.2009[0m   [32m0.7249[0m        [35m0.2534[0m       0.9250        [94m0.2366[0m     +  8.8188
      3   [36m0.2185[0m   [32m0.7546[0m        [35m0.2381[0m       0.9238        [94m0.2340[0m     +  9.1526
      4   0.2165   [32m0.7689[0m        [35m0.2336[0m       0.9238        0.2347        9.4238
      5   0.2119   [32m0.7754[0m        [35m0.2305[0m       0.9226        0.2362        8.8795
      6   0.2145   [32m0.7811[0m        [35m0.2277[0m       0.9190        0.2373        8.9929
      7   0.2095   [32m0.7830[0m        [35m0.2250[0m       0.9190        0.2406        9.3745
      8   0.2076   [32m0.7841[0m        [35m0.2240[0m       0.9166        0.2412        9.1608
      9   0.2066   [32m0.7859[0m        [35m0.2214[0m       0.9190        0.2423        8.9407
     10   0.2081   0.7829        [35m0.2177[0m       0.9178        0.2421        9.3077
     11   0.2103   0.7816        [35m0.2165[0m       0.9190        0.2432        9.5092
     12   0.2076   0.7790        [35m0.2154[0m       0.9202        0.2432        9.2278
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 23:34:34,032][0m Trial 689 finished with value: 0.23402113225108334 and parameters: {'lr': 0.00026478948098523626, 'dropout': 0.22399741807559098, 'd_model_multiplier': 4, 'num_layers': 2, 'n_heads': 8, 'dim_feedforward': 339, 'batch_size': 45, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0, 'top_n_features': 52}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 67
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.1717[0m   [32m0.6842[0m        [35m0.4551[0m       [31m0.9238[0m        [94m0.2646[0m     +  8.6022
      2   [36m0.2758[0m   [32m0.7645[0m        [35m0.2612[0m       0.9238        [94m0.2349[0m     +  9.2108
      3   [36m0.2856[0m   [32m0.7734[0m        [35m0.2499[0m       0.9238        [94m0.2306[0m     +  9.6766
      4   [36m0.2899[0m   [32m0.7765[0m        [35m0.2462[0m       0.9238        [94m0.2286[0m     +  9.5932
      5   0.2821   [32m0.7845[0m        [35m0.2429[0m       0.9226        [94m0.2272[0m     +  9.7381
      6   0.2872   [32m0.7901[0m        [35m0.2396[0m       0.9226        [94m0.2258[0m     +  9.9197
      7   0.2894   [32m0.7938[0m        [35m0.2363[0m       0.9214        [94m0.2243[0m     +  9.5345
      8   [36m0.2912[0m   [32m0.8003[0m        [35m0.2344[0m       0.9202        [94m0.2228[0m     +  9.6735
      9   [36m0.2976[0m   [32m0.8020[0m        [35m0.2317[0m       0.9226        [94m0.2221[0m     +  9.7067
     10   [36m0.3003[0m   [32m0.8039[0m        [35m0.2306[0m       0.9226        [94m0.2210[0m     +  9.9242
     11   [36m0.3045[0m   [32m0.8066[0m        [35m0.2278[0m       0.9214        0.2211        9.9561
     12   [36m0.3098[0m   [32m0.8067[0m        0.2278       0.9214        [94m0.2199[0m     +  9.9132
     13   0.3064   [32m0.8095[0m        [35m0.2249[0m       0.9214        [94m0.2193[0m     +  9.4163
     14   [36m0.3247[0m   0.8095        0.2263       0.9202        [94m0.2190[0m     +  9.5691
     15   0.3227   [32m0.8126[0m        [35m0.2247[0m       [31m0.9262[0m        [94m0.2182[0m     +  9.6821
     16   [36m0.3271[0m   0.8120        [35m0.2245[0m       [31m0.9287[0m        0.2196        9.8002
     17   [36m0.3351[0m   [32m0.8144[0m        [35m0.2221[0m       [31m0.9299[0m        [94m0.2176[0m     +  9.7011
     18   [36m0.3376[0m   [32m0.8161[0m        0.2227       0.9287        [94m0.2172[0m     +  9.7843
     19   [36m0.3493[0m   [32m0.8164[0m        [35m0.2207[0m       0.9299        0.2178        9.5121
     20   [36m0.3606[0m   0.8162        [35m0.2195[0m       0.9274        [94m0.2169[0m     +  9.9228
     21   [36m0.3682[0m   0.8152        [35m0.2178[0m       0.9262        0.2185        9.6700
     22   0.3534   [32m0.8176[0m        0.2182       0.9262        0.2173        9.8800
     23   0.3623   0.8136        [35m0.2169[0m       0.9274        0.2174        9.4886
     24   [36m0.3776[0m   0.8128        0.2177       0.9262        [94m0.2158[0m     +  9.4442
     25   [36m0.3799[0m   0.8159        [35m0.2151[0m       0.9274        0.2171        9.6791
     26   0.3707   0.8122        [35m0.2142[0m       0.9262        0.2180        9.6540
     27   0.3751   [32m0.8197[0m        [35m0.2129[0m       0.9250        0.2162        9.7164
     28   0.3780   0.8175        [35m0.2126[0m       0.9250        0.2172        9.6478
     29   0.3703   0.8148        [35m0.2120[0m       0.9274        0.2191        9.4885
     30   [36m0.3803[0m   0.8178        [35m0.2105[0m       0.9287        0.2183        9.9879
     31   0.3741   0.8185        [35m0.2104[0m       0.9262        0.2160        9.7568
     32   0.3608   0.8163        [35m0.2098[0m       0.9262        0.2192        9.9258
     33   [36m0.3848[0m   0.8176        [35m0.2089[0m       0.9250        0.2161        9.7942
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 23:40:03,827][0m Trial 690 finished with value: 0.21582063857776798 and parameters: {'lr': 0.0004998351393523609, 'dropout': 0.20092051607483177, 'd_model_multiplier': 1, 'num_layers': 4, 'n_heads': 8, 'dim_feedforward': 374, 'batch_size': 55, 'pos_encoding': 'fixed', 'activation': 'gelu', 'norm': 'BatchNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 67}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 131
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.2229[0m   [32m0.7229[0m        [35m0.2834[0m       [31m0.9202[0m        [94m0.2576[0m     +  9.3575
      2   [36m0.2446[0m   [32m0.7373[0m        [35m0.2537[0m       0.9190        0.2594        9.2699
      3   [36m0.2501[0m   [32m0.7469[0m        [35m0.2504[0m       0.9166        0.2589        9.5847
      4   [36m0.2645[0m   [32m0.7527[0m        [35m0.2425[0m       0.9190        [94m0.2554[0m     +  9.5866
      5   [36m0.2686[0m   [32m0.7629[0m        [35m0.2394[0m       0.9190        0.2558        9.4502
      6   [36m0.2807[0m   [32m0.7685[0m        [35m0.2387[0m       0.9190        0.2618        9.6415
      7   [36m0.2904[0m   0.7684        [35m0.2371[0m       [31m0.9238[0m        0.2568        9.2420
      8   0.2492   0.7664        [35m0.2346[0m       0.9190        0.2608        9.4028
      9   0.2821   0.7670        0.2350       0.9166        0.2655        9.5178
     10   0.2372   0.7590        [35m0.2346[0m       0.8900        0.2753        9.5284
     11   [36m0.3010[0m   [32m0.7776[0m        0.2365       0.9226        0.2634        9.9523
     12   0.2511   0.7590        0.2349       0.9081        0.2630        9.5844
     13   0.1926   0.7208        0.2361       0.9178        0.3015        9.3400
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 23:42:17,043][0m Trial 691 finished with value: 0.2553948120562024 and parameters: {'lr': 0.0035158131746471327, 'dropout': 0.4371445888866246, 'd_model_multiplier': 4, 'num_layers': 3, 'n_heads': 8, 'dim_feedforward': 391, 'batch_size': 50, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 131}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 55
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0749[0m   [32m0.3324[0m        [35m0.7137[0m       [31m0.3652[0m        [94m0.7012[0m     +  14.1399
      2   0.0727   0.3137        [35m0.6990[0m       [31m0.5574[0m        [94m0.6836[0m     +  14.5337
      3   0.0687   0.3084        [35m0.6802[0m       [31m0.8235[0m        [94m0.6621[0m     +  14.6197
      4   0.0665   0.3070        [35m0.6601[0m       [31m0.9093[0m        [94m0.6385[0m     +  14.3450
      5   0.0652   0.3107        [35m0.6360[0m       [31m0.9154[0m        [94m0.6140[0m     +  14.6708
      6   0.0657   0.3171        [35m0.6133[0m       0.9154        [94m0.5894[0m     +  14.5453
      7   0.0656   0.3198        [35m0.5894[0m       0.9154        [94m0.5652[0m     +  14.5480
      8   0.0656   0.3190        [35m0.5676[0m       0.9154        [94m0.5419[0m     +  14.6032
      9   0.0662   0.3169        [35m0.5447[0m       0.9154        [94m0.5197[0m     +  14.5045
     10   0.0671   0.3149        [35m0.5239[0m       0.9154        [94m0.4989[0m     +  14.5042
     11   0.0674   0.3140        [35m0.5027[0m       0.9154        [94m0.4797[0m     +  14.5822
     12   0.0670   0.3130        [35m0.4845[0m       0.9154        [94m0.4619[0m     +  14.7113
     13   0.0672   0.3142        [35m0.4655[0m       0.9154        [94m0.4457[0m     +  14.4797
     14   0.0676   0.3163        [35m0.4509[0m       0.9154        [94m0.4309[0m     +  14.6801
     15   0.0678   0.3183        [35m0.4358[0m       0.9154        [94m0.4174[0m     +  14.6508
     16   0.0685   0.3214        [35m0.4221[0m       0.9154        [94m0.4051[0m     +  14.6854
     17   0.0691   0.3241        [35m0.4105[0m       0.9154        [94m0.3941[0m     +  14.5416
     18   0.0696   0.3288        [35m0.3993[0m       0.9154        [94m0.3840[0m     +  14.5160
     19   0.0700   [32m0.3342[0m        [35m0.3870[0m       0.9154        [94m0.3749[0m     +  14.5483
     20   0.0704   [32m0.3403[0m        [35m0.3777[0m       0.9154        [94m0.3666[0m     +  14.5140
     21   0.0712   [32m0.3481[0m        [35m0.3694[0m       0.9154        [94m0.3591[0m     +  14.4453
     22   0.0719   [32m0.3567[0m        [35m0.3621[0m       0.9154        [94m0.3523[0m     +  14.6457
     23   0.0729   [32m0.3640[0m        [35m0.3542[0m       0.9154        [94m0.3462[0m     +  14.5335
     24   0.0740   [32m0.3734[0m        [35m0.3486[0m       0.9154        [94m0.3406[0m     +  14.4447
     25   [36m0.0751[0m   [32m0.3839[0m        [35m0.3411[0m       0.9154        [94m0.3355[0m     +  14.5891
     26   [36m0.0763[0m   [32m0.3943[0m        [35m0.3357[0m       0.9154        [94m0.3308[0m     +  14.6813
     27   [36m0.0778[0m   [32m0.4062[0m        [35m0.3300[0m       0.9154        [94m0.3265[0m     +  14.4922
     28   [36m0.0795[0m   [32m0.4182[0m        [35m0.3263[0m       0.9154        [94m0.3226[0m     +  14.5406
     29   [36m0.0812[0m   [32m0.4296[0m        [35m0.3214[0m       0.9154        [94m0.3190[0m     +  14.5946
     30   [36m0.0831[0m   [32m0.4411[0m        [35m0.3184[0m       0.9154        [94m0.3157[0m     +  14.5757
     31   [36m0.0856[0m   [32m0.4521[0m        [35m0.3136[0m       0.9154        [94m0.3126[0m     +  14.6976
     32   [36m0.0882[0m   [32m0.4630[0m        [35m0.3110[0m       0.9154        [94m0.3098[0m     +  14.6774
     33   [36m0.0911[0m   [32m0.4729[0m        [35m0.3071[0m       0.9154        [94m0.3072[0m     +  14.5466
     34   [36m0.0952[0m   [32m0.4840[0m        [35m0.3042[0m       0.9154        [94m0.3048[0m     +  14.5471
     35   [36m0.1056[0m   [32m0.4943[0m        [35m0.3022[0m       0.9154        [94m0.3025[0m     +  14.4800
     36   [36m0.1099[0m   [32m0.5039[0m        [35m0.2979[0m       0.9154        [94m0.3004[0m     +  14.5144
     37   [36m0.1142[0m   [32m0.5142[0m        [35m0.2964[0m       0.9154        [94m0.2984[0m     +  14.3903
     38   [36m0.1270[0m   [32m0.5229[0m        [35m0.2942[0m       0.9154        [94m0.2965[0m     +  14.7896
     39   [36m0.1334[0m   [32m0.5313[0m        [35m0.2909[0m       0.9154        [94m0.2948[0m     +  15.0474
     40   [36m0.1370[0m   [32m0.5399[0m        [35m0.2898[0m       0.9154        [94m0.2931[0m     +  14.5445
     41   [36m0.1422[0m   [32m0.5480[0m        [35m0.2884[0m       0.9154        [94m0.2915[0m     +  14.7027
     42   [36m0.1510[0m   [32m0.5566[0m        [35m0.2867[0m       0.9154        [94m0.2900[0m     +  14.5053
     43   [36m0.1553[0m   [32m0.5644[0m        [35m0.2837[0m       0.9154        [94m0.2886[0m     +  14.7984
     44   [36m0.1593[0m   [32m0.5724[0m        [35m0.2829[0m       0.9154        [94m0.2873[0m     +  14.6323
     45   [36m0.1633[0m   [32m0.5784[0m        [35m0.2813[0m       0.9154        [94m0.2861[0m     +  14.5254
     46   [36m0.1671[0m   [32m0.5843[0m        [35m0.2803[0m       0.9154        [94m0.2849[0m     +  14.4534
     47   [36m0.1732[0m   [32m0.5908[0m        [35m0.2784[0m       0.9154        [94m0.2837[0m     +  14.4242
     48   [36m0.1789[0m   [32m0.5977[0m        [35m0.2766[0m       0.9154        [94m0.2826[0m     +  14.6033
     49   [36m0.1818[0m   [32m0.6037[0m        [35m0.2755[0m       0.9154        [94m0.2816[0m     +  14.6523
     50   [36m0.1838[0m   [32m0.6092[0m        [35m0.2737[0m       0.9154        [94m0.2807[0m     +  14.4559
[32m[I 2023-05-05 23:54:33,501][0m Trial 692 finished with value: 0.280661236382165 and parameters: {'lr': 2.1566223327310967e-07, 'dropout': 0.3996006097805833, 'd_model_multiplier': 16, 'num_layers': 7, 'n_heads': 8, 'dim_feedforward': 358, 'batch_size': 32, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 55}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 138
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp     dur
-------  -------  -------  ------------  -----------  ------------  ----  ------
      1   [36m0.2340[0m   [32m0.7280[0m        [35m0.3122[0m       [31m0.9105[0m        [94m0.2656[0m     +  8.8900
      2   0.2283   0.7128        [35m0.2752[0m       [31m0.9141[0m        0.2659        9.3714
      3   [36m0.2422[0m   [32m0.7345[0m        [35m0.2674[0m       0.9141        [94m0.2610[0m     +  9.4986
      4   [36m0.2477[0m   0.7180        [35m0.2595[0m       0.9141        [94m0.2609[0m     +  9.4893
      5   0.2453   0.7054        [35m0.2559[0m       0.9141        0.2650        9.5713
      6   0.2350   0.7154        [35m0.2553[0m       0.9141        0.2628        9.4991
      7   [36m0.2478[0m   0.7153        [35m0.2542[0m       0.9141        0.2616        9.7061
      8   [36m0.2483[0m   0.7144        [35m0.2502[0m       0.9141        [94m0.2608[0m     +  9.3736
      9   0.2455   0.7133        0.2508       0.9141        0.2609        9.5404
     10   0.2404   0.7129        0.2503       0.9141        0.2613        9.3752
     11   0.2474   0.7148        [35m0.2499[0m       0.9141        [94m0.2608[0m     +  9.5040
     12   [36m0.2490[0m   0.7144        [35m0.2487[0m       0.9141        0.2610        9.5531
     13   [36m0.2592[0m   0.7134        0.2495       0.9141        0.2610        9.7322
     14   0.2462   0.7137        0.2500       0.9141        0.2609        9.3583
     15   0.2504   0.7178        0.2487       0.9141        [94m0.2605[0m     +  9.5306
     16   0.2587   0.7164        0.2491       0.9141        [94m0.2602[0m     +  9.6486
     17   0.2367   0.7189        0.2502       0.9141        0.2610        9.3973
     18   0.2474   0.7181        0.2490       0.9141        0.2602        9.6902
     19   0.2393   0.7180        0.2494       0.9141        0.2604        9.8336
     20   0.2494   0.7171        [35m0.2480[0m       0.9141        [94m0.2602[0m     +  9.4699
     21   0.2566   0.7182        0.2503       0.9141        [94m0.2596[0m     +  9.4829
     22   0.2485   0.7201        0.2494       0.9141        0.2603        9.6019
     23   0.2480   0.7188        0.2490       0.9141        0.2606        9.5984
     24   0.2522   0.7141        0.2508       0.9141        0.2602        10.0906
     25   0.2431   0.7136        0.2484       0.9141        0.2602        9.5702
     26   0.2397   0.7159        0.2491       0.9141        0.2606        9.5627
     27   0.2329   0.7151        0.2495       0.9141        0.2614        9.5748
     28   0.2346   0.7150        0.2552       0.9141        0.2613        9.6520
     29   0.2382   0.7173        0.2497       0.9141        0.2606        9.6172
     30   0.2313   0.7159        0.2501       0.9141        0.2614        9.9190
Stopping since valid_loss has not improved in the last 10 epochs.
[32m[I 2023-05-05 23:59:30,919][0m Trial 693 finished with value: 0.259568883689456 and parameters: {'lr': 0.014928377421767943, 'dropout': 0.21059367705137583, 'd_model_multiplier': 4, 'num_layers': 1, 'n_heads': 32, 'dim_feedforward': 336, 'batch_size': 25, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'LayerNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.001, 'top_n_features': 138}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 173
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0644[0m   [32m0.4110[0m        [35m0.6424[0m       [31m0.8041[0m        [94m0.6552[0m     +  12.9689
      2   0.0613   0.3876        [35m0.6306[0m       [31m0.8537[0m        [94m0.6446[0m     +  13.3278
      3   0.0506   0.3593        [35m0.6172[0m       [31m0.9045[0m        [94m0.6264[0m     +  13.4410
      4   0.0485   0.3402        [35m0.5983[0m       [31m0.9202[0m        [94m0.6126[0m     +  13.4276
      5   0.0459   0.3180        [35m0.5788[0m       [31m0.9262[0m        [94m0.5911[0m     +  13.6097
      6   0.0454   0.3074        [35m0.5583[0m       [31m0.9311[0m        [94m0.5751[0m     +  13.3604
      7   0.0449   0.2970        [35m0.5369[0m       [31m0.9347[0m        [94m0.5537[0m     +  13.5775
      8   0.0446   0.2922        [35m0.5198[0m       0.9347        [94m0.5358[0m     +  13.9028
      9   0.0446   0.2897        [35m0.4981[0m       0.9347        [94m0.5171[0m     +  13.4654
     10   0.0446   0.2891        [35m0.4808[0m       0.9347        [94m0.4995[0m     +  13.8214
     11   0.0447   0.2902        [35m0.4622[0m       0.9347        [94m0.4804[0m     +  13.5078
     12   0.0450   0.2920        [35m0.4440[0m       0.9347        [94m0.4649[0m     +  13.4791
     13   0.0456   0.2968        [35m0.4300[0m       0.9347        [94m0.4482[0m     +  13.4448
     14   0.0464   0.3032        [35m0.4144[0m       0.9347        [94m0.4336[0m     +  13.4219
     15   0.0477   0.3119        [35m0.3992[0m       0.9347        [94m0.4207[0m     +  13.4768
     16   0.0483   0.3218        [35m0.3891[0m       0.9347        [94m0.4075[0m     +  13.2548
     17   0.0501   0.3326        [35m0.3781[0m       0.9347        [94m0.3921[0m     +  13.5923
     18   0.0521   0.3492        [35m0.3666[0m       0.9347        [94m0.3805[0m     +  13.7895
     19   0.0535   0.3684        [35m0.3565[0m       0.9347        [94m0.3695[0m     +  13.5107
     20   0.0551   0.3884        [35m0.3466[0m       0.9347        [94m0.3584[0m     +  13.5107
     21   0.0566   0.4063        [35m0.3393[0m       0.9347        [94m0.3529[0m     +  13.6535
     22   0.0586   [32m0.4235[0m        [35m0.3319[0m       0.9347        [94m0.3441[0m     +  13.3610
     23   0.0601   [32m0.4423[0m        [35m0.3244[0m       0.9347        [94m0.3359[0m     +  13.6504
     24   0.0634   [32m0.4624[0m        [35m0.3178[0m       0.9347        [94m0.3262[0m     +  13.5809
     25   [36m0.0670[0m   [32m0.4770[0m        [35m0.3110[0m       0.9347        [94m0.3172[0m     +  13.6507
     26   [36m0.0719[0m   [32m0.4949[0m        [35m0.3073[0m       0.9347        [94m0.3125[0m     +  13.6745
     27   [36m0.0749[0m   [32m0.5089[0m        [35m0.3037[0m       0.9347        [94m0.3084[0m     +  13.5029
     28   [36m0.0819[0m   [32m0.5258[0m        [35m0.2985[0m       0.9347        [94m0.3021[0m     +  13.2731
     29   [36m0.0895[0m   [32m0.5393[0m        [35m0.2938[0m       0.9347        [94m0.2952[0m     +  13.4341
     30   [36m0.0994[0m   [32m0.5498[0m        [35m0.2914[0m       0.9347        [94m0.2934[0m     +  13.8053
     31   [36m0.1101[0m   [32m0.5624[0m        [35m0.2878[0m       0.9347        [94m0.2881[0m     +  13.5934
     32   [36m0.1188[0m   [32m0.5708[0m        [35m0.2862[0m       0.9347        [94m0.2841[0m     +  13.7309
     33   [36m0.1265[0m   [32m0.5798[0m        [35m0.2830[0m       0.9347        [94m0.2826[0m     +  13.5275
     34   [36m0.1327[0m   [32m0.5876[0m        [35m0.2804[0m       0.9347        [94m0.2770[0m     +  13.2429
     35   [36m0.1371[0m   [32m0.5953[0m        [35m0.2788[0m       0.9347        [94m0.2747[0m     +  13.3334
     36   [36m0.1430[0m   [32m0.6030[0m        [35m0.2764[0m       0.9335        [94m0.2702[0m     +  13.6813
     37   [36m0.1468[0m   [32m0.6092[0m        [35m0.2736[0m       0.9335        0.2709        13.6226
     38   [36m0.1526[0m   [32m0.6158[0m        [35m0.2724[0m       0.9335        [94m0.2657[0m     +  13.2653
     39   [36m0.1553[0m   [32m0.6222[0m        [35m0.2709[0m       0.9335        [94m0.2641[0m     +  13.4831
     40   [36m0.1583[0m   [32m0.6269[0m        [35m0.2694[0m       0.9335        [94m0.2634[0m     +  13.5057
     41   [36m0.1619[0m   [32m0.6325[0m        [35m0.2688[0m       0.9335        [94m0.2597[0m     +  13.1808
     42   [36m0.1640[0m   [32m0.6355[0m        [35m0.2670[0m       0.9335        [94m0.2584[0m     +  13.1758
     43   [36m0.1671[0m   [32m0.6401[0m        [35m0.2657[0m       0.9335        [94m0.2579[0m     +  13.4390
     44   [36m0.1704[0m   [32m0.6451[0m        0.2660       0.9335        [94m0.2553[0m     +  13.6378
     45   [36m0.1733[0m   [32m0.6495[0m        [35m0.2644[0m       0.9335        [94m0.2542[0m     +  13.6538
     46   [36m0.1743[0m   [32m0.6522[0m        [35m0.2643[0m       0.9335        [94m0.2537[0m     +  13.4530
     47   [36m0.1778[0m   [32m0.6552[0m        [35m0.2628[0m       0.9335        [94m0.2521[0m     +  13.2699
     48   [36m0.1800[0m   [32m0.6585[0m        [35m0.2614[0m       0.9335        [94m0.2519[0m     +  13.3190
     49   [36m0.1824[0m   [32m0.6619[0m        0.2624       0.9335        [94m0.2498[0m     +  13.4971
     50   [36m0.1832[0m   [32m0.6650[0m        [35m0.2586[0m       0.9335        0.2503        13.4779
[32m[I 2023-05-06 00:10:49,118][0m Trial 694 finished with value: 0.2497924273651197 and parameters: {'lr': 4.010088395200471e-07, 'dropout': 0.2654902001333327, 'd_model_multiplier': 4, 'num_layers': 2, 'n_heads': 32, 'dim_feedforward': 331, 'batch_size': 63, 'pos_encoding': 'fixed', 'activation': 'relu', 'norm': 'BatchNorm', 'optimizer_name': 'RAdam', 'weight_decay': 0.1, 'top_n_features': 173}. Best is trial 246 with value: 0.16447311791139763.[0m
[FileBasedDataset] Initializing dataset...
	Examples: 8264
	Features: 61
	Max length: 315
Standard scalar turned ON, loading params from cache
[FileBasedDataset] Dataset initialization complete
  epoch    auprc    auroc    train_loss    valid_acc    valid_loss    cp      dur
-------  -------  -------  ------------  -----------  ------------  ----  -------
      1   [36m0.0871[0m   [32m0.3192[0m        [35m0.6393[0m       [31m0.9166[0m        [94m0.5200[0m     +  13.9708
      2   0.0831   [32m0.3464[0m        [35m0.4368[0m       0.9166        [94m0.3549[0m     +  14.0254
      3   [36m0.1035[0m   [32m0.4815[0m        [35m0.3295[0m       0.9166        [94m0.3020[0m     +  14.0092
      4   [36m0.1872[0m   [32m0.6145[0m        [35m0.2909[0m       0.9166        [94m0.2789[0m     +  13.9932
      5   [36m0.2303[0m   [32m0.6935[0m        [35m0.2716[0m       0.9166        [94m0.2659[0m     +  14.0571
      6   [36m0.2640[0m   [32m0.7356[0m        [35m0.2622[0m       0.9154        [94m0.2578[0m     +  13.9501
      7   [36m0.2809[0m   [32m0.7505[0m        [35m0.2546[0m       0.9166        [94m0.2528[0m     +  13.8309
      8   [36m0.2849[0m   [32m0.7587[0m        [35m0.2490[0m       [31m0.9190[0m        [94m0.2498[0m     +  13.8778
      9   [36m0.2912[0m   [32m0.7634[0m        [35m0.2459[0m       0.9190        [94m0.2481[0m     +  14.0136
     10   [36m0.2939[0m   [32m0.7656[0m        [35m0.2429[0m       0.9190        [94m0.2474[0m     +  13.8675
     11   [36m0.2953[0m   [32m0.7669[0m        [35m0.2414[0m       0.9190        [94m0.2473[0m     +  13.8841
     12   0.2946   0.7662        [35m0.2405[0m       0.9190        0.2474        14.0377
     13   [36m0.2987[0m   [32m0.7679[0m        [35m0.2375[0m       [31m0.9202[0m        [94m0.2471[0m     +  13.9682
     14   [36m0.3030[0m   [32m0.7687[0m        0.2379       0.9190        0.2472        13.9520
     15   [36m0.3080[0m   [32m0.7693[0m        [35m0.2371[0m       0.9178        0.2474        13.8924
     16   [36m0.3081[0m   [32m0.7698[0m        [35m0.2363[0m       0.9178        0.2473        13.9997
     17   [36m0.3092[0m   [32m0.7703[0m        0.2372       0.9202        0.2474        14.0411
     18   [36m0.3095[0m   [32m0.7705[0m        [35m0.2355[0m       0.9190        0.2474        13.9384
     19   [36m0.3122[0m   [32m0.7710[0m        [35m0.2342[0m       [31m0.9226[0m        0.2474        13.9103
     20   0.3097   [32m0.7712[0m        0.2356       0.9226        0.2473        13.9654
slurmstepd: error: *** JOB 9851912 ON gpu2106 CANCELLED AT 2023-05-06T00:16:22 DUE TO TIME LIMIT ***
